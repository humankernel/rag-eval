[
  {
    "title": "Prime number",
    "source": "https://en.wikipedia.org/wiki/Prime_number",
    "language": "en",
    "chunks": [
      {
        "heading": "Definition and examples",
        "level": 1,
        "content": "A natural number (1, 2, 3, 4, 5, 6, etc.) is called a prime number (or a prime) if it is greater than 1 and cannot be written as the product of two smaller natural numbers. The numbers greater than 1 that are not prime are called composite numbers. In other words, ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ is prime if ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ items cannot be divided up into smaller equal-size groups of more than one item, or if it is not possible to arrange ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ dots into a rectangular grid that is more than one dot wide and more than one dot high. For example, among the numbers 1 through 6, the numbers 2, 3, and 5 are the prime numbers, as there are no other numbers that divide them evenly (without a remainder). 1 is not prime, as it is specifically excluded in the definition. 4 = 2 × 2 and 6 = 2 × 3 are both composite.\n\nThe divisors of a natural number ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ are the natural numbers that divide ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ evenly. Every natural number has both 1 and itself as a divisor. If it has any other divisor, it cannot be prime. This leads to an equivalent definition of prime numbers: they are the numbers with exactly two positive divisors. Those two are 1 and the number itself. As 1 has only one divisor, itself, it is not prime by this definition. Yet another way to express the same thing is that a number ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ is prime if it is greater than one and if none of the numbers \n  \n    \n      \n        2\n        ,\n        3\n        ,\n        …\n        ,\n        n\n        −\n        1\n      \n    \n    {\\displaystyle 2,3,\\dots ,n-1}\n  \n divides ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ evenly.\nThe first 25 prime numbers (all the prime numbers less than 100) are:\n\n2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97 (sequence A000040 in the OEIS).\nNo even number ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ greater than 2 is prime because any such number can be expressed as the product ⁠\n  \n    \n      \n        2\n        ×\n        n\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle 2\\times n/2}\n  \n⁠. Therefore, every prime number other than 2 is an odd number, and is called an odd prime. Similarly, when written in the usual decimal system, all prime numbers larger than 5 end in 1, 3, 7, or 9. The numbers that end with other digits are all composite: decimal numbers that end in 0, 2, 4, 6, or 8 are even, and decimal numbers that end in 0 or 5 are divisible by 5.\nThe set of all primes is sometimes denoted by \n  \n    \n      \n        \n          P\n        \n      \n    \n    {\\displaystyle \\mathbf {P} }\n  \n (a boldface capital P) or by \n  \n    \n      \n        \n          P\n        \n      \n    \n    {\\displaystyle \\mathbb {P} }\n  \n (a blackboard bold capital P)."
      },
      {
        "heading": "History",
        "level": 1,
        "content": "The Rhind Mathematical Papyrus, from around 1550 BC, has Egyptian fraction expansions of different forms for prime and composite numbers. However, the earliest surviving records of the study of prime numbers come from the ancient Greek mathematicians, who called them prōtos arithmòs (πρῶτος ἀριθμὸς). Euclid's Elements (c. 300 BC) proves the infinitude of primes and the fundamental theorem of arithmetic, and shows how to construct a perfect number from a Mersenne prime. Another Greek invention, the Sieve of Eratosthenes, is still used to construct lists of primes.\nAround 1000 AD, the Islamic mathematician Ibn al-Haytham (Alhazen) found Wilson's theorem, characterizing the prime numbers as the numbers ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ that evenly divide ⁠\n  \n    \n      \n        (\n        n\n        −\n        1\n        )\n        !\n        +\n        1\n      \n    \n    {\\displaystyle (n-1)!+1}\n  \n⁠. He also conjectured that all even perfect numbers come from Euclid's construction using Mersenne primes, but was unable to prove it. Another Islamic mathematician, Ibn al-Banna' al-Marrakushi, observed that the sieve of Eratosthenes can be sped up by considering only the prime divisors up to the square root of the upper limit. Fibonacci took the innovations from Islamic mathematics to Europe. His book Liber Abaci (1202) was the first to describe trial division for testing primality, again using divisors only up to the square root.\nIn 1640 Pierre de Fermat stated (without proof) Fermat's little theorem (later proved by Leibniz and Euler). Fermat also investigated the primality of the Fermat numbers ⁠\n  \n    \n      \n        \n          2\n          \n            \n              2\n              \n                n\n              \n            \n          \n        \n        +\n        1\n      \n    \n    {\\displaystyle 2^{2^{n}}+1}\n  \n⁠, and Marin Mersenne studied the Mersenne primes, prime numbers of the form \n  \n    \n      \n        \n          2\n          \n            p\n          \n        \n        −\n        1\n      \n    \n    {\\displaystyle 2^{p}-1}\n  \n with ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ itself a prime. Christian Goldbach formulated Goldbach's conjecture, that every even number is the sum of two primes, in a 1742 letter to Euler. Euler proved Alhazen's conjecture (now the Euclid–Euler theorem) that all even perfect numbers can be constructed from Mersenne primes. He introduced methods from mathematical analysis to this area in his proofs of the infinitude of the primes and the divergence of the sum of the reciprocals of the primes ⁠\n  \n    \n      \n        \n          \n            \n              1\n              2\n            \n          \n        \n        +\n        \n          \n            \n              1\n              3\n            \n          \n        \n        +\n        \n          \n            \n              1\n              5\n            \n          \n        \n        +\n        \n          \n            \n              1\n              7\n            \n          \n        \n        +\n        \n          \n            \n              1\n              11\n            \n          \n        \n        +\n        ⋯\n      \n    \n    {\\displaystyle {\\tfrac {1}{2}}+{\\tfrac {1}{3}}+{\\tfrac {1}{5}}+{\\tfrac {1}{7}}+{\\tfrac {1}{11}}+\\cdots }\n  \n⁠. At the start of the 19th century, Legendre and Gauss conjectured that as ⁠\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n⁠ tends to infinity, the number of primes up to ⁠\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n⁠ is asymptotic to ⁠\n  \n    \n      \n        x\n        \n          /\n        \n        log\n        ⁡\n        x\n      \n    \n    {\\displaystyle x/\\log x}\n  \n⁠, where \n  \n    \n      \n        log\n        ⁡\n        x\n      \n    \n    {\\displaystyle \\log x}\n  \n is the natural logarithm of ⁠\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n⁠. A weaker consequence of this high density of primes was Bertrand's postulate, that for every \n  \n    \n      \n        n\n        >\n        1\n      \n    \n    {\\displaystyle n>1}\n  \n there is a prime between ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ and ⁠\n  \n    \n      \n        2\n        n\n      \n    \n    {\\displaystyle 2n}\n  \n⁠, proved in 1852 by Pafnuty Chebyshev. Ideas of Bernhard Riemann in his 1859 paper on the zeta-function sketched an outline for proving the conjecture of Legendre and Gauss. Although the closely related Riemann hypothesis remains unproven, Riemann's outline was completed in 1896 by Hadamard and de la Vallée Poussin, and the result is now known as the prime number theorem. Another important 19th century result was Dirichlet's theorem on arithmetic progressions, that certain arithmetic progressions contain infinitely many primes.\nMany mathematicians have worked on primality tests for numbers larger than those where trial division is practicably applicable. Methods that are restricted to specific number forms include Pépin's test for Fermat numbers (1877), Proth's theorem (c. 1878), the Lucas–Lehmer primality test (originated 1856), and the generalized Lucas primality test.\nSince 1951 all the largest known primes have been found using these tests on computers. The search for ever larger primes has generated interest outside mathematical circles, through the Great Internet Mersenne Prime Search and other distributed computing projects. The idea that prime numbers had few applications outside of pure mathematics was shattered in the 1970s when public-key cryptography and the RSA cryptosystem were invented, using prime numbers as their basis.\nThe increased practical importance of computerized primality testing and factorization led to the development of improved methods capable of handling large numbers of unrestricted form. The mathematical theory of prime numbers also moved forward with the Green–Tao theorem (2004) that there are arbitrarily long arithmetic progressions of prime numbers, and Yitang Zhang's 2013 proof that there exist infinitely many prime gaps of bounded size."
      },
      {
        "heading": "Primality of one",
        "level": 2,
        "content": "Most early Greeks did not even consider 1 to be a number, so they could not consider its primality. A few scholars in the Greek and later Roman tradition, including Nicomachus, Iamblichus, Boethius, and Cassiodorus, also considered the prime numbers to be a subdivision of the odd numbers, so they did not consider ⁠\n  \n    \n      \n        2\n      \n    \n    {\\displaystyle 2}\n  \n⁠ to be prime either. However, Euclid and a majority of the other Greek mathematicians considered ⁠\n  \n    \n      \n        2\n      \n    \n    {\\displaystyle 2}\n  \n⁠ as prime. The medieval Islamic mathematicians largely followed the Greeks in viewing 1 as not being a number. By the Middle Ages and Renaissance, mathematicians began treating 1 as a number, and by the 17th century some of them included it as the first prime number. In the mid-18th century, Christian Goldbach listed 1 as prime in his correspondence with Leonhard Euler; however, Euler himself did not consider 1 to be prime. Many 19th century mathematicians still considered 1 to be prime, and Derrick Norman Lehmer included 1 in his list of primes less than ten million published in 1914. Lists of primes that included 1 continued to be published as recently as 1956. However, by the early 20th century mathematicians began to agree that 1 should not be listed as prime, but rather in its own special category as a \"unit\".\nIf 1 were to be considered a prime, many statements involving primes would need to be awkwardly reworded. For example, the fundamental theorem of arithmetic would need to be rephrased in terms of factorizations into primes greater than 1, because every number would have multiple factorizations with any number of copies of 1. Similarly, the sieve of Eratosthenes would not work correctly if it handled 1 as a prime, because it would eliminate all multiples of 1 (that is, all other numbers) and output only the single number 1. Some other more technical properties of prime numbers also do not hold for the number 1: for instance, the formulas for Euler's totient function or for the sum of divisors function are different for prime numbers than they are for 1."
      },
      {
        "heading": "Elementary properties",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Unique factorization",
        "level": 2,
        "content": "Writing a number as a product of prime numbers is called a prime factorization of the number. For example:\n\n  \n    \n      \n        \n          \n            \n              \n                50\n              \n              \n                \n                =\n                2\n                ×\n                5\n                ×\n                5\n              \n            \n            \n              \n              \n                \n                =\n                2\n                ×\n                \n                  5\n                  \n                    2\n                  \n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}50&=2\\times 5\\times 5\\\\&=2\\times 5^{2}.\\end{aligned}}}\n  \n\nThe terms in the product are called prime factors. The same prime factor may occur more than once; this example has two copies of the prime factor \n  \n    \n      \n        5.\n      \n    \n    {\\displaystyle 5.}\n  \n When a prime occurs multiple times, exponentiation can be used to group together multiple copies of the same prime number: for example, in the second way of writing the product above, \n  \n    \n      \n        \n          5\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle 5^{2}}\n  \n denotes the square or second power of ⁠\n  \n    \n      \n        >\n        5\n      \n    \n    {\\displaystyle >5}\n  \n⁠.\nThe central importance of prime numbers to number theory and mathematics in general stems from the fundamental theorem of arithmetic. This theorem states that every integer larger than 1 can be written as a product of one or more primes. More strongly, this product is unique in the sense that any two prime factorizations of the same number will have the same numbers of copies of the same primes, although their ordering may differ. So, although there are many different ways of finding a factorization using an integer factorization algorithm, they all must produce the same result. Primes can thus be considered the \"basic building blocks\" of the natural numbers.\nSome proofs of the uniqueness of prime factorizations are based on Euclid's lemma: If ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ is a prime number and ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ divides a product \n  \n    \n      \n        a\n        b\n      \n    \n    {\\displaystyle ab}\n  \n of integers ⁠\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n⁠ and \n  \n    \n      \n        b\n        ,\n      \n    \n    {\\displaystyle b,}\n  \n then ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ divides ⁠\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n⁠ or ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ divides ⁠\n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n⁠ (or both). Conversely, if a number ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ has the property that when it divides a product it always divides at least one factor of the product, then ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ must be prime."
      },
      {
        "heading": "Infinitude",
        "level": 2,
        "content": "There are infinitely many prime numbers. Another way of saying this is that the sequence\n\n  \n    \n      \n        2\n        ,\n        3\n        ,\n        5\n        ,\n        7\n        ,\n        11\n        ,\n        13\n        ,\n        .\n        .\n        .\n      \n    \n    {\\displaystyle 2,3,5,7,11,13,...}\n  \n\nof prime numbers never ends. This statement is referred to as Euclid's theorem in honor of the ancient Greek mathematician Euclid, since the first known proof for this statement is attributed to him. Many more proofs of the infinitude of primes are known, including an analytical proof by Euler, Goldbach's proof based on Fermat numbers, Furstenberg's proof using general topology, and Kummer's elegant proof.\nEuclid's proof shows that every finite list of primes is incomplete. The key idea is to multiply together the primes in any given list and add \n  \n    \n      \n        1.\n      \n    \n    {\\displaystyle 1.}\n  \n If the list consists of the primes \n  \n    \n      \n        \n          p\n          \n            1\n          \n        \n        ,\n        \n          p\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          p\n          \n            n\n          \n        \n        ,\n      \n    \n    {\\displaystyle p_{1},p_{2},\\ldots ,p_{n},}\n  \n this gives the number\n\n  \n    \n      \n        N\n        =\n        1\n        +\n        \n          p\n          \n            1\n          \n        \n        ⋅\n        \n          p\n          \n            2\n          \n        \n        ⋯\n        \n          p\n          \n            n\n          \n        \n        .\n      \n    \n    {\\displaystyle N=1+p_{1}\\cdot p_{2}\\cdots p_{n}.}\n  \n\nBy the fundamental theorem, ⁠\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n⁠ has a prime factorization\n\n  \n    \n      \n        N\n        =\n        \n          p\n          \n            1\n          \n          ′\n        \n        ⋅\n        \n          p\n          \n            2\n          \n          ′\n        \n        ⋯\n        \n          p\n          \n            m\n          \n          ′\n        \n      \n    \n    {\\displaystyle N=p'_{1}\\cdot p'_{2}\\cdots p'_{m}}\n  \n\nwith one or more prime factors. ⁠\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n⁠ is evenly divisible by each of these factors, but ⁠\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n⁠ has a remainder of one when divided by any of the prime numbers in the given list, so none of the prime factors of ⁠\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n⁠ can be in the given list. Because there is no finite list of all the primes, there must be infinitely many primes.\nThe numbers formed by adding one to the products of the smallest primes are called Euclid numbers. The first five of them are prime, but the sixth,\n\n  \n    \n      \n        1\n        +\n        \n          \n            (\n          \n        \n        2\n        ⋅\n        3\n        ⋅\n        5\n        ⋅\n        7\n        ⋅\n        11\n        ⋅\n        13\n        \n          \n            )\n          \n        \n        =\n        30031\n        =\n        59\n        ⋅\n        509\n        ,\n      \n    \n    {\\displaystyle 1+{\\big (}2\\cdot 3\\cdot 5\\cdot 7\\cdot 11\\cdot 13{\\big )}=30031=59\\cdot 509,}\n  \n\nis a composite number."
      },
      {
        "heading": "Formulas for primes",
        "level": 2,
        "content": "There is no known efficient formula for primes. For example, there is no non-constant polynomial, even in several variables, that takes only prime values. However, there are numerous expressions that do encode all primes, or only primes. One possible formula is based on Wilson's theorem and generates the number 2 many times and all other primes exactly once. There is also a set of Diophantine equations in nine variables and one parameter with the following property: the parameter is prime if and only if the resulting system of equations has a solution over the natural numbers. This can be used to obtain a single formula with the property that all its positive values are prime.\nOther examples of prime-generating formulas come from Mills' theorem and a theorem of Wright. These assert that there are real constants \n  \n    \n      \n        A\n        >\n        1\n      \n    \n    {\\displaystyle A>1}\n  \n and \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n such that\n\n  \n    \n      \n        \n          ⌊\n          \n            A\n            \n              \n                3\n                \n                  n\n                \n              \n            \n          \n          ⌋\n        \n        \n           and \n        \n        \n          ⌊\n          \n            2\n            \n              \n                ⋯\n                \n                  \n                    2\n                    \n                      \n                        2\n                        \n                          μ\n                        \n                      \n                    \n                  \n                \n              \n            \n          \n          ⌋\n        \n      \n    \n    {\\displaystyle \\left\\lfloor A^{3^{n}}\\right\\rfloor {\\text{ and }}\\left\\lfloor 2^{\\cdots ^{2^{2^{\\mu }}}}\\right\\rfloor }\n  \n\nare prime for any natural number ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ in the first formula, and any number of exponents in the second formula. Here \n  \n    \n      \n        ⌊\n        \n\n        \n        ⋅\n        \n\n        \n        ⌋\n      \n    \n    {\\displaystyle \\lfloor {}\\cdot {}\\rfloor }\n  \n represents the floor function, the largest integer less than or equal to the number in question. However, these are not useful for generating primes, as the primes must be generated first in order to compute the values of ⁠\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n⁠ or \n  \n    \n      \n        μ\n        .\n      \n    \n    {\\displaystyle \\mu .}"
      },
      {
        "heading": "Open questions",
        "level": 2,
        "content": "Many conjectures revolving about primes have been posed. Often having an elementary formulation, many of these conjectures have withstood proof for decades: all four of Landau's problems from 1912 are still unsolved. One of them is Goldbach's conjecture, which asserts that every even integer ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ greater than ⁠\n  \n    \n      \n        2\n      \n    \n    {\\displaystyle 2}\n  \n⁠ can be written as a sum of two primes. As of 2014, this conjecture has been verified for all numbers up to \n  \n    \n      \n        n\n        =\n        4\n        ⋅\n        \n          10\n          \n            18\n          \n        \n        .\n      \n    \n    {\\displaystyle n=4\\cdot 10^{18}.}\n  \n Weaker statements than this have been proven; for example, Vinogradov's theorem says that every sufficiently large odd integer can be written as a sum of three primes. Chen's theorem says that every sufficiently large even number can be expressed as the sum of a prime and a semiprime (the product of two primes). Also, any even integer greater than 10 can be written as the sum of six primes. The branch of number theory studying such questions is called additive number theory.\nAnother type of problem concerns prime gaps, the differences between consecutive primes.\nThe existence of arbitrarily large prime gaps can be seen by noting that the sequence \n  \n    \n      \n        n\n        !\n        +\n        2\n        ,\n        n\n        !\n        +\n        3\n        ,\n        …\n        ,\n        n\n        !\n        +\n        n\n      \n    \n    {\\displaystyle n!+2,n!+3,\\dots ,n!+n}\n  \n consists of \n  \n    \n      \n        n\n        −\n        1\n      \n    \n    {\\displaystyle n-1}\n  \n composite numbers, for any natural number \n  \n    \n      \n        n\n        .\n      \n    \n    {\\displaystyle n.}\n  \n However, large prime gaps occur much earlier than this argument shows. For example, the first prime gap of length 8 is between the primes 89 and 97, much smaller than \n  \n    \n      \n        8\n        !\n        =\n        40320.\n      \n    \n    {\\displaystyle 8!=40320.}\n  \n It is conjectured that there are infinitely many twin primes, pairs of primes with difference 2; this is the twin prime conjecture. Polignac's conjecture states more generally that for every positive integer \n  \n    \n      \n        k\n        ,\n      \n    \n    {\\displaystyle k,}\n  \n there are infinitely many pairs of consecutive primes that differ by \n  \n    \n      \n        2\n        k\n        .\n      \n    \n    {\\displaystyle 2k.}\n  \n\nAndrica's conjecture, Brocard's conjecture, Legendre's conjecture, and Oppermann's conjecture all suggest that the largest gaps between primes from 1 to ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ should be at most approximately \n  \n    \n      \n        \n          \n            n\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\sqrt {n}},}\n  \n a result that is known to follow from the Riemann hypothesis, while the much stronger Cramér conjecture sets the largest gap size at ⁠\n  \n    \n      \n        O\n        (\n        (\n        log\n        ⁡\n        n\n        \n          )\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O((\\log n)^{2})}\n  \n⁠. Prime gaps can be generalized to prime ⁠\n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n⁠-tuples, patterns in the differences among more than two prime numbers. Their infinitude and density are the subject of the first Hardy–Littlewood conjecture, which can be motivated by the heuristic that the prime numbers behave similarly to a random sequence of numbers with density given by the prime number theorem."
      },
      {
        "heading": "Analytic properties",
        "level": 1,
        "content": "Analytic number theory studies number theory through the lens of continuous functions, limits, infinite series, and the related mathematics of the infinite and infinitesimal.\nThis area of study began with Leonhard Euler and his first major result, the solution to the Basel problem.\nThe problem asked for the value of the infinite sum \n  \n    \n      \n        1\n        +\n        \n          \n            \n              1\n              4\n            \n          \n        \n        +\n        \n          \n            \n              1\n              9\n            \n          \n        \n        +\n        \n          \n            \n              1\n              16\n            \n          \n        \n        +\n        …\n        ,\n      \n    \n    {\\displaystyle 1+{\\tfrac {1}{4}}+{\\tfrac {1}{9}}+{\\tfrac {1}{16}}+\\dots ,}\n  \n\nwhich today can be recognized as the value \n  \n    \n      \n        ζ\n        (\n        2\n        )\n      \n    \n    {\\displaystyle \\zeta (2)}\n  \n of the Riemann zeta function. This function is closely connected to the prime numbers and to one of the most significant unsolved problems in mathematics, the Riemann hypothesis. Euler showed that ⁠\n  \n    \n      \n        ζ\n        (\n        2\n        )\n        =\n        \n          π\n          \n            2\n          \n        \n        \n          /\n        \n        6\n      \n    \n    {\\displaystyle \\zeta (2)=\\pi ^{2}/6}\n  \n⁠.\nThe reciprocal of this number, ⁠\n  \n    \n      \n        6\n        \n          /\n        \n        \n          π\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle 6/\\pi ^{2}}\n  \n⁠, is the limiting probability that two random numbers selected uniformly from a large range are relatively prime (have no factors in common).\nThe distribution of primes in the large, such as the question how many primes are smaller than a given, large threshold, is described by the prime number theorem, but no efficient formula for the ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠-th prime is known. Dirichlet's theorem on arithmetic progressions, in its basic form, asserts that linear polynomials\n\n  \n    \n      \n        p\n        (\n        n\n        )\n        =\n        a\n        +\n        b\n        n\n      \n    \n    {\\displaystyle p(n)=a+bn}\n  \n\nwith relatively prime integers ⁠\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n⁠ and ⁠\n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n⁠ take infinitely many prime values. Stronger forms of the theorem state that the sum of the reciprocals of these prime values diverges, and that different linear polynomials with the same ⁠\n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n⁠ have approximately the same proportions of primes.\nAlthough conjectures have been formulated about the proportions of primes in higher-degree polynomials, they remain unproven, and it is unknown whether there exists a quadratic polynomial that (for integer arguments) is prime infinitely often."
      },
      {
        "heading": "Analytical proof of Euclid's theorem",
        "level": 2,
        "content": "Euler's proof that there are infinitely many primes considers the sums of reciprocals of primes,\n\n  \n    \n      \n        \n          \n            1\n            2\n          \n        \n        +\n        \n          \n            1\n            3\n          \n        \n        +\n        \n          \n            1\n            5\n          \n        \n        +\n        \n          \n            1\n            7\n          \n        \n        +\n        ⋯\n        +\n        \n          \n            1\n            p\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {1}{2}}+{\\frac {1}{3}}+{\\frac {1}{5}}+{\\frac {1}{7}}+\\cdots +{\\frac {1}{p}}.}\n  \n\nEuler showed that, for any arbitrary real number ⁠\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n⁠, there exists a prime ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ for which this sum is bigger than ⁠\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n⁠. This shows that there are infinitely many primes, because if there were finitely many primes the sum would reach its maximum value at the biggest prime rather than growing past every ⁠\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n⁠.\nThe growth rate of this sum is described more precisely by Mertens' second theorem. For comparison, the sum\n\n  \n    \n      \n        \n          \n            1\n            \n              1\n              \n                2\n              \n            \n          \n        \n        +\n        \n          \n            1\n            \n              2\n              \n                2\n              \n            \n          \n        \n        +\n        \n          \n            1\n            \n              3\n              \n                2\n              \n            \n          \n        \n        +\n        ⋯\n        +\n        \n          \n            1\n            \n              n\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{1^{2}}}+{\\frac {1}{2^{2}}}+{\\frac {1}{3^{2}}}+\\cdots +{\\frac {1}{n^{2}}}}\n  \n\ndoes not grow to infinity as ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ goes to infinity (see the Basel problem). In this sense, prime numbers occur more often than squares of natural numbers,\nalthough both sets are infinite. Brun's theorem states that the sum of the reciprocals of twin primes,\n\n  \n    \n      \n        \n          (\n          \n            \n              \n                1\n                3\n              \n            \n            +\n            \n              \n                1\n                5\n              \n            \n          \n          )\n        \n        +\n        \n          (\n          \n            \n              \n                1\n                5\n              \n            \n            +\n            \n              \n                1\n                7\n              \n            \n          \n          )\n        \n        +\n        \n          (\n          \n            \n              \n                1\n                11\n              \n            \n            +\n            \n              \n                1\n                13\n              \n            \n          \n          )\n        \n        +\n        ⋯\n        ,\n      \n    \n    {\\displaystyle \\left({{\\frac {1}{3}}+{\\frac {1}{5}}}\\right)+\\left({{\\frac {1}{5}}+{\\frac {1}{7}}}\\right)+\\left({{\\frac {1}{11}}+{\\frac {1}{13}}}\\right)+\\cdots ,}\n  \n\nis finite. Because of Brun's theorem, it is not possible to use Euler's method to solve the twin prime conjecture, that there exist infinitely many twin primes."
      },
      {
        "heading": "Number of primes below a given bound",
        "level": 2,
        "content": "The prime-counting function \n  \n    \n      \n        π\n        (\n        n\n        )\n      \n    \n    {\\displaystyle \\pi (n)}\n  \n is defined as the number of primes not greater than ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠. For example, ⁠\n  \n    \n      \n        π\n        (\n        11\n        )\n        =\n        5\n      \n    \n    {\\displaystyle \\pi (11)=5}\n  \n⁠, since there are five primes less than or equal to 11. Methods such as the Meissel–Lehmer algorithm can compute exact values of \n  \n    \n      \n        π\n        (\n        n\n        )\n      \n    \n    {\\displaystyle \\pi (n)}\n  \n faster than it would be possible to list each prime up to ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠. The prime number theorem states that \n  \n    \n      \n        π\n        (\n        n\n        )\n      \n    \n    {\\displaystyle \\pi (n)}\n  \n is asymptotic to ⁠\n  \n    \n      \n        n\n        \n          /\n        \n        log\n        ⁡\n        n\n      \n    \n    {\\displaystyle n/\\log n}\n  \n⁠, which is denoted as\n\n  \n    \n      \n        π\n        (\n        n\n        )\n        ∼\n        \n          \n            n\n            \n              log\n              ⁡\n              n\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\pi (n)\\sim {\\frac {n}{\\log n}},}\n  \n\nand means that the ratio of \n  \n    \n      \n        π\n        (\n        n\n        )\n      \n    \n    {\\displaystyle \\pi (n)}\n  \n to the right-hand fraction approaches 1 as ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ grows to infinity. This implies that the likelihood that a randomly chosen number less than ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ is prime is (approximately) inversely proportional to the number of digits in ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠.\nIt also implies that the ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠th prime number is proportional to \n  \n    \n      \n        n\n        log\n        ⁡\n        n\n      \n    \n    {\\displaystyle n\\log n}\n  \n\nand therefore that the average size of a prime gap is proportional to ⁠\n  \n    \n      \n        log\n        ⁡\n        n\n      \n    \n    {\\displaystyle \\log n}\n  \n⁠.\nA more accurate estimate for \n  \n    \n      \n        π\n        (\n        n\n        )\n      \n    \n    {\\displaystyle \\pi (n)}\n  \n is given by the offset logarithmic integral\n\n  \n    \n      \n        π\n        (\n        n\n        )\n        ∼\n        Li\n        ⁡\n        (\n        n\n        )\n        =\n        \n          ∫\n          \n            2\n          \n          \n            n\n          \n        \n        \n          \n            \n              d\n              t\n            \n            \n              log\n              ⁡\n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\pi (n)\\sim \\operatorname {Li} (n)=\\int _{2}^{n}{\\frac {dt}{\\log t}}.}"
      },
      {
        "heading": "Arithmetic progressions",
        "level": 2,
        "content": "An arithmetic progression is a finite or infinite sequence of numbers such that consecutive numbers in the sequence all have the same difference. This difference is called the modulus of the progression. For example,\n\n  \n    \n      \n        3\n        ,\n        12\n        ,\n        21\n        ,\n        30\n        ,\n        39\n        ,\n        .\n        .\n        .\n        ,\n      \n    \n    {\\displaystyle 3,12,21,30,39,...,}\n  \n\nis an infinite arithmetic progression with modulus 9. In an arithmetic progression, all the numbers have the same remainder when divided by the modulus; in this example, the remainder is 3. Because both the modulus 9 and the remainder 3 are multiples of 3, so is every element in the sequence. Therefore, this progression contains only one prime number, 3 itself. In general, the infinite progression\n\n  \n    \n      \n        a\n        ,\n        a\n        +\n        q\n        ,\n        a\n        +\n        2\n        q\n        ,\n        a\n        +\n        3\n        q\n        ,\n        …\n      \n    \n    {\\displaystyle a,a+q,a+2q,a+3q,\\dots }\n  \n\ncan have more than one prime only when its remainder ⁠\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n⁠ and modulus ⁠\n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n⁠ are relatively prime. If they are relatively prime, Dirichlet's theorem on arithmetic progressions asserts that the progression contains infinitely many primes.\n\nThe Green–Tao theorem shows that there are arbitrarily long finite arithmetic progressions consisting only of primes."
      },
      {
        "heading": "Prime values of quadratic polynomials",
        "level": 2,
        "content": "Euler noted that the function\n\n  \n    \n      \n        \n          n\n          \n            2\n          \n        \n        −\n        n\n        +\n        41\n      \n    \n    {\\displaystyle n^{2}-n+41}\n  \n\nyields prime numbers for ⁠\n  \n    \n      \n        1\n        ≤\n        n\n        ≤\n        40\n      \n    \n    {\\displaystyle 1\\leq n\\leq 40}\n  \n⁠, although composite numbers appear among its later values. The search for an explanation for this phenomenon led to the deep algebraic number theory of Heegner numbers and the class number problem. The Hardy–Littlewood conjecture F predicts the density of primes among the values of quadratic polynomials with integer coefficients in terms of the logarithmic integral and the polynomial coefficients. No quadratic polynomial has been proven to take infinitely many prime values.\nThe Ulam spiral arranges the natural numbers in a two-dimensional grid, spiraling in concentric squares surrounding the origin with the prime numbers highlighted. Visually, the primes appear to cluster on certain diagonals and not others, suggesting that some quadratic polynomials take prime values more often than others."
      },
      {
        "heading": "Zeta function and the Riemann hypothesis",
        "level": 2,
        "content": "One of the most famous unsolved questions in mathematics, dating from 1859, and one of the Millennium Prize Problems, is the Riemann hypothesis, which asks where the zeros of the Riemann zeta function \n  \n    \n      \n        ζ\n        (\n        s\n        )\n      \n    \n    {\\displaystyle \\zeta (s)}\n  \n are located.\nThis function is an analytic function on the complex numbers. For complex numbers ⁠\n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n⁠ with real part greater than one it equals both an infinite sum over all integers, and an infinite product over the prime numbers,\n\n  \n    \n      \n        ζ\n        (\n        s\n        )\n        =\n        \n          ∑\n          \n            n\n            =\n            1\n          \n          \n            ∞\n          \n        \n        \n          \n            1\n            \n              n\n              \n                s\n              \n            \n          \n        \n        =\n        \n          ∏\n          \n            p\n            \n               prime\n            \n          \n        \n        \n          \n            1\n            \n              1\n              −\n              \n                p\n                \n                  −\n                  s\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\zeta (s)=\\sum _{n=1}^{\\infty }{\\frac {1}{n^{s}}}=\\prod _{p{\\text{ prime}}}{\\frac {1}{1-p^{-s}}}.}\n  \n\nThis equality between a sum and a product, discovered by Euler, is called an Euler product. The Euler product can be derived from the fundamental theorem of arithmetic, and shows the close connection between the zeta function and the prime numbers.\nIt leads to another proof that there are infinitely many primes: if there were only finitely many,\nthen the sum-product equality would also be valid at ⁠\n  \n    \n      \n        s\n        =\n        1\n      \n    \n    {\\displaystyle s=1}\n  \n⁠, but the sum would diverge (it is the harmonic series ⁠\n  \n    \n      \n        1\n        +\n        \n          \n            \n              1\n              2\n            \n          \n        \n        +\n        \n          \n            \n              1\n              3\n            \n          \n        \n        +\n        …\n      \n    \n    {\\displaystyle 1+{\\tfrac {1}{2}}+{\\tfrac {1}{3}}+\\dots }\n  \n⁠) while the product would be finite, a contradiction.\nThe Riemann hypothesis states that the zeros of the zeta-function are all either negative even numbers, or complex numbers with real part equal to 1/2. The original proof of the prime number theorem was based on a weak form of this hypothesis, that there are no zeros with real part equal to 1, although other more elementary proofs have been found. The prime-counting function can be expressed by Riemann's explicit formula as a sum in which each term comes from one of the zeros of the zeta function; the main term of this sum is the logarithmic integral, and the remaining terms cause the sum to fluctuate above and below the main term. In this sense, the zeros control how regularly the prime numbers are distributed. If the Riemann hypothesis is true, these fluctuations will be small, and the\nasymptotic distribution of primes given by the prime number theorem will also hold over much shorter intervals (of length about the square root of ⁠\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n⁠ for intervals near a number ⁠\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n⁠)."
      },
      {
        "heading": "Abstract algebra",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Modular arithmetic and finite fields",
        "level": 2,
        "content": "Modular arithmetic modifies usual arithmetic by only using the numbers ⁠\n  \n    \n      \n        {\n        0\n        ,\n        1\n        ,\n        2\n        ,\n        …\n        ,\n        n\n        −\n        1\n        }\n      \n    \n    {\\displaystyle \\{0,1,2,\\dots ,n-1\\}}\n  \n⁠, for a natural number ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ called the modulus.\nAny other natural number can be mapped into this system by replacing it by its remainder after division by ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠. Modular sums, differences and products are calculated by performing the same replacement by the remainder on the result of the usual sum, difference, or product of integers. Equality of integers corresponds to congruence in modular arithmetic: ⁠\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n⁠ and ⁠\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n⁠ are congruent (written \n  \n    \n      \n        x\n        ≡\n        y\n      \n    \n    {\\displaystyle x\\equiv y}\n  \n mod ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠) when they have the same remainder after division by ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠. However, in this system of numbers, division by all nonzero numbers is possible if and only if the modulus is prime. For instance, with the prime number 7 as modulus, division by 3 is possible: ⁠\n  \n    \n      \n        2\n        \n          /\n        \n        3\n        ≡\n        3\n        \n          mod\n          \n            7\n          \n        \n      \n    \n    {\\displaystyle 2/3\\equiv 3{\\bmod {7}}}\n  \n⁠, because clearing denominators by multiplying both sides by 3 gives the valid formula ⁠\n  \n    \n      \n        2\n        ≡\n        9\n        \n          mod\n          \n            7\n          \n        \n      \n    \n    {\\displaystyle 2\\equiv 9{\\bmod {7}}}\n  \n⁠. However, with the composite modulus 6, division by 3 is impossible. There is no valid solution to \n  \n    \n      \n        2\n        \n          /\n        \n        3\n        ≡\n        x\n        \n          mod\n          \n            6\n          \n        \n      \n    \n    {\\displaystyle 2/3\\equiv x{\\bmod {6}}}\n  \n: clearing denominators by multiplying by 3 causes the left-hand side to become 2 while the right-hand side becomes either 0 or 3. In the terminology of abstract algebra, the ability to perform division means that modular arithmetic modulo a prime number forms a field or, more specifically, a finite field, while other moduli only give a ring but not a field.\nSeveral theorems about primes can be formulated using modular arithmetic. For instance, Fermat's little theorem states that if \n  \n    \n      \n        a\n        ≢\n        0\n      \n    \n    {\\displaystyle a\\not \\equiv 0}\n  \n (mod ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠), then \n  \n    \n      \n        \n          a\n          \n            p\n            −\n            1\n          \n        \n        ≡\n        1\n      \n    \n    {\\displaystyle a^{p-1}\\equiv 1}\n  \n (mod ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠). Summing this over all choices of ⁠\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n⁠ gives the equation\n\n  \n    \n      \n        \n          ∑\n          \n            a\n            =\n            1\n          \n          \n            p\n            −\n            1\n          \n        \n        \n          a\n          \n            p\n            −\n            1\n          \n        \n        ≡\n        (\n        p\n        −\n        1\n        )\n        ⋅\n        1\n        ≡\n        −\n        1\n        \n          \n          (\n          mod\n          \n          p\n          )\n        \n        ,\n      \n    \n    {\\displaystyle \\sum _{a=1}^{p-1}a^{p-1}\\equiv (p-1)\\cdot 1\\equiv -1{\\pmod {p}},}\n  \n\nvalid whenever ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ is prime.\nGiuga's conjecture says that this equation is also a sufficient condition for ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ to be prime. Wilson's theorem says that an integer \n  \n    \n      \n        p\n        >\n        1\n      \n    \n    {\\displaystyle p>1}\n  \n is prime if and only if the factorial \n  \n    \n      \n        (\n        p\n        −\n        1\n        )\n        !\n      \n    \n    {\\displaystyle (p-1)!}\n  \n is congruent to \n  \n    \n      \n        −\n        1\n      \n    \n    {\\displaystyle -1}\n  \n mod ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠. For a composite number ⁠\n  \n    \n      \n        n\n        =\n        r\n        ⋅\n        s\n      \n    \n    {\\displaystyle n=r\\cdot s}\n  \n⁠ this cannot hold, since one of its factors divides both n and ⁠\n  \n    \n      \n        (\n        n\n        −\n        1\n        )\n        !\n      \n    \n    {\\displaystyle (n-1)!}\n  \n⁠, and so \n  \n    \n      \n        (\n        n\n        −\n        1\n        )\n        !\n        ≡\n        −\n        1\n        \n          \n          (\n          mod\n          \n          n\n          )\n        \n      \n    \n    {\\displaystyle (n-1)!\\equiv -1{\\pmod {n}}}\n  \n is impossible."
      },
      {
        "heading": "p-adic numbers",
        "level": 2,
        "content": "The ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠-adic order \n  \n    \n      \n        \n          ν\n          \n            p\n          \n        \n        (\n        n\n        )\n      \n    \n    {\\displaystyle \\nu _{p}(n)}\n  \n of an integer ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ is the number of copies of ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ in the prime factorization of ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠. The same concept can be extended from integers to rational numbers by defining the ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠-adic order of a fraction \n  \n    \n      \n        m\n        \n          /\n        \n        n\n      \n    \n    {\\displaystyle m/n}\n  \n to be ⁠\n  \n    \n      \n        \n          ν\n          \n            p\n          \n        \n        (\n        m\n        )\n        −\n        \n          ν\n          \n            p\n          \n        \n        (\n        n\n        )\n      \n    \n    {\\displaystyle \\nu _{p}(m)-\\nu _{p}(n)}\n  \n⁠. The ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠-adic absolute value \n  \n    \n      \n        \n          |\n        \n        q\n        \n          \n            |\n          \n          \n            p\n          \n        \n      \n    \n    {\\displaystyle |q|_{p}}\n  \n of any rational number ⁠\n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n⁠ is then defined as ⁠\n  \n    \n      \n        |\n        q\n        \n          |\n          \n            p\n          \n        \n        =\n        \n          p\n          \n            −\n            \n              ν\n              \n                p\n              \n            \n            (\n            q\n            )\n          \n        \n      \n    \n    {\\displaystyle \\vert q\\vert _{p}=p^{-\\nu _{p}(q)}}\n  \n⁠. Multiplying an integer by its ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠-adic absolute value cancels out the factors of ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ in its factorization, leaving only the other primes. Just as the distance between two real numbers can be measured by the absolute value of their distance, the distance between two rational numbers can be measured by their ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠-adic distance, the ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠-adic absolute value of their difference. For this definition of distance, two numbers are close together (they have a small distance) when their difference is divisible by a high power of ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠. In the same way that the real numbers can be formed from the rational numbers and their distances, by adding extra limiting values to form a complete field, the rational numbers with the ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠-adic distance can be extended to a different complete field, the ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠-adic numbers.\nThis picture of an order, absolute value, and complete field derived from them can be generalized to algebraic number fields and their valuations (certain mappings from the multiplicative group of the field to a totally ordered additive group, also called orders), absolute values (certain multiplicative mappings from the field to the real numbers, also called norms), and places (extensions to complete fields in which the given field is a dense set, also called completions). The extension from the rational numbers to the real numbers, for instance, is a place in which the distance between numbers is the usual absolute value of their difference. The corresponding mapping to an additive group would be the logarithm of the absolute value, although this does not meet all the requirements of a valuation. According to Ostrowski's theorem, up to a natural notion of equivalence, the real numbers and ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠-adic numbers, with their orders and absolute values, are the only valuations, absolute values, and places on the rational numbers. The local–global principle allows certain problems over the rational numbers to be solved by piecing together solutions from each of their places, again underlining the importance of primes to number theory."
      },
      {
        "heading": "Prime elements of a ring",
        "level": 2,
        "content": "A commutative ring is an algebraic structure where addition, subtraction and multiplication are defined. The integers are a ring, and the prime numbers in the integers have been generalized to rings in two different ways, prime elements and irreducible elements. An element ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ of a ring ⁠\n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n⁠ is called prime if it is nonzero, has no multiplicative inverse (that is, it is not a unit), and satisfies the following requirement: whenever ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ divides the product \n  \n    \n      \n        x\n        y\n      \n    \n    {\\displaystyle xy}\n  \n of two elements of ⁠\n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n⁠, it also divides at least one of ⁠\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n⁠ or ⁠\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n⁠. An element is irreducible if it is neither a unit nor the product of two other non-unit elements. In the ring of integers, the prime and irreducible elements form the same set,\n\n  \n    \n      \n        {\n        …\n        ,\n        −\n        11\n        ,\n        −\n        7\n        ,\n        −\n        5\n        ,\n        −\n        3\n        ,\n        −\n        2\n        ,\n        2\n        ,\n        3\n        ,\n        5\n        ,\n        7\n        ,\n        11\n        ,\n        …\n        }\n        \n        .\n      \n    \n    {\\displaystyle \\{\\dots ,-11,-7,-5,-3,-2,2,3,5,7,11,\\dots \\}\\,.}\n  \n\nIn an arbitrary ring, all prime elements are irreducible. The converse does not hold in general, but does hold for unique factorization domains.\nThe fundamental theorem of arithmetic continues to hold (by definition) in unique factorization domains. An example of such a domain is the Gaussian integers ⁠\n  \n    \n      \n        \n          Z\n        \n        [\n        i\n        ]\n      \n    \n    {\\displaystyle \\mathbb {Z} [i]}\n  \n⁠, the ring of complex numbers of the form \n  \n    \n      \n        a\n        +\n        b\n        i\n      \n    \n    {\\displaystyle a+bi}\n  \n where ⁠\n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n⁠ denotes the imaginary unit and ⁠\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n⁠ and ⁠\n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n⁠ are arbitrary integers. Its prime elements are known as Gaussian primes. Not every number that is prime among the integers remains prime in the Gaussian integers; for instance, the number 2 can be written as a product of the two Gaussian primes \n  \n    \n      \n        1\n        +\n        i\n      \n    \n    {\\displaystyle 1+i}\n  \n and ⁠\n  \n    \n      \n        1\n        −\n        i\n      \n    \n    {\\displaystyle 1-i}\n  \n⁠. Rational primes (the prime elements in the integers) congruent to 3 mod 4 are Gaussian primes, but rational primes congruent to 1 mod 4 are not. This is a consequence of Fermat's theorem on sums of two squares,\nwhich states that an odd prime ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ is expressible as the sum of two squares, ⁠\n  \n    \n      \n        p\n        =\n        \n          x\n          \n            2\n          \n        \n        +\n        \n          y\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle p=x^{2}+y^{2}}\n  \n⁠, and therefore factorable as ⁠\n  \n    \n      \n        p\n        =\n        (\n        x\n        +\n        i\n        y\n        )\n        (\n        x\n        −\n        i\n        y\n        )\n      \n    \n    {\\displaystyle p=(x+iy)(x-iy)}\n  \n⁠, exactly when ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ is 1 mod 4."
      },
      {
        "heading": "Prime ideals",
        "level": 2,
        "content": "Not every ring is a unique factorization domain. For instance, in the ring of numbers \n  \n    \n      \n        a\n        +\n        b\n        \n          \n            −\n            5\n          \n        \n      \n    \n    {\\displaystyle a+b{\\sqrt {-5}}}\n  \n (for integers ⁠\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n⁠ and ⁠\n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n⁠) the number \n  \n    \n      \n        21\n      \n    \n    {\\displaystyle 21}\n  \n has two factorizations ⁠\n  \n    \n      \n        21\n        =\n        3\n        ⋅\n        7\n        =\n        (\n        1\n        +\n        2\n        \n          \n            −\n            5\n          \n        \n        )\n        (\n        1\n        −\n        2\n        \n          \n            −\n            5\n          \n        \n        )\n      \n    \n    {\\displaystyle 21=3\\cdot 7=(1+2{\\sqrt {-5}})(1-2{\\sqrt {-5}})}\n  \n⁠, where neither of the four factors can be reduced any further, so it does not have a unique factorization. In order to extend unique factorization to a larger class of rings, the notion of a number can be replaced with that of an ideal, a subset of the elements of a ring that contains all sums of pairs of its elements, and all products of its elements with ring elements.\nPrime ideals, which generalize prime elements in the sense that the principal ideal generated by a prime element is a prime ideal, are an important tool and object of study in commutative algebra, algebraic number theory and algebraic geometry. The prime ideals of the ring of integers are the ideals ⁠\n  \n    \n      \n        (\n        0\n        )\n      \n    \n    {\\displaystyle (0)}\n  \n⁠, ⁠\n  \n    \n      \n        (\n        2\n        )\n      \n    \n    {\\displaystyle (2)}\n  \n⁠, ⁠\n  \n    \n      \n        (\n        3\n        )\n      \n    \n    {\\displaystyle (3)}\n  \n⁠, ⁠\n  \n    \n      \n        (\n        5\n        )\n      \n    \n    {\\displaystyle (5)}\n  \n⁠, ⁠\n  \n    \n      \n        (\n        7\n        )\n      \n    \n    {\\displaystyle (7)}\n  \n⁠, ⁠\n  \n    \n      \n        (\n        11\n        )\n      \n    \n    {\\displaystyle (11)}\n  \n⁠, ... The fundamental theorem of arithmetic generalizes to the Lasker–Noether theorem, which expresses every ideal in a Noetherian commutative ring as an intersection of primary ideals, which are the appropriate generalizations of prime powers.\nThe spectrum of a ring is a geometric space whose points are the prime ideals of the ring. Arithmetic geometry also benefits from this notion, and many concepts exist in both geometry and number theory. For example, factorization or ramification of prime ideals when lifted to an extension field, a basic problem of algebraic number theory, bears some resemblance with ramification in geometry. These concepts can even assist with in number-theoretic questions solely concerned with integers. For example, prime ideals in the ring of integers of quadratic number fields can be used in proving quadratic reciprocity, a statement that concerns the existence of square roots modulo integer prime numbers. Early attempts to prove Fermat's Last Theorem led to Kummer's introduction of regular primes, integer prime numbers connected with the failure of unique factorization in the cyclotomic integers. The question of how many integer prime numbers factor into a product of multiple prime ideals in an algebraic number field is addressed by Chebotarev's density theorem, which (when applied to the cyclotomic integers) has Dirichlet's theorem on primes in arithmetic progressions as a special case."
      },
      {
        "heading": "Group theory",
        "level": 2,
        "content": "In the theory of finite groups the Sylow theorems imply that, if a power of a prime number \n  \n    \n      \n        \n          p\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle p^{n}}\n  \n divides the order of a group, then the group has a subgroup of order ⁠\n  \n    \n      \n        \n          p\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle p^{n}}\n  \n⁠. By Lagrange's theorem, any group of prime order is a cyclic group,\nand by Burnside's theorem any group whose order is divisible by only two primes is solvable."
      },
      {
        "heading": "Computational methods",
        "level": 1,
        "content": "For a long time, number theory in general, and the study of prime numbers in particular, was seen as the canonical example of pure mathematics, with no applications outside of mathematics other than the use of prime numbered gear teeth to distribute wear evenly. In particular, number theorists such as British mathematician G. H. Hardy prided themselves on doing work that had absolutely no military significance.\nThis vision of the purity of number theory was shattered in the 1970s, when it was publicly announced that prime numbers could be used as the basis for the creation of public-key cryptography algorithms.\nThese applications have led to significant study of algorithms for computing with prime numbers, and in particular of primality testing, methods for determining whether a given number is prime. The most basic primality testing routine, trial division, is too slow to be useful for large numbers. One group of modern primality tests is applicable to arbitrary numbers, while more efficient tests are available for numbers of special types. Most primality tests only tell whether their argument is prime or not. Routines that also provide a prime factor of composite arguments (or all of its prime factors) are called factorization algorithms. Prime numbers are also used in computing for checksums, hash tables, and pseudorandom number generators."
      },
      {
        "heading": "Trial division",
        "level": 2,
        "content": "The most basic method of checking the primality of a given integer ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ is called trial division. This method divides ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ by each integer from 2 up to the square root of ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠. Any such integer dividing ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ evenly establishes ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ as composite; otherwise it is prime. Integers larger than the square root do not need to be checked because, whenever ⁠\n  \n    \n      \n        n\n        =\n        a\n        ⋅\n        b\n      \n    \n    {\\displaystyle n=a\\cdot b}\n  \n⁠, one of the two factors ⁠\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n⁠ and ⁠\n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n⁠ is less than or equal to the square root of ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠. Another optimization is to check only primes as factors in this range. For instance, to check whether 37 is prime, this method divides it by the primes in the range from 2 to ⁠\n  \n    \n      \n        \n          \n            37\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {37}}}\n  \n⁠, which are 2, 3, and 5. Each division produces a nonzero remainder, so 37 is indeed prime.\nAlthough this method is simple to describe, it is impractical for testing the primality of large integers, because the number of tests that it performs grows exponentially as a function of the number of digits of these integers. However, trial division is still used, with a smaller limit than the square root on the divisor size, to quickly discover composite numbers with small factors, before using more complicated methods on the numbers that pass this filter."
      },
      {
        "heading": "Sieves",
        "level": 2,
        "content": "Before computers, mathematical tables listing all of the primes or prime factorizations up to a given limit were commonly printed. The oldest known method for generating a list of primes is called the sieve of Eratosthenes. The animation shows an optimized variant of this method. Another more asymptotically efficient sieving method for the same problem is the sieve of Atkin. In advanced mathematics, sieve theory applies similar methods to other problems."
      },
      {
        "heading": "Primality testing versus primality proving",
        "level": 2,
        "content": "Some of the fastest modern tests for whether an arbitrary given number ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ is prime are probabilistic (or Monte Carlo) algorithms, meaning that they have a small random chance of producing an incorrect answer. For instance the Solovay–Strassen primality test on a given number ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ chooses a number ⁠\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n⁠ randomly from 2 through \n  \n    \n      \n        p\n        −\n        2\n      \n    \n    {\\displaystyle p-2}\n  \n and uses modular exponentiation to check whether \n  \n    \n      \n        \n          a\n          \n            (\n            p\n            −\n            1\n            )\n            \n              /\n            \n            2\n          \n        \n        ±\n        1\n      \n    \n    {\\displaystyle a^{(p-1)/2}\\pm 1}\n  \n is divisible by ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠. If so, it answers yes and otherwise it answers no. If ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ really is prime, it will always answer yes, but if ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠ is composite then it answers yes with probability at most 1/2 and no with probability at least 1/2. If this test is repeated ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ times on the same number, the probability that a composite number could pass the test every time is at most ⁠\n  \n    \n      \n        1\n        \n          /\n        \n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 1/2^{n}}\n  \n⁠. Because this decreases exponentially with the number of tests, it provides high confidence (although not certainty) that a number that passes the repeated test is prime. On the other hand, if the test ever fails, then the number is certainly composite.\nA composite number that passes such a test is called a pseudoprime.\nIn contrast, some other algorithms guarantee that their answer will always be correct: primes will always be determined to be prime and composites will always be determined to be composite. For instance, this is true of trial division. The algorithms with guaranteed-correct output include both deterministic (non-random) algorithms, such as the AKS primality test,\nand randomized Las Vegas algorithms where the random choices made by the algorithm do not affect its final answer, such as some variations of elliptic curve primality proving.\nWhen the elliptic curve method concludes that a number is prime, it provides primality certificate that can be verified quickly.\nThe elliptic curve primality test is the fastest in practice of the guaranteed-correct primality tests, but its runtime analysis is based on heuristic arguments rather than rigorous proofs. The AKS primality test has mathematically proven time complexity, but is slower than elliptic curve primality proving in practice. These methods can be used to generate large random prime numbers, by generating and testing random numbers until finding one that is prime; when doing this, a faster probabilistic test can quickly eliminate most composite numbers before a guaranteed-correct algorithm is used to verify that the remaining numbers are prime.\nThe following table lists some of these tests. Their running time is given in terms of ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠, the number to be tested and, for probabilistic algorithms, the number ⁠\n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n⁠ of tests performed. Moreover, \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n is an arbitrarily small positive number, and log is the logarithm to an unspecified base. The big O notation means that each time bound should be multiplied by a constant factor to convert it from dimensionless units to units of time; this factor depends on implementation details such as the type of computer used to run the algorithm, but not on the input parameters ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ and ⁠\n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n⁠."
      },
      {
        "heading": "Special-purpose algorithms and the largest known prime",
        "level": 2,
        "content": "In addition to the aforementioned tests that apply to any natural number, some numbers of a special form can be tested for primality more quickly. For example, the Lucas–Lehmer primality test can determine whether a Mersenne number (one less than a power of two) is prime, deterministically, in the same time as a single iteration of the Miller–Rabin test. This is why since 1992 (as of October 2024) the largest known prime has always been a Mersenne prime. It is conjectured that there are infinitely many Mersenne primes.\nThe following table gives the largest known primes of various types. Some of these primes have been found using distributed computing. In 2009, the Great Internet Mersenne Prime Search project was awarded a US$100,000 prize for first discovering a prime with at least 10 million digits. The Electronic Frontier Foundation also offers $150,000 and $250,000 for primes with at least 100 million digits and 1 billion digits, respectively."
      },
      {
        "heading": "Integer factorization",
        "level": 2,
        "content": "Given a composite integer ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠, the task of providing one (or all) prime factors is referred to as factorization of ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠. It is significantly more difficult than primality testing, and although many factorization algorithms are known, they are slower than the fastest primality testing methods. Trial division and Pollard's rho algorithm can be used to find very small factors of ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠, and elliptic curve factorization can be effective when ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ has factors of moderate size. Methods suitable for arbitrary large numbers that do not depend on the size of its factors include the quadratic sieve and general number field sieve. As with primality testing, there are also factorization algorithms that require their input to have a special form, including the special number field sieve. As of December 2019 the largest number known to have been factored by a general-purpose algorithm is RSA-240, which has 240 decimal digits (795 bits) and is the product of two large primes.\nShor's algorithm can factor any integer in a polynomial number of steps on a quantum computer. However, current technology can only run this algorithm for very small numbers. As of October 2012, the largest number that has been factored by a quantum computer running Shor's algorithm is 21."
      },
      {
        "heading": "Other computational applications",
        "level": 2,
        "content": "Several public-key cryptography algorithms, such as RSA and the Diffie–Hellman key exchange, are based on large prime numbers (2048-bit primes are common). RSA relies on the assumption that it is much easier (that is, more efficient) to perform the multiplication of two (large) numbers ⁠\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n⁠ and ⁠\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n⁠ than to calculate ⁠\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n⁠ and ⁠\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n⁠ (assumed coprime) if only the product \n  \n    \n      \n        x\n        y\n      \n    \n    {\\displaystyle xy}\n  \n is known. The Diffie–Hellman key exchange relies on the fact that there are efficient algorithms for modular exponentiation (computing ⁠\n  \n    \n      \n        \n          a\n          \n            b\n          \n        \n        \n          mod\n          \n            c\n          \n        \n      \n    \n    {\\displaystyle a^{b}{\\bmod {c}}}\n  \n⁠), while the reverse operation (the discrete logarithm) is thought to be a hard problem.\nPrime numbers are frequently used for hash tables. For instance the original method of Carter and Wegman for universal hashing was based on computing hash functions by choosing random linear functions modulo large prime numbers. Carter and Wegman generalized this method to ⁠\n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n⁠-independent hashing by using higher-degree polynomials, again modulo large primes. As well as in the hash function, prime numbers are used for the hash table size in quadratic probing based hash tables to ensure that the probe sequence covers the whole table.\nSome checksum methods are based on the mathematics of prime numbers. For instance the checksums used in International Standard Book Numbers are defined by taking the rest of the number modulo 11, a prime number. Because 11 is prime this method can detect both single-digit errors and transpositions of adjacent digits. Another checksum method, Adler-32, uses arithmetic modulo 65521, the largest prime number less than ⁠\n  \n    \n      \n        \n          2\n          \n            16\n          \n        \n      \n    \n    {\\displaystyle 2^{16}}\n  \n⁠. Prime numbers are also used in pseudorandom number generators including linear congruential generators and the Mersenne Twister."
      },
      {
        "heading": "Other applications",
        "level": 1,
        "content": "Prime numbers are of central importance to number theory but also have many applications to other areas within mathematics, including abstract algebra and elementary geometry. For example, it is possible to place prime numbers of points in a two-dimensional grid so that no three are in a line, or so that every triangle formed by three of the points has large area. Another example is Eisenstein's criterion, a test for whether a polynomial is irreducible based on divisibility of its coefficients by a prime number and its square.\n\nThe concept of a prime number is so important that it has been generalized in different ways in various branches of mathematics. Generally, \"prime\" indicates minimality or indecomposability, in an appropriate sense. For example, the prime field of a given field is its smallest subfield that contains both 0 and 1. It is either the field of rational numbers or a finite field with a prime number of elements, whence the name. Often a second, additional meaning is intended by using the word prime, namely that any object can be, essentially uniquely, decomposed into its prime components. For example, in knot theory, a prime knot is a knot that is indecomposable in the sense that it cannot be written as the connected sum of two nontrivial knots. Any knot can be uniquely expressed as a connected sum of prime knots. The prime decomposition of 3-manifolds is another example of this type.\nBeyond mathematics and computing, prime numbers have potential connections to quantum mechanics, and have been used metaphorically in the arts and literature. They have also been used in evolutionary biology to explain the life cycles of cicadas."
      },
      {
        "heading": "Constructible polygons and polygon partitions",
        "level": 2,
        "content": "Fermat primes are primes of the form\n\n  \n    \n      \n        \n          F\n          \n            k\n          \n        \n        =\n        \n          2\n          \n            \n              2\n              \n                k\n              \n            \n          \n        \n        +\n        1\n        ,\n      \n    \n    {\\displaystyle F_{k}=2^{2^{k}}+1,}\n  \n\nwith ⁠\n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n⁠ a nonnegative integer. They are named after Pierre de Fermat, who conjectured that all such numbers are prime. The first five of these numbers – 3, 5, 17, 257, and 65,537 – are prime, but \n  \n    \n      \n        \n          F\n          \n            5\n          \n        \n      \n    \n    {\\displaystyle F_{5}}\n  \n is composite and so are all other Fermat numbers that have been verified as of 2017. A regular ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠-gon is constructible using straightedge and compass if and only if the odd prime factors of ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ (if any) are distinct Fermat primes. Likewise, a regular ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠-gon may be constructed using straightedge, compass, and an angle trisector if and only if the prime factors of ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ are any number of copies of 2 or 3 together with a (possibly empty) set of distinct Pierpont primes, primes of the form ⁠\n  \n    \n      \n        \n          2\n          \n            a\n          \n        \n        \n          3\n          \n            b\n          \n        \n        +\n        1\n      \n    \n    {\\displaystyle 2^{a}3^{b}+1}\n  \n⁠.\nIt is possible to partition any convex polygon into ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ smaller convex polygons of equal area and equal perimeter, when ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ is a power of a prime number, but this is not known for other values of ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠."
      },
      {
        "heading": "Quantum mechanics",
        "level": 2,
        "content": "Beginning with the work of Hugh Montgomery and Freeman Dyson in the 1970s, mathematicians and physicists have speculated that the zeros of the Riemann zeta function are connected to the energy levels of quantum systems. Prime numbers are also significant in quantum information science, thanks to mathematical structures such as mutually unbiased bases and symmetric informationally complete positive-operator-valued measures."
      },
      {
        "heading": "Biology",
        "level": 2,
        "content": "The evolutionary strategy used by cicadas of the genus Magicicada makes use of prime numbers. These insects spend most of their lives as grubs underground. They only pupate and then emerge from their burrows after 7, 13 or 17 years, at which point they fly about, breed, and then die after a few weeks at most. Biologists theorize that these prime-numbered breeding cycle lengths have evolved in order to prevent predators from synchronizing with these cycles. In contrast, the multi-year periods between flowering in bamboo plants are hypothesized to be smooth numbers, having only small prime numbers in their factorizations."
      },
      {
        "heading": "Arts and literature",
        "level": 2,
        "content": "Prime numbers have influenced many artists and writers. The French composer Olivier Messiaen used prime numbers to create ametrical music through \"natural phenomena\". In works such as La Nativité du Seigneur (1935) and Quatre études de rythme (1949–1950), he simultaneously employs motifs with lengths given by different prime numbers to create unpredictable rhythms: the primes 41, 43, 47 and 53 appear in the third étude, \"Neumes rythmiques\". According to Messiaen this way of composing was \"inspired by the movements of nature, movements of free and unequal durations\".\nIn his science fiction novel Contact, scientist Carl Sagan suggested that prime factorization could be used as a means of establishing two-dimensional image planes in communications with aliens, an idea that he had first developed informally with American astronomer Frank Drake in 1975. In the novel The Curious Incident of the Dog in the Night-Time by Mark Haddon, the narrator arranges the sections of the story by consecutive prime numbers as a way to convey the mental state of its main character, a mathematically gifted teen with Asperger syndrome. Prime numbers are used as a metaphor for loneliness and isolation in the Paolo Giordano novel The Solitude of Prime Numbers, in which they are portrayed as \"outsiders\" among integers."
      },
      {
        "heading": "Notes",
        "level": 1,
        "content": ""
      },
      {
        "heading": "References",
        "level": 1,
        "content": ""
      },
      {
        "heading": "External links",
        "level": 1,
        "content": "\"Prime number\". Encyclopedia of Mathematics. EMS Press. 2001 [1994].\nCaldwell, Chris, The Prime Pages at primes.utm.edu.\nPrime Numbers on In Our Time at the BBC.\n\"Teacher package: Prime numbers\" from Plus, December 1, 2008, produced by the Millennium Mathematics Project at the University of Cambridge."
      },
      {
        "heading": "Generators and calculators",
        "level": 2,
        "content": "Prime factors calculator can factorize any positive integer up to 20 digits.\nFast Online primality test with factorization makes use of the Elliptic Curve Method (up to thousand-digits numbers, requires Java).\nHuge database of prime numbers.\nPrime Numbers up to 1 trillion. Archived 2021-02-27 at the Wayback Machine."
      }
    ],
    "summary": "A prime number (or a prime) is a natural number greater than 1 that is not a product of two smaller natural numbers. A natural number greater than 1 that is not prime is called a composite number. For example, 5 is prime because the only ways of writing it as a product, 1 × 5 or 5 × 1, involve 5 itself. However, 4 is composite because it is a product (2 × 2) in which both numbers are smaller than 4. Primes are central in number theory because of the fundamental theorem of arithmetic: every natural number greater than 1 is either a prime itself or can be factorized as a product of primes that is unique up to their order.\nThe property of being prime is called primality. A simple but slow method of checking the primality of a given number ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠, called trial division, tests whether ⁠\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n⁠ is a multiple of any integer between 2 and ⁠\n  \n    \n      \n        \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {n}}}\n  \n⁠. Faster algorithms include the Miller–Rabin primality test, which is fast but has a small chance of error, and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical. Particularly fast methods are available for numbers of special forms, such as Mersenne numbers. As of October 2024 the largest known prime number is a Mersenne prime with 41,024,320 decimal digits.\nThere are infinitely many primes, as demonstrated by Euclid around 300 BC. No known simple formula separates prime numbers from composite numbers. However, the distribution of primes within the natural numbers in the large can be statistically modelled. The first result in that direction is the prime number theorem, proven at the end of the 19th century, which says roughly that the probability of a randomly chosen large number being prime is inversely proportional to its number of digits, that is, to its logarithm.\nSeveral historical questions regarding prime numbers are still unsolved. These include Goldbach's conjecture, that every even integer greater than 2 can be expressed as the sum of two primes, and the twin prime conjecture, that there are infinitely many pairs of primes that differ by two. Such questions spurred the development of various branches of number theory, focusing on analytic or algebraic aspects of numbers. Primes are used in several routines in information technology, such as public-key cryptography, which relies on the difficulty of factoring large numbers into their prime factors. In abstract algebra, objects that behave in a generalized way like prime numbers include prime elements and prime ideals."
  },
  {
    "title": "Número primo",
    "source": "https://es.wikipedia.org/wiki/N%C3%BAmero_primo",
    "language": "es",
    "chunks": [
      {
        "heading": "Historia",
        "level": 1,
        "content": ""
      },
      {
        "heading": "El Oriente prehelénico",
        "level": 2,
        "content": "Las muescas presentes en el hueso de Ishango, que data de hace más de &&&&&&&&&&020000.&&&&&020 000 años (anterior por tanto a la aparición de la escritura) y que fue hallado por el arqueólogo Jean de Heinzelin de Braucourt,[4]​ parecen aislar cuatro números primos: 11, 13, 17 y 19. Algunos arqueólogos interpretan este hecho como la prueba del conocimiento de los números primos. Con todo, existen muy pocos hallazgos que permitan discernir los conocimientos que tenía realmente el hombre de aquella época.[5]​\nNumerosas tablillas de arcilla cocida atribuidas a las civilizaciones que se fueron sucediendo en Mesopotamia a lo largo del II milenio a. C. muestran la resolución de problemas aritméticos y atestiguan los conocimientos de la época. Los cálculos requerían conocer los inversos de los naturales, que también se han hallado en tablillas.[6]​\nEn el sistema sexagesimal que empleaban los babilonios para escribir los números, los inversos de los divisores de potencias de 60 (números regulares) se calculan fácilmente; por ejemplo, dividir entre 24 equivale a multiplicar por 150 (2·60+30) y correr la coma sexagesimal dos lugares. El conocimiento matemático de los babilonios necesitaba una sólida comprensión de la multiplicación, la división y la factorización de los naturales.\nEn las matemáticas egipcias, el cálculo de fracciones requería conocimientos sobre las operaciones, la división de naturales y la factorización. Los egipcios solo operaban con las llamadas fracciones egipcias, suma de fracciones unitarias, es decir, aquellas cuyo numerador es 1, como \n  \n    \n      \n        \n          \n            \n              1\n              2\n            \n          \n        \n        ,\n        \n          \n            \n              1\n              3\n            \n          \n        \n        ,\n        \n          \n            \n              1\n              4\n            \n          \n        \n        ,\n        \n          \n            \n              1\n              5\n            \n          \n        \n        ,\n        …\n      \n    \n    {\\displaystyle {\\tfrac {1}{2}},{\\tfrac {1}{3}},{\\tfrac {1}{4}},{\\tfrac {1}{5}},\\dots }\n  \n, por lo que las fracciones de numerador distinto de 1 se escribían como suma de inversos de naturales, a ser posible sin repetición \n  \n    \n      \n        \n          (\n          \n            \n              \n                \n                  2\n                  3\n                \n              \n            \n            =\n            \n              \n                \n                  1\n                  2\n                \n              \n            \n            +\n            \n              \n                \n                  1\n                  6\n                \n              \n            \n          \n          \n        \n      \n    \n    {\\displaystyle \\left({\\tfrac {2}{3}}={\\tfrac {1}{2}}+{\\tfrac {1}{6}}\\right.}\n  \n en lugar de \n  \n    \n      \n        \n          \n          \n            \n              \n                \n                  1\n                  3\n                \n              \n            \n            +\n            \n              \n                \n                  1\n                  3\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left.{\\tfrac {1}{3}}+{\\tfrac {1}{3}}\\right)}\n  \n.[7]​ Es por ello que, en cierta manera, tenían que conocer o intuir los números primos.[8]​"
      },
      {
        "heading": "Antigua Grecia",
        "level": 2,
        "content": "La primera prueba indiscutible del conocimiento de los números primos se remonta a alrededor del año 300 a. C. y se encuentra en los Elementos de Euclides (tomos VII a IX). Euclides define los números primos, demuestra que hay infinitos de ellos, define el máximo común divisor y el mínimo común múltiplo y proporciona un método para determinarlos que hoy en día se conoce como el algoritmo de Euclides. Los Elementos contienen asimismo el teorema fundamental de la aritmética y la manera de construir un número perfecto a partir de un número primo de Mersenne.\nLa criba de Eratóstenes, atribuida a Eratóstenes de Cirene, es un método sencillo que permite encontrar números primos. Hoy en día, empero, los mayores números primos que se encuentran con la ayuda de ordenadores emplean otros algoritmos más rápidos y complejos."
      },
      {
        "heading": "Desde la época del Renacimiento",
        "level": 2,
        "content": "Después de las matemáticas griegas hubo pocos avances en el estudio de los números primos hasta el siglo xvii. En 1640 Pierre de Fermat estableció (aunque sin demostración) el pequeño teorema de Fermat, posteriormente demostrado por Leibniz y Euler. Es posible que mucho antes se conociera un caso especial de dicho teorema en China.\nFermat conjeturó que todos los números de la forma 22n+1 eran primos (debido a lo cual se los conoce como números de Fermat) y verificó esta propiedad hasta n = 4 (es decir, 216 + 1). Sin embargo, el número de Fermat 232 + 1 es compuesto (uno de sus factores primos es 641), como demostró Euler. De hecho, hasta nuestros días no se conoce ningún número de Fermat que sea primo aparte de los que ya conocía el propio Fermat.\nEl monje francés Marin Mersenne investigó los números primos de la forma 2p − 1, con p primo. En su honor, se los conoce como números de Mersenne.\nEn el trabajo de Euler en teoría de números se encuentran muchos resultados que conciernen a los números primos. Demostró la divergencia de la serie \n  \n    \n      \n        \n          \n            \n              1\n              2\n            \n          \n        \n        +\n        \n          \n            \n              1\n              3\n            \n          \n        \n        +\n        \n          \n            \n              1\n              5\n            \n          \n        \n        +\n        \n          \n            \n              1\n              7\n            \n          \n        \n        +\n        …\n      \n    \n    {\\displaystyle {\\tfrac {1}{2}}+{\\tfrac {1}{3}}+{\\tfrac {1}{5}}+{\\tfrac {1}{7}}+\\dots }\n  \n, y en 1747 demostró que todos los números perfectos pares son de la forma 2p-1(2p - 1), donde el segundo factor es un número primo de Mersenne. Se cree que no existen números perfectos impares, pero todavía es una cuestión abierta.\nA comienzos del siglo xix, Legendre y Gauss conjeturaron de forma independiente que, cuando n tiende a infinito, el número de primos menores o iguales que n es asintótico a \n  \n    \n      \n        \n          \n            \n              n\n              \n                ln\n                ⁡\n                (\n                n\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {n}{\\ln(n)}}}\n  \n, donde ln(n) es el logaritmo natural de n. Las ideas que Bernhard Riemann plasmó en un trabajo de 1859 sobre la función zeta describieron el camino que conduciría a la demostración del teorema de los números primos. Hadamard y De la Vallée-Poussin, cada uno por separado, dieron forma a este esquema y consiguieron demostrar el teorema en 1896.\nActualmente no se comprueba la primalidad de un número por divisiones sucesivas, al menos no si el número es relativamente grande.\nDurante el siglo xix se desarrollaron algoritmos para saber si un número es primo o no factorizando completamente el número siguiente (p+1) o el anterior (p-1). Dentro del primer caso se encuentra el test de Lucas-Lehmer, desarrollado a partir de 1856. Dentro del segundo caso se encuentra el test de Pépin para los números de Fermat (1877). El caso general de test de primalidad cuando el número inmediatamente anterior se encuentra completamente factorizado se denomina test de Lucas.\nPosteriormente se encontraron algoritmos de primalidad con solo obtener una factorización parcial de p+1 o p-1. Ejemplos de estos algoritmos son el test de Proth (desarrollado alrededor de 1878) y el test de Pocklington (1914). En estos algoritmos se requiere que el producto de los factores primos conocidos de p-1 sea mayor que la raíz cuadrada de p. Más recientemente, en 1975, Brillhart, Lehmer y Selfridge desarrollaron el test de primalidad BLS que solo requiere que dicho producto sea mayor que la raíz cúbica de p. El mejor método conocido de esta clase es el test de Koniaguin y Pomerance del año 1997, que requiere que dicho producto sea mayor que p3/10.[9]​[10]​\nA partir de la década de 1970 varios investigadores descubrieron algoritmos para determinar si cualquier número es primo o no con complejidad subexponencial, lo que permite realizar tests en números de miles de dígitos, aunque son mucho más lentos que los métodos anteriores. Ejemplos de estos algoritmos son el test APRT-CL (desarrollado en 1979 por Adleman, Pomerance y Rumely, con mejoras introducidas por Cohen y Lenstra en 1984), donde se usan los factores de pm-1, donde el exponente m depende del tamaño del número cuya primalidad se desea verificar, el test de primalidad por curvas elípticas (desarrollado en 1986 por S. Goldwasser, J. Kilian y mejorado por A. O. L. Atkin), que entrega un certificado consistente en una serie de números que permite después confirmar rápidamente si el número es primo o no. El desarrollo más reciente es el test de primalidad AKS (2002), que si bien su complejidad es polinómica, para los números que puede manejar la tecnología actual es el más lento de los tres.\nDurante mucho tiempo, se pensaba que la aplicación de los números primos era muy limitada fuera de la matemática pura.[11]​[12]​ Esto cambió en los años 1970 con el desarrollo de la criptografía de clave pública, en la que los números primos formaban la base de los primeros algoritmos, tales como el algoritmo RSA.\nDesde 1951, el mayor número primo conocido siempre ha sido descubierto con la ayuda de ordenadores. La búsqueda de números primos cada vez mayores ha suscitado interés incluso fuera de la comunidad matemática. En los últimos años han ganado popularidad proyectos de computación distribuida tales como el GIMPS, mientras los matemáticos siguen investigando las propiedades de los números primos."
      },
      {
        "heading": "El número 1 no se considera primo",
        "level": 1,
        "content": "La cuestión acerca de si el número 1 debe o no considerarse primo está basada en la convención. Ambas posturas tienen sus ventajas y sus inconvenientes. De hecho, hasta el siglo xix, los matemáticos en su mayoría lo consideraban primo. Muchos trabajos matemáticos siguen siendo válidos a pesar de considerar el 1 como un número primo, como, por ejemplo, el de Stern y Zeisel. La lista de Derrick Norman Lehmer de números primos hasta el 10.006.721, reimpresa hasta el año 1956[13]​ empezaba con el 1 como primer número primo.[14]​\nActualmente, la comunidad matemática se inclina por no considerar al 1 en la lista de los números primos. Esta convención, por ejemplo, permite una formulación muy económica del teorema fundamental de la aritmética: «todo número natural tiene una representación única como producto de factores primos, salvo el orden».[15]​[16]​ Además, los números primos tienen numerosas propiedades de las que carece el 1, tales como la relación del número con el valor correspondiente de la función φ de Euler o la función divisor.[17]​ Cabe también la igualdad para todo \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n entero positivo, \n  \n    \n      \n        1\n        =\n        \n          1\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 1=1^{n}}\n  \n, lo que permitiría decir que tiene \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n factores.[18]​"
      },
      {
        "heading": "Propiedades de los números primos",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Teorema fundamental de la aritmética",
        "level": 2,
        "content": "El teorema fundamental de la aritmética establece que todo número natural tiene una representación única como producto de factores primos, salvo el orden. Un mismo factor primo puede aparecer varias veces. El 1 se representa entonces como un producto vacío.\nSe puede considerar que los números primos son los «ladrillos» con los que se construye cualquier número natural. Por ejemplo, se puede escribir el número 23.244 como producto de 22·3·13·149, y cualquier otra factorización del 23.244 como producto de números primos será idéntica excepto por el orden de los factores.\nLa importancia de este teorema es una de las razones para excluir el 1 del conjunto de los números primos. Si se admitiera el 1 como número primo, el enunciado del teorema requeriría aclaraciones adicionales.\nA partir de esta unicidad en la factorización en factores primos se desarrollan otros conceptos muy utilizados en matemáticas, tales como el mínimo común múltiplo, el máximo común divisor y la coprimalidad de dos o más números. Así,\n\nEl mínimo común múltiplo de dos o más números es el menor de los múltiplos comunes de todos ellos. Para calcularlo, se descomponen los números en factores primos y se toman los factores comunes y no comunes con su máximo exponente. Por ejemplo, el mínimo común múltiplo de 10=2·5 y 12=22·3 es 60=22·3·5.\nEl máximo común divisor de dos o más números es el mayor de los divisores comunes de todos ellos. Es igual al producto de los factores comunes con su mínimo exponente. En el ejemplo anterior, el máximo común divisor de 10 y 12 es 2.\nFinalmente, dos o más números son coprimos, o primos entre sí, si no tienen ningún factor primo común; es decir, si su máximo común divisor es 1. Un número primo es, así, coprimo con cualquier número natural que no sea múltiplo de él mismo."
      },
      {
        "heading": "Otras propiedades",
        "level": 2,
        "content": "En su escritura en el sistema de numeración decimal, todos los números primos, salvo el 2 y el 5, tiene como el guarismo de las unidades uno de estos: 1, 3, 7 o 9. En general, en cualquier sistema de numeración, todos los números primos salvo un número finito acaban en una cifra que es coprima con la base.\nDe lo anterior se deduce que todos los números primos salvo el 2 son de la forma 4n + 1 o bien 4n + 3. Igualmente, todos los números primos salvo el 2 y el 3 son de la forma 6n + 1 o 6n - 1.\nEn la progresión aritmética 3, 7, 11, 15, 19, 23, 27, 31, …, hay una cantidad infinita de números primos de la forma 4n-1, n natural.[19]​\nEn la progresión aritmética 7, 13, 19, 25, 31, 37, 43, 49, 55, 61, 67, …, hay una cantidad infinita de números primos de la forma 6k+1, k natural.[20]​\nLema de Euclides: Si p es un número primo y divisor del producto de números enteros ab, entonces p es divisor de a o de b.\nPequeño teorema de Fermat: Si p es primo y a es algún número natural diferente de 1, entonces ap - a es divisible por p.\nSi un número p no divide al número m, entonces (p; m) =1[21]​\nSi p es primo distinto de 2 y 5, \n  \n    \n      \n        \n          \n            \n              1\n              p\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {1}{p}}}\n  \n siempre es un número periódico en su representación decimal, de periodo p − 1 o un divisor de p − 1. Esto se puede deducir directamente a partir del pequeño teorema de Fermat. \n  \n    \n      \n        \n          \n            \n              1\n              p\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {1}{p}}}\n  \n expresado en base q (en lugar de en base 10) tiene propiedades similares, siempre que p no sea un factor primo de q.\nTeorema de Wilson: Un número natural n > 1 es primo si y solo si el factorial (n - 1)! + 1 es divisible por n. Asimismo, un número natural n > 4 es compuesto si y solo si (n - 1)! es divisible por n.\nLa característica de todo cuerpo es, o bien cero, o bien un número primo.\nPrimer teorema de Sylow: Si G es un grupo finito, p primo y pn es la mayor potencia de p que divide el orden de G. Entonces, existe un subgrupo de G de orden pn.\nTeorema de Cauchy: Si G es un grupo finito y p es un número primo que divide al orden de G, entonces G contiene un elemento de orden p.\nLa constante de Copeland-Erdős 0,235711131719232931374143…, obtenida por concatenación de los números primos en el sistema decimal, es un número irracional.\nEl valor de la función zeta de Riemann en cada punto del plano complejo se da como una continuación meromorfa de una función definida por un producto sobre el conjunto de todos los primos para Re(s) > 1:\n\n  \n    \n      \n        ζ\n        (\n        s\n        )\n        =\n        \n          ∑\n          \n            n\n            =\n            1\n          \n          \n            ∞\n          \n        \n        \n          \n            1\n            \n              n\n              \n                s\n              \n            \n          \n        \n        =\n        \n          ∏\n          \n            p\n          \n        \n        \n          \n            1\n            \n              1\n              −\n              \n                p\n                \n                  −\n                  s\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\zeta (s)=\\sum _{n=1}^{\\infty }{\\frac {1}{n^{s}}}=\\prod _{p}{\\frac {1}{1-p^{-s}}}.}\n  \n\nEn la región donde es convergente, este producto indexado por los números primos se puede calcular, obteniéndose diversos valores, algunos de ellos importantes en teoría de números. Los dos primeros son:\n\n  \n    \n      \n        \n          ∏\n          \n            p\n          \n        \n        \n          \n            1\n            \n              1\n              −\n              \n                p\n                \n                  −\n                  1\n                \n              \n            \n          \n        \n        =\n        ∞\n      \n    \n    {\\displaystyle \\prod _{p}{\\frac {1}{1-p^{-1}}}=\\infty }\n  \n (correspondiente a la serie armónica, relacionado con la infinitud de números primos).\n\n  \n    \n      \n        \n          ∏\n          \n            p\n          \n        \n        \n          \n            1\n            \n              1\n              −\n              \n                p\n                \n                  −\n                  2\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              π\n              \n                2\n              \n            \n            6\n          \n        \n      \n    \n    {\\displaystyle \\prod _{p}{\\frac {1}{1-p^{-2}}}={\\frac {\\pi ^{2}}{6}}}\n  \n (correspondiente al problema de Basilea).\nEn general \n  \n    \n      \n        \n          \n            1\n            \n              π\n              \n                n\n              \n            \n          \n        \n        \n          ∏\n          \n            p\n          \n        \n        \n          \n            1\n            \n              1\n              −\n              \n                p\n                \n                  −\n                  n\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{\\pi ^{n}}}\\prod _{p}{\\frac {1}{1-p^{-n}}}}\n  \n es un número racional cuando n es un número entero positivo par.\nEl anillo \n  \n    \n      \n        \n          Z\n        \n        \n          /\n        \n        p\n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbb {Z} /p\\mathbb {Z} }\n  \n es un cuerpo si y solo si p es primo. Equivalentemente: p es primo si y solo si φ(p) = p − 1.\nSi p > 1, el polinomio x p-1+x p-2+ ··· + 1 es irreducible sobre \n  \n    \n      \n        \n          Z\n        \n        \n          /\n        \n        p\n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbb {Z} /p\\mathbb {Z} }\n  \n si y solo si p es primo.\nUn número natural n es primo si y solo si el n-ésimo polinomio de Chebyshov de la primera especie Tn(x), dividido entre x, es \n  \n    \n      \n        2\n        =\n        i\n        (\n        1\n        −\n        i\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle 2=i(1-i)^{2}}\n  \n irreducible en \n  \n    \n      \n        \n          Z\n        \n        [\n        x\n        ]\n      \n    \n    {\\displaystyle \\mathbb {Z} [x]}\n  \n. Además, Tn(x) ≡ xn si y solo si n es primo.\nNo todo número primo es un número gaussiano primo; tal el caso de 2, que como entero gaussiano admite la descomposición \n  \n    \n      \n        2\n        =\n        i\n        (\n        1\n        −\n        i\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle 2=i(1-i)^{2}}\n  \n don de la norma de \n  \n    \n      \n        1\n        −\n        i\n      \n    \n    {\\displaystyle 1-i}\n  \n es 2, por lo tanto no es unidad en Z[i].\nLos números primos de la forma \n  \n    \n      \n        4\n        n\n        +\n        1\n      \n    \n    {\\displaystyle 4n+1}\n  \n son igual a la suma de dos cuadrados perfectos; por lo que no son números gaussianos primos. En tanto que los números primos de la forma \n  \n    \n      \n        4\n        n\n        +\n        3\n      \n    \n    {\\displaystyle 4n+3}\n  \n sí son números gaussianos primos.\nTodo número racional primo es un número gaussiano entero, sin ser necesariamente número gaussiano primo.[22]​"
      },
      {
        "heading": "Números primos y funciones aritméticas",
        "level": 2,
        "content": "Las funciones aritméticas, es decir, funciones reales o complejas, definidas sobre un conjunto de números naturales, desempeñan un papel crucial en la teoría de números. Las más importantes son las funciones multiplicativas, que son aquellas funciones f en las cuales, para cada par de números coprimos (a,b) se tiene\n\n  \n    \n      \n        f\n        (\n        a\n        b\n        )\n        =\n        f\n        (\n        a\n        )\n        f\n        (\n        b\n        )\n        \n        \n      \n    \n    {\\displaystyle f(ab)=f(a)f(b)\\,\\!}\n  \n.\nAlgunos ejemplos de funciones multiplicativas son la función φ de Euler, que a cada n asocia el número de enteros positivos menores y coprimos con n, y las funciones τ y σ, que a cada n asocian respectivamente el número de divisores de n y la suma de todos ellos. El valor de estas funciones en las potencias de números primos es\n\n  \n    \n      \n        \n          φ\n        \n        ⁡\n        (\n        \n          p\n          \n            m\n          \n        \n        )\n        =\n        \n          p\n          \n            m\n          \n        \n        −\n        \n          p\n          \n            m\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle \\operatorname {\\varphi } (p^{m})=p^{m}-p^{m-1}}\n  \n,\n\n  \n    \n      \n        \n          τ\n        \n        ⁡\n        (\n        \n          p\n          \n            m\n          \n        \n        )\n        =\n        m\n        +\n        1\n      \n    \n    {\\displaystyle \\operatorname {\\tau } (p^{m})=m+1}\n  \n,\n\n  \n    \n      \n        \n          σ\n        \n        ⁡\n        (\n        \n          p\n          \n            m\n          \n        \n        )\n        =\n        1\n        +\n        \n          p\n          \n            2\n          \n        \n        +\n        \n          p\n          \n            3\n          \n        \n        +\n        ⋯\n        +\n        \n          p\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle \\operatorname {\\sigma } (p^{m})=1+p^{2}+p^{3}+\\cdots +p^{m}}\n  \n.\nGracias a la propiedad que las define, las funciones aritméticas pueden calcularse fácilmente a partir del valor que toman en las potencias de números primos. De hecho, dado un número natural n de factorización\n\n  \n    \n      \n        n\n        =\n        \n          p\n          \n            1\n          \n          \n            \n              q\n              \n                1\n              \n            \n          \n        \n        ⋯\n        \n          p\n          \n            a\n          \n          \n            \n              q\n              \n                a\n              \n            \n          \n        \n      \n    \n    {\\displaystyle n=p_{1}^{q_{1}}\\cdots p_{a}^{q_{a}}}\n  \n\nse tiene que\n\n  \n    \n      \n        f\n        (\n        n\n        )\n        =\n        f\n        (\n        \n          p\n          \n            1\n          \n          \n            \n              q\n              \n                1\n              \n            \n          \n        \n        )\n        ⋯\n        f\n        (\n        \n          p\n          \n            a\n          \n          \n            \n              q\n              \n                a\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle f(n)=f(p_{1}^{q_{1}})\\cdots f(p_{a}^{q_{a}})}\n  \n\ncon lo que se ha reconducido el problema de calcular f(n) al de calcular f sobre las potencias de los números primos que dividen n, valores que son generalmente más fáciles de obtener mediante una fórmula general. Por ejemplo, para conocer el valor de la función φ sobre n=450=2·32·52 basta con calcular\n\n  \n    \n      \n        \n          φ\n        \n        ⁡\n        (\n        450\n        )\n        =\n        \n          φ\n        \n        ⁡\n        (\n        2\n        )\n        ⋅\n        \n          φ\n        \n        ⁡\n        (\n        \n          3\n          \n            2\n          \n        \n        )\n        ⋅\n        \n          φ\n        \n        ⁡\n        (\n        \n          5\n          \n            2\n          \n        \n        )\n        =\n        (\n        2\n        −\n        1\n        )\n        ⋅\n        (\n        9\n        −\n        3\n        )\n        ⋅\n        (\n        25\n        −\n        5\n        )\n        =\n        120\n      \n    \n    {\\displaystyle \\operatorname {\\varphi } (450)=\\operatorname {\\varphi } (2)\\cdot \\operatorname {\\varphi } (3^{2})\\cdot \\operatorname {\\varphi } (5^{2})=(2-1)\\cdot (9-3)\\cdot (25-5)=120}\n  \n."
      },
      {
        "heading": "Características del conjunto de los números primos",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Infinitud de los números primos",
        "level": 2,
        "content": "Existen infinitos números primos. Euclides realizó la primera demostración alrededor del año 300 a. C. en el libro IX de su obra Elementos.[23]​ Una adaptación común de esta demostración original sigue así: Se toma un conjunto arbitrario pero finito de números primos p1, p2, p3, ···, pn, y se considera el producto de todos ellos más uno, \n  \n    \n      \n        q\n        =\n        \n          p\n          \n            1\n          \n        \n        ×\n        \n          p\n          \n            2\n          \n        \n        ×\n        \n          p\n          \n            3\n          \n        \n        …\n        ×\n        \n          p\n          \n            n\n          \n        \n        +\n        1\n      \n    \n    {\\displaystyle q=p_{1}\\times p_{2}\\times p_{3}\\ldots \\times p_{n}+1}\n  \n. Este número es obviamente mayor que 1 y distinto de todos los primos pi de la lista. El número q puede ser primo o compuesto. Si es primo tendremos un número primo que no está en el conjunto original. Si, por el contrario, es compuesto, entonces existirá algún factor p que divida a q. Suponiendo que p es alguno de los pi, se deduce entonces que p divide a la diferencia \n  \n    \n      \n        q\n        −\n        \n          p\n          \n            1\n          \n        \n        ×\n        \n          p\n          \n            2\n          \n        \n        ×\n        \n          p\n          \n            3\n          \n        \n        …\n        ×\n        \n          p\n          \n            n\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle q-p_{1}\\times p_{2}\\times p_{3}\\ldots \\times p_{n}=1}\n  \n, pero ningún número primo divide a 1, es decir, se ha llegado a un absurdo por suponer que p está en el conjunto original. La consecuencia es que el conjunto que se escogió no es exhaustivo, ya que existen números primos que no pertenecen a él, y esto es independiente del conjunto finito que se tome.\nPor tanto, el conjunto de los números primos es infinito.\nSi se toma como conjunto el de los n primeros números primos, entonces \n  \n    \n      \n        q\n        =\n        \n          p\n          \n            1\n          \n        \n        ×\n        \n          p\n          \n            2\n          \n        \n        ×\n        \n          p\n          \n            3\n          \n        \n        …\n        ×\n        \n          p\n          \n            n\n          \n        \n        +\n        1\n        =\n        \n          p\n          \n            n\n          \n        \n        ♯\n        +\n        1\n      \n    \n    {\\displaystyle q=p_{1}\\times p_{2}\\times p_{3}\\ldots \\times p_{n}+1=p_{n}\\sharp +1}\n  \n, donde pn# es lo que se llama primorial de pn. Un número primo de la forma pn# +1 se denomina número primo de Euclides en honor al matemático griego. También se puede elaborar una demostración similar a la de Euclides tomando el producto de un número dado de números primos menos uno, el lugar del producto de esos números primos más uno. En ese sentido, se denomina número primo primorial a un número primo de la forma pn# ± 1.\nNo todos los números de la forma pn# +1 son primos. En este caso, como se sigue de la demostración anterior, todos los factores primos deberán ser mayores que n. Por ejemplo: 2·3·5·7·11·13+1=30031=59·509\nOtros matemáticos han demostrado la infinitud de los números primos con diversos métodos procedentes de áreas de las matemáticas tales como al álgebra conmutativa y la topología.[24]​\nAlgunas de estas demostraciones se basan en el uso de sucesiones infinitas con la propiedad de que cada uno de sus términos es coprimo con todos los demás, por lo que se crea una biyección entre los términos de la sucesión y un subconjunto (infinito) del conjunto de los primos.\nUna sucesión que cumple dicha propiedad es la sucesión de Euclides-Mullin, que deriva de la demostración euclídea de la infinitud de los números primos, ya que cada uno de sus términos se define como el factor primo más pequeño de uno más el producto de todos los términos anteriores. La sucesión de Sylvester se define de forma similar, puesto que cada uno de sus términos es igual a uno más el producto de todos los anteriores. Aunque los términos de esta última sucesión no son necesariamente todos primos, cada uno de ellos es coprimo con todos los demás, por lo que se puede escoger cualquiera de sus factores primos, por ejemplo, el menor de ellos, y el conjunto resultante será un conjunto infinito cuyos términos son todos primos."
      },
      {
        "heading": "Otros enunciados que implican la infinitud de los números primos",
        "level": 3,
        "content": "Un resultado aún más fuerte, y que implica directamente la infinitud de los números primos, fue descubierto por Euler en el siglo xviii. Establece que la serie \n  \n    \n      \n        \n          \n            \n              1\n              2\n            \n          \n        \n        +\n        \n          \n            \n              1\n              3\n            \n          \n        \n        +\n        \n          \n            \n              1\n              5\n            \n          \n        \n        +\n        \n          \n            \n              1\n              7\n            \n          \n        \n        +\n        …\n      \n    \n    {\\displaystyle {\\tfrac {1}{2}}+{\\tfrac {1}{3}}+{\\tfrac {1}{5}}+{\\tfrac {1}{7}}+\\dots }\n  \n es divergente. Uno de los teoremas de Mertens concreta más, estableciendo que\n\n  \n    \n      \n        \n          ∑\n          \n            p\n            ≤\n            n\n          \n        \n        \n          \n            1\n            p\n          \n        \n        =\n        ln\n        ⁡\n        ln\n        ⁡\n        n\n        +\n        O\n        (\n        1\n        )\n      \n    \n    {\\displaystyle \\sum _{p\\leq n}{\\frac {1}{p}}=\\ln \\ln n+O(1)}\n  \n[25]​\ndonde la expresión O(1) indica que ese término está acotado entre -C y C para n mayor que n0, donde los valores de C y n0 no están especificados.[26]​\nOtro resultado es el teorema de Dirichlet, que dice así:\n\nEl postulado de Bertrand enuncia así:\n\nUna manera más débil pero elegante de formularlo es que, si n es un número natural mayor que 1, entonces siempre existe un número primo p tal que n < p < 2n. Esto supone que, en una progresión geométrica de primer término entero mayor que 3 y razón igual a 2, entre cada término de la progresión y el siguiente, se tiene al menos un número primo."
      },
      {
        "heading": "Frecuencia de los números primos",
        "level": 2,
        "content": "Una vez demostrado la infinitud de los números primos, cabe preguntarse cómo se distribuyen los primos entre los números naturales, es decir, cuán frecuentes son y dónde se espera encontrar el n-ésimo número primo. Este estudio lo iniciaron Gauss y Legendre de forma independiente a finales del siglo xviii, para el cual introdujeron la función enumerativa de los números primos π(n), y conjeturaron que su valor fuese aproximadamente\n\n  \n    \n      \n        π\n        (\n        n\n        )\n        ∼\n        \n          \n            n\n            \n              ln\n              ⁡\n              n\n            \n          \n        \n      \n    \n    {\\displaystyle \\pi (n)\\sim {\\frac {n}{\\ln n}}}\n  \n.[27]​\nEl empeño de demostrar esta conjetura abarcó todo el siglo xix. Los primeros resultados fueron obtenidos entre 1848 y 1859 por Chebyshov, quien demostró utilizando métodos puramente aritméticos la existencia de dos constantes A y B tales que\n\n  \n    \n      \n        A\n        ≤\n        \n          \n            \n              π\n              (\n              n\n              )\n            \n            \n              n\n              \n                ln\n                ⁡\n                n\n              \n            \n          \n        \n        ≤\n        B\n      \n    \n    {\\displaystyle A\\leq {\\frac {\\pi (n)}{\\frac {n}{\\ln n}}}\\leq B}\n  \n\npara n suficientemente grande. Consiguió demostrar que, si existía el límite del cociente de aquellas expresiones, este debía ser 1.\nHadamard y De la Vallée-Poussin elaboraron una demostración en 1896, independientemente el uno del otro, usando métodos similares, basados en el uso de la función zeta de Riemann, que había sido introducida por Bernhard Riemann en 1859. Hubo que esperar hasta 1949 para encontrar una demostración que usara solo métodos elementales (es decir, sin usar el análisis complejo). Esta demostración fue ideada por Selberg y Erdős. Actualmente, se conoce el teorema como teorema de los números primos.\nEl mismo Gauss introdujo una estimación más precisa, utilizando la función logaritmo integral:\n\n  \n    \n      \n        π\n        (\n        n\n        )\n        ≈\n        \n          L\n          i\n        \n        (\n        n\n        )\n        =\n        \n          ∫\n          \n            2\n          \n          \n            n\n          \n        \n        \n          \n            1\n            \n              ln\n              ⁡\n              x\n            \n          \n        \n        \n          d\n        \n        x\n      \n    \n    {\\displaystyle \\pi (n)\\approx \\mathrm {Li} (n)=\\int _{2}^{n}{\\frac {1}{\\ln x}}\\mathrm {d} x}\n  \n.\nEn 1899 De la Vallée-Poussin demostró que el error que se comete aproximando \n  \n    \n      \n        π\n        (\n        n\n        )\n      \n    \n    {\\displaystyle \\pi (n)}\n  \n de esta forma es\n\n  \n    \n      \n        π\n        (\n        n\n        )\n        −\n        \n          L\n          i\n        \n        (\n        n\n        )\n        =\n        O\n        \n          (\n          \n            n\n            \n              \n                e\n              \n              \n                −\n                a\n                \n                  \n                    ln\n                    ⁡\n                    n\n                  \n                \n              \n            \n          \n          )\n        \n        =\n        O\n        \n          (\n          \n            \n              n\n              \n                (\n                ln\n                ⁡\n                n\n                \n                  )\n                  \n                    m\n                  \n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\pi (n)-\\mathrm {Li} (n)=O\\left(n\\mathrm {e} ^{-a{\\sqrt {\\ln n}}}\\right)=O\\left({\\frac {n}{(\\ln n)^{m}}}\\right)}\n  \n\npara una constante positiva a y para cada entero m. Este resultado fue ligeramente mejorado a lo largo de los años. Por otra parte, en 1901 Von Koch mostró que si la hipótesis de Riemann era cierta, se tenía la siguiente estimación, más precisa:[28]​\n\n  \n    \n      \n        \n          |\n          \n            π\n            (\n            n\n            )\n            −\n            \n              L\n              i\n            \n            (\n            n\n            )\n          \n          |\n        \n        =\n        O\n        \n          (\n          \n            \n              \n                n\n              \n            \n            ln\n            ⁡\n            n\n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\left|\\pi (n)-\\mathrm {Li} (n)\\right|=O\\left({\\sqrt {n}}\\ln n\\right).}\n  \n\nUna forma equivalente al teorema de los números primos es que pn, el n-ésimo número primo, queda bien aproximado por nln(n). En efecto, pn es estrictamente mayor que este valor."
      },
      {
        "heading": "Diferencia entre dos primos consecutivos",
        "level": 2,
        "content": "Ligado a la distribución de los números primos se encuentra el estudio de los intervalos entre dos primos consecutivos. Este intervalo, con la única salvedad del que hay entre el 2 y el 3, debe ser siempre igual o mayor que 2, ya que entre dos números primos consecutivos al menos hay un número par y por tanto compuesto. Si dos números primos tienen por diferencia 2, se dice que son gemelos, y con la salvedad del «triplete» formado por los números 3, 5 y 7, los números gemelos se presentan siempre de dos en dos. Esto también es fácil de demostrar: entre tres números impares consecutivos mayores que 3 siempre hay uno que es múltiplo de 3, y por tanto compuesto. Los primeros pares de números primos gemelos son (3,5), (5,7), (11, 13), (17, 19) y (29, 31).\nPor otra parte, la diferencia entre primos consecutivos puede ser tan grande como se quiera. La demostración es relativamente sencilla: \nSea un número natural \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n. Entonces, todos los números de la forma \n\n  \n    \n      \n        (\n        n\n        +\n        1\n        )\n        !\n        +\n        i\n      \n    \n    {\\textstyle (n+1)!+i}\n  \n\nson números compuestos si \n  \n    \n      \n        2\n        ≤\n        i\n        ≤\n        n\n        +\n        1\n      \n    \n    {\\displaystyle 2\\leq i\\leq n+1}\n  \n, pues \n  \n    \n      \n        1\n        <\n        i\n        <\n        (\n        n\n        +\n        1\n        )\n        +\n        i\n      \n    \n    {\\displaystyle 1<i<(n+1)+i}\n  \n y \n  \n    \n      \n        \n          \n            \n              (\n              n\n              +\n              1\n              )\n              !\n              +\n              i\n            \n            i\n          \n        \n        =\n        (\n        i\n        −\n        1\n        )\n        !\n        ⋅\n        (\n        i\n        +\n        1\n        )\n        (\n        i\n        +\n        2\n        )\n        .\n        .\n        .\n        (\n        n\n        )\n        (\n        n\n        +\n        1\n        )\n        +\n        1\n        ∈\n        \n          N\n        \n      \n    \n    {\\displaystyle {\\frac {(n+1)!+i}{i}}=(i-1)!\\cdot (i+1)(i+2)...(n)(n+1)+1\\in \\mathbb {N} }\n  \n.\nSe puede construir así una lista con \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n números compuestos, y dado que \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n es un número natural arbitrario, entonces el intervalo puede hacerse tan grande como se desee. \nPor ejemplo, si se requiere construir un intervalo de cinco números consecutivos donde ninguno sea un número primo, se hace \n  \n    \n      \n        n\n        =\n        5\n      \n    \n    {\\displaystyle n=5}\n  \n. Estos valores corresponden a:\n\n  \n    \n      \n        6\n        !\n        +\n        2\n        =\n        722\n        =\n        2\n        ⋅\n        361\n      \n    \n    {\\displaystyle 6!+2=722=2\\cdot 361}\n  \n\n  \n    \n      \n        6\n        !\n        +\n        3\n        =\n        723\n        =\n        3\n        ⋅\n        241\n      \n    \n    {\\displaystyle 6!+3=723=3\\cdot 241}\n  \n\n  \n    \n      \n        6\n        !\n        +\n        4\n        =\n        724\n        =\n        4\n        ⋅\n        181\n      \n    \n    {\\displaystyle 6!+4=724=4\\cdot 181}\n  \n\n  \n    \n      \n        6\n        !\n        +\n        5\n        =\n        725\n        =\n        5\n        ⋅\n        145\n      \n    \n    {\\displaystyle 6!+5=725=5\\cdot 145}\n  \n\n  \n    \n      \n        6\n        !\n        +\n        6\n        =\n        726\n        =\n        6\n        ⋅\n        121\n      \n    \n    {\\displaystyle 6!+6=726=6\\cdot 121}\n  \n\nEl siguiente valor, 6!+7=727, es primo.[29]​ De todas formas, el menor número primo que dista del siguiente en n es generalmente mucho menor que el factorial, por ejemplo, el caso más pequeño de dos primos consecutivos separados de ocho unidades es (89, 97), mientras que 8! es igual a 40.320.\nLa sucesión de las diferencias entre primos consecutivos[30]​ ha sido profusamente estudiada en matemáticas, y alrededor de este concepto se han establecido muchas conjeturas que permanecen sin resolver."
      },
      {
        "heading": "Conclusión",
        "level": 2,
        "content": "El modelado de la distribución de los números primos es un tema de investigación recurrente entre los teóricos de números. La primalidad de un número concreto es (hasta ahora) impredecible a pesar de que existen leyes, como el teorema de los números primos y el postulado de Bertrand, que gobiernan su distribución a gran escala. Leonhard Euler comentó:\n\n \n\nEn una conferencia de 1975, el matemático germano-estadounidense Don Zagier comentó:"
      },
      {
        "heading": "Encontrar números primos",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Tests de primalidad",
        "level": 2,
        "content": "La criba de Eratóstenes es una manera sencilla de hallar todos los números primos menores o iguales que un número dado. Se basa en confeccionar una lista de todos los números naturales desde el 2 hasta ese número y tachar repetidamente los múltiplos de los números primos ya descubiertos. La criba de Atkin, más moderna, tiene una mayor complejidad, pero si se optimiza apropiadamente también es más rápida. También existe una reciente criba de Sundaram que genera únicamente números compuestos, siendo los primos los números faltantes.\nEn la práctica, lo que se desea es determinar si un número dado es primo sin tener que confeccionar una lista de números primos. Un método para determinar la primalidad de un número es la división por tentativa, que consiste en dividir sucesivamente ese número entre los números primos menores o iguales a su raíz cuadrada. Si alguna de las divisiones es exacta, entonces el número no es primo; en caso contrario, es primo. Por ejemplo, dado n menor o igual que 120, para determinar su primalidad basta comprobar si es divisible entre 2, 3, 5 y 7, ya que el siguiente número primo, 11, ya es mayor que √120. Es el test de primalidad más sencillo, y rápidamente pierde su utilidad a la hora de comprobar la primalidad de números grandes, ya que el número de factores posibles crece demasiado rápido a medida que crece el número potencialmente primo.\nEn efecto, el número de números primos menores que n es aproximadamente\n\n  \n    \n      \n        \n          \n            n\n            \n              ln\n              ⁡\n              n\n              −\n              1\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {n}{\\ln n-1}}}\n  \n.\nDe esta forma, para determinar la primalidad de n, el mayor factor primo que se necesita no es mayor que √n, dejando el número de candidatos a factor primo en cerca de\n\n  \n    \n      \n        \n          \n            \n              n\n            \n            \n              ln\n              ⁡\n              \n                \n                  n\n                \n              \n              −\n              1\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\sqrt {n}}{\\ln {\\sqrt {n}}-1}}}\n  \n.\nEsta expresión crece cada vez más lentamente en función de n, pero, como los n grandes son de interés, el número de candidatos también se hace grande: por ejemplo, para n = 1020 se tienen 450 millones de candidatos.\nAsimismo, existen otros muchos tests de primalidad deterministas que se basan en propiedades que caracterizan a los números primos, pero su utilidad computacional depende mucho del test usado. Por ejemplo, se podría emplear el teorema de Wilson para calcular la primalidad de un número, pero tiene el inconveniente de requerir el cálculo de un factorial, una operación computacionalmente prohibitiva cuando se manejan números grandes. Aquí entra en juego el tiempo de ejecución del algoritmo empleado, que se expresa en la notación de Landau. Para poder determinar la primalidad de números cada vez más grandes (de miles de cifras) se buscan aquellos algoritmos cuyo tiempo de ejecución crezca lo más lentamente posible, a ser posible, que se pueda expresar como un polinomio. Si bien el test de primalidad AKS cumple con esta condición, para el rango de números que se usa en la práctica este algoritmo es extremadamente lento.\nPor otra parte, a menudo basta con tener una respuesta más rápida con una alta probabilidad (aunque no segura) de ser cierta. Se puede comprobar rápidamente la primalidad de un número relativamente grande mediante tests de primalidad probabilísticos. Estos tests suelen tomar un número aleatorio llamado \"testigo\" e introducirlo en una fórmula junto con el número potencialmente primo n. Después de varias iteraciones, se resuelve que n es \"definitivamente compuesto\" o bien \"probablemente primo\". Estos últimos números pueden ser primos o bien pseudoprimos (números compuestos que pasan el test de primalidad). Algunos de estos tests no son perfectos: puede haber números compuestos que el test considere \"probablemente primos\" independientemente del testigo utilizado. Esos números reciben el nombre de pseudoprimos absolutos para ese test. Por ejemplo, los números de Carmichael son números compuestos, pero el test de Fermat los evalúa como probablemente primos. Sin embargo, los tests probabilísticos más utilizados, como el test de Miller-Rabin o el obsoleto test de Solovay-Strassen, superado por el anterior, no tienen este inconveniente, aun siendo igualmente tests probabilísticos.\nAlgunos tests probabilísticos podrían pasar a ser determinísticos y algunos tests pueden mejorar su tiempo de ejecución si se verifican algunas hipótesis matemáticas. Por ejemplo, si se verifica la hipótesis generalizada de Riemann, se puede emplear una versión determinística del test de Miller-Rabin, y el test de primalidad por curvas elípticas podría mejorar notablemente su tiempo de ejecución si se verificaran algunas hipótesis de teoría analítica de números."
      },
      {
        "heading": "Algoritmos de factorización",
        "level": 2,
        "content": "Un algoritmo de factorización es un algoritmo que separa uno a uno los factores primos de un número. Los algoritmos de factorización pueden funcionar también a modo de tests de primalidad, pero en general tienen un tiempo de ejecución menos ventajoso. Por ejemplo, se puede modificar el algoritmo de división por tentativa de forma que no se detenga cuando se obtenga una división exacta, sino que siga realizando nuevas divisiones, y no sobre el número original, sino sobre el cociente obtenido. Después de la división por tentativa, los métodos más antiguos que se conocen son el método de Fermat, que se basa en las diferencias entre cuadrados y que es especialmente eficaz cuando n es el producto de dos números primos próximos entre sí, y el método de Euler, que se basa en la representación de n como suma de dos cuadrados de dos formas distintas.\nMás recientemente, se han elaborado algoritmos basados en una gran variedad de técnicas, como las fracciones continuas o las curvas elípticas, aunque algunos son mejoras de métodos anteriores (la criba cuadrática, por ejemplo, se basa en una mejora del método de Fermat y posee complejidad computacional subexponencial sobre el número de cifras de n). Otros, como el método rho de Pollard, son probabilísticos, y no garantizan hallar los divisores de un número compuesto.\nHoy por hoy, el algoritmo determinístico más rápido de uso general es la criba general del cuerpo de números (GNFS por las siglas de su nombre en inglés: General number field sieve), que también posee complejidad computacional subexponencial sobre el número de cifras de n.[33]​ Se ha propuesto un algoritmo cuyo tiempo de ejecución es polinómico sobre el número de cifras de n (el algoritmo de Shor), pero requiere ser ejecutado en un ordenador cuántico, ya que su simulación en un ordenador normal requiere un tiempo exponencial. No se conocen algoritmos para factorizar en una computadora tradicional en tiempo polinómico y tampoco se demostró que esto sea imposible."
      },
      {
        "heading": "Fórmulas que solo generasen números primos",
        "level": 2,
        "content": "A lo largo de la historia, se han buscado numerosas fórmulas para generar los números primos. El nivel más alto de exigencia para una fórmula así sería que asociara a cada número natural n el n-ésimo número primo. De forma más indulgente, se puede pedir una función f inyectiva que asocie a cada número natural n un número primo de tal forma que cada uno de los valores tomados aparezca solo una vez.\nAdemás, se exige que la función se pueda aplicar, efectiva y eficazmente, en la práctica.[34]​ Por ejemplo, el teorema de Wilson asegura que p es un número primo si y solo si (p-1)!≡-1 (mod p). Otro ejemplo: la función f(n) = 2 + ( 2(n!) mod (n+1)) genera todos los números primos, solo los números primos, y solo el valor 2 se toma más de una vez. Sin embargo, ambas fórmulas se basan en el cálculo de un factorial, lo que las hace computacionalmente inviables.\nEn la búsqueda de estas funciones, se han investigado, notablemente, las funciones polinómicas. Cabe subrayar que ningún polinomio, aun en varias variables, devuelve solo valores primos.[35]​ Por ejemplo, el polinomio en una variable f(n) = n² + n + 41, estudiada por Leonardo Euler, devuelve valores primos para n = 0, …, 39, sin embargo para n= 40, resulta \n  \n    \n      \n        \n          40\n          \n            2\n          \n        \n        +\n        40\n        +\n        41\n        =\n        \n          41\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle 40^{2}+40+41=41^{2}}\n  \n un número compuesto.[36]​ Si el término constante vale cero, entonces el polinomio es múltiplo de n, por lo que el polinomio es compuesto para valores compuestos de n. En caso contrario, si c es el término constante, entonces f(cn) es múltiplo de c, por lo que si el polinomio no es constante, necesariamente deberá incluir valores compuestos.\nSin embargo, hay polinomios en varias variables cuyos valores positivos (cuando las variables recorren números naturales) son precisamente números primos. Un ejemplo, es este polinomio descubierto por Jones, Sato, Wada y Wiens en 1976:[35]​\n\n  \n    \n      \n        (\n        1\n        −\n        (\n        w\n        ⋅\n        z\n        +\n        h\n        +\n        j\n        −\n        q\n        \n          )\n          \n            2\n          \n        \n        −\n        (\n        2\n        ⋅\n        n\n        +\n        p\n        +\n        q\n        +\n        z\n        −\n        e\n        \n          )\n          \n            2\n          \n        \n        −\n        (\n        \n          a\n          \n            2\n          \n        \n        ⋅\n        \n          y\n          \n            2\n          \n        \n        −\n        \n          y\n          \n            2\n          \n        \n        +\n        1\n        −\n        \n          x\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        …\n      \n    \n    {\\displaystyle (1-(w\\cdot z+h+j-q)^{2}-(2\\cdot n+p+q+z-e)^{2}-(a^{2}\\cdot y^{2}-y^{2}+1-x^{2})^{2}\\dots }\n  \n\n  \n    \n      \n        −\n        (\n        \n          e\n          \n            3\n          \n        \n        ⋅\n        (\n        e\n        +\n        2\n        )\n        ⋅\n        (\n        a\n        +\n        1\n        \n          )\n          \n            2\n          \n        \n        +\n        1\n        −\n        \n          o\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        −\n        (\n        16\n        ⋅\n        (\n        k\n        +\n        1\n        \n          )\n          \n            3\n          \n        \n        ⋅\n        (\n        k\n        +\n        2\n        )\n        ⋅\n        (\n        n\n        +\n        1\n        \n          )\n          \n            2\n          \n        \n        +\n        1\n        −\n        \n          f\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        …\n      \n    \n    {\\displaystyle -(e^{3}\\cdot (e+2)\\cdot (a+1)^{2}+1-o^{2})^{2}-(16\\cdot (k+1)^{3}\\cdot (k+2)\\cdot (n+1)^{2}+1-f^{2})^{2}\\dots }\n  \n\n  \n    \n      \n        −\n        (\n        (\n        (\n        a\n        +\n        \n          u\n          \n            2\n          \n        \n        ⋅\n        (\n        \n          u\n          \n            2\n          \n        \n        −\n        a\n        )\n        \n          )\n          \n            2\n          \n        \n        −\n        1\n        )\n        ⋅\n        (\n        n\n        +\n        4\n        ⋅\n        d\n        ⋅\n        y\n        \n          )\n          \n            2\n          \n        \n        +\n        1\n        −\n        (\n        x\n        +\n        c\n        ⋅\n        u\n        \n          )\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        −\n        (\n        a\n        ⋅\n        i\n        +\n        k\n        +\n        1\n        −\n        l\n        −\n        i\n        \n          )\n          \n            2\n          \n        \n        …\n      \n    \n    {\\displaystyle -(((a+u^{2}\\cdot (u^{2}-a))^{2}-1)\\cdot (n+4\\cdot d\\cdot y)^{2}+1-(x+c\\cdot u)^{2})^{2}-(a\\cdot i+k+1-l-i)^{2}\\dots }\n  \n\n  \n    \n      \n        −\n        (\n        (\n        g\n        ⋅\n        k\n        +\n        2\n        ⋅\n        g\n        +\n        k\n        +\n        1\n        )\n        ⋅\n        (\n        h\n        +\n        j\n        )\n        +\n        h\n        −\n        z\n        \n          )\n          \n            2\n          \n        \n        −\n        (\n        16\n        ⋅\n        \n          r\n          \n            2\n          \n        \n        ⋅\n        \n          y\n          \n            4\n          \n        \n        ⋅\n        (\n        \n          a\n          \n            2\n          \n        \n        −\n        1\n        )\n        +\n        1\n        −\n        \n          u\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        …\n      \n    \n    {\\displaystyle -((g\\cdot k+2\\cdot g+k+1)\\cdot (h+j)+h-z)^{2}-(16\\cdot r^{2}\\cdot y^{4}\\cdot (a^{2}-1)+1-u^{2})^{2}\\dots }\n  \n\n  \n    \n      \n        −\n        (\n        p\n        −\n        m\n        +\n        l\n        ⋅\n        (\n        a\n        −\n        n\n        −\n        1\n        )\n        +\n        b\n        ⋅\n        (\n        2\n        ⋅\n        a\n        ⋅\n        n\n        +\n        2\n        ⋅\n        a\n        −\n        \n          n\n          \n            2\n          \n        \n        −\n        2\n        ⋅\n        n\n        −\n        2\n        )\n        \n          )\n          \n            2\n          \n        \n        −\n        (\n        z\n        −\n        p\n        ⋅\n        m\n        +\n        p\n        ⋅\n        l\n        ⋅\n        a\n        −\n        \n          p\n          \n            2\n          \n        \n        l\n        +\n        t\n        ⋅\n        (\n        2\n        ⋅\n        a\n        ⋅\n        p\n        −\n        \n          p\n          \n            2\n          \n        \n        −\n        1\n        )\n        \n          )\n          \n            2\n          \n        \n        …\n      \n    \n    {\\displaystyle -(p-m+l\\cdot (a-n-1)+b\\cdot (2\\cdot a\\cdot n+2\\cdot a-n^{2}-2\\cdot n-2))^{2}-(z-p\\cdot m+p\\cdot l\\cdot a-p^{2}l+t\\cdot (2\\cdot a\\cdot p-p^{2}-1))^{2}\\dots }\n  \n\n  \n    \n      \n        −\n        (\n        q\n        −\n        x\n        +\n        y\n        ⋅\n        (\n        a\n        −\n        p\n        −\n        1\n        )\n        +\n        s\n        ⋅\n        (\n        2\n        ⋅\n        a\n        ⋅\n        p\n        +\n        2\n        ⋅\n        a\n        −\n        \n          p\n          \n            2\n          \n        \n        −\n        2\n        ⋅\n        p\n        −\n        2\n        )\n        \n          )\n          \n            2\n          \n        \n        −\n        (\n        \n          a\n          \n            2\n          \n        \n        ⋅\n        \n          l\n          \n            2\n          \n        \n        −\n        \n          l\n          \n            2\n          \n        \n        +\n        1\n        −\n        \n          m\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        −\n        (\n        n\n        +\n        l\n        +\n        v\n        −\n        y\n        \n          )\n          \n            2\n          \n        \n        )\n        ⋅\n        (\n        k\n        +\n        2\n        )\n      \n    \n    {\\displaystyle -(q-x+y\\cdot (a-p-1)+s\\cdot (2\\cdot a\\cdot p+2\\cdot a-p^{2}-2\\cdot p-2))^{2}-(a^{2}\\cdot l^{2}-l^{2}+1-m^{2})^{2}-(n+l+v-y)^{2})\\cdot (k+2)}\n  \n\nAl igual que ocurre con las fórmulas con factoriales, este polinomio no es práctico de calcular, ya que, aunque los valores positivos que toma son todos primos, prácticamente no devuelve otra cosa que valores negativos cuando se hacen variar las variables a a z de 0 a infinito.\nOtro enfoque al problema de encontrar una función que solo genere números primos viene dado a partir del teorema de Mills, que indica que existe una constante θ tal que\n\n  \n    \n      \n        ⌊\n        \n          θ\n          \n            \n              3\n              \n                n\n              \n            \n          \n        \n        ⌋\n      \n    \n    {\\displaystyle \\lfloor \\theta ^{3^{n}}\\rfloor }\n  \n\nes siempre un número primo, donde \n  \n    \n      \n        ⌊\n        ⌋\n      \n    \n    {\\displaystyle \\lfloor \\rfloor }\n  \n es la función piso.[37]​ Todavía no se conoce ninguna fórmula para calcular la constante de Mills, y las aproximaciones que se emplean en la actualidad se basa en la sucesión de los así llamados números primos de Mills (los números primos generados mediante esta fórmula), que no pueden ser obtenidos rigurosamente, sino solo de manera probabilística, suponiendo cierta la hipótesis de Riemann."
      },
      {
        "heading": "Clases de números primos",
        "level": 1,
        "content": "De mayor interés son otras fórmulas que, aunque no solo generen números primos, son más rápidas de implementar, sobre todo si existe un algoritmo especializado que permita calcular rápidamente la primalidad de los valores que van tomando. A partir de estas fórmulas se obtienen subconjuntos relativamente pequeños del conjunto de los números primos, que suelen recibir un nombre colectivo."
      },
      {
        "heading": "Primos primoriales y primos factoriales",
        "level": 2,
        "content": "Los números primos primoriales, directamente relacionados con la demostración euclidiana de la infinitud de los números primos, son los de la forma p = n# ± 1 para algún número natural n, donde n# es igual al producto 2 · 3 · 5 · 7 · 11 · … de todos los primos ≤ n. Asimismo, un número primo se dice primo factorial si es de la forma n! ± 1. Los primeros primos factoriales son:\n\nn! − 1 es primo para n = 3, 4, 6, 7, 12, 14, 30, 32, 33, 38, 94, 166, 324, …[38]​\nn! + 1 es primo para n = 0, 1, 2, 3, 11, 27, 37, 41, 73, 77, 116, 154, 320, …[39]​"
      },
      {
        "heading": "Números primos de Fermat",
        "level": 2,
        "content": "Los números de Fermat, ligados a la construcción de polígonos regulares con regla y compás, son los números de la forma \n  \n    \n      \n        \n          F\n          \n            n\n          \n        \n        =\n        \n          2\n          \n            \n              2\n              \n                n\n              \n            \n          \n        \n        +\n        1\n      \n    \n    {\\displaystyle F_{n}=2^{2^{n}}+1}\n  \n, con n natural. Los únicos números primos de Fermat que se conocen hasta la fecha son los cinco que ya conocía el propio Fermat, correspondientes a n = 0, 1, 2, 3 y 4, mientras que para valores de n entre 5 y 32 estos números son compuestos.[40]​\nPara determinar su primalidad, existe un test especializado cuyo tiempo de ejecución es polinómico: el test de Pépin. Sin embargo, los propios números de Fermat crecen tan rápidamente que solo se lo ha podido aplicar para valores de n pequeños. En 1999 se lo aplicó para n = 24. Para determinar el carácter de otros números de Fermat mayores se utiliza el método de divisiones sucesivas y de esa manera a fecha de junio de 2009 se conocen 241 números de Fermat compuestos, aunque en la mayoría de los casos se desconozca su factorización completa.[40]​"
      },
      {
        "heading": "Números primos de Mersenne",
        "level": 2,
        "content": "Los números de Mersenne son los de forma Mp = 2p – 1, donde p es primo.[41]​ Los mayores números primos conocidos son generalmente de esta forma, ya que existe un test de primalidad muy eficaz, el test de Lucas-Lehmer, para determinar si un número de Mersenne es primo o no.\nActualmente, el mayor número primo que se conoce es M136.279.841 = 2136.279.841 - 1, que tiene 41 024 320 cifras en el sistema decimal. Se trata cronológicamente del 52.o número primo de Mersenne conocido y su descubrimiento se anunció el 21 de octubre de 2024[42]​ gracias al proyecto de computación distribuida «Great Internet Mersenne Prime Search» (GIMPS)."
      },
      {
        "heading": "Otras clases de números primos",
        "level": 2,
        "content": "Existen literalmente decenas de apellidos que se pueden añadir al concepto de número primo para referirse a un subconjunto que cumple alguna propiedad concreta. Por ejemplo, los números primos pitagóricos son los que se pueden expresar en la forma 4n+1. Dicho de otra forma, se trata de los números primos cuyo resto al dividirlos entre 4 es 1. Otro ejemplo es el de los números primos de Wieferich, que son aquellos números primos p tales que p2 divide a 2p-1 - 1.\nAlgunas de estas propiedades se refieren a una relación concreta con otro número primo:\n\nNúmeros primos gemelos: p y p+2 lo son si son los dos primos.\nNúmero primo de Sophie Germain: dado p primo, es de Sophie Germain si 2p + 1 también es primo. Una sucesión de números p1,p2,p3,··· ,pn todos ellos primos, tales que pi+1=2pi+1 para todo i ∈ {1,2,···,n-1 }, se denomina cadena (completa) de Cunningham de primera especie, y cumple por definición que cada uno de los términos, salvo el último, es un número primo de Sophie Germain. Se cree que para todo n natural existen infinitas cadenas de Cunningham de longitud n,[43]​ aunque hasta la fecha nadie ha proporcionado prueba de que dicha afirmación sea cierta.\nNúmero primo de Wagstaff: p lo es si \n  \n    \n      \n        \n          p\n          =\n          \n            \n              \n                \n                  2\n                  \n                    q\n                  \n                \n                +\n                1\n              \n              3\n            \n          \n        \n      \n    \n    {\\displaystyle \\textstyle p={{2^{q}+1} \\over 3}}\n  \n, donde q es otro número primo.[44]​[45]​\nTambién se les da nombres especiales a algunas clases de primos que dependen de la base de numeración empleada o de la forma de escribir los dígitos, y no de una fórmula matemática. Es el caso de los números somirp (primos al revés), que son aquellos números primos tales que el número obtenido al invertir el orden de sus cifras también es primo. También es el caso de los números primos repunit, que son aquellos números primos que son concatenación de unos. Si, en lugar de considerarse el sistema de numeración decimal se considera el binario, se obtiene otro conjunto distinto de números primos repunit que, además, coincide con el de los números primos de Mersenne. Finalmente, los números primos triádicos son aquellos números que son primos, capicúas y simétricos respecto de una recta horizontal.\nEl que se le dé un nombre a una clase de números primos con una definición precisa no significa que se conozca algún número primo que sea de esa clase. Por ejemplo, no se conoce hasta el momento ningún número primo de Wall-Sun-Sun, pero su relevancia radica en que en 1992, antes de la demostración de Wiles del último teorema de Fermat, se descubrió que la falsedad del teorema para un número primo p dado implicaba que p era un número primo de Wall-Sun-Sun. Esto hizo que, durante un tiempo, la búsqueda de números primos de esta clase fuera también la búsqueda de un contraejemplo del último teorema de Fermat.[46]​"
      },
      {
        "heading": "Cuadro resumen",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Conjeturas",
        "level": 1,
        "content": "Existen numerosas preguntas abiertas acerca de los números primos. Muchas de ellas son problemas bien antiguos, y una de las más significativas es la hipótesis de Riemann, varias veces mencionada en este artículo como una conjetura que, de ser cierta, permitiría conocer numerosos resultados relevantes en diversos campos de las matemáticas."
      },
      {
        "heading": "Hipótesis de Riemann",
        "level": 2,
        "content": "Para entender la hipótesis de Riemann, una conjetura enunciada en 1859 pero que, hasta la fecha (2025), sigue sin resolverse, es necesario entender la función zeta de Riemann. Sea \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n un número complejo con parte real mayor que 1. Entonces,\n\n  \n    \n      \n        ζ\n        (\n        s\n        )\n        =\n        \n          ∑\n          \n            n\n            =\n            1\n          \n          \n            ∞\n          \n        \n        \n          \n            1\n            \n              n\n              \n                s\n              \n            \n          \n        \n        =\n        \n          ∏\n          \n            p\n          \n        \n        \n          \n            1\n            \n              1\n              −\n              \n                p\n                \n                  −\n                  s\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\zeta (s)=\\sum _{n=1}^{\\infty }{\\frac {1}{n^{s}}}=\\prod _{p}{\\frac {1}{1-p^{-s}}}.}\n  \n\nLa segunda igualdad es una consecuencia del teorema fundamental de la aritmética, y muestra que la función zeta está íntimamente relacionada con los números primos.\nExisten dos tipos de ceros de la función zeta, es decir, valores s para los cuales ζ(s) = 0: los triviales, que son s=-2, s=-4, s=-6, etc., (los enteros pares negativos) y los no triviales, que son aquellos ceros que no se encuentran en el eje real. Lo que indica la hipótesis de Riemann es que la parte real de todos los ceros no triviales es igual a 1/2.\nLa veracidad de la hipótesis implica una profunda conexión con los números primos, en esencia, en el caso de verificarse, dice que los números primos están distribuidos de la forma más regular posible. Desde un punto de vista «físico», dice grosso modo que las irregularidades en la distribución de los números primos solo proceden de ruido aleatorio. Desde un punto de vista matemático, dice que la distribución asintótica de los números primos (según el teorema de los números primos, la proporción de primos menores que n es \n  \n    \n      \n        \n          \n            \n              1\n              \n                ln\n                ⁡\n                (\n                n\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {1}{\\ln(n)}}}\n  \n) también es cierta para intervalos mucho menores, con un error de aproximadamente la raíz cuadrada de n (para intervalos próximos a n). Está ampliamente extendido en la comunidad matemática que la hipótesis sea cierta. En concreto, la presunción más simple es que los números primos no deberían tener irregularidades significativas en su distribución sin una buena razón.[47]​"
      },
      {
        "heading": "Otras conjeturas",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Infinitud de ciertos tipos de números primos",
        "level": 3,
        "content": "Muchas conjeturas tratan sobre si hay infinitos números primos de una determinada forma. Así, se conjetura que hay infinitos números primos de Fibonacci[48]​ e infinitos primos de Mersenne, pero solo un número finito de primos de Fermat.[49]​ No se sabe si hay infinitos números primos de Euclides."
      },
      {
        "heading": "Distribución de los números primos",
        "level": 3,
        "content": "También hay numerosas conjeturas que se ocupan de determinadas propiedades de la distribución de los números primos. Así, la conjetura de los números primos gemelos enuncia que hay infinitos números primos gemelos, que son pares de primos cuya diferencia es de 2. La conjetura de Polignac es una versión más general y más fuerte de la anterior, ya que enuncia que, para cada entero positivo n, hay infinitos pares de primos consecutivos que difieren en 2n. A su vez, una versión más débil de la conjetura de Polignac dice que todo número par es la diferencia de dos números primos.\nAsimismo, se conjetura la infinidad de los primos de la forma n2 + 1. Según la conjetura de Brocard, entre los cuadrados de primos consecutivos mayores que 2 existen siempre al menos cuatro números primos. La conjetura de Legendre establece que, para cada n natural, existe un número primo entre n2 y (n+1)2. Finalmente, la conjetura de Cramér, cuya veracidad implicaría la de Legendre, dice que:\n\n  \n    \n      \n        \n          lim sup\n          \n            n\n            →\n            ∞\n          \n        \n        \n          \n            \n              \n                p\n                \n                  n\n                  +\n                  1\n                \n              \n              −\n              \n                p\n                \n                  n\n                \n              \n            \n            \n              (\n              log\n              ⁡\n              \n                p\n                \n                  n\n                \n              \n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle \\limsup _{n\\rightarrow \\infty }{\\frac {p_{n+1}-p_{n}}{(\\log p_{n})^{2}}}=1}"
      },
      {
        "heading": "Teoría aditiva de números",
        "level": 3,
        "content": "Otras conjeturas relacionan algunas propiedades aditivas de los números con los números primos. Así, la conjetura de Goldbach dice que todo número par mayor que 2 se puede escribir como suma de dos números primos, aunque también existe una versión más débil de la misma conjetura según la cual todo número impar mayor que 5 se puede escribir como suma de tres números primos. El matemático chino Chen Jingrun demostró, en 1966, que en efecto, todo número par suficientemente grande puede expresarse como suma de dos primos o como la suma de un primo y de un número que es el producto de dos primos. (\"semi-primo\").[50]​"
      },
      {
        "heading": "Los cuatro problemas de Landau",
        "level": 2,
        "content": "En 1912, Landau estableció en el Quinto Congreso Internacional de Matemáticos de Cambridge una lista de cuatro de los problemas ya mencionados sobre números primos, que se conocen como los problemas de Landau. Ninguno de ellos está resuelto hasta la fecha. Se trata de la conjetura de Goldbach, la de los números primos gemelos, la de Legendre y la de los primos de la forma n2 + 1.[51]​"
      },
      {
        "heading": "Generalización del concepto de número primo",
        "level": 1,
        "content": "El concepto de número primo es tan importante que se ha visto generalizado de varias maneras en diversas ramas de las matemáticas."
      },
      {
        "heading": "Elementos primos en un anillo",
        "level": 2,
        "content": "Se pueden definir los elementos primos y los elementos irreducibles en cualquier dominio de integridad.[52]​\nEn cualquier dominio de factorización única, como por ejemplo, el anillo \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbb {Z} }\n  \n de los enteros, el conjunto de elementos primos equivale al conjunto de los elementos irreducibles, que en \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbb {Z} }\n  \n es {…, −11, −7, −5, −3, −2, 2, 3, 5, 7, 11, …}.\nConsidérense por ejemplo los enteros gaussianos \n  \n    \n      \n        \n          Z\n        \n        [\n        i\n        ]\n      \n    \n    {\\displaystyle \\mathbb {Z} [i]}\n  \n, es decir, los números complejos de la forma a+bi con a, b ∈ \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbb {Z} }\n  \n. Este es un dominio de integridad, y sus elementos primos son los primos gaussianos. Cabe destacar que el 2 no es un primo gaussiano, porque admite factorización como producto de los primos gaussianos (1+i) y (1-i). Sin embargo, el elemento 3 sí es primo en los enteros gaussianos, pero no lo es en otro dominio entero. En general, los primos racionales (es decir, los elementos primos del anillo \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbb {Z} }\n  \n) de la forma 4k+3 son primos gaussianos, pero no lo son aquellos de la forma 4k+1."
      },
      {
        "heading": "Ideales primos",
        "level": 2,
        "content": "En teoría de anillos, un ideal I es un subconjunto de un anillo A tal que\n\nsi i, j ∈ I, entonces la suma i + j pertenece a I\ny si x ∈ A, i ∈ I, entonces los productos a × i, i × a pertenecen a I.\nUn ideal primo se define entonces como un ideal que cumple también que:\n\npara cualquier par de elementos a, b del anillo A tales que su producto a × b pertenece a I, entonces, al menos uno de los dos elementos, a o b, está en I.\nI no es el anillo A entero.\nLos ideales primos son una herramienta relevante en álgebra conmutativa, teoría algebraica de números y geometría algebraica. Los ideales primos del anillo de enteros son los ideales (0), (2), (3), (5), (7), (11), …\nUn problema central en teoría algebraica de números es la manera en que se factorizan los ideales primos cuando se ven sometidos a una extensión de cuerpos. En el ejemplo de los enteros gaussianos, (2) se ramifica en potencia de un primo (ya que \n  \n    \n      \n        1\n        +\n        i\n      \n    \n    {\\displaystyle 1+i}\n  \n y \n  \n    \n      \n        1\n        −\n        i\n      \n    \n    {\\displaystyle 1-i}\n  \n generan el mismo ideal primo), los ideales primos de la forma \n  \n    \n      \n        4\n        k\n        +\n        3\n      \n    \n    {\\displaystyle 4k+3}\n  \n son inertes (mantienen su primalidad) y los de la forma \n  \n    \n      \n        4\n        k\n        +\n        1\n      \n    \n    {\\displaystyle 4k+1}\n  \n pasan a ser producto de dos ideales primos distintos."
      },
      {
        "heading": "Primos en teoría de la valoración",
        "level": 2,
        "content": "En teoría algebraica de números surge otra generalización más. Dado un cuerpo \n  \n    \n      \n        \n          K\n        \n      \n    \n    {\\displaystyle \\mathbb {K} }\n  \n, reciben el nombre de valoraciones sobre \n  \n    \n      \n        \n          K\n        \n      \n    \n    {\\displaystyle \\mathbb {K} }\n  \n determinadas funciones de \n  \n    \n      \n        \n          K\n        \n      \n    \n    {\\displaystyle \\mathbb {K} }\n  \n en \n  \n    \n      \n        \n          R\n        \n      \n    \n    {\\displaystyle \\mathbb {R} }\n  \n. Cada una de estas valoraciones genera una topología sobre \n  \n    \n      \n        \n          K\n        \n      \n    \n    {\\displaystyle \\mathbb {K} }\n  \n, y se dice que dos valoraciones son equivalentes si generan la misma topología. Un primo de \n  \n    \n      \n        \n          K\n        \n      \n    \n    {\\displaystyle \\mathbb {K} }\n  \n es una clase de equivalencia de valoraciones. Con esta definición, los primos del cuerpo \n  \n    \n      \n        \n          Q\n        \n      \n    \n    {\\displaystyle \\mathbb {Q} }\n  \n de los números racionales quedan representados por la función valor absoluto así como por las valoraciones p-ádicas sobre \n  \n    \n      \n        \n          Q\n        \n      \n    \n    {\\displaystyle \\mathbb {Q} }\n  \n para cada número primo p."
      },
      {
        "heading": "Nudos primos",
        "level": 2,
        "content": "En teoría de nudos, un nudo primo es un nudo no trivial que no se puede descomponer en dos nudos más pequeños. De forma más precisa, se trata de un nudo que no se puede escribir como suma conexa de dos nudos no triviales.\nEn 1949 Horst Schubert demostró un teorema de factorización análogo al teorema fundamental de la aritmética, que asegura que cada nudo se puede obtener de forma única como suma conexa de nudos primos.[53]​ Por este motivo, los nudos primos desempeñan un papel central en la teoría de nudos: una clasificación de los nudos ha sido desde finales del siglo xix el tema central de la teoría."
      },
      {
        "heading": "Aplicaciones en la matemática",
        "level": 1,
        "content": "En el estudio de los números complejos, se acude al concepto de \"primos relativos\" para definir raíces primitivas de la unidad .[54]​ Si n es un número primo todas las raíces enésimas de 1 son raíces primitivas, salvo la raíz 1.\nEn la definición de un cuerpo finito, se exige que el número de elementos de un anillo sea entero primo. En tal caso, eliminando el cero, cada elemento tiene inverso multiplicativo y se obtiene la estructura de un cuerpo.[55]​\nEn la definición de un polígono estrellado de n lados, para tomar los puntos de m en m, se exige que m sea menor que n/2 y primo con n.[56]​\nAl definir el representante canónico de un número racional, usando clases de equivalencia de pares ordenados de números enteros, necesariamente, el par ordenado definente tiene que involucrar dos enteros primos relativos. A fortiori, por lo menos uno de ellos, un primo absoluto.[57]​"
      },
      {
        "heading": "Aplicaciones en la computación",
        "level": 1,
        "content": "El algoritmo RSA se basa en la obtención de la clave pública mediante la multiplicación de dos números grandes (mayores que 10100) que sean primos. La seguridad de este algoritmo radica en que no se conocen maneras rápidas de factorizar un número grande en sus factores primos utilizando computadoras tradicionales."
      },
      {
        "heading": "Números primos en el arte y la literatura",
        "level": 1,
        "content": "Los números primos han influido en numerosos artistas y escritores. El compositor francés Olivier Messiaen se valió de ellos para crear música no métrica. En obras tales como La Nativité du Seigneur (1935) o Quatre Études de rythme (1949-50) emplea simultáneamente motivos cuya duración es un número primo para crear ritmos impredecibles. Según Messiaen, esta forma de componer fue «inspirada por los movimientos de la naturaleza, movimientos de duraciones libres y desiguales».[58]​\nEn la novela escrita en 1968 2001: Una Odisea Espacial, Arthur C. Clarke menciona que el monolito de origen extraterrestre tiene la proporción del cuadrado de los primeros tres números primos: 1,4,9.\nEn su novela de ciencia ficción Contact, posteriormente adaptada al cine, Carl Sagan sugiere que los números primos podrían ser empleados para comunicarse con inteligencias extraterrestres, una idea que había desarrollado de manera informal con el astrónomo estadounidense Frank Drake en 1975.[59]​\nEl curioso incidente del perro a medianoche, de Mark Haddon, que describe en primera persona la vida de un joven autista muy dotado en matemáticas y cálculo mental, utiliza únicamente los números primos para numerar los capítulos.\nEn la novela PopCo de Scarlett Thomas, la abuela de Alice Butler trabaja en la demostración de la hipótesis de Riemann. El libro ilustra una tabla de los mil primeros números primos.[60]​\nLa soledad de los números primos, novela escrita por Paolo Giordano, ganó el premio Strega en 2008.\nTambién son muchas las películas que reflejan la fascinación popular hacia los misterios de los números primos y la criptografía, por ejemplo, Cube, Sneakers, El amor tiene dos caras y Una mente maravillosa. Esta última se basa en la biografía del matemático y premio Nobel John Forbes Nash, escrita por Sylvia Nasar.[61]​\nEl escritor griego Apostolos Doxiadis, escribió El tío Petros y la conjetura de Goldbach, que narra cómo un ficticio matemático prodigio de principios del siglo xx se sumerge en el mundo de las matemáticas de una forma apasionante, tratando de resolver uno de los problemas más difíciles y aún no resueltos de la matemática, la conjetura de Goldbach, la cual reza: «Todo número par puede expresarse como la suma de dos números primos»."
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": "Portal:Matemática. Contenido relacionado con Matemática."
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": " Wikilibros alberga un libro o manual sobre  cálculo de números primos.\n«Criba de Eratóstenes para buscar los números primos aplicada en C/C++». Brainum Code. \nCalculador en línea de factores primos, por www.mathstools.com\nThe Prime Pages Archivado el 7 de junio de 2004 en Wayback Machine.\nSobre el artículo de Manindra Agrawal et al. PRIMES IS IN P, en donde afirman: \"We present a deterministic polynomial-time algorithm that determines whether an input number n is prime or composite\" mathmistakes\nAlgoritmos eficientes para calcular números primos, por Steve Litt\n¿Es este número primo?"
      }
    ],
    "summary": "En matemáticas, un número primo es un número natural mayor que 1 que tiene únicamente dos divisores positivos distintos: él mismo y el 1.[1]​ Por el contrario, los números compuestos son los números naturales que tienen algún divisor natural aparte de sí mismos y del 1, y, por lo tanto, pueden factorizarse. El número 1, por convenio, no se considera ni primo ni compuesto.\nLos 168 números primos menores que 1000 son: \n2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599, 601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683, 691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797, 809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991 y 997 (sucesión A000040 en OEIS). \nEl primer número primo a partir del número mil es el 1009, después de diez mil es el &&&&&&&&&&010007.&&&&&010 007, a partir de cien mil es el &&&&&&&&&0100003.&&&&&0100 003 e inmediatamente tras un millón es el &&&&&&&&01000003.&&&&&01 000 003.\nLa propiedad de ser número primo se denomina primalidad.\nEn la teoría algebraica de números, los números primos se denominan números racionales primos para distinguirlos de los números gaussianos primos.[2]​ La primalidad no depende del sistema de numeración, pero sí del anillo donde se estudia la primalidad. Dos es primo racional; sin embargo tiene factores como entero gaussiano: 2 = (1+i)*(1-i).\nEl estudio de los números primos es una parte importante de la teoría de números, rama de las matemáticas que trata las propiedades, básicamente aritméticas,[3]​ de los números enteros.\nLos números primos están presentes en algunas conjeturas centenarias tales como la hipótesis de Riemann y la conjetura de Goldbach, resuelta por Harald Helfgott en su forma débil.\nLa distribución de los números primos es un asunto reiterativo de investigación en la teoría de números: si se consideran números aisladamente, los primos parecieran estar distribuidos de modo probabilístico, pero la distribución «global» de los números primos se ajusta a leyes bien definidas."
  },
  {
    "title": "Linear algebra",
    "source": "https://en.wikipedia.org/wiki/Linear_algebra",
    "language": "en",
    "chunks": [
      {
        "heading": "History",
        "level": 1,
        "content": "The procedure (using counting rods) for solving simultaneous linear equations now called Gaussian elimination appears in the ancient Chinese mathematical text Chapter Eight: Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations.\nSystems of linear equations arose in Europe with the introduction in 1637 by René Descartes of coordinates in geometry. In fact, in this new geometry, now called Cartesian geometry, lines and planes are represented by linear equations, and computing their intersections amounts to solving systems of linear equations.\nThe first systematic methods for solving linear systems used determinants and were first considered by Leibniz in 1693. In 1750, Gabriel Cramer used them for giving explicit solutions of linear systems, now called Cramer's rule. Later, Gauss further described the method of elimination, which was initially listed as an advancement in geodesy.\nIn 1844 Hermann Grassmann published his \"Theory of Extension\" which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for womb.\nLinear algebra grew with ideas noted in the complex plane. For instance, two numbers w and z in \n  \n    \n      \n        \n          C\n        \n      \n    \n    {\\displaystyle \\mathbb {C} }\n  \n have a difference w – z, and the line segments wz and 0(w − z) are of the same length and direction. The segments are equipollent. The four-dimensional system \n  \n    \n      \n        \n          H\n        \n      \n    \n    {\\displaystyle \\mathbb {H} }\n  \n of quaternions was discovered by W.R. Hamilton in 1843. The term vector was introduced as v = xi + yj + zk representing a point in space. The quaternion difference p – q also produces a segment equipollent to pq. Other hypercomplex number systems also used the idea of a linear space with a basis.\nArthur Cayley introduced matrix multiplication and the inverse matrix in 1856, making possible the general linear group. The mechanism of group representation became available for describing complex and hypercomplex numbers. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\".\nBenjamin Peirce published his Linear Associative Algebra (1872), and his son Charles Sanders Peirce extended the work later.\nThe telegraph required an explanatory system, and the 1873 publication by James Clerk Maxwell of A Treatise on Electricity and Magnetism instituted a field theory of forces and required differential geometry for expression. Linear algebra is flat differential geometry and serves in tangent spaces to manifolds. Electromagnetic symmetries of spacetime are expressed by the Lorentz transformations, and much of the history of linear algebra is the history of Lorentz transformations.\nThe first modern and more precise definition of a vector space was introduced by Peano in 1888; by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century when many ideas and methods of previous centuries were generalized as abstract algebra. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modeling and simulations."
      },
      {
        "heading": "Vector spaces",
        "level": 1,
        "content": "Until the 19th century, linear algebra was introduced through systems of linear equations and matrices. In modern mathematics, the presentation through vector spaces is generally preferred, since it is more synthetic, more general (not limited to the finite-dimensional case), and conceptually simpler, although more abstract.\nA vector space over a field F (often the field of the real numbers) is a set V equipped with two binary operations. Elements of V are called vectors, and elements of F are called scalars. The first operation, vector addition, takes any two vectors v and w and outputs a third vector v + w. The second operation, scalar multiplication, takes any scalar a and any vector v and outputs a new vector av. The axioms that addition and scalar multiplication must satisfy are the following. (In the list below, u, v and w are arbitrary elements of V, and a and b are arbitrary scalars in the field F.)\n\nThe first four axioms mean that V is an abelian group under addition.\nAn element of a specific vector space may have various natures; for example, it could be a sequence, a function, a polynomial, or a matrix. Linear algebra is concerned with the properties of such objects that are common to all vector spaces."
      },
      {
        "heading": "Linear maps",
        "level": 2,
        "content": "Linear maps are mappings between vector spaces that preserve the vector-space structure. Given two vector spaces V and W over a field F, a linear map (also called, in some contexts, linear transformation or linear mapping) is a map\n\n  \n    \n      \n        T\n        :\n        V\n        →\n        W\n      \n    \n    {\\displaystyle T:V\\to W}\n  \n\nThat is compatible with addition and scalar multiplication, that is\n\n  \n    \n      \n        T\n        (\n        \n          u\n        \n        +\n        \n          v\n        \n        )\n        =\n        T\n        (\n        \n          u\n        \n        )\n        +\n        T\n        (\n        \n          v\n        \n        )\n        ,\n        \n        T\n        (\n        a\n        \n          v\n        \n        )\n        =\n        a\n        T\n        (\n        \n          v\n        \n        )\n      \n    \n    {\\displaystyle T(\\mathbf {u} +\\mathbf {v} )=T(\\mathbf {u} )+T(\\mathbf {v} ),\\quad T(a\\mathbf {v} )=aT(\\mathbf {v} )}\n  \n\nfor any vectors u,v in V and scalar a in F.\nThis implies that for any vectors u, v in V and scalars a, b in F, one has\n\n  \n    \n      \n        T\n        (\n        a\n        \n          u\n        \n        +\n        b\n        \n          v\n        \n        )\n        =\n        T\n        (\n        a\n        \n          u\n        \n        )\n        +\n        T\n        (\n        b\n        \n          v\n        \n        )\n        =\n        a\n        T\n        (\n        \n          u\n        \n        )\n        +\n        b\n        T\n        (\n        \n          v\n        \n        )\n      \n    \n    {\\displaystyle T(a\\mathbf {u} +b\\mathbf {v} )=T(a\\mathbf {u} )+T(b\\mathbf {v} )=aT(\\mathbf {u} )+bT(\\mathbf {v} )}\n  \n\nWhen V = W are the same vector space, a linear map T : V → V is also known as a linear operator on V.\nA bijective linear map between two vector spaces (that is, every vector from the second space is associated with exactly one in the first) is an isomorphism. Because an isomorphism preserves linear structure, two isomorphic vector spaces are \"essentially the same\" from the linear algebra point of view, in the sense that they cannot be distinguished by using vector space properties. An essential question in linear algebra is testing whether a linear map is an isomorphism or not, and, if it is not an isomorphism, finding its range (or image) and the set of elements that are mapped to the zero vector, called the kernel of the map. All these questions can be solved by using Gaussian elimination or some variant of this algorithm."
      },
      {
        "heading": "Subspaces, span, and basis",
        "level": 2,
        "content": "The study of those subsets of vector spaces that are in themselves vector spaces under the induced operations is fundamental, similarly as for many mathematical structures. These subsets are called linear subspaces. More precisely, a linear subspace of a vector space V over a field F is a subset W of V such that u + v and au are in W, for every u, v in W, and every a in F. (These conditions suffice for implying that W is a vector space.)\nFor example, given a linear map T : V → W, the image T(V) of V, and the inverse image T−1(0) of 0 (called kernel or null space), are linear subspaces of W and V, respectively.\nAnother important way of forming a subspace is to consider linear combinations of a set S of vectors: the set of all sums \n\n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          \n            v\n          \n          \n            1\n          \n        \n        +\n        \n          a\n          \n            2\n          \n        \n        \n          \n            v\n          \n          \n            2\n          \n        \n        +\n        ⋯\n        +\n        \n          a\n          \n            k\n          \n        \n        \n          \n            v\n          \n          \n            k\n          \n        \n        ,\n      \n    \n    {\\displaystyle a_{1}\\mathbf {v} _{1}+a_{2}\\mathbf {v} _{2}+\\cdots +a_{k}\\mathbf {v} _{k},}\n  \n\nwhere v1, v2, ..., vk are in S, and a1, a2, ..., ak are in F form a linear subspace called the span of S. The span of S is also the intersection of all linear subspaces containing S. In other words, it is the smallest (for the inclusion relation) linear subspace containing S.\nA set of vectors is linearly independent if none is in the span of the others. Equivalently, a set S of vectors is linearly independent if the only way to express the zero vector as a linear combination of elements of S is to take zero for every coefficient ai.\nA set of vectors that spans a vector space is called a spanning set or generating set. If a spanning set S is linearly dependent (that is not linearly independent), then some element w of S is in the span of the other elements of S, and the span would remain the same if one were to remove w from S. One may continue to remove elements of S until getting a linearly independent spanning set. Such a linearly independent set that spans a vector space V is called a basis of V. The importance of bases lies in the fact that they are simultaneously minimal-generating sets and maximal independent sets. More precisely, if S is a linearly independent set, and T is a spanning set such that S ⊆ T, then there is a basis B such that S ⊆ B ⊆ T.\nAny two bases of a vector space V have the same cardinality, which is called the dimension of V; this is the dimension theorem for vector spaces. Moreover, two vector spaces over the same field F are isomorphic if and only if they have the same dimension.\nIf any basis of V (and therefore every basis) has a finite number of elements, V is a finite-dimensional vector space. If U is a subspace of V, then dim U ≤ dim V. In the case where V is finite-dimensional, the equality of the dimensions implies U = V.\nIf U1 and U2 are subspaces of V, then\n\n  \n    \n      \n        dim\n        ⁡\n        (\n        \n          U\n          \n            1\n          \n        \n        +\n        \n          U\n          \n            2\n          \n        \n        )\n        =\n        dim\n        ⁡\n        \n          U\n          \n            1\n          \n        \n        +\n        dim\n        ⁡\n        \n          U\n          \n            2\n          \n        \n        −\n        dim\n        ⁡\n        (\n        \n          U\n          \n            1\n          \n        \n        ∩\n        \n          U\n          \n            2\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle \\dim(U_{1}+U_{2})=\\dim U_{1}+\\dim U_{2}-\\dim(U_{1}\\cap U_{2}),}\n  \n\nwhere U1 + U2 denotes the span of U1 ∪ U2."
      },
      {
        "heading": "Matrices",
        "level": 1,
        "content": "Matrices allow explicit manipulation of finite-dimensional vector spaces and linear maps. Their theory is thus an essential part of linear algebra.\nLet V be a finite-dimensional vector space over a field F, and (v1, v2, ..., vm) be a basis of V (thus m is the dimension of V). By definition of a basis, the map\n\n  \n    \n      \n        \n          \n            \n              \n                (\n                \n                  a\n                  \n                    1\n                  \n                \n                ,\n                …\n                ,\n                \n                  a\n                  \n                    m\n                  \n                \n                )\n              \n              \n                \n                ↦\n                \n                  a\n                  \n                    1\n                  \n                \n                \n                  \n                    v\n                  \n                  \n                    1\n                  \n                \n                +\n                ⋯\n                \n                  a\n                  \n                    m\n                  \n                \n                \n                  \n                    v\n                  \n                  \n                    m\n                  \n                \n              \n            \n            \n              \n                \n                  F\n                  \n                    m\n                  \n                \n              \n              \n                \n                →\n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}(a_{1},\\ldots ,a_{m})&\\mapsto a_{1}\\mathbf {v} _{1}+\\cdots a_{m}\\mathbf {v} _{m}\\\\F^{m}&\\to V\\end{aligned}}}\n  \n\nis a bijection from Fm, the set of the sequences of m elements of F, onto V. This is an isomorphism of vector spaces, if Fm is equipped with its standard structure of vector space, where vector addition and scalar multiplication are done component by component.\nThis isomorphism allows representing a vector by its inverse image under this isomorphism, that is by the coordinate vector (a1, ..., am) or by the column matrix\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    a\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    a\n                    \n                      m\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\begin{bmatrix}a_{1}\\\\\\vdots \\\\a_{m}\\end{bmatrix}}.}\n  \n\nIf W is another finite dimensional vector space (possibly the same), with a basis (w1, ..., wn), a linear map f from W to V is well defined by its values on the basis elements, that is (f(w1), ..., f(wn)). Thus, f is well represented by the list of the corresponding column matrices. That is, if \n\n  \n    \n      \n        f\n        (\n        \n          w\n          \n            j\n          \n        \n        )\n        =\n        \n          a\n          \n            1\n            ,\n            j\n          \n        \n        \n          v\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          a\n          \n            m\n            ,\n            j\n          \n        \n        \n          v\n          \n            m\n          \n        \n        ,\n      \n    \n    {\\displaystyle f(w_{j})=a_{1,j}v_{1}+\\cdots +a_{m,j}v_{m},}\n  \n\nfor j = 1, ..., n, then f is represented by the matrix\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    a\n                    \n                      1\n                      ,\n                      1\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    a\n                    \n                      1\n                      ,\n                      n\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    a\n                    \n                      m\n                      ,\n                      1\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    a\n                    \n                      m\n                      ,\n                      n\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\begin{bmatrix}a_{1,1}&\\cdots &a_{1,n}\\\\\\vdots &\\ddots &\\vdots \\\\a_{m,1}&\\cdots &a_{m,n}\\end{bmatrix}},}\n  \n\nwith m rows and n columns.\nMatrix multiplication is defined in such a way that the product of two matrices is the matrix of the composition of the corresponding linear maps, and the product of a matrix and a column matrix is the column matrix representing the result of applying the represented linear map to the represented vector. It follows that the theory of finite-dimensional vector spaces and the theory of matrices are two different languages for expressing the same concepts.\nTwo matrices that encode the same linear transformation in different bases are called similar. It can be proved that two matrices are similar if and only if one can transform one into the other by elementary row and column operations. For a matrix representing a linear map from W to V, the row operations correspond to change of bases in V and the column operations correspond to change of bases in W. Every matrix is similar to an identity matrix possibly bordered by zero rows and zero columns. In terms of vector spaces, this means that, for any linear map from W to V, there are bases such that a part of the basis of W is mapped bijectively on a part of the basis of V, and that the remaining basis elements of W, if any, are mapped to zero. Gaussian elimination is the basic algorithm for finding these elementary operations, and proving these results."
      },
      {
        "heading": "Linear systems",
        "level": 1,
        "content": "A finite set of linear equations in a finite set of variables, for example, x1, x2, ..., xn, or x, y, ..., z is called a  system of linear equations or a linear system.\nSystems of linear equations form a fundamental part of linear algebra. Historically, linear algebra and matrix theory have been developed for solving such systems. In the modern presentation of linear algebra through vector spaces and matrices, many problems may be interpreted in terms of linear systems.\nFor example, let\n\nbe a linear system.\nTo such a system, one may associate its matrix \n\n  \n    \n      \n        M\n        =\n        \n          [\n          \n            \n              \n                \n                  2\n                \n                \n                  1\n                \n                \n                  −\n                  1\n                \n              \n              \n                \n                  −\n                  3\n                \n                \n                  −\n                  1\n                \n                \n                  2\n                \n              \n              \n                \n                  −\n                  2\n                \n                \n                  1\n                \n                \n                  2\n                \n              \n            \n          \n          ]\n        \n        .\n      \n    \n    {\\displaystyle M=\\left[{\\begin{array}{rrr}2&1&-1\\\\-3&-1&2\\\\-2&1&2\\end{array}}\\right].}\n  \n\nand its right member vector\n\n  \n    \n      \n        \n          v\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  8\n                \n              \n              \n                \n                  −\n                  11\n                \n              \n              \n                \n                  −\n                  3\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {v} ={\\begin{bmatrix}8\\\\-11\\\\-3\\end{bmatrix}}.}\n  \n\nLet T be the linear transformation associated with the matrix M. A solution of the system (S) is a vector \n\n  \n    \n      \n        \n          X\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  x\n                \n              \n              \n                \n                  y\n                \n              \n              \n                \n                  z\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {X} ={\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}}}\n  \n\nsuch that \n\n  \n    \n      \n        T\n        (\n        \n          X\n        \n        )\n        =\n        \n          v\n        \n        ,\n      \n    \n    {\\displaystyle T(\\mathbf {X} )=\\mathbf {v} ,}\n  \n\nthat is an element of the preimage of v by T.\nLet (S′) be the associated homogeneous system, where the right-hand sides of the equations are put to zero:\n\nThe solutions of (S′) are exactly the elements of the kernel of T or, equivalently, M.\nThe Gaussian-elimination consists of performing elementary row operations on the augmented matrix\n\n  \n    \n      \n        \n          [\n          \n            \n            \n              \n                \n                  \n                    M\n                  \n                  \n                    \n                      v\n                    \n                  \n                \n              \n            \n            \n          \n          ]\n        \n        =\n        \n          [\n          \n            \n              \n                \n                  2\n                \n                \n                  1\n                \n                \n                  −\n                  1\n                \n                \n                  8\n                \n              \n              \n                \n                  −\n                  3\n                \n                \n                  −\n                  1\n                \n                \n                  2\n                \n                \n                  −\n                  11\n                \n              \n              \n                \n                  −\n                  2\n                \n                \n                  1\n                \n                \n                  2\n                \n                \n                  −\n                  3\n                \n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\left[\\!{\\begin{array}{c|c}M&\\mathbf {v} \\end{array}}\\!\\right]=\\left[{\\begin{array}{rrr|r}2&1&-1&8\\\\-3&-1&2&-11\\\\-2&1&2&-3\\end{array}}\\right]}\n  \n\nfor putting it in reduced row echelon form. These row operations do not change the set of solutions of the system of equations. In the example, the reduced echelon form is \n\n  \n    \n      \n        \n          [\n          \n            \n            \n              \n                \n                  \n                    M\n                  \n                  \n                    \n                      v\n                    \n                  \n                \n              \n            \n            \n          \n          ]\n        \n        =\n        \n          [\n          \n            \n              \n                \n                  1\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  2\n                \n              \n              \n                \n                  0\n                \n                \n                  1\n                \n                \n                  0\n                \n                \n                  3\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  1\n                \n                \n                  −\n                  1\n                \n              \n            \n          \n          ]\n        \n        ,\n      \n    \n    {\\displaystyle \\left[\\!{\\begin{array}{c|c}M&\\mathbf {v} \\end{array}}\\!\\right]=\\left[{\\begin{array}{rrr|r}1&0&0&2\\\\0&1&0&3\\\\0&0&1&-1\\end{array}}\\right],}\n  \n\nshowing that the system (S) has the unique solution\n\n  \n    \n      \n        \n          \n            \n              \n                x\n              \n              \n                \n                =\n                2\n              \n            \n            \n              \n                y\n              \n              \n                \n                =\n                3\n              \n            \n            \n              \n                z\n              \n              \n                \n                =\n                −\n                1.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}x&=2\\\\y&=3\\\\z&=-1.\\end{aligned}}}\n  \n\nIt follows from this matrix interpretation of linear systems that the same methods can be applied for solving linear systems and for many operations on matrices and linear transformations, which include the computation of the ranks, kernels, matrix inverses."
      },
      {
        "heading": "Endomorphisms and square matrices",
        "level": 1,
        "content": "A linear endomorphism is a linear map that maps a vector space V to itself. \nIf V has a basis of n elements, such an endomorphism is represented by a square matrix of size n.\nConcerning general linear maps, linear endomorphisms, and square matrices have some specific properties that make their study an important part of linear algebra, which is used in many parts of mathematics, including geometric transformations, coordinate changes, quadratic forms, and many other parts of mathematics."
      },
      {
        "heading": "Determinant",
        "level": 2,
        "content": "The determinant of a square matrix A is defined to be\n\n  \n    \n      \n        \n          ∑\n          \n            σ\n            ∈\n            \n              S\n              \n                n\n              \n            \n          \n        \n        (\n        −\n        1\n        \n          )\n          \n            σ\n          \n        \n        \n          a\n          \n            1\n            σ\n            (\n            1\n            )\n          \n        \n        ⋯\n        \n          a\n          \n            n\n            σ\n            (\n            n\n            )\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\sum _{\\sigma \\in S_{n}}(-1)^{\\sigma }a_{1\\sigma (1)}\\cdots a_{n\\sigma (n)},}\n  \n\nwhere Sn is the group of all permutations of n elements, σ is a permutation, and (−1)σ the parity of the permutation. A matrix is invertible if and only if the determinant is invertible (i.e., nonzero if the scalars belong to a field).\nCramer's rule is a closed-form expression, in terms of determinants, of the solution of a system of n linear equations in n unknowns. Cramer's rule is useful for reasoning about the solution, but, except for n = 2 or 3, it is rarely used for computing a solution, since Gaussian elimination is a faster algorithm.\nThe determinant of an endomorphism is the determinant of the matrix representing the endomorphism in terms of some ordered basis. This definition makes sense since this determinant is independent of the choice of the basis."
      },
      {
        "heading": "Eigenvalues and eigenvectors",
        "level": 2,
        "content": "If f is a linear endomorphism of a vector space V over a field F, an eigenvector of f is a nonzero vector v of V such that f(v) = av for some scalar a in F. This scalar a is an eigenvalue of f.\nIf the dimension of V is finite, and a basis has been chosen, f and v may be represented, respectively, by a square matrix M and a column matrix z; the equation defining eigenvectors and eigenvalues becomes\n\n  \n    \n      \n        M\n        z\n        =\n        a\n        z\n        .\n      \n    \n    {\\displaystyle Mz=az.}\n  \n\nUsing the identity matrix I, whose entries are all zero, except those of the main diagonal, which are equal to one, this may be rewritten\n\n  \n    \n      \n        (\n        M\n        −\n        a\n        I\n        )\n        z\n        =\n        0.\n      \n    \n    {\\displaystyle (M-aI)z=0.}\n  \n\nAs z is supposed to be nonzero, this means that M – aI is a singular matrix, and thus that its determinant det (M − aI) equals zero. The eigenvalues are thus the roots of the polynomial\n\n  \n    \n      \n        det\n        (\n        x\n        I\n        −\n        M\n        )\n        .\n      \n    \n    {\\displaystyle \\det(xI-M).}\n  \n\nIf V is of dimension n, this is a monic polynomial of degree n, called the characteristic polynomial of the matrix (or of the endomorphism), and there are, at most, n eigenvalues.\nIf a basis exists that consists only of eigenvectors, the matrix of f on this basis has a very simple structure: it is a diagonal matrix such that the entries on the main diagonal are eigenvalues, and the other entries are zero. In this case, the endomorphism and the matrix are said to be diagonalizable. More generally, an endomorphism and a matrix are also said diagonalizable, if they become diagonalizable after extending the field of scalars. In this extended sense, if the characteristic polynomial is square-free, then the matrix is diagonalizable.\nA symmetric matrix is always diagonalizable. There are non-diagonalizable matrices, the simplest being\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  0\n                \n                \n                  1\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}}}\n  \n\n(it cannot be diagonalizable since its square is the zero matrix, and the square of a nonzero diagonal matrix is never zero).\nWhen an endomorphism is not diagonalizable, there are bases on which it has a simple form, although not as simple as the diagonal form. The Frobenius normal form does not need to extend the field of scalars and makes the characteristic polynomial immediately readable on the matrix. The Jordan normal form requires to extension of the field of scalar for containing all eigenvalues and differs from the diagonal form only by some entries that are just above the main diagonal and are equal to 1."
      },
      {
        "heading": "Duality",
        "level": 1,
        "content": "A linear form is a linear map from a vector space V over a field F to the field of scalars F, viewed as a vector space over itself. Equipped by pointwise addition and multiplication by a scalar, the linear forms form a vector space, called the dual space of V, and usually denoted V* or V′.\nIf v1, ..., vn is a basis of V (this implies that V is finite-dimensional), then one can define, for i = 1, ..., n, a linear map vi* such that vi*(vi) = 1 and vi*(vj) = 0 if j ≠ i. These linear maps form a basis of V*, called the dual basis of v1, ..., vn. (If V is not finite-dimensional, the vi* may be defined similarly; they are linearly independent, but do not form a basis.)\nFor v in V, the map\n\n  \n    \n      \n        f\n        →\n        f\n        (\n        \n          v\n        \n        )\n      \n    \n    {\\displaystyle f\\to f(\\mathbf {v} )}\n  \n\nis a linear form on V*. This defines the canonical linear map from V into (V*)*, the dual of V*, called the double dual or bidual of V. This canonical map is an isomorphism if V is finite-dimensional, and this allows identifying V with its bidual. (In the infinite-dimensional case, the canonical map is injective, but not surjective.)\nThere is thus a complete symmetry between a finite-dimensional vector space and its dual. This motivates the frequent use, in this context, of the bra–ket notation\n\n  \n    \n      \n        ⟨\n        f\n        ,\n        \n          x\n        \n        ⟩\n      \n    \n    {\\displaystyle \\langle f,\\mathbf {x} \\rangle }\n  \n\nfor denoting f(x)."
      },
      {
        "heading": "Dual map",
        "level": 2,
        "content": "Let \n\n  \n    \n      \n        f\n        :\n        V\n        →\n        W\n      \n    \n    {\\displaystyle f:V\\to W}\n  \n\nbe a linear map. For every linear form h on W, the composite function h ∘ f is a linear form on V. This defines a linear map\n\n  \n    \n      \n        \n          f\n          \n            ∗\n          \n        \n        :\n        \n          W\n          \n            ∗\n          \n        \n        →\n        \n          V\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle f^{*}:W^{*}\\to V^{*}}\n  \n\nbetween the dual spaces, which is called the dual or the transpose of f.\nIf V and W are finite-dimensional, and M is the matrix of f in terms of some ordered bases, then the matrix of f* over the dual bases is the transpose MT of M, obtained by exchanging rows and columns.\nIf elements of vector spaces and their duals are represented by column vectors, this duality may be expressed in bra–ket notation by \n\n  \n    \n      \n        ⟨\n        \n          h\n          \n            \n              T\n            \n          \n        \n        ,\n        M\n        \n          v\n        \n        ⟩\n        =\n        ⟨\n        \n          h\n          \n            \n              T\n            \n          \n        \n        M\n        ,\n        \n          v\n        \n        ⟩\n        .\n      \n    \n    {\\displaystyle \\langle h^{\\mathsf {T}},M\\mathbf {v} \\rangle =\\langle h^{\\mathsf {T}}M,\\mathbf {v} \\rangle .}\n  \n\nTo highlight this symmetry, the two members of this equality are sometimes written \n\n  \n    \n      \n        ⟨\n        \n          h\n          \n            \n              T\n            \n          \n        \n        ∣\n        M\n        ∣\n        \n          v\n        \n        ⟩\n        .\n      \n    \n    {\\displaystyle \\langle h^{\\mathsf {T}}\\mid M\\mid \\mathbf {v} \\rangle .}"
      },
      {
        "heading": "Inner-product spaces",
        "level": 2,
        "content": "Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an inner product is a map.\n\n  \n    \n      \n        ⟨\n        ⋅\n        ,\n        ⋅\n        ⟩\n        :\n        V\n        ×\n        V\n        →\n        F\n      \n    \n    {\\displaystyle \\langle \\cdot ,\\cdot \\rangle :V\\times V\\to F}\n  \n\nthat satisfies the following three axioms for all vectors u, v, w in V and all scalars a in F:\n\nConjugate symmetry:\n\n  \n    \n      \n        ⟨\n        \n          u\n        \n        ,\n        \n          v\n        \n        ⟩\n        =\n        \n          \n            \n              ⟨\n              \n                v\n              \n              ,\n              \n                u\n              \n              ⟩\n            \n            ¯\n          \n        \n        .\n      \n    \n    {\\displaystyle \\langle \\mathbf {u} ,\\mathbf {v} \\rangle ={\\overline {\\langle \\mathbf {v} ,\\mathbf {u} \\rangle }}.}\n  \n\nIn \n  \n    \n      \n        \n          R\n        \n      \n    \n    {\\displaystyle \\mathbb {R} }\n  \n, it is symmetric.\nLinearity in the first argument:\n\n  \n    \n      \n        \n          \n            \n              \n                ⟨\n                a\n                \n                  u\n                \n                ,\n                \n                  v\n                \n                ⟩\n              \n              \n                \n                =\n                a\n                ⟨\n                \n                  u\n                \n                ,\n                \n                  v\n                \n                ⟩\n                .\n              \n            \n            \n              \n                ⟨\n                \n                  u\n                \n                +\n                \n                  v\n                \n                ,\n                \n                  w\n                \n                ⟩\n              \n              \n                \n                =\n                ⟨\n                \n                  u\n                \n                ,\n                \n                  w\n                \n                ⟩\n                +\n                ⟨\n                \n                  v\n                \n                ,\n                \n                  w\n                \n                ⟩\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\langle a\\mathbf {u} ,\\mathbf {v} \\rangle &=a\\langle \\mathbf {u} ,\\mathbf {v} \\rangle .\\\\\\langle \\mathbf {u} +\\mathbf {v} ,\\mathbf {w} \\rangle &=\\langle \\mathbf {u} ,\\mathbf {w} \\rangle +\\langle \\mathbf {v} ,\\mathbf {w} \\rangle .\\end{aligned}}}\n  \n\nPositive-definiteness:\n\n  \n    \n      \n        ⟨\n        \n          v\n        \n        ,\n        \n          v\n        \n        ⟩\n        ≥\n        0\n      \n    \n    {\\displaystyle \\langle \\mathbf {v} ,\\mathbf {v} \\rangle \\geq 0}\n  \n\nwith equality only for v = 0.\nWe can define the length of a vector v in V by\n\n  \n    \n      \n        ‖\n        \n          v\n        \n        \n          ‖\n          \n            2\n          \n        \n        =\n        ⟨\n        \n          v\n        \n        ,\n        \n          v\n        \n        ⟩\n        ,\n      \n    \n    {\\displaystyle \\|\\mathbf {v} \\|^{2}=\\langle \\mathbf {v} ,\\mathbf {v} \\rangle ,}\n  \n\nand we can prove the Cauchy–Schwarz inequality:\n\n  \n    \n      \n        \n          |\n        \n        ⟨\n        \n          u\n        \n        ,\n        \n          v\n        \n        ⟩\n        \n          |\n        \n        ≤\n        ‖\n        \n          u\n        \n        ‖\n        ⋅\n        ‖\n        \n          v\n        \n        ‖\n        .\n      \n    \n    {\\displaystyle |\\langle \\mathbf {u} ,\\mathbf {v} \\rangle |\\leq \\|\\mathbf {u} \\|\\cdot \\|\\mathbf {v} \\|.}\n  \n\nIn particular, the quantity\n\n  \n    \n      \n        \n          \n            \n              \n                |\n              \n              ⟨\n              \n                u\n              \n              ,\n              \n                v\n              \n              ⟩\n              \n                |\n              \n            \n            \n              ‖\n              \n                u\n              \n              ‖\n              ⋅\n              ‖\n              \n                v\n              \n              ‖\n            \n          \n        \n        ≤\n        1\n        ,\n      \n    \n    {\\displaystyle {\\frac {|\\langle \\mathbf {u} ,\\mathbf {v} \\rangle |}{\\|\\mathbf {u} \\|\\cdot \\|\\mathbf {v} \\|}}\\leq 1,}\n  \n\nand so we can call this quantity the cosine of the angle between the two vectors.\nTwo vectors are orthogonal if ⟨u, v⟩ = 0. An orthonormal basis is a basis where all basis vectors have length 1 and are orthogonal to each other. Given any finite-dimensional vector space, an orthonormal basis could be found by the Gram–Schmidt procedure. Orthonormal bases are particularly easy to deal with, since if v = a1 v1 + ⋯ + an vn, then\n\n  \n    \n      \n        \n          a\n          \n            i\n          \n        \n        =\n        ⟨\n        \n          v\n        \n        ,\n        \n          \n            v\n          \n          \n            i\n          \n        \n        ⟩\n        .\n      \n    \n    {\\displaystyle a_{i}=\\langle \\mathbf {v} ,\\mathbf {v} _{i}\\rangle .}\n  \n\nThe inner product facilitates the construction of many useful concepts. For instance, given a transform T, we can define its Hermitian conjugate T* as the linear transform satisfying\n\n  \n    \n      \n        ⟨\n        T\n        \n          u\n        \n        ,\n        \n          v\n        \n        ⟩\n        =\n        ⟨\n        \n          u\n        \n        ,\n        \n          T\n          \n            ∗\n          \n        \n        \n          v\n        \n        ⟩\n        .\n      \n    \n    {\\displaystyle \\langle T\\mathbf {u} ,\\mathbf {v} \\rangle =\\langle \\mathbf {u} ,T^{*}\\mathbf {v} \\rangle .}\n  \n\nIf T satisfies TT* = T*T, we call T normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span V."
      },
      {
        "heading": "Relationship with geometry",
        "level": 1,
        "content": "There is a strong relationship between linear algebra and geometry, which started with the introduction by René Descartes, in 1637, of Cartesian coordinates. In this new (at that time) geometry, now called Cartesian geometry, points are represented by Cartesian coordinates, which are sequences of three real numbers (in the case of the usual three-dimensional space). The basic objects of geometry, which are lines and planes are represented by linear equations. Thus, computing intersections of lines and planes amounts to solving systems of linear equations. This was one of the main motivations for developing linear algebra.\nMost geometric transformation, such as translations, rotations, reflections, rigid motions, isometries, and projections transform lines into lines. It follows that they can be defined, specified, and studied in terms of linear maps. This is also the case of homographies and Möbius transformations when considered as transformations of a projective space.\nUntil the end of the 19th century, geometric spaces were defined by axioms relating points, lines, and planes (synthetic geometry). Around this date, it appeared that one may also define geometric spaces by constructions involving vector spaces (see, for example, Projective space and Affine space). It has been shown that the two approaches are essentially equivalent. In classical geometry, the involved vector spaces are vector spaces over the reals, but the constructions may be extended to vector spaces over any field, allowing considering geometry over arbitrary fields, including finite fields.\nPresently, most textbooks introduce geometric spaces from linear algebra, and geometry is often presented, at the elementary level, as a subfield of linear algebra."
      },
      {
        "heading": "Usage and applications",
        "level": 1,
        "content": "Linear algebra is used in almost all areas of mathematics, thus making it relevant in almost all scientific domains that use mathematics. These applications may be divided into several wide categories."
      },
      {
        "heading": "Functional analysis",
        "level": 2,
        "content": "Functional analysis studies function spaces. These are vector spaces with additional structure, such as Hilbert spaces. Linear algebra is thus a fundamental part of functional analysis and its applications, which include, in particular, quantum mechanics (wave functions) and Fourier analysis (orthogonal basis)."
      },
      {
        "heading": "Scientific computation",
        "level": 2,
        "content": "Nearly all scientific computations involve linear algebra. Consequently, linear algebra algorithms have been highly optimized. BLAS and LAPACK are the best known implementations. For improving efficiency, some of them configure the algorithms automatically, at run time, to adapt them to the specificities of the computer (cache size, number of available cores, ...).\nSince the 1960s there have been processors with specialized instructions for optimizing the operations of linear algebra, optional array processors under the control of a conventional processor, supercomputers designed for array processing and conventional processors augmented with vector registers.\nSome contemporary processors, typically graphics processing units (GPU), are designed with a matrix structure, for optimizing the operations of linear algebra."
      },
      {
        "heading": "Geometry of ambient space",
        "level": 2,
        "content": "The modeling of ambient space is based on geometry. Sciences concerned with this space use geometry widely. This is the case with mechanics and robotics, for describing rigid body dynamics; geodesy for describing Earth shape; perspectivity, computer vision, and computer graphics, for describing the relationship between a scene and its plane representation; and many other scientific domains.\nIn all these applications, synthetic geometry is often used for general descriptions and a qualitative approach, but for the study of explicit situations, one must compute with coordinates. This requires the heavy use of linear algebra."
      },
      {
        "heading": "Study of complex systems",
        "level": 2,
        "content": "Most physical phenomena are modeled by partial differential equations. To solve them, one usually decomposes the space in which the solutions are searched into small, mutually interacting cells. For linear systems this interaction involves linear functions. For nonlinear systems, this interaction is often approximated by linear functions.This is called a linear model or first-order approximation. Linear models are frequently used for complex nonlinear real-world systems because they make parametrization more manageable. In both cases, very large matrices are generally involved. Weather forecasting (or more specifically, parametrization for atmospheric modeling) is a typical example of a real-world application, where the whole Earth atmosphere is divided into cells of, say, 100 km of width and 100 km of height."
      },
      {
        "heading": "Fluid mechanics, fluid dynamics, and thermal energy systems",
        "level": 2,
        "content": "Linear algebra, a branch of mathematics dealing with vector spaces and linear mappings between these spaces, plays a critical role in various engineering disciplines, including fluid mechanics, fluid dynamics, and thermal energy systems. Its application in these fields is multifaceted and indispensable for solving complex problems.\nIn fluid mechanics, linear algebra is integral to understanding and solving problems related to the behavior of fluids. It assists in the modeling and simulation of fluid flow, providing essential tools for the analysis of fluid dynamics problems. For instance, linear algebraic techniques are used to solve systems of differential equations that describe fluid motion. These equations, often complex and non-linear, can be linearized using linear algebra methods, allowing for simpler solutions and analyses.\nIn the field of fluid dynamics, linear algebra finds its application in computational fluid dynamics (CFD), a branch that uses numerical analysis and data structures to solve and analyze problems involving fluid flows. CFD relies heavily on linear algebra for the computation of fluid flow and heat transfer in various applications. For example, the Navier–Stokes equations, fundamental in fluid dynamics, are often solved using techniques derived from linear algebra. This includes the use of matrices and vectors to represent and manipulate fluid flow fields.\nFurthermore, linear algebra plays a crucial role in thermal energy systems, particularly in power systems analysis. It is used to model and optimize the generation, transmission, and distribution of electric power. Linear algebraic concepts such as matrix operations and eigenvalue problems are employed to enhance the efficiency, reliability, and economic performance of power systems. The application of linear algebra in this context is vital for the design and operation of modern power systems, including renewable energy sources and smart grids.\nOverall, the application of linear algebra in fluid mechanics, fluid dynamics, and thermal energy systems is an example of the profound interconnection between mathematics and engineering. It provides engineers with the necessary tools to model, analyze, and solve complex problems in these domains, leading to advancements in technology and industry."
      },
      {
        "heading": "Extensions and generalizations",
        "level": 1,
        "content": "This section presents several related topics that do not appear generally in elementary textbooks on linear algebra but are commonly considered, in advanced mathematics, as parts of linear algebra."
      },
      {
        "heading": "Module theory",
        "level": 2,
        "content": "The existence of multiplicative inverses in fields is not involved in the axioms defining a vector space. One may thus replace the field of scalars by a ring R, and this gives the structure called a module over R, or R-module.\nThe concepts of linear independence, span, basis, and linear maps (also called module homomorphisms) are defined for modules exactly as for vector spaces, with the essential difference that, if R is not a field, there are modules that do not have any basis. The modules that have a basis are the free modules, and those that are spanned by a finite set are the finitely generated modules. Module homomorphisms between finitely generated free modules may be represented by matrices. The theory of matrices over a ring is similar to that of matrices over a field, except that determinants exist only if the ring is commutative, and that a square matrix over a commutative ring is invertible only if its determinant has a multiplicative inverse in the ring.\nVector spaces are completely characterized by their dimension (up to an isomorphism). In general, there is not such a complete classification for modules, even if one restricts oneself to finitely generated modules. However, every module is a cokernel of a homomorphism of free modules.\nModules over the integers can be identified with abelian groups, since the multiplication by an integer may be identified as a repeated addition. Most of the theory of abelian groups may be extended to modules over a principal ideal domain. In particular, over a principal ideal domain, every submodule of a free module is free, and the fundamental theorem of finitely generated abelian groups may be extended straightforwardly to finitely generated modules over a principal ring.\nThere are many rings for which there are algorithms for solving linear equations and systems of linear equations. However, these algorithms have generally a computational complexity that is much higher than similar algorithms over a field. For more details, see Linear equation over a ring."
      },
      {
        "heading": "Multilinear algebra and tensors",
        "level": 2,
        "content": "In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of several different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space V* consisting of linear maps f : V → F where F is the field of scalars. Multilinear maps T : Vn → F can be described via tensor products of elements of V*.\nIf, in addition to vector addition and scalar multiplication, there is a bilinear vector product V × V → V, the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials)."
      },
      {
        "heading": "Topological vector spaces",
        "level": 2,
        "content": "Vector spaces that are not finite-dimensional often require additional structure to be tractable. A normed vector space is a vector space along with a function called a norm, which measures the \"size\" of elements. The norm induces a metric, which measures the distance between elements, and induces a topology, which allows for a definition of continuous maps. The metric also allows for a definition of limits and completeness – a normed vector space that is complete is known as a Banach space. A complete metric space along with the additional structure of an inner product (a conjugate symmetric sesquilinear form) is known as a Hilbert space, which is in some sense a particularly well-behaved Banach space. Functional analysis applies the methods of linear algebra alongside those of mathematical analysis to study various function spaces; the central objects of study in functional analysis are Lp spaces, which are Banach spaces, and especially the L2 space of square-integrable functions, which is the only Hilbert space among them. Functional analysis is of particular importance to quantum mechanics, the theory of partial differential equations, digital signal processing, and electrical engineering. It also provides the foundation and theoretical framework that underlies the Fourier transform and related methods."
      },
      {
        "heading": "See also",
        "level": 1,
        "content": "Fundamental matrix (computer vision)\nGeometric algebra\nLinear programming\nLinear regression, a statistical estimation method\nNumerical linear algebra\nOutline of linear algebra\nTransformation matrix"
      },
      {
        "heading": "Explanatory notes",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Citations",
        "level": 1,
        "content": ""
      },
      {
        "heading": "General and cited sources",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Further reading",
        "level": 1,
        "content": ""
      },
      {
        "heading": "History",
        "level": 2,
        "content": "Fearnley-Sander, Desmond, \"Hermann Grassmann and the Creation of Linear Algebra\", American Mathematical Monthly 86 (1979), pp. 809–817.\nGrassmann, Hermann (1844), Die lineale Ausdehnungslehre ein neuer Zweig der Mathematik: dargestellt und durch Anwendungen auf die übrigen Zweige der Mathematik, wie auch auf die Statik, Mechanik, die Lehre vom Magnetismus und die Krystallonomie erläutert, Leipzig: O. Wigand"
      },
      {
        "heading": "Introductory textbooks",
        "level": 2,
        "content": "Anton, Howard (2005), Elementary Linear Algebra (Applications Version) (9th ed.), Wiley International\nBanerjee, Sudipto; Roy, Anindya (2014), Linear Algebra and Matrix Analysis for Statistics, Texts in Statistical Science (1st ed.), Chapman and Hall/CRC, ISBN 978-1420095388\nBretscher, Otto (2004), Linear Algebra with Applications (3rd ed.), Prentice Hall, ISBN 978-0-13-145334-0\nFarin, Gerald; Hansford, Dianne (2004), Practical Linear Algebra: A Geometry Toolbox, AK Peters, ISBN 978-1-56881-234-2\nHefferon, Jim (2020). Linear Algebra (4th ed.). Ann Arbor, Michigan: Orthogonal Publishing. ISBN 978-1-944325-11-4. OCLC 1178900366. OL 30872051M.\nKolman, Bernard; Hill, David R. (2007), Elementary Linear Algebra with Applications (9th ed.), Prentice Hall, ISBN 978-0-13-229654-0\nLay, David C. (2005), Linear Algebra and Its Applications (3rd ed.), Addison Wesley, ISBN 978-0-321-28713-7\nLeon, Steven J. (2006), Linear Algebra With Applications (7th ed.), Pearson Prentice Hall, ISBN 978-0-13-185785-8\nMurty, Katta G. (2014) Computational and Algorithmic Linear Algebra and n-Dimensional Geometry, World Scientific Publishing, ISBN 978-981-4366-62-5. Chapter 1: Systems of Simultaneous Linear Equations\nNoble, B. & Daniel, J.W. (2nd Ed. 1977) [1], Pearson Higher Education, ISBN 978-0130413437.\nPoole, David (2010), Linear Algebra: A Modern Introduction (3rd ed.), Cengage – Brooks/Cole, ISBN 978-0-538-73545-2\nRicardo, Henry (2010), A Modern Introduction To Linear Algebra (1st ed.), CRC Press, ISBN 978-1-4398-0040-9\nSadun, Lorenzo (2008), Applied Linear Algebra: the decoupling principle (2nd ed.), AMS, ISBN 978-0-8218-4441-0\nStrang, Gilbert (2016), Introduction to Linear Algebra (5th ed.), Wellesley-Cambridge Press, ISBN 978-09802327-7-6\nThe Manga Guide to Linear Algebra (2012), by Shin Takahashi, Iroha Inoue and Trend-Pro Co., Ltd., ISBN 978-1-59327-413-9"
      },
      {
        "heading": "Advanced textbooks",
        "level": 2,
        "content": "Bhatia, Rajendra (November 15, 1996), Matrix Analysis, Graduate Texts in Mathematics, Springer, ISBN 978-0-387-94846-1\nDemmel, James W. (August 1, 1997), Applied Numerical Linear Algebra, SIAM, ISBN 978-0-89871-389-3\nDym, Harry (2007), Linear Algebra in Action, AMS, ISBN 978-0-8218-3813-6\nGantmacher, Felix R. (2005), Applications of the Theory of Matrices, Dover Publications, ISBN 978-0-486-44554-0\nGantmacher, Felix R. (1990), Matrix Theory Vol. 1 (2nd ed.), American Mathematical Society, ISBN 978-0-8218-1376-8\nGantmacher, Felix R. (2000), Matrix Theory Vol. 2 (2nd ed.), American Mathematical Society, ISBN 978-0-8218-2664-5\nGelfand, Israel M. (1989), Lectures on Linear Algebra, Dover Publications, ISBN 978-0-486-66082-0\nGlazman, I. M.; Ljubic, Ju. I. (2006), Finite-Dimensional Linear Analysis, Dover Publications, ISBN 978-0-486-45332-3\nGolan, Johnathan S. (January 2007), The Linear Algebra a Beginning Graduate Student Ought to Know (2nd ed.), Springer, ISBN 978-1-4020-5494-5\nGolan, Johnathan S. (August 1995), Foundations of Linear Algebra, Kluwer, ISBN 0-7923-3614-3\nGreub, Werner H. (October 16, 1981), Linear Algebra, Graduate Texts in Mathematics (4th ed.), Springer, ISBN 978-0-8018-5414-9\nHoffman, Kenneth; Kunze, Ray (1971), Linear algebra (2nd ed.), Englewood Cliffs, N.J.: Prentice-Hall, Inc., MR 0276251\nHalmos, Paul R. (August 20, 1993), Finite-Dimensional Vector Spaces, Undergraduate Texts in Mathematics, Springer, ISBN 978-0-387-90093-3\nFriedberg, Stephen H.; Insel, Arnold J.; Spence, Lawrence E. (September 7, 2018), Linear Algebra (5th ed.), Pearson, ISBN 978-0-13-486024-4\nHorn, Roger A.; Johnson, Charles R. (February 23, 1990), Matrix Analysis, Cambridge University Press, ISBN 978-0-521-38632-6\nHorn, Roger A.; Johnson, Charles R. (June 24, 1994), Topics in Matrix Analysis, Cambridge University Press, ISBN 978-0-521-46713-1\nLang, Serge (March 9, 2004), Linear Algebra, Undergraduate Texts in Mathematics (3rd ed.), Springer, ISBN 978-0-387-96412-6\nMarcus, Marvin; Minc, Henryk (2010), A Survey of Matrix Theory and Matrix Inequalities, Dover Publications, ISBN 978-0-486-67102-4\nMeyer, Carl D. (February 15, 2001), Matrix Analysis and Applied Linear Algebra, Society for Industrial and Applied Mathematics (SIAM), ISBN 978-0-89871-454-8, archived from the original on October 31, 2009\nMirsky, L. (1990), An Introduction to Linear Algebra, Dover Publications, ISBN 978-0-486-66434-7\nShafarevich, I. R.; Remizov, A. O (2012), Linear Algebra and Geometry, Springer, ISBN 978-3-642-30993-9\nShilov, Georgi E. (June 1, 1977), Linear algebra, Dover Publications, ISBN 978-0-486-63518-7\nShores, Thomas S. (December 6, 2006), Applied Linear Algebra and Matrix Analysis, Undergraduate Texts in Mathematics, Springer, ISBN 978-0-387-33194-2\nSmith, Larry (May 28, 1998), Linear Algebra, Undergraduate Texts in Mathematics, Springer, ISBN 978-0-387-98455-1\nTrefethen, Lloyd N.; Bau, David (1997), Numerical Linear Algebra, SIAM, ISBN 978-0-898-71361-9"
      },
      {
        "heading": "Study guides and outlines",
        "level": 2,
        "content": "Leduc, Steven A. (May 1, 1996), Linear Algebra (Cliffs Quick Review), Cliffs Notes, ISBN 978-0-8220-5331-6\nLipschutz, Seymour; Lipson, Marc (December 6, 2000), Schaum's Outline of Linear Algebra (3rd ed.), McGraw-Hill, ISBN 978-0-07-136200-9\nLipschutz, Seymour (January 1, 1989), 3,000 Solved Problems in Linear Algebra, McGraw–Hill, ISBN 978-0-07-038023-3\nMcMahon, David (October 28, 2005), Linear Algebra Demystified, McGraw–Hill Professional, ISBN 978-0-07-146579-3\nZhang, Fuzhen (April 7, 2009), Linear Algebra: Challenging Problems for Students, The Johns Hopkins University Press, ISBN 978-0-8018-9125-0"
      },
      {
        "heading": "External links",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Online Resources",
        "level": 2,
        "content": "MIT Linear Algebra Video Lectures, a series of 34 recorded lectures by Professor Gilbert Strang (Spring 2010)\nInternational Linear Algebra Society\n\"Linear algebra\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nLinear Algebra on MathWorld\nMatrix and Linear Algebra Terms on Earliest Known Uses of Some of the Words of Mathematics\nEarliest Uses of Symbols for Matrices and Vectors on Earliest Uses of Various Mathematical Symbols\nEssence of linear algebra, a video presentation from 3Blue1Brown of the basics of linear algebra, with emphasis on the relationship between the geometric, the matrix and the abstract points of view"
      },
      {
        "heading": "Online books",
        "level": 2,
        "content": "Beezer, Robert A. (2009) [2004]. A First Course in Linear Algebra. Gainesville, Florida: University Press of Florida. ISBN 9781616100049.\nConnell, Edwin H. (2004) [1999]. Elements of Abstract and Linear Algebra. University of Miami, Coral Gables, Florida: Self-published.\nHefferon, Jim (2020). Linear Algebra (4th ed.). Ann Arbor, Michigan: Orthogonal Publishing. ISBN 978-1-944325-11-4. OCLC 1178900366. OL 30872051M.\nMargalit, Dan; Rabinoff, Joseph (2019). Interactive Linear Algebra. Georgia Institute of Technology, Atlanta, Georgia: Self-published.\nMatthews, Keith R. (2013) [1991]. Elementary Linear Algebra. University of Queensland, Brisbane, Australia: Self-published.\nMikaelian, Vahagn H. (2020) [2017]. Linear Algebra: Theory and Algorithms. Yerevan, Armenia: Self-published – via ResearchGate.\nSharipov, Ruslan, Course of linear algebra and multidimensional geometry\nTreil, Sergei, Linear Algebra Done Wrong"
      }
    ],
    "summary": "Linear algebra is the branch of mathematics concerning linear equations such as\n\n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        =\n        b\n        ,\n      \n    \n    {\\displaystyle a_{1}x_{1}+\\cdots +a_{n}x_{n}=b,}\n  \n\nlinear maps such as\n\n  \n    \n      \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n        ↦\n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        ,\n      \n    \n    {\\displaystyle (x_{1},\\ldots ,x_{n})\\mapsto a_{1}x_{1}+\\cdots +a_{n}x_{n},}\n  \n\nand their representations in vector spaces and through matrices.\nLinear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis, a branch of mathematical analysis, may be viewed as the application of linear algebra to function spaces.\nLinear algebra is also used in most sciences and fields of engineering because it allows modeling many natural phenomena, and computing efficiently with such models. For nonlinear systems, which cannot be modeled with linear algebra, it is often used for dealing with first-order approximations, using the fact that the differential of a multivariate function at a point is the linear map that best approximates the function near that point."
  },
  {
    "title": "Álgebra lineal",
    "source": "https://es.wikipedia.org/wiki/%C3%81lgebra_lineal",
    "language": "es",
    "chunks": [
      {
        "heading": "Historia",
        "level": 1,
        "content": "El procedimiento para resolver ecuaciones lineales simultáneas que ahora se denomina eliminación gaussiana aparece en el antiguo texto matemático chino Cálculo de barras#Sistema de ecuaciones lineales; Capítulo octavo: Matrices rectangulares de Los nueve capítulos sobre el arte matemático. Su uso se ilustra en dieciocho problemas, con dos a cinco ecuaciones.[6]​\nLos Sistemas de ecuaciones lineales surgieron en Europa con la introducción en 1637 por René Descartes de las coordenadas en la geometría. De hecho, en esta nueva geometría, ahora llamada geometría cartesiana, las líneas y los planos están representados por ecuaciones lineales, y calcular sus intersecciones equivale a resolver sistemas de ecuaciones lineales.\nLos primeros métodos sistemáticos para resolver sistemas lineales utilizaban  determinantes, considerados por primera vez por Leibniz en 1693. En 1750, Gabriel Cramer los utilizó para dar soluciones explícitas de sistemas lineales, lo que ahora se llama regla de Cramer. Más tarde, Gauss describió aún más el método de eliminación, que inicialmente fue catalogado como un avance en geodesia.[7]​\nEn 1844 Hermann Grassmann publicó su \"Teoría de la Extensión\" que incluía nuevos temas fundacionales de lo que hoy se llama álgebra lineal. En 1848, James Joseph Sylvester introdujo el término matriz, que en latín significa vientre.\nEl álgebra lineal creció con las ideas anotadas en el plano complejo. Por ejemplo, dos números w y z en ℂ tienen una diferencia w - z, y los segmentos de línea \n  \n    \n      \n        \n          \n            \n              w\n              z\n            \n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {wz}}}\n  \n y \n  \n    \n      \n        \n          \n            \n              0\n              (\n              w\n              −\n              z\n              )\n            \n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {0(w-z)}}}\n  \n tienen la misma longitud y dirección. Los segmentos son equipolentes. El sistema cuatridimensional ℍ de cuaterniones se inició en 1843. El término vector fue introducido como v = x i + y j + z k representando un punto en el espacio. La diferencia de cuaterniones p - q también produce un segmento equipolente a la \n  \n    \n      \n        \n          \n            \n              p\n              q\n            \n            ¯\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\overline {pq}}.}\n  \n  \nOtros sistemas de números hipercomplejos también utilizaron la idea de un espacio lineal con una base.\nArthur Cayley introdujo la multiplicación matricial y la matriz inversa en 1856, haciendo posible el grupo lineal general. El mecanismo de representación de grupo se hizo disponible para describir los números complejos e hipercomplejos. Fundamentalmente, Cayley utilizó una sola letra para denotar una matriz, tratando así una matriz como un objeto agregado. También se dio cuenta de la conexión entre las matrices y los determinantes, y escribió \"Habría muchas cosas que decir sobre esta teoría de las matrices que deberían, me parece, preceder a la teoría de los determinantes\".[7]​\nBenjamin Peirce publicó su Álgebra lineal asociativa (1872), y su hijo Charles Sanders Peirce amplió el trabajo posteriormente.[8]​\nEl telégrafo requería un sistema explicativo, y la publicación en 1873 de A Treatise on Electricity and Magnetism instituyó una teoría de campos de fuerzas y requirió la geometría diferencial para su expresión. El álgebra lineal es geometría diferencial plana y sirve en los espacios tangentes a los colectores. Las simetrías electromagnéticas del espaciotiempo se expresan mediante las transformaciones de Lorentzs, y gran parte de la historia del álgebra lineal es la historia de las transformaciones de Lorentz.\nLa primera definición moderna y más precisa de un espacio vectorial fue introducida por Peano en 1888;[7]​ en 1900 había surgido una teoría de las transformaciones lineales de los espacios vectoriales de dimensión finita. El álgebra lineal tomó su forma moderna en la primera mitad del siglo XX, cuando muchas ideas y métodos de siglos anteriores se generalizaron como álgebra abstracta. El desarrollo de los ordenadores hizo que aumentara la investigación de algoritmos eficientes para la eliminación gaussiana y las descomposiciones matriciales, y el álgebra lineal se convirtió en una herramienta esencial para la modelización y las simulaciones.[7]​"
      },
      {
        "heading": "Contexto general",
        "level": 1,
        "content": "De manera más formal, el álgebra lineal estudia conjuntos denominados espacios vectoriales, los cuales constan de un conjunto de vectores y un conjunto de escalares que tiene estructura de campo, con una operación de suma de vectores y otra de producto entre escalares y vectores que satisfacen ciertas propiedades (por ejemplo, que la suma es conmutativa).\nEstudia también transformaciones lineales, que son funciones entre espacios vectoriales que satisfacen las condiciones de linealidad:\n\n  \n    \n      \n        T\n        (\n        u\n        +\n        v\n        )\n        =\n        T\n        (\n        u\n        )\n        +\n        T\n        (\n        v\n        )\n        ,\n        \n        T\n        (\n        r\n        ⋅\n        u\n        )\n        =\n        r\n        ⋅\n        T\n        (\n        u\n        )\n        .\n      \n    \n    {\\displaystyle T(u+v)=T(u)+T(v),\\qquad T(r\\cdot u)=r\\cdot T(u).}\n  \n\nA diferencia del ejemplo desarrollado en la sección anterior, los vectores no necesariamente son n-adas de escalares, sino que pueden ser elementos de un conjunto cualquiera (de hecho, a partir de todo conjunto puede construirse un espacio vectorial sobre un campo fijo).\nFinalmente, el álgebra lineal estudia también las propiedades que aparecen cuando se impone estructura adicional sobre los espacios vectoriales, siendo una de las más frecuentes la existencia de un producto interno (una especie de producto entre dos vectores) que permite introducir nociones como longitud de vectores y ángulo entre un par de los mismos."
      },
      {
        "heading": "Espacios vectoriales",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Antecedentes",
        "level": 2,
        "content": "Hasta el siglo XIX, el álgebra lineal se presentaba a través de sistemas de ecuaciones lineales y  matrices. En la matemática moderna, se prefiere generalmente la presentación a través de espacios vectoriales, ya que es más sintética, más general (no se limita al caso de dimensión finita) y conceptualmente más sencilla, aunque más abstracta."
      },
      {
        "heading": "Algunas operaciones básicas",
        "level": 2,
        "content": "Un espacio vectorial sobre un campo F, con frecuencia el campo de los  números reales, es un Conjunto V dotado de dos  operaciones binarias que satisfacen los siguientes axiomas. Los  elementos de V se llaman vectores, y los elementos de F se llaman escalares. \nLa primera operación, la suma de vectores, se expresa de la siguiente manera: tómese dos vectores cualesquiera v y w; la suma tiene como resultado un tercer vector v + w. \nLa segunda operación, multiplicación escalar, se expresa de la siguiente manera: tómese cualquier escalar a y cualquier vector v y produce un nuevo vector av. Los axiomas que deben satisfacer la suma y la multiplicación escalar son los siguientes, siendo en la lista siguiente, u, v y w elementos arbitrarios de V; y a y b son escalares arbitrarios en el campo F.[9]​"
      },
      {
        "heading": "Aplicaciones lineales",
        "level": 2,
        "content": "Las  aplicaciones lineales son  mapeos entre espacios vectoriales que preservan la estructura del espacio vectorial. Dados dos espacios vectoriales V y W sobre un campo F, un mapa lineal, también llamado en algunos contextos, transformación lineal o mapeo lineal, es un mapa o aplicación\n\n  \n    \n      \n        T\n        :\n        V\n        →\n        W\n      \n    \n    {\\displaystyle T:V\\to W}\n  \n\nque es compatible con la suma y la multiplicación escalar, es decir\n\n  \n    \n      \n        T\n        (\n        \n          u\n        \n        +\n        \n          v\n        \n        )\n        =\n        T\n        (\n        \n          u\n        \n        )\n        +\n        T\n        (\n        \n          v\n        \n        )\n        ,\n        \n        T\n        (\n        a\n        \n          v\n        \n        )\n        =\n        a\n        T\n        (\n        \n          v\n        \n        )\n      \n    \n    {\\displaystyle T(\\mathbf {u} +\\mathbf {v} )=T(\\mathbf {u} )+T(\\mathbf {v} ),\\quad T(a\\mathbf {v} )=aT(\\mathbf {v} )}\n  \n\npara los vectores u,v in V y escalares a en F.\nEsto implica que para cualquier vector u, v en V y escalares a, b en F, se tiene\n\n  \n    \n      \n        T\n        (\n        a\n        \n          u\n        \n        +\n        b\n        \n          v\n        \n        )\n        =\n        T\n        (\n        a\n        \n          u\n        \n        )\n        +\n        T\n        (\n        b\n        \n          v\n        \n        )\n        =\n        a\n        T\n        (\n        \n          u\n        \n        )\n        +\n        b\n        T\n        (\n        \n          v\n        \n        )\n      \n    \n    {\\displaystyle T(a\\mathbf {u} +b\\mathbf {v} )=T(a\\mathbf {u} )+T(b\\mathbf {v} )=aT(\\mathbf {u} )+bT(\\mathbf {v} )}\n  \n\nDonde V = W son el mismo espacio vectorial, un mapa lineal \n  \n    \n      \n        T\n        :\n        V\n        →\n        V\n      \n    \n    {\\displaystyle T:V\\to V}\n  \n también se conoce como un operador lineal en V.\nUn mapa lineal biyectivo  entre dos espacios vectoriales, es decir, cada vector del segundo espacio se asocia exactamente con uno en el primero, es un isomorfismo. Dado que un isomorfismo preserva la estructura lineal, dos espacios vectoriales isomorfos son \"esencialmente iguales\" desde el punto de vista del álgebra lineal, en el sentido de que no pueden distinguirse utilizando las propiedades del espacio vectorial. Una cuestión esencial en el álgebra lineal es probar si un mapa lineal es un isomorfismo o no, y, si no es un isomorfismo, encontrar su  rango (o imagen) y el conjunto de elementos que son mapeados al vector cero, llamado el  núcleo del mapa. Todas estas cuestiones pueden resolverse mediante el uso de la eliminación gaussiana o alguna variante de este algoritmo."
      },
      {
        "heading": "Subespacios, intervalo y base",
        "level": 2,
        "content": "El estudio de aquellos subconjuntos de espacios vectoriales que son en sí mismos espacios vectoriales bajo las operaciones inducidas es fundamental, al igual que para muchas estructuras matemáticas. Estos subconjuntos se denominan  subespacios lineales. Más precisamente, un subespacio lineal de un espacio vectorial V sobre un campo F es un subconjunto de W of V tal que u + v y au están en W, para todo u, v en W, y todo a in F. Estas condiciones son suficientes para implicar que W es un espacio vectorial.\nPor ejemplo, dado un campo lineal \n  \n    \n      \n        T\n        :\n        V\n        →\n        W\n      \n    \n    {\\displaystyle T:V\\to W}\n  \n, la  imagen T(V) de V, y la  imagen inversa T−1(0) de 0, llamada  núcleo o kernel, son subespacios lineales de W y V, respectivamente.\nOtra forma importante de formar un subespacio es considerar las combinaciones lineales de un conjunto S de vectores: el conjunto de todas las sumas\n\n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          \n            v\n          \n          \n            1\n          \n        \n        +\n        \n          a\n          \n            2\n          \n        \n        \n          \n            v\n          \n          \n            2\n          \n        \n        +\n        ⋯\n        +\n        \n          a\n          \n            k\n          \n        \n        \n          \n            v\n          \n          \n            k\n          \n        \n        ,\n      \n    \n    {\\displaystyle a_{1}\\mathbf {v} _{1}+a_{2}\\mathbf {v} _{2}+\\cdots +a_{k}\\mathbf {v} _{k},}\n  \n\ndonde v1, v2, …, vk están en S, y a1, a2, ..., ak están en F forman un subespacio lineal llamado Sistema generador de S. El sistema generador de S es también la intersección de todos los subespacios lineales que contienen a S. En otras palabras, es el subespacio lineal, más pequeño para la relación de inclusión, que contiene a S.\nUn conjunto de vectores es  linealmente independiente si ninguno está en el intervalo de los demás. De manera equivalente, un conjunto S de vectores es linealmente independiente si la única forma de expresar el vector cero como una combinación lineal de elementos de S es tomar cero para cada coeficiente \n  \n    \n      \n        \n          a\n          \n            i\n          \n        \n        .\n      \n    \n    {\\displaystyle a_{i}.}\n  \n\nUn conjunto de vectores que abarca un espacio vectorial se denomina conjunto de expansión o sistema generador. Si un conjunto generador S es linealmente dependiente (que no es linealmente independiente), entonces algún elemento w de S es en el lapso de los otros elementos de S , y el lapso seguiría siendo el mismo si uno remove w de S. Se puede continuar eliminando elementos de S hasta obtener un conjunto de expansión linealmente independiente. Un conjunto linealmente independiente que abarca un espacio vectorial V se llama base de V. La importancia de las bases radica en el hecho de que hay juntos grupos electrógenos mínimos y grupos independientes máximos. Más precisamente, si S es un conjunto linealmente independiente y T es un conjunto de expansión tal que \n  \n    \n      \n        S\n        ⊆\n        T\n        ,\n      \n    \n    {\\displaystyle S\\subseteq T,}\n  \n, entonces hay una base B tal que \n  \n    \n      \n        S\n        ⊆\n        B\n        ⊆\n        T\n        .\n      \n    \n    {\\displaystyle S\\subseteq B\\subseteq T.}\n  \n\nSi dos bases cualesquiera de un espacio vectorial V tienen la misma cardinalidad que se llama  dimensión; este es el Teorema de la dimensión de espacios vectoriales. Además, dos espacios vectoriales sobre el mismo campo F son  isomorfos si y solo si tienen la misma dimensión.[11]​\nSi alguna base de V (y por lo tanto cada base) tiene un número finito de elementos, V es un espacio vectorial de dimensión finita. Si U es un subespacio de V, entonces dim U ≤ dim V. En el caso en el que V es de dimensión finita, la igualdad de las dimensiones implica que U = V.\nSi U1 y U2 son subespacios de V , entonces\n\n  \n    \n      \n        dim\n        ⁡\n        (\n        \n          U\n          \n            1\n          \n        \n        +\n        \n          U\n          \n            2\n          \n        \n        )\n        =\n        dim\n        ⁡\n        \n          U\n          \n            1\n          \n        \n        +\n        dim\n        ⁡\n        \n          U\n          \n            2\n          \n        \n        −\n        dim\n        ⁡\n        (\n        \n          U\n          \n            1\n          \n        \n        ∩\n        \n          U\n          \n            2\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle \\dim(U_{1}+U_{2})=\\dim U_{1}+\\dim U_{2}-\\dim(U_{1}\\cap U_{2}),}\n  \n\ndonde \n  \n    \n      \n        \n          U\n          \n            1\n          \n        \n        +\n        \n          U\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle U_{1}+U_{2}}\n  \n denota el lapso de \n  \n    \n      \n        \n          U\n          \n            1\n          \n        \n        ∪\n        \n          U\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle U_{1}\\cup U_{2}.}\n  \n[12]​"
      },
      {
        "heading": "Matrices",
        "level": 1,
        "content": "La matriz es una disposición rectangular de números, símbolos o expresiones, cuyas dimensiones son descritas en las cantidades de filas (usualmente m) por las de columnas (n) que poseen. Las  disposiciones matriciales son particularmente estudiados por el álgebra lineal y son bastante usados en ciencias e ingeniería.\nLas matrices permiten la manipulación explícita de espacios vectoriales de dimensión finita y mapas lineales . Por tanto, su teoría es una parte esencial del álgebra lineal.\nSea V un espacio vectorial de dimensión finita sobre un campo F, y (v1, v2, …, vm) es una base de V, por lo tanto m es la dimensión de V). Por definición, de una base, el mapa\n\n  \n    \n      \n        \n          \n            \n              \n                (\n                \n                  a\n                  \n                    1\n                  \n                \n                ,\n                …\n                ,\n                \n                  a\n                  \n                    m\n                  \n                \n                )\n              \n              \n                \n                ↦\n                \n                  a\n                  \n                    1\n                  \n                \n                \n                  \n                    v\n                  \n                  \n                    1\n                  \n                \n                +\n                ⋯\n                \n                  a\n                  \n                    m\n                  \n                \n                \n                  \n                    v\n                  \n                  \n                    m\n                  \n                \n              \n            \n            \n              \n                \n                  F\n                  \n                    m\n                  \n                \n              \n              \n                \n                →\n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}(a_{1},\\ldots ,a_{m})&\\mapsto a_{1}\\mathbf {v} _{1}+\\cdots a_{m}\\mathbf {v} _{m}\\\\F^{m}&\\to V\\end{aligned}}}\n  \n\nes una  biyección de \n  \n    \n      \n        \n          F\n          \n            m\n          \n        \n        ,\n      \n    \n    {\\displaystyle F^{m},}\n  \n el conjunto de las  secuencias de m elementos de V , sobre la V. Este es un isomorfismo de espacios vectoriales, si \n  \n    \n      \n        \n          F\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle F^{m}}\n  \n está equipado con su estructura estándar de espacio vectorial, donde la suma de vectores y la multiplicación escalar se realizan componente por componente.\nEste isomorfismo permite representar un vector por su imagen inversa bajo este isomorfismo, es decir por las componentes de un vector de coordenadas {\\ Displaystyle (a_ {1}, \\ ldots, a_ {m})}{\\ Displaystyle (a_ {1}, \\ ldots, a_ {m})}o por la matriz de columnas \n  \n    \n      \n        (\n        \n          a\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          a\n          \n            m\n          \n        \n        )\n      \n    \n    {\\displaystyle (a_{1},\\ldots ,a_{m})}\n  \n o mediante la  matriz vertical\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    a\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    a\n                    \n                      m\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\begin{bmatrix}a_{1}\\\\\\vdots \\\\a_{m}\\end{bmatrix}}.}\n  \n\nSi W es otro espacio vectorial de dimensión finita (posiblemente el mismo), con una base \n  \n    \n      \n        (\n        \n          \n            w\n          \n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          \n            w\n          \n          \n            n\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle (\\mathbf {w} _{1},\\ldots ,\\mathbf {w} _{n}),}\n  \n, un mapa lineal f de W a V está bien definido por sus valores en los elementos base, es decir {\\ Displaystyle (f (\\ mathbf {w} _ {1}), \\ ldots, f (\\ mathbf {w} _ {n})).}{\\ Displaystyle (f (\\ mathbf {w} _ {1}), \\ ldots, f (\\ mathbf {w} _ {n})).}Por tanto, f está bien representada por la lista de las matrices de columna correspondientes. Es decir, si\n\n  \n    \n      \n        f\n        (\n        \n          w\n          \n            j\n          \n        \n        )\n        =\n        \n          a\n          \n            1\n            ,\n            j\n          \n        \n        \n          v\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          a\n          \n            m\n            ,\n            j\n          \n        \n        \n          v\n          \n            m\n          \n        \n        ,\n      \n    \n    {\\displaystyle f(w_{j})=a_{1,j}v_{1}+\\cdots +a_{m,j}v_{m},}\n  \n\ncon j = 1, ..., n, entonces f viene representada por una matriz:\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    a\n                    \n                      1\n                      ,\n                      1\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    a\n                    \n                      1\n                      ,\n                      n\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    a\n                    \n                      m\n                      ,\n                      1\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    a\n                    \n                      m\n                      ,\n                      n\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\begin{bmatrix}a_{1,1}&\\cdots &a_{1,n}\\\\\\vdots &\\ddots &\\vdots \\\\a_{m,1}&\\cdots &a_{m,n}\\end{bmatrix}},}\n  \n\ncon m filas y n columnas.\nLa multiplicación de matrices se define de forma que el producto de dos matrices es la matriz de la composición de los mapas lineales correspondientes, y el producto de una matriz y una matriz columna es la matriz columna que representa el resultado de aplicar el mapa lineal representado al vector representado. Se deduce que la teoría de los espacios vectoriales de dimensión finita y la teoría de las matrices son dos lenguajes diferentes para expresar exactamente los mismos conceptos.\nDos matrices que codifican la misma transformación lineal en bases diferentes se llaman  matrices similares. Se puede demostrar que dos matrices son similares si y solo si se puede transformar una en la otra mediante operaciones elementales de filas y columnas. Para una matriz que representa un mapa lineal de W a V, las operaciones de fila corresponden a cambio de bases en V y las operaciones de columna corresponden a cambio de bases en W. Toda matriz es similar a una matriz identidad  bordeada por filas y columnas nulas. En términos de espacios vectoriales, esto significa que, para cualquier mapa lineal de W a V, hay bases tales que una parte de la base de W se mapea biyectivamente en una parte de la base de V, y que los elementos restantes de la base de W, si los hay, se mapean a cero. La eliminación gaussiana es el algoritmo básico para encontrar estas operaciones elementales y demostrar estos resultados."
      },
      {
        "heading": "Sistemas lineales",
        "level": 1,
        "content": "Un conjunto finito de ecuaciones lineales en un conjunto finito de variables, por ejemplo \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},x_{2},\\ldots ,x_{n}}\n  \n or \n  \n    \n      \n        x\n        ,\n        y\n        ,\n        …\n        ,\n        z\n      \n    \n    {\\displaystyle x,y,\\ldots ,z}\n  \n  se llama sistema de ecuaciones lineales o sistema lineal .[13]​[14]​[15]​[16]​[17]​\nLos sistemas de ecuaciones lineales constituyen una parte fundamental del álgebra lineal. Históricamente, el álgebra lineal y la teoría de matrices se han desarrollado para resolver dichos sistemas. En la presentación moderna del álgebra lineal mediante espacios vectoriales y matrices, muchos problemas pueden interpretarse en términos de sistemas lineales.\nPor ejemplo, \n\nes un sistema lineal.\nA dicho sistema se le puede asociar su matriz \n\n  \n    \n      \n        M\n        =\n        \n          [\n          \n            \n              \n                \n                  2\n                \n                \n                  1\n                \n                \n                  −\n                  1\n                \n              \n              \n                \n                  −\n                  3\n                \n                \n                  −\n                  1\n                \n                \n                  2\n                \n              \n              \n                \n                  −\n                  2\n                \n                \n                  1\n                \n                \n                  2\n                \n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle M=\\left[{\\begin{array}{rrr}2&1&-1\\\\-3&-1&2\\\\-2&1&2\\end{array}}\\right]}\n  \n\ny su vector de miembro derecho\n\n  \n    \n      \n        \n          v\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  8\n                \n              \n              \n                \n                  −\n                  11\n                \n              \n              \n                \n                  −\n                  3\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {v} ={\\begin{bmatrix}8\\\\-11\\\\-3\\end{bmatrix}}.}\n  \n\nSea T la transformación lineal asociada a la matriz M. Una solución del sistema (S) es un vector\n\n  \n    \n      \n        \n          X\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  x\n                \n              \n              \n                \n                  y\n                \n              \n              \n                \n                  z\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {X} ={\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}}}\n  \n\ntal que\n\n  \n    \n      \n        T\n        (\n        \n          X\n        \n        )\n        =\n        \n          v\n        \n        ,\n      \n    \n    {\\displaystyle T(\\mathbf {X} )=\\mathbf {v} ,}\n  \n\nque es un elemento de la imagen inversa de v por T.\nSea (S') el sistema homogéneo asociado, donde los lados derechos de las ecuaciones se ponen a cero:\n\nLas soluciones de (S') son exactamente los elementos del  núcleo de T o, equivalentemente, M.\nLa eliminación gaussiana consiste en realizar  operaciones elementales de filas en la matriz aumentada.\n\n  \n    \n      \n        \n          [\n          \n            \n            \n              \n                \n                  \n                    M\n                  \n                  \n                    \n                      v\n                    \n                  \n                \n              \n            \n            \n          \n          ]\n        \n        =\n        \n          [\n          \n            \n              \n                \n                  2\n                \n                \n                  1\n                \n                \n                  −\n                  1\n                \n                \n                  8\n                \n              \n              \n                \n                  −\n                  3\n                \n                \n                  −\n                  1\n                \n                \n                  2\n                \n                \n                  −\n                  11\n                \n              \n              \n                \n                  −\n                  2\n                \n                \n                  1\n                \n                \n                  2\n                \n                \n                  −\n                  3\n                \n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\left[\\!{\\begin{array}{c|c}M&\\mathbf {v} \\end{array}}\\!\\right]=\\left[{\\begin{array}{rrr|r}2&1&-1&8\\\\-3&-1&2&-11\\\\-2&1&2&-3\\end{array}}\\right]}\n  \n\npara ponerlo en  forma escalonada reducida. Estas operaciones de fila no cambian el conjunto de soluciones del sistema de ecuaciones. En el ejemplo, la forma escalonada reducida es\n\n  \n    \n      \n        \n          [\n          \n            \n            \n              \n                \n                  \n                    M\n                  \n                  \n                    \n                      v\n                    \n                  \n                \n              \n            \n            \n          \n          ]\n        \n        =\n        \n          [\n          \n            \n              \n                \n                  1\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  2\n                \n              \n              \n                \n                  0\n                \n                \n                  1\n                \n                \n                  0\n                \n                \n                  3\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  1\n                \n                \n                  −\n                  1\n                \n              \n            \n          \n          ]\n        \n        ,\n      \n    \n    {\\displaystyle \\left[\\!{\\begin{array}{c|c}M&\\mathbf {v} \\end{array}}\\!\\right]=\\left[{\\begin{array}{rrr|r}1&0&0&2\\\\0&1&0&3\\\\0&0&1&-1\\end{array}}\\right],}\n  \n\nmostrando\n\n  \n    \n      \n        \n          \n            \n              \n                x\n              \n              \n                \n                =\n                2\n              \n            \n            \n              \n                y\n              \n              \n                \n                =\n                3\n              \n            \n            \n              \n                z\n              \n              \n                \n                =\n                −\n                1.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}x&=2\\\\y&=3\\\\z&=-1.\\end{aligned}}}\n  \n\nDe esta interpretación matricial de los sistemas lineales se deduce que los mismos métodos pueden aplicarse para resolver sistemas lineales y para muchas operaciones sobre matrices y transformaciones lineales, que incluyen el cálculo del rangos, núcleos, y matriz inversa que el sistema (S) tiene la solución única\n\n  \n    \n      \n        \n          \n            \n              \n                x\n              \n              \n                \n                =\n                2\n              \n            \n            \n              \n                y\n              \n              \n                \n                =\n                3\n              \n            \n            \n              \n                z\n              \n              \n                \n                =\n                −\n                1.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}x&=2\\\\y&=3\\\\z&=-1.\\end{aligned}}}\n  \n\nDe esta interpretación matricial de los sistemas lineales se deduce que los mismos métodos pueden aplicarse para resolver sistemas lineales y para muchas operaciones sobre matrices y transformaciones lineales, que incluyen el cálculo del rangos, núcleos y matriz inversa."
      },
      {
        "heading": "Endomorfismos y matrices cuadradas",
        "level": 1,
        "content": "Un endomorfismo lineal es un mapa lineal que mapea un espacio vectorial V a sí mismo. Si V tiene una base de n elementos, tal endomorfismo se representa mediante una matriz cuadrada de tamaño n.\nCon respecto a los mapas lineales generales, los endomorfismos lineales y las matrices cuadradas tienen algunas propiedades específicas que hacen que su estudio sea una parte importante del álgebra lineal, que se usa en muchas partes de las matemáticas, incluidas las  transformaciones geométricas , los  cambios de coordenadas, las  formas cuadráticas y muchas otras. de las matemáticas."
      },
      {
        "heading": "Determinantes",
        "level": 2,
        "content": "El determinante de una matriz cuadrada A se define como[18]​ \n\n  \n    \n      \n        \n          ∑\n          \n            σ\n            ∈\n            \n              S\n              \n                n\n              \n            \n          \n        \n        (\n        −\n        1\n        \n          )\n          \n            σ\n          \n        \n        \n          a\n          \n            1\n            σ\n            (\n            1\n            )\n          \n        \n        ⋯\n        \n          a\n          \n            n\n            σ\n            (\n            n\n            )\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\sum _{\\sigma \\in S_{n}}(-1)^{\\sigma }a_{1\\sigma (1)}\\cdots a_{n\\sigma (n)},}\n  \n\ndonde \n\n  \n    \n      \n        \n          S\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle S_{n}}\n  \n es el grupo de todas las permutaciones de n elementos,\n\n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n es una permutación y\ny \n  \n    \n      \n        (\n        −\n        1\n        \n          )\n          \n            σ\n          \n        \n      \n    \n    {\\displaystyle (-1)^{\\sigma }}\n  \n la paridad de la permutación.\nUna matriz es invertible si y solo si el determinante es invertible (es decir, distinto de cero si los escalares pertenecen a un campo).\nLa regla de Cramer es una  expresión de forma cerrada, en términos de determinantes, de la solución de un sistema de n ecuaciones lineales en n incógnitas. La regla de Cramer es útil para razonar sobre la solución, pero, excepto para n = 2 o 3, rara vez se utiliza para calcular una solución, ya que la eliminación gaussiana es un algoritmo más rápido.\nEl determinante de un endomorfismo es el determinante de la matriz que representa el endomorfismo en términos de alguna base ordenada. Esta definición tiene sentido, ya que este determinante es independiente de la elección de la base."
      },
      {
        "heading": "Valores propios y vectores propios",
        "level": 2,
        "content": "Si f es un endomorfismo lineal de un espacio vectorial V sobre un campo F, un vector propio de f es un vector v de V no nulo tal que f(v) = av para algún escalar a en F. Este escalar a es un valor propio de f.\nSi la dimensión de V es finita, y se ha elegido una base, f y v pueden representarse, respectivamente, por una matriz cuadrada M y una matriz de columnas z; la ecuación que define los vectores y valores propios se convierte en\n\n  \n    \n      \n        M\n        z\n        =\n        a\n        z\n        .\n      \n    \n    {\\displaystyle Mz=az.}\n  \n\nUtilizando la matriz identidad I, cuyas entradas son todas cero, excepto las de la diagonal principal, que son iguales a uno, esto puede reescribirse\n\n  \n    \n      \n        (\n        M\n        −\n        a\n        I\n        )\n        z\n        =\n        0.\n      \n    \n    {\\displaystyle (M-aI)z=0.}\n  \n\nComo se supone que z es distinto de cero, esto significa que M - aI es una matriz singular, y por tanto que su determinante \n  \n    \n      \n        det\n        (\n        M\n        −\n        a\n        I\n        )\n      \n    \n    {\\displaystyle \\det(M-aI)}\n  \n es igual a cero. Los valores propios son, pues, la raíces del polinomio\n\n  \n    \n      \n        det\n        (\n        x\n        I\n        −\n        M\n        )\n        .\n      \n    \n    {\\displaystyle \\det(xI-M).}\n  \n\nSi V es de dimensión n, se trata de un polinomio mónico de grado n, llamado polinomio característico de la matriz (o del endomorfismo), y hay, como máximo, n valores propios.\nSi existe una base que consiste solo en vectores propios, la matriz de f en esta base tiene una estructura muy simple: es una matriz diagonal tal que las entradas en la diagonal principal son valores propios, y las otras entradas son cero. En este caso, se dice que el endomorfismo y la matriz son diagonalizable. De forma más general, un endomorfismo y una matriz también se dicen diagonalizables, si se convierten en diagonalizables después de extender el campo de escalares. En este sentido extendido, si el polinomio característico es square-free, entonces la matriz es diagonalizable.\nUna matriz simétrica es siempre diagonalizable. Existen matrices no diagonalizables, siendo la más sencilla\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  0\n                \n                \n                  1\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}}}\n  \n\n(no puede ser diagonalizable ya que su cuadrado es la matriz cero, y el cuadrado de una matriz diagonal no nula nunca es cero).\nCuando un endomorfismo no es diagonalizable, hay bases en las que tiene una forma simple, aunque no tan simple como la forma diagonal. La forma normal de Frobenius no necesita extender el campo de escalares y hace que el polinomio característico sea inmediatamente legible sobre la matriz. La forma normal de Jordan requiere extender el campo de escalares para contener todos los valores propios, y difiere de la forma diagonal solo por algunas entradas que están justo encima de la diagonal principal y son iguales a 1."
      },
      {
        "heading": "Dualidad",
        "level": 1,
        "content": "Una forma lineal es un mapa lineal desde un espacio vectorial \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n sobre un campo \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  \n al campo de escalares \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  \n, visto como un espacio vectorial sobre sí mismo. Equipadas por la adición puntual y la multiplicación por un escalar, las formas lineales forman un espacio vectorial, llamado espacio dual de \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n, y usualmente denotado \n  \n    \n      \n        \n          V\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle V^{*}}\n  \n(Katznelson, Katznelson y 2008,  p. 37 §2.1.3) o \n  \n    \n      \n        \n          V\n          ′\n        \n      \n    \n    {\\displaystyle V'}\n  \n.(Halmos y 1974,  p. 20, §13)(Axler y 2015,  p. 101, §3.94)"
      },
      {
        "heading": "Mapa lineal de un espacio vectorial a su campo de escalares",
        "level": 2,
        "content": "Si \n  \n    \n      \n        \n          \n            v\n          \n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          \n            v\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {v} _{1},\\ldots ,\\mathbf {v} _{n}}\n  \n es una base de \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n (esto implica que V es de dimensión finita), entonces se puede definir, para i = 1, . .., n, un mapa lineal \n  \n    \n      \n        \n          v\n          \n            i\n          \n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle v_{i}^{*}}\n  \n tal que \n  \n    \n      \n        \n          v\n          \n            i\n          \n          \n            ∗\n          \n        \n        (\n        \n          \n            e\n          \n          \n            i\n          \n        \n        )\n        =\n        1\n      \n    \n    {\\displaystyle v_{i}^{*}(\\mathbf {e} _{i})=1}\n  \n y \n  \n    \n      \n        \n          v\n          \n            i\n          \n          \n            ∗\n          \n        \n        (\n        \n          \n            e\n          \n          \n            j\n          \n        \n        )\n        =\n        0\n      \n    \n    {\\displaystyle v_{i}^{*}(\\mathbf {e} _{j})=0}\n  \n si j ≠ i. Estos mapas lineales forman una base de \n  \n    \n      \n        \n          V\n          \n            ∗\n          \n        \n        ,\n      \n    \n    {\\displaystyle V^{*},}\n  \n llamada la base dual de \n  \n    \n      \n        \n          \n            v\n          \n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          \n            v\n          \n          \n            n\n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {v} _{1},\\ldots ,\\mathbf {v} _{n}.}\n  \n (Si V no es de dimensión finita, los \n  \n    \n      \n        \n          v\n          \n            i\n          \n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle v_{i}^{*}}\n  \n pueden definirse de forma similar; son linealmente independientes, pero no forman una base).\nPara \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  \n en \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n, el mapa\n\n  \n    \n      \n        f\n        →\n        f\n        (\n        \n          v\n        \n        )\n      \n    \n    {\\displaystyle f\\to f(\\mathbf {v} )}\n  \n\nes una forma lineal en \n  \n    \n      \n        \n          V\n          \n            ∗\n          \n        \n        .\n      \n    \n    {\\displaystyle V^{*}.}\n  \n Esto define el mapa lineal canónico de \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n en \n  \n    \n      \n        \n          V\n          \n            ∗\n            ∗\n          \n        \n        ,\n      \n    \n    {\\displaystyle V^{**},}\n  \n el dual de \n  \n    \n      \n        \n          V\n          \n            ∗\n          \n        \n        ,\n      \n    \n    {\\displaystyle V^{*},}\n  \n llamado el bidual of \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n. Este mapa canónico es un isomorfismo si \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n es finito-dimensional, y esto permite identificar \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n con su bidual.\nExiste, pues, una simetría completa entre un espacio vectorial de dimensión finita y su dual. Esto motiva el uso frecuente, en este contexto, de la notación bra-ket\n\n  \n    \n      \n        ⟨\n        f\n        ,\n        \n          x\n        \n        ⟩\n      \n    \n    {\\displaystyle \\langle f,\\mathbf {x} \\rangle }\n  \n  para denotar  \n  \n    \n      \n        f\n        (\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle f(\\mathbf {x} )}\n  \n."
      },
      {
        "heading": "Mapa dual",
        "level": 2,
        "content": "Dejemos que \n\n  \n    \n      \n        f\n        :\n        V\n        →\n        W\n      \n    \n    {\\displaystyle f:V\\to W}\n  \n\nsea un mapa lineal. Para toda forma lineal h sobre W, la función compuesta h ∘ f es una forma lineal sobre V. Esto define un mapa lineal\n\n  \n    \n      \n        \n          f\n          \n            ∗\n          \n        \n        :\n        \n          W\n          \n            ∗\n          \n        \n        →\n        \n          V\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle f^{*}:W^{*}\\to V^{*}}\n  \n\nentre los espacios duales, que se llama el dual o la transposición' de f.\nSi V y W son de dimensión finita, y M es la matriz de f en términos de algunas bases ordenadas, entonces la matriz de \n  \n    \n      \n        \n          f\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle f^{*}}\n  \n sobre las bases duales es la transposición \n  \n    \n      \n        \n          M\n          \n            \n              T\n            \n          \n        \n      \n    \n    {\\displaystyle M^{\\mathsf {T}}}\n  \n de M, obtenida intercambiando filas y columnas.\nSi los elementos de los espacios vectoriales y sus duales se representan mediante vectores columna, esta dualidad puede expresarse en notación bra-ket mediante \n\n  \n    \n      \n        ⟨\n        \n          h\n          \n            \n              T\n            \n          \n        \n        ,\n        M\n        \n          v\n        \n        ⟩\n        =\n        ⟨\n        \n          h\n          \n            \n              T\n            \n          \n        \n        M\n        ,\n        \n          v\n        \n        ⟩\n        .\n      \n    \n    {\\displaystyle \\langle h^{\\mathsf {T}},M\\mathbf {v} \\rangle =\\langle h^{\\mathsf {T}}M,\\mathbf {v} \\rangle .}\n  \n\nPara resaltar esta simetría, los dos miembros de esta igualdad se escriben a veces \n\n  \n    \n      \n        ⟨\n        \n          h\n          \n            \n              T\n            \n          \n        \n        ∣\n        M\n        ∣\n        \n          v\n        \n        ⟩\n        .\n      \n    \n    {\\displaystyle \\langle h^{\\mathsf {T}}\\mid M\\mid \\mathbf {v} \\rangle .}"
      },
      {
        "heading": "Espacios vectoriales de uso común",
        "level": 1,
        "content": "Dentro de los espacios vectoriales de dimensión finita, son de amplio uso los dos tipos siguientes de espacios vectoriales:"
      },
      {
        "heading": "Vectores en Rn",
        "level": 2,
        "content": "Este espacio vectorial está formado por el conjunto de vectores de n dimensiones (es decir con n número de componentes). Podemos encontrar un ejemplo de ellos en los vectores R2, que son famosos por representar las coordenadas cartesianas: (2,3), (3,4),..."
      },
      {
        "heading": "Espacio vectorial de polinomios en una misma variable",
        "level": 2,
        "content": "Un ejemplo de espacio vectorial está dado por todos los polinomios cuyo grado es menor o igual a 2 con coeficientes reales sobre una variable x.\nEjemplos de tales polinomios son:\n\n  \n    \n      \n        4\n        \n          x\n          \n            2\n          \n        \n        −\n        5\n        x\n        +\n        1\n        ,\n        \n        \n          \n            \n              2\n              \n                x\n                \n                  2\n                \n              \n            \n            7\n          \n        \n        −\n        3\n        ,\n        \n        8\n        x\n        +\n        4\n        ,\n        \n        5\n      \n    \n    {\\displaystyle 4x^{2}-5x+1,\\quad {\\frac {2x^{2}}{7}}-3,\\quad 8x+4,\\quad 5}\n  \n\nLa suma de dos polinomios cuyo grado no excede a 2 es otro polinomio cuyo grado no excede a 2:\n\n  \n    \n      \n        (\n        3\n        \n          x\n          \n            2\n          \n        \n        −\n        5\n        x\n        +\n        1\n        )\n        +\n        (\n        4\n        x\n        −\n        8\n        )\n        =\n        3\n        \n          x\n          \n            2\n          \n        \n        −\n        x\n        −\n        7\n      \n    \n    {\\displaystyle (3x^{2}-5x+1)+(4x-8)=3x^{2}-x-7}\n  \n\nEl campo de escalares es naturalmente el de los números reales, y es posible multiplicar un número por un polinomio:\n\n  \n    \n      \n        5\n        ⋅\n        (\n        2\n        x\n        +\n        3\n        )\n        =\n        10\n        x\n        +\n        15\n      \n    \n    {\\displaystyle 5\\cdot (2x+3)=10x+15}\n  \n\ndonde el resultado nuevamente es un polinomio (es decir, un vector).\nUn ejemplo de transformación lineal es el operador derivada D, que asigna a cada polinomio el resultado de derivarlo:\n\n  \n    \n      \n        D\n        (\n        3\n        \n          x\n          \n            2\n          \n        \n        −\n        5\n        x\n        +\n        7\n        )\n        =\n        6\n        x\n        −\n        5.\n      \n    \n    {\\displaystyle D(3x^{2}-5x+7)=6x-5.}\n  \n\nEl operador derivada satisface las condiciones de linealidad, y aunque es posible demostrarlo con rigor, simplemente lo ilustramos con un ejemplo la primera condición de linealidad:\n\n  \n    \n      \n        D\n        (\n        (\n        4\n        \n          x\n          \n            2\n          \n        \n        +\n        5\n        x\n        −\n        3\n        )\n        +\n        (\n        \n          x\n          \n            2\n          \n        \n        −\n        x\n        −\n        1\n        )\n        )\n        =\n        D\n        (\n        5\n        \n          x\n          \n            2\n          \n        \n        +\n        4\n        x\n        −\n        4\n        )\n        =\n        10\n        x\n        +\n        4\n      \n    \n    {\\displaystyle D((4x^{2}+5x-3)+(x^{2}-x-1))=D(5x^{2}+4x-4)=10x+4}\n  \n\ny por otro lado:\n\n  \n    \n      \n        D\n        (\n        4\n        \n          x\n          \n            2\n          \n        \n        +\n        5\n        x\n        −\n        3\n        )\n        +\n        D\n        (\n        \n          x\n          \n            2\n          \n        \n        −\n        x\n        −\n        1\n        )\n        =\n        (\n        8\n        x\n        +\n        5\n        )\n        +\n        (\n        2\n        x\n        −\n        1\n        )\n        =\n        10\n        x\n        +\n        4.\n      \n    \n    {\\displaystyle D(4x^{2}+5x-3)+D(x^{2}-x-1)=(8x+5)+(2x-1)=10x+4.}\n  \n\nCualquier espacio vectorial tiene una representación en coordenadas similar a  \n  \n    \n      \n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{n}}\n  \n, lo cual se obtiene mediante la elección de una base (álgebra) (es decir, un conjunto especial de vectores), y uno de los temas recurrentes en el álgebra lineal es la elección de bases apropiadas para que los vectores de coordenadas y las matrices que representan las transformaciones lineales tengan formas sencillas o propiedades específicas."
      },
      {
        "heading": "Generalización y temas relacionados",
        "level": 1,
        "content": "Puesto que el álgebra lineal es una teoría muy exitosa, sus métodos se han proliferado por otras áreas de la matemática: en la teoría de módulos, que reemplaza al cuerpo en los escalares por un anillo; en el álgebra multilineal, uno lidia con 'múltiples variables' en un problema de mapeo lineal, en el que cada número de las diferentes variables se dirige al concepto de tensor, e incluso en el ámbito de la programación ya que hoy en día la indexación de páginas web se basa en métodos del álgebra lineal;[19]​ en la teoría del espectro de los operadores de control de matrices de dimensión infinita, aplicando el análisis matemático en una teoría que no es puramente algebraica. En todos estos casos las dificultades técnicas son mucho más grandes."
      },
      {
        "heading": "Relación con la geometría",
        "level": 1,
        "content": "Existe una fuerte relación entre el álgebra lineal y la geometría, que comenzó con la introducción por René Descartes, en 1637, de las coordenadas cartesianas. En esta nueva (en ese momento) geometría, ahora llamada geometría cartesiana, los puntos se representan mediante coordenadas cartesianas, que son secuencias de tres números reales (en el caso del espacio tridimensional habitual). Los objetos básicos de la geometría, que son las líneas y los planos se representan mediante ecuaciones lineales. Por tanto, calcular las intersecciones de líneas y planos equivale a resolver sistemas de ecuaciones lineales. Esta fue una de las principales motivaciones para desarrollar el álgebra lineal.\nLa mayoría de las transformaciones geométricas, como las traslaciones, rotaciones, reflexiones, movimientos rígidos, isometrías y proyecciones transforman líneas en líneas. De ello se deduce que se pueden definir, especificar y estudiar en términos de mapas lineales. Este es también el caso de las homografías y las transformaciones de Möbius, cuando se consideran como transformaciones de un espacio proyectivo.\nHasta finales del siglo XIX, los espacios geométricos se definían mediante axiomas que relacionaban puntos, líneas y planos (geometría sintética). Alrededor de esta fecha, apareció que también se pueden definir los espacios geométricos mediante construcciones que implican espacios vectoriales (véase, por ejemplo, Espacio proyectivo y Espacio afín). Se ha demostrado que los dos enfoques son esencialmente equivalentes.[20]​ En la geometría clásica, los espacios vectoriales implicados son espacios vectoriales sobre los reales, pero las construcciones pueden extenderse a espacios vectoriales sobre cualquier campo, permitiendo considerar la geometría sobre campos arbitrarios, incluyendo campos finitos.\nActualmente, la mayoría de los libros de texto introducen los espacios geométricos desde el álgebra lineal, y la geometría se presenta a menudo, a nivel elemental, como un subcampo del álgebra lineal."
      },
      {
        "heading": "Utilización y aplicaciones",
        "level": 1,
        "content": "El álgebra lineal se utiliza en casi todas las áreas de las matemáticas, por lo que es relevante en casi todos los ámbitos científicos que utilizan las matemáticas. Estas aplicaciones pueden dividirse en varias categorías amplias."
      },
      {
        "heading": "Geometría del espacio ambiental",
        "level": 2,
        "content": "El modelo del espacio ambiental se basa en la geometría. Las ciencias que se ocupan de este espacio utilizan ampliamente la geometría. Es el caso de la mecánica y la robótica, para describir la dinámica de cuerpos rígidos; la geodesia para describir la forma de la Tierra; la perspectiva, la Visión artificial y los gráficos por ordenador, para describir la relación entre una escena y su representación en el plano; y muchos otros dominios científicos.\nEn todas estas aplicaciones, la geometría sintética suele utilizarse para descripciones generales y un enfoque cualitativo, pero para el estudio de situaciones explícitas, hay que calcular con coordenadas. Esto requiere el uso intensivo del álgebra lineal."
      },
      {
        "heading": "Análisis funcional",
        "level": 2,
        "content": "El Análisis funcional estudia los espacios de funciones. Estos son espacios vectoriales con estructura adicional, como los espacios de Hilbert. El álgebra lineal es, por tanto, una parte fundamental del análisis funcional y sus aplicaciones, que incluyen, en particular, la mecánica cuántica (función de onda)."
      },
      {
        "heading": "Estudio de sistemas complejos",
        "level": 2,
        "content": "La mayoría de los fenómenos físicos se modelan mediante ecuaciones diferenciales parciales. Para resolverlas, se suele descomponer el espacio en el que se buscan las soluciones en pequeñas células que interactúan entre sí. Para los  sistemas lineales esta interacción implica función lineal. Para  sistemas no lineales, esta interacción suele aproximarse mediante funciones lineales.[21]​ En ambos casos, suelen intervenir matrices muy grandes. Un ejemplo típico es la predicción meteorológica, en la que toda la atmósfera de la Tierra se divide en celdas de, por ejemplo, 100 km de ancho y 100 m de alto."
      },
      {
        "heading": "Cálculo científico",
        "level": 2,
        "content": "Casi todos los  cálculos científicos implican álgebra lineal. En consecuencia, los algoritmos de álgebra lineal han sido altamente optimizados. Los BLAS y LAPACK son las implementaciones más conocidas. Para mejorar la eficiencia, algunas de ellas configuran los algoritmos automáticamente, en tiempo de ejecución, para adaptarlos a las especificidades del ordenador (caché tamaño, número de núcleos disponible, ...).\nAlgunos procesador, normalmente Unidad de procesamiento gráfico (GPU), están diseñados con una estructura matricial, para optimizar las operaciones de álgebra lineal."
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": "Portal:Matemática. Contenido relacionado con Matemática."
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Fuentes principales",
        "level": 1,
        "content": "Anton, Howard (1987), Elementary Linear Algebra (5th edición), New York: Wiley, ISBN 0-471-84819-0 .\nAxler, Sheldon (2015), Linear Algebra Done Right, Undergraduate Texts in Mathematics (3rd edición), Springer Publishing, ISBN 978-3-319-11079-0 .\nBeauregard, Raymond A.; Fraleigh, John B. (1973), A First Course In Linear Algebra: with Optional Introduction to Groups, Rings, and Fields, Boston: Houghton Mifflin Company, ISBN 0-395-14017-X, (requiere registro) .\nBurden, Richard L.; Faires, J. Douglas (1993), Numerical Analysis (5th edición), Boston: Prindle, Weber and Schmidt, ISBN 0-534-93219-3, (requiere registro) .\nGolub, Gene H.; Van Loan, Charles F. (1996), Matrix Computations, Johns Hopkins Studies in Mathematical Sciences (3rd edición), Baltimore: Johns Hopkins University Press, ISBN 978-0-8018-5414-9 .\nHalmos, Paul Richard (1974), Finite-Dimensional Vector Spaces, Undergraduate Texts in Mathematics (1958 2nd edición), Springer Publishing, ISBN 0-387-90093-4 .\nHarper, Charlie (1976), Introduction to Mathematical Physics, New Jersey: Prentice-Hall, ISBN 0-13-487538-9 .\nKatznelson, Yitzhak; Katznelson, Yonatan R. (2008), A (Terse) Introduction to Linear Algebra, American Mathematical Society, ISBN 978-0-8218-4419-9 .\nRoman, Steven (22 de marzo de 2005), Advanced Linear Algebra, Graduate Texts in Mathematics (2nd edición), Springer, ISBN 978-0-387-24766-3 ."
      },
      {
        "heading": "Bibliografía",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Historia",
        "level": 2,
        "content": "Fearnley-Sander, Desmond, \"Hermann Grassmann and the Creation of Linear Algebra\", American Mathematical Monthly 86 (1979), pp. 809–817.\nGrassmann, Hermann (1844), Die lineale Ausdehnungslehre ein neuer Zweig der Mathematik: dargestellt und durch Anwendungen auf die übrigen Zweige der Mathematik, wie auch auf die Statik, Mechanik, die Lehre vom Magnetismus und die Krystallonomie erläutert, Leipzig: O. Wigand ."
      },
      {
        "heading": "Libros de texto introductorios",
        "level": 2,
        "content": "Anton, Howard (2005), Elementary Linear Algebra (Applications Version) (9th edición), Wiley International .\nBanerjee, Sudipto; Roy, Anindya (2014), Linear Algebra and Matrix Analysis for Statistics, Texts in Statistical Science (1st edición), Chapman and Hall/CRC, ISBN 978-1420095388 .\nBretscher, Otto (2004), Linear Algebra with Applications (3rd edición), Prentice Hall, ISBN 978-0-13-145334-0 .\nFarin, Gerald; Hansford, Dianne (2004), Practical Linear Algebra: A Geometry Toolbox, AK Peters, ISBN 978-1-56881-234-2 .\nKolman, Bernard; Hill, David R. (2007), Elementary Linear Algebra with Applications (9th edición), Prentice Hall, ISBN 978-0-13-229654-0 .\nLay, David C. (2005), Linear Algebra and Its Applications (3rd edición), Addison Wesley, ISBN 978-0-321-28713-7 .\nLeon, Steven J. (2006), Linear Algebra With Applications (7th edición), Pearson Prentice Hall, ISBN 978-0-13-185785-8, (requiere registro) .\nMurty, Katta G. (2014) Computational and Algorithmic Linear Algebra and n-Dimensional Geometry, World Scientific Publishing, ISBN 978-981-4366-62-5}}. Chapter 1: Systems of Simultaneous Linear Equations\nPoole, David (2010), Linear Algebra: A Modern Introduction (3rd edición), Cengage – Brooks/Cole, ISBN 978-0-538-73545-2 .\nRicardo, Henry (2010), A Modern Introduction To Linear Algebra (1st edición), CRC Press, ISBN 978-1-4398-0040-9 .\nSadun, Lorenzo (2008), Applied Linear Algebra: the decoupling principle (2nd edición), AMS, ISBN 978-0-8218-4441-0 .\nStrang, Gilbert (2016), Introduction to Linear Algebra (5th edición), Wellesley-Cambridge Press, ISBN 978-09802327-7-6 .\nThe Manga Guide to Linear Algebra (2012), by Shin Takahashi, Iroha Inoue and Trend-Pro Co., Ltd., ISBN  978-1-59327-413-9"
      },
      {
        "heading": "Libros de texto avanzados",
        "level": 2,
        "content": "Bhatia, Rajendra (15 de noviembre de 1996), Matrix Analysis, Graduate Texts in Mathematics, Springer, ISBN 978-0-387-94846-1 .\nDemmel, James W. (1 de agosto de 1997), Applied Numerical Linear Algebra, SIAM, ISBN 978-0-89871-389-3 .\nDym, Harry (2007), Linear Algebra in Action, AMS, ISBN 978-0-8218-3813-6 .\nGantmacher, Felix R. (2005), Applications of the Theory of Matrices, Dover Publications, ISBN 978-0-486-44554-0 .\nGantmacher, Felix R. (1990), Matrix Theory Vol. 1 (2nd edición), American Mathematical Society, ISBN 978-0-8218-1376-8 .\nGantmacher, Felix R. (2000), Matrix Theory Vol. 2 (2nd edición), American Mathematical Society, ISBN 978-0-8218-2664-5 .\nGelfand, Israel M. (1989), Lectures on Linear Algebra, Dover Publications, ISBN 978-0-486-66082-0 .\nGlazman, I. M.; Ljubic, Ju. I. (2006), Finite-Dimensional Linear Analysis, Dover Publications, ISBN 978-0-486-45332-3 .\nGolan, Johnathan S. (January 2007), The Linear Algebra a Beginning Graduate Student Ought to Know (2nd edición), Springer, ISBN 978-1-4020-5494-5 .\nGolan, Johnathan S. (August 1995), Foundations of Linear Algebra, Kluwer, ISBN 0-7923-3614-3 .\nGreub, Werner H. (16 de octubre de 1981), Linear Algebra, Graduate Texts in Mathematics (4th edición), Springer, ISBN 978-0-8018-5414-9 .\nHoffman, Kenneth; Kunze, Ray (1971), Linear algebra (2nd edición), Englewood Cliffs, N.J.: Prentice-Hall, Inc., MR 0276251 .\nHalmos, Paul R. (20 de agosto de 1993), Finite-Dimensional Vector Spaces, Undergraduate Texts in Mathematics, Springer, ISBN 978-0-387-90093-3 .\nFriedberg, Stephen H.; Insel, Arnold J.; Spence, Lawrence E. (7 de septiembre de 2018), Linear Algebra (5th edición), Pearson, ISBN 978-0-13-486024-4 .\nHorn, Roger A.; Johnson, Charles R. (23 de febrero de 1990), Matrix Analysis, Cambridge University Press, ISBN 978-0-521-38632-6 .\nHorn, Roger A.; Johnson, Charles R. (24 de junio de 1994), Topics in Matrix Analysis, Cambridge University Press, ISBN 978-0-521-46713-1 .\nLang, Serge (9 de marzo de 2004), Linear Algebra, Undergraduate Texts in Mathematics (3rd edición), Springer, ISBN 978-0-387-96412-6 .\nMarcus, Marvin; Minc, Henryk (2010), A Survey of Matrix Theory and Matrix Inequalities, Dover Publications, ISBN 978-0-486-67102-4 .\nMeyer, Carl D. (15 de febrero de 2001), Matrix Analysis and Applied Linear Algebra, Society for Industrial and Applied Mathematics (SIAM), ISBN 978-0-89871-454-8, archivado desde el original el 31 de octubre de 2009 .\nMirsky, L. (1990), An Introduction to Linear Algebra, Dover Publications, ISBN 978-0-486-66434-7 .\nShafarevich, I. R.; Remizov, A. O (2012), Linear Algebra and Geometry, Springer, ISBN 978-3-642-30993-9 .\nShilov, Georgi E. (1 de junio de 1977), Linear algebra, Dover Publications, ISBN 978-0-486-63518-7 .\nShores, Thomas S. (6 de diciembre de 2006), Applied Linear Algebra and Matrix Analysis, Undergraduate Texts in Mathematics, Springer, ISBN 978-0-387-33194-2 .\nSmith, Larry (28 de mayo de 1998), Linear Algebra, Undergraduate Texts in Mathematics, Springer, ISBN 978-0-387-98455-1 .\nTrefethen, Lloyd N.; Bau, David (1997), Numerical Linear Algebra, SIAM, ISBN 978-0-898-71361-9 ."
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": " Wikimedia Commons alberga una categoría multimedia sobre Álgebra lineal.\nÁlgebra lineal por Elmer G. Wiens (en inglés)\nÁlgebra Lineal: Conceptos Básicos\nIntroducción al Álgebra Lineal en Contexto por José Arturo Barreto Archivado el 23 de mayo de 2010 en Wayback Machine.\nÁlgebra Lineal por René A Hernández-Toledo, 2008\nMIT Linear Algebra Video Lectures,\nInternational Linear Algebra Society Archivado el 5 de julio de 2020 en Wayback Machine.\nHazewinkel, Michiel, ed. (2001), «Álgebra lineal», Encyclopaedia of Mathematics (en inglés), Springer, ISBN 978-1556080104 .\nLinear Algebra on MathWorld\nMatrix and Linear Algebra Terms on Earliest Known Uses of Some of the Words of Mathematics\nEarliest Uses of Symbols for Matrices and Vectors on Earliest Uses of Various Mathematical Symbols\nEssence of linear algebra, a video presentation from 3Blue1Brown of the basics of linear algebra, with emphasis on the relationship between the geometric, the matrix and the abstract points of view\nUn Curso de Álgebra Lineal con notación asociativa y un módulo para Python por Marcos Bujosa (con vídeos y notebooks de jupyter)"
      }
    ],
    "summary": "El álgebra lineal es una rama de las matemáticas que estudia conceptos tales como vectores, matrices, espacio dual, sistemas de ecuaciones lineales y en su enfoque de manera más formal, espacios vectoriales y sus transformaciones lineales.\nDicho de otra forma, el Álgebra lineal es la rama de las matemáticas que se ocupa de las ecuaciones lineales como: \n\n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        =\n        b\n        ,\n      \n    \n    {\\displaystyle a_{1}x_{1}+\\cdots +a_{n}x_{n}=b,}\n  \n\ny  aplicaciones lineales tales como:\n\n  \n    \n      \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n        ↦\n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        ,\n      \n    \n    {\\displaystyle (x_{1},\\ldots ,x_{n})\\mapsto a_{1}x_{1}+\\cdots +a_{n}x_{n},}\n  \n\ny sus representaciones en  espacios vectoriales y a través de matrices.[1]​[2]​[3]​\nEl álgebra lineal es fundamental en casi todas las áreas de las matemáticas. Por ejemplo, el álgebra lineal es fundamental en las presentaciones modernas de la geometría, incluso para definir objetos básicos como líneas, planos y rotaciones. Además, el análisis funcional, una rama del análisis matemático, puede considerarse básicamente como la aplicación del álgebra lineal al espacios de funciones.\nEl álgebra lineal también se utiliza en la mayoría de las ciencias y campos de la ingeniería, porque permite modelar muchos fenómenos naturales, y computar eficientemente con dichos modelos. Para los  sistemas no lineales, que no pueden ser modelados con el álgebra lineal, se utiliza a menudo para tratar las aproximaciones de primer orden, utilizando el hecho de que la diferencial de una 'función multivariante' en un punto es el mapa lineal que mejor aproxima la función cerca de ese punto así como el análisis funcional, las ecuaciones diferenciales, la investigación de operaciones, las gráficas por computadora, la ingeniería entre otras más.\nLa historia del álgebra lineal moderna se remonta a 1843, cuando William Rowan Hamilton (de quien proviene el uso del término vector) creó los cuaterniones inspirado en los números complejos;[4]​ y a 1844, cuando Hermann Grassmann publicó su libro Die lineare Ausdehnungslehre (La teoría lineal de extensión).[5]​"
  },
  {
    "title": "Calculus",
    "source": "https://en.wikipedia.org/wiki/Calculus",
    "language": "en",
    "chunks": [
      {
        "heading": "Etymology",
        "level": 1,
        "content": "In mathematics education, calculus is an abbreviation of both infinitesimal calculus and integral calculus, which denotes courses of elementary mathematical analysis. \nIn Latin, the word calculus means “small pebble”, (the diminutive of calx, meaning \"stone\"), a meaning which still persists in medicine. Because such pebbles were used for counting out distances, tallying votes, and doing abacus arithmetic, the word came to be the Latin word for calculation. In this sense, it was used in English at least as early as 1672, several years before the publications of Leibniz and Newton, who wrote their mathematical texts in Latin.\nIn addition to differential calculus and integral calculus, the term is also used for naming specific methods of computation or theories that imply some sort of computation. Examples of this usage include propositional calculus, Ricci calculus, calculus of variations, lambda calculus, sequent calculus, and process calculus. Furthermore, the term \"calculus\" has variously been applied in ethics and philosophy, for such systems as Bentham's felicific calculus, and the ethical calculus."
      },
      {
        "heading": "History",
        "level": 1,
        "content": "Modern calculus was developed in 17th-century Europe by Isaac Newton and Gottfried Wilhelm Leibniz (independently of each other, first publishing around the same time) but elements of it first appeared in ancient Egypt and later Greece, then in China and the Middle East, and still later again in medieval Europe and India."
      },
      {
        "heading": "Ancient precursors",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Egypt",
        "level": 3,
        "content": "Calculations of volume and area, one goal of integral calculus, can be found in the Egyptian Moscow papyrus (c. 1820 BC), but the formulae are simple instructions, with no indication as to how they were obtained."
      },
      {
        "heading": "Greece",
        "level": 3,
        "content": "Laying the foundations for integral calculus and foreshadowing the concept of the limit, ancient Greek mathematician Eudoxus of Cnidus (c. 390–337 BC) developed  the method of exhaustion to prove the formulas for cone and pyramid volumes.\nDuring the Hellenistic period, this method was further developed by Archimedes (c. 287 – c. 212 BC), who combined it with a concept of the indivisibles—a precursor to infinitesimals—allowing him to solve several problems now treated by integral calculus. In The Method of Mechanical Theorems he describes, for example, calculating the center of gravity of a solid hemisphere, the center of gravity of a frustum of a circular paraboloid, and the area of a region bounded by a parabola and one of its secant lines."
      },
      {
        "heading": "China",
        "level": 3,
        "content": "The method of exhaustion was later discovered independently in China by Liu Hui in the 3rd century AD to find the area of a circle. In the 5th century AD, Zu Gengzhi, son of Zu Chongzhi, established a method that would later be called Cavalieri's principle to find the volume of a sphere."
      },
      {
        "heading": "Medieval",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Middle East",
        "level": 3,
        "content": "In the Middle East, Hasan Ibn al-Haytham, Latinized as Alhazen (c. 965 – c. 1040 AD) derived a formula for the sum of fourth powers. He determined the equations to calculate the area enclosed by the curve represented by \n  \n    \n      \n        y\n        =\n        \n          x\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle y=x^{k}}\n  \n (which translates to the integral \n  \n    \n      \n        ∫\n        \n          x\n          \n            k\n          \n        \n        \n        d\n        x\n      \n    \n    {\\displaystyle \\int x^{k}\\,dx}\n  \n in contemporary notation), for any given non-negative integer value of \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n.He used the results to carry out what would now be called an integration of this function, where the formulae for the sums of integral squares and fourth powers allowed him to calculate the volume of a paraboloid."
      },
      {
        "heading": "India",
        "level": 3,
        "content": "Bhāskara II (c. 1114–1185) was acquainted with some ideas of differential calculus and suggested that the \"differential coefficient\" vanishes at an extremum value of the function. In his astronomical work, he gave a procedure that looked like a precursor to infinitesimal methods. Namely, if \n  \n    \n      \n        x\n        ≈\n        y\n      \n    \n    {\\displaystyle x\\approx y}\n  \n then \n  \n    \n      \n        sin\n        ⁡\n        (\n        y\n        )\n        −\n        sin\n        ⁡\n        (\n        x\n        )\n        ≈\n        (\n        y\n        −\n        x\n        )\n        cos\n        ⁡\n        (\n        y\n        )\n        .\n      \n    \n    {\\displaystyle \\sin(y)-\\sin(x)\\approx (y-x)\\cos(y).}\n  \n This can be interpreted as the discovery that cosine is the derivative of sine. In the 14th century, Indian mathematicians gave a non-rigorous method, resembling differentiation, applicable to some trigonometric functions. Madhava of Sangamagrama and the Kerala School of Astronomy and Mathematics stated components of calculus. They studied series equivalent to the Maclaurin expansions of ⁠\n  \n    \n      \n        sin\n        ⁡\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\sin(x)}\n  \n⁠, ⁠\n  \n    \n      \n        cos\n        ⁡\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\cos(x)}\n  \n⁠, and ⁠\n  \n    \n      \n        arctan\n        ⁡\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\arctan(x)}\n  \n⁠ more than two hundred years before their introduction in Europe. According to Victor J. Katz they were not able to \"combine many differing ideas under the two unifying themes of the derivative and the integral, show the connection between the two, and turn calculus into the great problem-solving tool we have today\"."
      },
      {
        "heading": "Modern",
        "level": 2,
        "content": "Johannes Kepler's work Stereometria Doliorum (1615) formed the basis of integral calculus. Kepler developed a method to calculate the area of an ellipse by adding up the lengths of many radii drawn from a focus of the ellipse.\nSignificant work was a treatise, the origin being Kepler's methods, written by Bonaventura Cavalieri, who argued that volumes and areas should be computed as the sums of the volumes and areas of infinitesimally thin cross-sections. The ideas were similar to Archimedes' in The Method, but this treatise is believed to have been lost in the 13th century and was only rediscovered in the early 20th century, and so would have been unknown to Cavalieri. Cavalieri's work was not well respected since his methods could lead to erroneous results, and the infinitesimal quantities he introduced were disreputable at first.\nThe formal study of calculus brought together Cavalieri's infinitesimals with the calculus of finite differences developed in Europe at around the same time. Pierre de Fermat, claiming that he borrowed from Diophantus, introduced the concept of adequality, which represented equality up to an infinitesimal error term. The combination was achieved by John Wallis, Isaac Barrow, and James Gregory, the latter two proving predecessors to the second fundamental theorem of calculus around 1670.\nThe product rule and chain rule, the notions of higher derivatives and Taylor series, and of analytic functions were used by Isaac Newton in an idiosyncratic notation which he applied to solve problems of mathematical physics. In his works, Newton rephrased his ideas to suit the mathematical idiom of the time, replacing calculations with infinitesimals by equivalent geometrical arguments which were considered beyond reproach. He used the methods of calculus to solve the problem of planetary motion, the shape of the surface of a rotating fluid, the oblateness of the earth, the motion of a weight sliding on a cycloid, and many other problems discussed in his Principia Mathematica (1687). In other work, he developed series expansions for functions, including fractional and irrational powers, and it was clear that he understood the principles of the Taylor series. He did not publish all these discoveries, and at this time infinitesimal methods were still considered disreputable.\n\nThese ideas were arranged into a true calculus of infinitesimals by Gottfried Wilhelm Leibniz, who was originally accused of plagiarism by Newton. He is now regarded as an independent inventor of and contributor to calculus. His contribution was to provide a clear set of rules for working with infinitesimal quantities, allowing the computation of second and higher derivatives, and providing the product rule and chain rule, in their differential and integral forms. Unlike Newton, Leibniz put painstaking effort into his choices of notation.\nToday, Leibniz and Newton are usually both given credit for independently inventing and developing calculus. Newton was the first to apply calculus to general physics. Leibniz developed much of the notation used in calculus today.: 51–52  The basic insights that both Newton and Leibniz provided were the laws of differentiation and integration, emphasizing that differentiation and integration are inverse processes, second and higher derivatives, and the notion of an approximating polynomial series.\nWhen Newton and Leibniz first published their results, there was great controversy over which mathematician (and therefore which country) deserved credit. Newton derived his results first (later to be published in his Method of Fluxions), but Leibniz published his \"Nova Methodus pro Maximis et Minimis\" first. Newton claimed Leibniz stole ideas from his unpublished notes, which Newton had shared with a few members of the Royal Society. This controversy divided English-speaking mathematicians from continental European mathematicians for many years, to the detriment of English mathematics. A careful examination of the papers of Leibniz and Newton shows that they arrived at their results independently, with Leibniz starting first with integration and Newton with differentiation. It is Leibniz, however, who gave the new discipline its name. Newton called his calculus \"the science of fluxions\", a term that endured in English schools into the 19th century.: 100  The first complete treatise on calculus to be written in English and use the Leibniz notation was not published until 1815.\n\nSince the time of Leibniz and Newton, many mathematicians have contributed to the continuing development of calculus. One of the first and most complete works on both infinitesimal and integral calculus was written in 1748 by Maria Gaetana Agnesi."
      },
      {
        "heading": "Foundations",
        "level": 2,
        "content": "In calculus, foundations refers to the rigorous development of the subject from axioms and definitions.  In early calculus, the use of infinitesimal quantities was thought unrigorous and was fiercely criticized by several authors, most notably Michel Rolle and Bishop Berkeley. Berkeley famously described infinitesimals as the ghosts of departed quantities in his book The Analyst in 1734.  Working out a rigorous foundation for calculus occupied mathematicians for much of the century following Newton and Leibniz, and is still to some extent an active area of research today.\nSeveral mathematicians, including Maclaurin, tried to prove the soundness of using infinitesimals, but it would not be until 150 years later when, due to the work of Cauchy and Weierstrass, a way was finally found to avoid mere \"notions\" of infinitely small quantities. The foundations of differential and integral calculus had been laid. In Cauchy's Cours d'Analyse, we find a broad range of foundational approaches, including a definition of continuity in terms of infinitesimals, and a (somewhat imprecise) prototype of an (ε, δ)-definition of limit in the definition of differentiation. In his work, Weierstrass formalized the concept of limit and eliminated infinitesimals (although his definition can validate nilsquare infinitesimals). Following the work of Weierstrass, it eventually became common to base calculus on limits instead of infinitesimal quantities, though the subject is still occasionally called \"infinitesimal calculus\". Bernhard Riemann used these ideas to give a precise definition of the integral. It was also during this period that the ideas of calculus were generalized to the complex plane with the development of complex analysis.\nIn modern mathematics, the foundations of calculus are included in the field of real analysis, which contains full definitions and proofs of the theorems of calculus. The reach of calculus has also been greatly extended. Henri Lebesgue invented measure theory, based on earlier developments by Émile Borel, and used it to define integrals of all but the most pathological functions. Laurent Schwartz introduced distributions, which can be used to take the derivative of any function whatsoever.\nLimits are not the only rigorous approach to the foundation of calculus. Another way is to use Abraham Robinson's non-standard analysis. Robinson's approach, developed in the 1960s, uses technical machinery from mathematical logic to augment the real number system with infinitesimal and infinite numbers, as in the original Newton-Leibniz conception. The resulting numbers are called hyperreal numbers, and they can be used to give a Leibniz-like development of the usual rules of calculus. There is also smooth infinitesimal analysis, which differs from non-standard analysis in that it mandates neglecting higher-power infinitesimals during derivations. Based on the ideas of F. W. Lawvere and employing the methods of category theory, smooth infinitesimal analysis views all functions as being continuous and incapable of being expressed in terms of discrete entities. One aspect of this formulation is that the law of excluded middle does not hold. The law of excluded middle is also rejected in constructive mathematics, a branch of mathematics that insists that proofs of the existence of a number, function, or other mathematical object should give a construction of the object. Reformulations of calculus in a constructive framework are generally part of the subject of constructive analysis."
      },
      {
        "heading": "Significance",
        "level": 2,
        "content": "While many of the ideas of calculus had been developed earlier in Greece, China, India, Iraq, Persia, and Japan, the use of calculus began in Europe, during the 17th century, when Newton and Leibniz built on the work of earlier mathematicians to introduce its basic principles. The Hungarian polymath John von Neumann wrote of this work,\n\nThe calculus was the first achievement of modern mathematics and it is difficult to overestimate its importance. I think it defines more unequivocally than anything else the inception of modern mathematics, and the system of mathematical analysis, which is its logical development, still constitutes the greatest technical advance in exact thinking.\nApplications of differential calculus include computations involving velocity and acceleration, the slope of a curve, and optimization.: 341–453  Applications of integral calculus include computations involving area, volume, arc length, center of mass, work, and pressure.: 685–700  More advanced applications include power series and Fourier series.\nCalculus is also used to gain a more precise understanding of the nature of space, time, and motion. For centuries, mathematicians and philosophers wrestled with paradoxes involving division by zero or sums of infinitely many numbers. These questions arise in the study of motion and area. The ancient Greek philosopher Zeno of Elea gave several famous examples of such paradoxes. Calculus provides tools, especially the limit and the infinite series, that resolve the paradoxes."
      },
      {
        "heading": "Principles",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Limits and infinitesimals",
        "level": 2,
        "content": "Calculus is usually developed by working with very small quantities. Historically, the first method of doing so was by infinitesimals. These are objects which can be treated like real numbers but which are, in some sense, \"infinitely small\".  For example, an infinitesimal number could be greater than 0, but less than any number in the sequence 1, 1/2, 1/3, ... and thus less than any positive real number. From this point of view, calculus is a collection of techniques for manipulating infinitesimals. The symbols \n  \n    \n      \n        d\n        x\n      \n    \n    {\\displaystyle dx}\n  \n and \n  \n    \n      \n        d\n        y\n      \n    \n    {\\displaystyle dy}\n  \n were taken to be infinitesimal, and the derivative \n  \n    \n      \n        d\n        y\n        \n          /\n        \n        d\n        x\n      \n    \n    {\\displaystyle dy/dx}\n  \n was their ratio.\nThe infinitesimal approach fell out of favor in the 19th century because it was difficult to make the notion of an infinitesimal precise. In the late 19th century, infinitesimals were replaced within academia by the epsilon, delta approach to limits. Limits describe the behavior of a function at a certain input in terms of its values at nearby inputs. They capture small-scale behavior using the intrinsic structure of the real number system (as a metric space with the least-upper-bound property). In this treatment, calculus is a collection of techniques for manipulating certain limits. Infinitesimals get replaced by sequences of smaller and smaller numbers, and the infinitely small behavior of a function is found by taking the limiting behavior for these sequences. Limits were thought to provide a more rigorous foundation for calculus, and for this reason, they became the standard approach during the 20th century. However, the infinitesimal concept was revived in the 20th century with the introduction of non-standard analysis and smooth infinitesimal analysis, which provided solid foundations for the manipulation of infinitesimals."
      },
      {
        "heading": "Differential calculus",
        "level": 2,
        "content": "Differential calculus is the study of the definition, properties, and applications of the derivative of a function. The process of finding the derivative is called differentiation. Given a function and a point in the domain, the derivative at that point is a way of encoding the small-scale behavior of the function near that point. By finding the derivative of a function at every point in its domain, it is possible to produce a new function, called the derivative function or just the derivative of the original function. In formal terms, the derivative is a linear operator which takes a function as its input and produces a second function as its output. This is more abstract than many of the processes studied in elementary algebra, where functions usually input a number and output another number. For example, if the doubling function is given the input three, then it outputs six, and if the squaring function is given the input three, then it outputs nine. The derivative, however, can take the squaring function as an input. This means that the derivative takes all the information of the squaring function—such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on—and uses this information to produce another function. The function produced by differentiating the squaring function turns out to be the doubling function.: 32 \nIn more explicit terms the \"doubling function\" may be denoted by g(x) = 2x and the \"squaring function\" by f(x) = x2. The \"derivative\" now takes the function f(x), defined by the expression \"x2\", as an input, that is all the information—such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on—and uses this information to output another function, the function g(x) = 2x, as will turn out.\nIn Lagrange's notation, the symbol for a derivative is an apostrophe-like mark called a prime. Thus, the derivative of a function called f is denoted by f′, pronounced \"f prime\" or \"f dash\". For instance, if f(x) = x2 is the squaring function, then f′(x) = 2x is its derivative (the doubling function g from above).\nIf the input of the function represents time, then the derivative represents change concerning time. For example, if f is a function that takes time as input and gives the position of a ball at that time as output, then the derivative of f is how the position is changing in time, that is, it is the velocity of the ball.: 18–20 \nIf a function is linear (that is if the graph of the function is a straight line), then the function can be written as y = mx + b, where x is the independent variable, y is the dependent variable, b is the y-intercept, and:\n\n  \n    \n      \n        m\n        =\n        \n          \n            rise\n            run\n          \n        \n        =\n        \n          \n            \n              \n                change in \n              \n              y\n            \n            \n              \n                change in \n              \n              x\n            \n          \n        \n        =\n        \n          \n            \n              Δ\n              y\n            \n            \n              Δ\n              x\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle m={\\frac {\\text{rise}}{\\text{run}}}={\\frac {{\\text{change in }}y}{{\\text{change in }}x}}={\\frac {\\Delta y}{\\Delta x}}.}\n  \n\nThis gives an exact value for the slope of a straight line.: 6  If the graph of the function is not a straight line, however, then the change in y divided by the change in x varies. Derivatives give an exact meaning to the notion of change in output concerning change in input. To be concrete, let f be a function, and fix a point a in the domain of f. (a, f(a)) is a point on the graph of the function. If h is a number close to zero, then a + h is a number close to a. Therefore, (a + h, f(a + h)) is close to (a, f(a)). The slope between these two points is\n\n  \n    \n      \n        m\n        =\n        \n          \n            \n              f\n              (\n              a\n              +\n              h\n              )\n              −\n              f\n              (\n              a\n              )\n            \n            \n              (\n              a\n              +\n              h\n              )\n              −\n              a\n            \n          \n        \n        =\n        \n          \n            \n              f\n              (\n              a\n              +\n              h\n              )\n              −\n              f\n              (\n              a\n              )\n            \n            h\n          \n        \n        .\n      \n    \n    {\\displaystyle m={\\frac {f(a+h)-f(a)}{(a+h)-a}}={\\frac {f(a+h)-f(a)}{h}}.}\n  \n\nThis expression is called a difference quotient. A line through two points on a curve is called a secant line, so m is the slope of the secant line between (a, f(a)) and (a + h, f(a + h)). The second line is only an approximation to the behavior of the function at the point  a because it does not account for what happens between  a and  a + h. It is not possible to discover the behavior at  a by setting h to zero because this would require dividing by zero, which is undefined. The derivative is defined by taking the limit as h tends to zero, meaning that it considers the behavior of f for all small values of h and extracts a consistent value for the case when h equals zero:\n\n  \n    \n      \n        \n          lim\n          \n            h\n            →\n            0\n          \n        \n        \n          \n            \n              f\n              (\n              a\n              +\n              h\n              )\n              −\n              f\n              (\n              a\n              )\n            \n            \n              h\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\lim _{h\\to 0}{f(a+h)-f(a) \\over {h}}.}\n  \n\nGeometrically, the derivative is the slope of the tangent line to the graph of f at a. The tangent line is a limit of secant lines just as the derivative is a limit of difference quotients. For this reason, the derivative is sometimes called the slope of the function f.: 61–63 \nHere is a particular example, the derivative of the squaring function at the input 3. Let f(x) = x2 be the squaring function.\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  f\n                  ′\n                \n                (\n                3\n                )\n              \n              \n                \n                =\n                \n                  lim\n                  \n                    h\n                    →\n                    0\n                  \n                \n                \n                  \n                    \n                      (\n                      3\n                      +\n                      h\n                      \n                        )\n                        \n                          2\n                        \n                      \n                      −\n                      \n                        3\n                        \n                          2\n                        \n                      \n                    \n                    \n                      h\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  lim\n                  \n                    h\n                    →\n                    0\n                  \n                \n                \n                  \n                    \n                      9\n                      +\n                      6\n                      h\n                      +\n                      \n                        h\n                        \n                          2\n                        \n                      \n                      −\n                      9\n                    \n                    \n                      h\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  lim\n                  \n                    h\n                    →\n                    0\n                  \n                \n                \n                  \n                    \n                      6\n                      h\n                      +\n                      \n                        h\n                        \n                          2\n                        \n                      \n                    \n                    \n                      h\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  lim\n                  \n                    h\n                    →\n                    0\n                  \n                \n                (\n                6\n                +\n                h\n                )\n              \n            \n            \n              \n              \n                \n                =\n                6\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}f'(3)&=\\lim _{h\\to 0}{(3+h)^{2}-3^{2} \\over {h}}\\\\&=\\lim _{h\\to 0}{9+6h+h^{2}-9 \\over {h}}\\\\&=\\lim _{h\\to 0}{6h+h^{2} \\over {h}}\\\\&=\\lim _{h\\to 0}(6+h)\\\\&=6\\end{aligned}}}\n  \n\nThe slope of the tangent line to the squaring function at the point (3, 9) is 6, that is to say, it is going up six times as fast as it is going to the right. The limit process just described can be performed for any point in the domain of the squaring function. This defines the derivative function of the squaring function or just the derivative of the squaring function for short. A computation similar to the one above shows that the derivative of the squaring function is the doubling function.: 63"
      },
      {
        "heading": "Leibniz notation",
        "level": 2,
        "content": "A common notation, introduced by Leibniz, for the derivative in the example above is\n\n  \n    \n      \n        \n          \n            \n              \n                y\n              \n              \n                \n                =\n                \n                  x\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n                \n                  \n                    \n                      d\n                      y\n                    \n                    \n                      d\n                      x\n                    \n                  \n                \n              \n              \n                \n                =\n                2\n                x\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}y&=x^{2}\\\\{\\frac {dy}{dx}}&=2x.\\end{aligned}}}\n  \n\nIn an approach based on limits, the symbol ⁠dy/ dx⁠ is to be interpreted not as the quotient of two numbers but as a shorthand for the limit computed above.: 74  Leibniz, however, did intend it to represent the quotient of two infinitesimally small numbers, dy being the infinitesimally small change in y caused by an infinitesimally small change  dx applied to x. We can also think of ⁠d/ dx⁠ as a differentiation operator, which takes a function as an input and gives another function, the derivative, as the output. For example:\n\n  \n    \n      \n        \n          \n            d\n            \n              d\n              x\n            \n          \n        \n        (\n        \n          x\n          \n            2\n          \n        \n        )\n        =\n        2\n        x\n        .\n      \n    \n    {\\displaystyle {\\frac {d}{dx}}(x^{2})=2x.}\n  \n\nIn this usage, the dx in the denominator is read as \"with respect to x\".: 79  Another example of correct notation could be:\n\n  \n    \n      \n        \n          \n            \n              \n                g\n                (\n                t\n                )\n              \n              \n                \n                =\n                \n                  t\n                  \n                    2\n                  \n                \n                +\n                2\n                t\n                +\n                4\n              \n            \n            \n              \n                \n                  \n                    d\n                    \n                      d\n                      t\n                    \n                  \n                \n                g\n                (\n                t\n                )\n              \n              \n                \n                =\n                2\n                t\n                +\n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}g(t)&=t^{2}+2t+4\\\\{d \\over dt}g(t)&=2t+2\\end{aligned}}}\n  \n\nEven when calculus is developed using limits rather than infinitesimals, it is common to manipulate symbols like  dx and dy as if they were real numbers; although it is possible to avoid such manipulations, they are sometimes notationally convenient in expressing operations such as the total derivative."
      },
      {
        "heading": "Integral calculus",
        "level": 2,
        "content": "Integral calculus is the study of the definitions, properties, and applications of two related concepts, the indefinite integral and the definite integral. The process of finding the value of an integral is called integration.: 508  The indefinite integral, also known as the antiderivative, is the inverse operation to the derivative.: 163–165  F is an indefinite integral of f when f is a derivative of F.  (This use of lower- and upper-case letters for a function and its indefinite integral is common in calculus.) The definite integral inputs a function and outputs a number, which gives the algebraic sum of areas between the graph of the input and the x-axis. The technical definition of the definite integral involves the limit of a sum of areas of rectangles, called a Riemann sum.: 282 \nA motivating example is the distance traveled in a given time.: 153  If the speed is constant, only multiplication is needed:\n\n  \n    \n      \n        \n          D\n          i\n          s\n          t\n          a\n          n\n          c\n          e\n        \n        =\n        \n          S\n          p\n          e\n          e\n          d\n        \n        ⋅\n        \n          T\n          i\n          m\n          e\n        \n      \n    \n    {\\displaystyle \\mathrm {Distance} =\\mathrm {Speed} \\cdot \\mathrm {Time} }\n  \n\nBut if the speed changes, a more powerful method of finding the distance is necessary. One such method is to approximate the distance traveled by breaking up the time into many short intervals of time, then multiplying the time elapsed in each interval by one of the speeds in that interval, and then taking the sum (a Riemann sum) of the approximate distance traveled in each interval. The basic idea is that if only a short time elapses, then the speed will stay more or less the same. However, a Riemann sum only gives an approximation of the distance traveled. We must take the limit of all such Riemann sums to find the exact distance traveled.\nWhen velocity is constant, the total distance traveled over the given time interval can be computed by multiplying velocity and time.  For example, traveling a steady 50 mph for 3 hours results in a total distance of 150 miles.  Plotting the velocity as a function of time yields a rectangle with a height equal to the velocity and a width equal to the time elapsed.  Therefore, the product of velocity and time also calculates the rectangular area under the (constant) velocity curve.: 535   This connection between the area under a curve and the distance traveled can be extended to any irregularly shaped region exhibiting a fluctuating velocity over a given period. If f(x) represents speed as it varies over time, the distance traveled between the times represented by  a and b is the area of the region between f(x) and the x-axis, between x = a and x = b.\nTo approximate that area, an intuitive method would be to divide up the distance between  a and b into several equal segments, the length of each segment represented by the symbol Δx. For each small segment, we can choose one value of the function f(x). Call that value h. Then the area of the rectangle with base Δx and height h gives the distance (time Δx multiplied by speed h) traveled in that segment.   Associated with each segment is the average value of the function above it, f(x) = h. The sum of all such rectangles gives an approximation of the area between the axis and the curve, which is an approximation of the total distance traveled. A smaller value for Δx will give more rectangles and in most cases a better approximation, but for an exact answer, we need to take a limit as Δx approaches zero.: 512–522 \nThe symbol of integration is \n  \n    \n      \n        ∫\n      \n    \n    {\\displaystyle \\int }\n  \n, an elongated S chosen to suggest summation.: 529  The definite integral is written as:\n\n  \n    \n      \n        \n          ∫\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle \\int _{a}^{b}f(x)\\,dx}\n  \n\nand is read \"the integral from a to b of f-of-x with respect to x.\" The Leibniz notation dx is intended to suggest dividing the area under the curve into an infinite number of rectangles so that their width Δx becomes the infinitesimally small dx.: 44 \nThe indefinite integral, or antiderivative, is written:\n\n  \n    \n      \n        ∫\n        f\n        (\n        x\n        )\n        \n        d\n        x\n        .\n      \n    \n    {\\displaystyle \\int f(x)\\,dx.}\n  \n\nFunctions differing by only a constant have the same derivative, and it can be shown that the antiderivative of a given function is a family of functions differing only by a constant.: 326  Since the derivative of the function y = x2 + C, where C is any constant, is y′ = 2x, the antiderivative of the latter is given by:\n\n  \n    \n      \n        ∫\n        2\n        x\n        \n        d\n        x\n        =\n        \n          x\n          \n            2\n          \n        \n        +\n        C\n        .\n      \n    \n    {\\displaystyle \\int 2x\\,dx=x^{2}+C.}\n  \n\nThe unspecified constant C present in the indefinite integral or antiderivative is known as the constant of integration.: 135"
      },
      {
        "heading": "Fundamental theorem",
        "level": 2,
        "content": "The fundamental theorem of calculus states that differentiation and integration are inverse operations.: 290  More precisely, it relates the values of antiderivatives to definite integrals. Because it is usually easier to compute an antiderivative than to apply the definition of a definite integral, the fundamental theorem of calculus provides a practical way of computing definite integrals. It can also be interpreted as a precise statement of the fact that differentiation is the inverse of integration.\nThe fundamental theorem of calculus states: If a function f is continuous on the interval [a, b] and if F is a function whose derivative is f on the interval (a, b), then\n\n  \n    \n      \n        \n          ∫\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        =\n        F\n        (\n        b\n        )\n        −\n        F\n        (\n        a\n        )\n        .\n      \n    \n    {\\displaystyle \\int _{a}^{b}f(x)\\,dx=F(b)-F(a).}\n  \n\nFurthermore, for every x in the interval (a, b),\n\n  \n    \n      \n        \n          \n            d\n            \n              d\n              x\n            \n          \n        \n        \n          ∫\n          \n            a\n          \n          \n            x\n          \n        \n        f\n        (\n        t\n        )\n        \n        d\n        t\n        =\n        f\n        (\n        x\n        )\n        .\n      \n    \n    {\\displaystyle {\\frac {d}{dx}}\\int _{a}^{x}f(t)\\,dt=f(x).}\n  \n\nThis realization, made by both Newton and Leibniz, was key to the proliferation of analytic results after their work became known. (The extent to which Newton and Leibniz were influenced by immediate predecessors, and particularly what Leibniz may have learned from the work of Isaac Barrow, is difficult to determine because of the priority dispute between them.) The fundamental theorem provides an algebraic method of computing many definite integrals—without performing limit processes—by finding formulae for antiderivatives. It is also a prototype solution of a differential equation. Differential equations relate an unknown function to its derivatives and are ubiquitous in the sciences.: 351–352"
      },
      {
        "heading": "Applications",
        "level": 1,
        "content": "Calculus is used in every branch of the physical sciences,: 1  actuarial science, computer science, statistics, engineering, economics, business, medicine, demography, and in other fields wherever a problem can be mathematically modeled and an optimal solution is desired. It allows one to go from (non-constant) rates of change to the total change or vice versa, and many times in studying a problem we know one and are trying to find the other. Calculus can be used in conjunction with other mathematical disciplines. For example, it can be used with linear algebra to find the \"best fit\" linear approximation for a set of points in a domain. Or, it can be used in probability theory to determine the expectation value of a continuous random variable given a probability density function.: 37  In analytic geometry, the study of graphs of functions, calculus is used to find high points and low points (maxima and minima), slope, concavity and inflection points. Calculus is also used to find approximate solutions to equations; in practice, it is the standard way to solve differential equations and do root finding in most applications. Examples are methods such as Newton's method, fixed point iteration, and linear approximation. For instance, spacecraft use a variation of the Euler method to approximate curved courses within zero-gravity environments.\nPhysics makes particular use of calculus; all concepts in classical mechanics and electromagnetism are related through calculus. The mass of an object of known density, the moment of inertia of objects, and the potential energies due to gravitational and electromagnetic forces can all be found by the use of calculus. An example of the use of calculus in mechanics is Newton's second law of motion, which states that the derivative of an object's momentum concerning time equals the net force upon it. Alternatively, Newton's second law can be expressed by saying that the net force equals the object's mass times its acceleration, which is the time derivative of velocity and thus the second time derivative of spatial position. Starting from knowing how an object is accelerating, we use calculus to derive its path.\nMaxwell's theory of electromagnetism and Einstein's theory of general relativity are also expressed in the language of differential calculus.: 52–55  Chemistry also uses calculus in determining reaction rates: 599  and in studying radioactive decay.: 814  In biology, population dynamics starts with reproduction and death rates to model population changes.: 631 \nGreen's theorem, which gives the relationship between a line integral around a simple closed curve C and a double integral over the plane region D bounded by C, is applied in an instrument known as a planimeter, which is used to calculate the area of a flat surface on a drawing. For example, it can be used to calculate the amount of area taken up by an irregularly shaped flower bed or swimming pool when designing the layout of a piece of property.\nIn the realm of medicine, calculus can be used to find the optimal branching angle of a blood vessel to maximize flow. Calculus can be applied to understand how quickly a drug is eliminated from a body or how quickly a cancerous tumor grows.\nIn economics, calculus allows for the determination of maximal profit by providing a way to easily calculate both marginal cost and marginal revenue.: 387"
      },
      {
        "heading": "See also",
        "level": 1,
        "content": "Glossary of calculus\nList of calculus topics\nList of derivatives and integrals in alternative calculi\nList of differentiation identities\nPublications in calculus\nTable of integrals"
      },
      {
        "heading": "References",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Further reading",
        "level": 1,
        "content": ""
      },
      {
        "heading": "External links",
        "level": 1,
        "content": "\n\"Calculus\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nWeisstein, Eric W. \"Calculus\". MathWorld.\nTopics on Calculus at PlanetMath.\nCalculus Made Easy (1914) by Silvanus P. Thompson Full text in PDF\nCalculus on In Our Time at the BBC\nCalculus.org: The Calculus page at University of California, Davis – contains resources and links to other sites\nEarliest Known Uses of Some of the Words of Mathematics: Calculus & Analysis\nThe Role of Calculus in College Mathematics Archived 26 July 2021 at the Wayback Machine from ERICDigests.org\nOpenCourseWare Calculus from the Massachusetts Institute of Technology\nInfinitesimal Calculus – an article on its historical development, in Encyclopedia of Mathematics, ed. Michiel Hazewinkel.\nDaniel Kleitman, MIT. \"Calculus for Beginners and Artists\".\nCalculus training materials at imomath.com\n(in English and Arabic) The Excursion of Calculus, 1772"
      }
    ],
    "summary": "Calculus is the mathematical study of continuous change, in the same way that geometry is the study of shape, and algebra is the study of generalizations of arithmetic operations.\nOriginally called infinitesimal calculus or \"the calculus of infinitesimals\", it has two major branches, differential calculus and integral calculus. The former concerns instantaneous rates of change, and the slopes of curves, while the latter concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus. They make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit. It is the \"mathematical backbone\" for dealing with problems where variables change with time or another reference variable.\nInfinitesimal calculus was formulated separately in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz. Later work, including codifying the idea of limits, put these developments on a more solid conceptual footing. Today, calculus is widely used in science, engineering, biology, and even has applications in social science and other branches of math."
  },
  {
    "title": "Cálculo infinitesimal",
    "source": "https://es.wikipedia.org/wiki/C%C3%A1lculo_infinitesimal",
    "language": "es",
    "chunks": [
      {
        "heading": "Historia",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Edad Antigua",
        "level": 2,
        "content": "El período antiguo introdujo algunas de las ideas del cálculo integral, pero no parece haber desarrollado estas ideas en una manera rigurosa o sistemática. En el cálculo de áreas y volúmenes, la función básica del cálculo integral puede ser rastreada en el tiempo hasta los papiros matemáticos de Moscú que datan del año 1890 a. C, en los que un egipcio calculó satisfactoriamente el volumen del tronco de una pirámide.[2]​[3]​ Los babilonios pueden haber descubierto la regla trapezoidal mientras hacían observaciones astronómicas de Júpiter.[4]​\nDe la escuela de los matemáticos griegos, Eudoxo (408−355 a. C.) usó el método exhaustivo, el cual prefiguraba el concepto de límite, para calcular áreas y volúmenes, mientras que Arquímedes (287−212 a. C.) desarrolló más allá su idea inventando un método heurístico, denominado exhaustación, que se asemeja al cálculo infinitesimal.[5]​\nEl método exhaustivo fue más tarde usado en China por Liu Hui en el siglo III a. C. para encontrar el área de un círculo. En el siglo V d. C., Zu Chongzhi usó lo que más tarde sería llamado la teoría de los indivisibles por el matemático italiano Bonaventura Cavalieri para encontrar el volumen de una esfera.[3]​"
      },
      {
        "heading": "Edad Media",
        "level": 2,
        "content": "Cerca del año 1000 d. C., el matemático islámico Alhacén fue el primero en derivar la fórmula para la suma de la cuarta potencia de una progresión aritmética, usando un método a partir del cual es fácil encontrar la fórmula para la suma de cualquier potencia integral de mayor orden.[6]​\nEn el siglo XI, el polímata chino Shen Kuo desarrolló ecuaciones que se encargaban de integrar. En el siglo XII, el matemático indio, Bhaskara II, desarrolló una derivada temprana representando el cambio infinitesimal, y describió una forma temprana del «teorema de Rolle».[7]​\nTambién en el siglo XII, el matemático persa Sharaf al-Din al-Tusi descubrió la derivada de la función cúbica, un importante acontecimiento en el cálculo diferencial.[8]​\nEn el siglo XIV, Madhava de Sangamagrama, en conjunto con otros matemáticos y astrónomos de la Escuela de Kerala, describieron casos especiales de las series de Taylor,[9]​ los cuales están referidos en el texto Yuktibhasa.[10]​[11]​"
      },
      {
        "heading": "Modernidad",
        "level": 2,
        "content": "En la época moderna, descubrimientos independientes relacionados con el cálculo se estaban llevando a cabo por la matemática japonesa del siglo XVII, gracias al aporte de matemáticos como Seki Kōwa, quien expandió el método exhaustivo.\nEn Europa, el trabajo fundacional fue un tratado del clérigo y matemático italiano Bonaventura Cavalieri, quien argumentó que los volúmenes y áreas deberían ser calculados como las sumas de los volúmenes y áreas de delgadas secciones infinitesimales. Estas ideas eran similares a las expuestas en el trabajo El método de los teoremas mecánicos de Arquímedes, el cual estuvo perdido hasta principios del siglo XX. El trabajo de Cavalieri no fue bien respetado ya que sus métodos pueden llevar a resultados erróneos, y porque las cantidades infinitesimales que introdujo eran desacreditadas al principio.\nEl estudio formal del cálculo combinó los infinitesimales de Cavalieri con el cálculo de diferencias finitas desarrollado en Europa más o menos al mismo tiempo. La combinación fue lograda por John Wallis, Isaac Barrow y James Gregory, probando estos últimos el teorema fundamental del cálculo integral cerca del año 1675.\nLa regla del producto y la regla de la cadena, la noción de derivada de mayor orden, las series de Taylor, y las funciones analíticas fueron introducidas por Isaac Newton en una notación idiosincrásica que usó para resolver problemas de física matemática. En sus publicaciones, Newton formuló sus ideas para acomodar el idioma matemático de la época, utilizando argumentos informales como el de las fluxiones, que generaron gran escozor y escepticismo en otros filósofos de la época; notablemente Berkeley. Usó los métodos del cálculo para resolver el problema del movimiento planetario, la forma de la superficie de un fluido rotante, y se refirió a lo achatada que es la tierra por los polos, así como a muchos otros problemas, los cuales discutió en Principia mathematica. En otro trabajo, desarrolló una serie de expansiones para las funciones, incluyendo las potencias fraccionarias e irracionales. Fue claro que Newton entendía los principios de las series de Taylor. No publicó todos estos descubrimientos. En su tiempo los sistemas infinitesimales eran considerados como reprochables.\nEstas ideas fueron sistematizadas en un verdadero cálculo de infinitesimales por Gottfried Wilhelm Leibniz, quien fue originalmente acusado de plagio por Newton. Es ahora reconocido como inventor independiente del cálculo y un gran contribuyente a este. Su principal contribución fue el proveer un conjunto de reglas claras para la manipulación de cantidades infinitesimales, permitiendo el cómputo de derivadas de segundo orden y de orden superior, y estableciendo la regla del producto y regla de la cadena en su forma diferencial e integral. A diferencia de Newton, Leibniz le puso mucha atención al formalismo y a menudo le dedicaba varios días a determinar los símbolos apropiados para los conceptos.\nUsualmente se le acredita a ambos Leibniz y Newton la invención del cálculo. Newton fue el primero en aplicar el cálculo a la física general y Leibniz desarrolló mucho de la notación usada en cálculo hasta al menos principio del siglo XIX. Las ideas principales que ambos Newton y Leibniz estipularon fueron las leyes de diferenciación e integración, las segundas derivadas, las derivadas de orden superior, y la noción de una aproximación de series de polinomios. Ya por la época de Newton, el teorema fundamental de cálculo era conocido.\nCuando Newton y Leibniz primero publicaron sus resultados, hubo gran controversia sobre qué matemático (y por ende qué país) merecía el crédito por la invención de esta disciplina. Newton llegó primero a sus resultados, pero Leibniz publicó primero. Newton acusó a Leibniz de robar sus ideas de sus notas inéditas, las cuales Newton había compartido con unos cuantos miembros de la Royal Society. Esta controversia dividió a los matemáticos de habla inglesa de los matemáticos continentales por varios años, causando un retraso de las matemáticas inglesas. Un cuidadoso examen de los papeles de ambos matemáticos demuestra que ellos llegaron a sus resultados independientemente, con Leibniz empezando primero con la integración y Newton con la diferenciación. Hoy, se les da crédito a ambos matemáticos por desarrollar el cálculo independientemente. Fue Leibniz, sin embargo, quien le dio el nuevo nombre a su disciplina. Newton llamó su cálculo el «método de las fluxiones». La simbología usada por Newton, tal como \n  \n    \n      \n        \n          \n            \n              x\n              ˙\n            \n          \n        \n      \n    \n    {\\displaystyle {\\dot {x}}}\n  \n (derivada primera), \n  \n    \n      \n        \n          \n            \n              x\n              ¨\n            \n          \n        \n      \n    \n    {\\displaystyle {\\ddot {x}}}\n  \n (derivada segunda) a veces aparece en física y situaciones que no requieren de formalismo matemático; mientras que la notación de Leibniz es preferida por los libros de texto sobre cálculo.\nDesde los tiempos de Leibniz y Newton, muchos matemáticos han contribuido al desarrollo continuo del cálculo. En el siglo XIX, el cálculo comenzó a ser planteado más rigurosamente por matemáticos como Cauchy, Riemann y Weierstrass. También fue en este período que las ideas del cálculo fueron generalizadas al espacio euclidiano y al plano complejo. Lebesgue generalizó la noción de la integral de tal manera que virtualmente cualquier función tenga una integral, mientras que Laurent Schwartz extendió la diferenciación casi de la misma manera.\nEl cálculo es un tema omnipresente en la mayoría de los programas de educación superior y en las universidades. Los matemáticos alrededor del mundo continúan contribuyendo al desarrollo de esta disciplina, la cual ha sido considerada como uno de los logros más grandes del intelecto humano.[12]​ El desarrollo de las ecuaciones diferenciales ha jugado un gran papel de cambio cualitativo en la ciencia y la tecnología, comparable con el control del fuego en la época primitiva, las ecuaciones diferenciales son un salto enorme para la ciencia."
      },
      {
        "heading": "Conjunto",
        "level": 1,
        "content": "O\n          \n            x\n            ,\n            α\n          \n          \n            n\n          \n        \n        (\n        h\n        )\n      \n    \n    {\\displaystyle O_{x,\\alpha }^{n}(h)}\n  \n de Operadores Fraccionales\nEl cálculo fraccional de conjuntos (Fractional Calculus of Sets (FCS)), mencionado por primera vez en el artículo titulado \"Sets of Fractional Operators and Numerical Estimation of the Order of Convergence of a Family of Fractional Fixed-Point Methods\",[13]​ es una metodología derivada del cálculo fraccional.[14]​ El concepto principal detrás del FCS es la caracterización de los elementos del cálculo fraccional utilizando conjuntos debido a la gran cantidad de operadores fraccionales disponibles.[15]​[16]​[17]​ Esta metodología se originó a partir del desarrollo del método de Newton-Raphson fraccional [18]​ y trabajos relacionados posteriores.[19]​[20]​[21]​\n\nEl cálculo fraccional, una rama de las matemáticas que trata con derivadas de orden no entero, surgió casi simultáneamente con el cálculo tradicional. Esta emergencia fue en parte debido a la notación de Leibniz para derivadas de orden entero: \n  \n    \n      \n        \n          \n            \n              d\n              \n                n\n              \n            \n            \n              d\n              \n                x\n                \n                  n\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {d^{n}}{dx^{n}}}}\n  \n. Gracias a esta notación, L’Hopital pudo preguntar en una carta a Leibniz sobre la interpretación de tomar \n  \n    \n      \n        n\n        =\n        \n          \n            1\n            2\n          \n        \n      \n    \n    {\\displaystyle n={\\frac {1}{2}}}\n  \n en una derivada. En ese momento, Leibniz no pudo proporcionar una interpretación física o geométrica para esta pregunta, por lo que simplemente respondió a L’Hopital en una carta que \"... es una aparente paradoja de la cual, algún día, se derivarán consecuencias útiles\".\nEl nombre \"cálculo fraccional\" se origina a partir de una pregunta histórica, ya que esta rama del análisis matemático estudia derivadas e integrales de un cierto orden \n  \n    \n      \n        α\n        ∈\n        \n          R\n        \n      \n    \n    {\\displaystyle \\alpha \\in \\mathbb {R} }\n  \n. Actualmente, el cálculo fraccional carece de una definición unificada de lo que constituye una derivada fraccional. En consecuencia, cuando no es necesario especificar explícitamente la forma de una derivada fraccional, típicamente se denota de la siguiente manera:\n\n  \n    \n      \n        \n          \n            \n              d\n              \n                α\n              \n            \n            \n              d\n              \n                x\n                \n                  α\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {d^{\\alpha }}{dx^{\\alpha }}}.}\n  \n\nLos operadores fraccionales tienen varias representaciones, pero una de sus propiedades fundamentales es que recuperan los resultados del cálculo tradicional a medida que \n  \n    \n      \n        α\n        →\n        n\n      \n    \n    {\\displaystyle \\alpha \\to n}\n  \n. Considerando una función escalar \n  \n    \n      \n        h\n        :\n        \n          \n            R\n          \n          \n            m\n          \n        \n        →\n        \n          R\n        \n      \n    \n    {\\displaystyle h:\\mathbb {R} ^{m}\\to \\mathbb {R} }\n  \n y la base canónica de \n  \n    \n      \n        \n          \n            R\n          \n          \n            m\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{m}}\n  \n denotada por \n  \n    \n      \n        {\n        \n          \n            \n              \n                e\n                ^\n              \n            \n          \n          \n            k\n          \n        \n        \n          }\n          \n            k\n            ≥\n            1\n          \n        \n      \n    \n    {\\displaystyle \\{{\\hat {e}}_{k}\\}_{k\\geq 1}}\n  \n, el siguiente operador fraccional de orden \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n se define utilizando notación de Einstein:[22]​\n\n  \n    \n      \n        \n          o\n          \n            x\n          \n          \n            α\n          \n        \n        h\n        (\n        x\n        )\n        :=\n        \n          \n            \n              \n                e\n                ^\n              \n            \n          \n          \n            k\n          \n        \n        \n          o\n          \n            k\n          \n          \n            α\n          \n        \n        h\n        (\n        x\n        )\n        .\n      \n    \n    {\\displaystyle o_{x}^{\\alpha }h(x):={\\hat {e}}_{k}o_{k}^{\\alpha }h(x).}\n  \n\nDenotando \n  \n    \n      \n        \n          ∂\n          \n            k\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\partial _{k}^{n}}\n  \n como la derivada parcial de orden \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n con respecto al componente \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n-ésimo del vector \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n, se define el siguiente conjunto de operadores fraccionales:[23]​[19]​\n\ncuyo complemento es:\n\nComo consecuencia, se define el siguiente conjunto:\n\n  \n    \n      \n        \n          O\n          \n            x\n            ,\n            α\n          \n          \n            n\n            ,\n            u\n          \n        \n        (\n        h\n        )\n        :=\n        \n          O\n          \n            x\n            ,\n            α\n          \n          \n            n\n          \n        \n        (\n        h\n        )\n        ∪\n        \n          O\n          \n            x\n            ,\n            α\n          \n          \n            n\n            ,\n            c\n          \n        \n        (\n        h\n        )\n        .\n      \n    \n    {\\displaystyle O_{x,\\alpha }^{n,u}(h):=O_{x,\\alpha }^{n}(h)\\cup O_{x,\\alpha }^{n,c}(h).}"
      },
      {
        "heading": "Extensión a Funciones Vectoriales",
        "level": 2,
        "content": "Para una función \n  \n    \n      \n        h\n        :\n        Ω\n        ⊂\n        \n          \n            R\n          \n          \n            m\n          \n        \n        →\n        \n          \n            R\n          \n          \n            m\n          \n        \n      \n    \n    {\\displaystyle h:\\Omega \\subset \\mathbb {R} ^{m}\\to \\mathbb {R} ^{m}}\n  \n, el conjunto se define como:\n\n  \n    \n      \n        \n          \n\n          \n          \n            m\n          \n        \n        \n          O\n          \n            x\n            ,\n            α\n          \n          \n            n\n            ,\n            u\n          \n        \n        (\n        h\n        )\n        :=\n        \n          {\n          \n            \n              o\n              \n                x\n              \n              \n                α\n              \n            \n            :\n            \n              o\n              \n                x\n              \n              \n                α\n              \n            \n            ∈\n            \n              O\n              \n                x\n                ,\n                α\n              \n              \n                n\n                ,\n                u\n              \n            \n            (\n            [\n            h\n            \n              ]\n              \n                k\n              \n            \n            )\n             \n            ∀\n            k\n            ≤\n            m\n          \n          }\n        \n        ,\n      \n    \n    {\\displaystyle {}_{m}O_{x,\\alpha }^{n,u}(h):=\\left\\{o_{x}^{\\alpha }:o_{x}^{\\alpha }\\in O_{x,\\alpha }^{n,u}([h]_{k})\\ \\forall k\\leq m\\right\\},}\n  \n\ndonde \n  \n    \n      \n        [\n        h\n        \n          ]\n          \n            k\n          \n        \n        :\n        Ω\n        ⊂\n        \n          \n            R\n          \n          \n            m\n          \n        \n        →\n        \n          R\n        \n      \n    \n    {\\displaystyle [h]_{k}:\\Omega \\subset \\mathbb {R} ^{m}\\to \\mathbb {R} }\n  \n denota el \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n-ésimo componente de la función \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n."
      },
      {
        "heading": "Significado y aplicaciones",
        "level": 2,
        "content": "Mientras que algunas ideas del cálculo fueron desarrolladas tempranamente en las matemáticas griegas, chinas, indias e islámicas, el uso moderno del cálculo comenzó en Europa, durante el siglo XVII, cuando Isaac Newton y Gottfried Leibniz construyeron con base al trabajo de antiguos matemáticos los principios básicos de esta disciplina. El desarrollo del cálculo fue constituido con base en los conceptos de movimiento instantáneo y el área bajo las curvas.\nLas aplicaciones del cálculo diferencial incluyen cómputos que involucran velocidad, aceleración, la pendiente de una recta tangente a una curva y optimización. Las aplicaciones del cálculo integral están en cómputos que incluyen elementos de área, volumen, centro de masa, longitud de arco, trabajo y presión. Aplicaciones más avanzadas incluyen series de potencias y series de Fourier. El cálculo puede ser usado para computar la trayectoria de una nave acoplándose a una estación espacial o la cantidad de nieve en una calzada para coches.\nEl cálculo es también usado para obtener un entendimiento más preciso de la naturaleza del espacio, el tiempo y del movimiento. Por siglos, matemáticos y filósofos lucharon con paradojas que involucraban la división por cero o sumas de series infinitas de números. Estas preguntas surgen en el estudio del movimiento y área. El antiguo filósofo griego Zenón dio varios ejemplos famosos de tales paradojas. El cálculo provee herramientas que pueden resolver tales paradojas, especialmente los límites y las series infinitas."
      },
      {
        "heading": "Fundamentos",
        "level": 2,
        "content": "En matemáticas, los fundamentos se refiere al desarrollo riguroso de un tema desde axiomas y definiciones precisas. El obtener un fundamento riguroso para el cálculo ocupó a los matemáticos por la mayor parte del siglo que siguió a Leibniz y Newton y todavía es un área activa en la actualidad. Los fundamentos del cálculo fueron objeto de diversas especulaciones filosóficas e interpretaciones informales, la falta de rigor y laxitud con que fueron afrontados ciertos problemas de fundamentación contribuyeron a la crisis de los fundamentos de las matemáticas.\nSin embargo, ya durante el siglo XIX se empezó a trabajar en una aproximación rigurosa para los fundamentos del cálculo. El más usual hoy en día es el concepto de límite definido en la continuidad de los números reales (el concepto de límite es esencialmente un concepto topológico). Una alternativa es el análisis no estándar, en el cual el sistema de números reales es aumentado con infinitesimales y números infinitos, como en la concepción original de Newton y Leibniz. Los fundamentos del cálculo son incluidos en el campo del análisis real, el cual contiene las definiciones completas y pruebas matemáticas de los teoremas del cálculo, así como también generalizaciones tales como la teoría de la medida y la teoría de distribuciones."
      },
      {
        "heading": "Principios",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Límites e infinitesimales",
        "level": 2,
        "content": "El cálculo es usualmente desarrollado mediante la manipulación de cantidades muy pequeñas. Históricamente, el primer método para lograr eso se basaba en infinitesimales. Estos son objetos que pueden ser tratados como números reales pero que son, en algún sentido, «infinitamente pequeños». Por ejemplo, un número infinitesimal podría ser mayor que 0, pero menor que cualquer número en la secuencia 1, 1/2, 1/3, ... y por lo tanto menor que cualquier número real positivo. Desde este punto de vista, el cálculo es una colección de técnicas para manipular infinitesimales. Se asumía que los símbolos \n  \n    \n      \n        d\n        x\n      \n    \n    {\\displaystyle dx}\n  \n y \n  \n    \n      \n        d\n        y\n      \n    \n    {\\displaystyle dy}\n  \n eran infinitesimales, y que la derivada \n  \n    \n      \n        d\n        y\n        \n          /\n        \n        d\n        x\n      \n    \n    {\\displaystyle dy/dx}\n  \n era su razón o proporción.[24]​\nLa perspectiva infinitesimal perdió popularidad en el siglo XIX  porque era difícil lograr una noción precisa del infinitesimal. A finales del siglo XIX, los infinitesimales fueron reemplazados dentro de la academia por la perspectiva «epsilon, delta» de los límites. Los límites describen el comportamiento de una función en una cierta entrada en términos de los valores de entradas cercanas. Capturan el comportamiento a pequeña escala utilizando la estructura intrínseca del sistema de números reales (como un espacio métrico con la propiedad del límite superior mínimo). En este contexto, el cálculo es una colección de técnicas para la manipulación de ciertos límites. Los infinitesimales se reemplazan por secuencias de números cada vez más pequeños, y el comportamiento infinitamente pequeño de una función se encuentra tomando el comportamiento límite para estas secuencias. Se creía que los límites ofrecían una base más rigurosa para el cálculo y, por este motivo, se convirtieron en el enfoque estándar durante el siglo XX. Sin embargo, el concepto de infinitesimal cobró fuerza nuevamente en el siglo XX con la introducción del análisis no estándar y del «análisis infinitesimal suave» (del inglés smooth infinitesimal analysis) , los que proporcionaron fundamentos sólidos para la manipulación de infinitesimales.[24]​"
      },
      {
        "heading": "Cálculo diferencial",
        "level": 2,
        "content": "El cálculo diferencial es el estudio de la definición, propiedades, y aplicaciones de la derivada de una función, o lo que es lo mismo, la pendiente de la tangente a lo largo de su gráfica. El proceso de encontrar la derivada se llama derivación o diferenciación. Dada una función y un punto en su dominio, la derivada en ese punto es una forma de codificar el comportamiento a pequeña-escala de la función cerca del punto. Encontrando la derivada de una función para cada punto en su dominio, es posible producir una nueva función, llamada la «función derivada» o simplemente la «derivada» de la función original. En lenguaje técnico, la derivada es un operador lineal, el cual toma una función y devuelve una segunda función, de manera que para cada punto de la primera función, la segunda obtiene la pendiente a la tangente en ese punto.\nEl concepto de derivada es fundamentalmente más avanzado que los conceptos encontrados en el álgebra.\nPara entender la derivada, los estudiantes deben aprender la notación matemática. En notación matemática, un símbolo común para la derivada de una función es una marca parecida a un acento o apóstrofo llamada símbolo primo. Así la derivada de f es f′ (pronunciado «efe prima»). En lo siguiente la segunda función es la derivada de la primera:\n\n  \n    \n      \n        \n          \n            \n              \n                f\n                (\n                x\n                )\n              \n              \n                \n                =\n                \n                  x\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n                \n                  f\n                  ′\n                \n                (\n                x\n                )\n              \n              \n                \n                =\n                2\n                x\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}f(x)&=x^{2}\\\\f'(x)&=2x.\\end{aligned}}}\n  \n\nSi la entrada de la función representa el tiempo, entonces la derivada representa el cambio con respecto del tiempo. Por ejemplo, si f es una función que toma el tiempo como entrada y da la posición de la pelota en ese momento como salida, entonces la derivada de f es cuánto la posición está cambiando en el tiempo, esto es, es la velocidad de la pelota.\nSi la función es lineal (esto es, la gráfica de la función es una línea recta), entonces la función puede ser escrita de la forma y = mx + b, donde:\n\n  \n    \n      \n        m\n        =\n        \n          \n            \n              cambio en y\n            \n            \n              cambio en x\n            \n          \n        \n        =\n        \n          \n            \n              Δ\n              y\n            \n            \n              Δ\n              x\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle m={\\frac {\\mbox{cambio en y}}{\\mbox{cambio en x}}}={\\Delta y \\over {\\Delta x}}.}"
      },
      {
        "heading": "Cálculo integral",
        "level": 2,
        "content": "El cálculo integral es el estudio de las definiciones, propiedades, y aplicaciones de dos conceptos relacionados, la integral indefinida y la integral definida. El proceso de encontrar el valor de una integral es llamado integración. En lenguaje técnico, el cálculo integral estudia dos operadores lineales relacionados.\nLa integral indefinida es la antiderivada, es decir, la operación inversa de la derivada. La función F es una integral indefinida de la función f cuando f es una derivada de F. (El uso de mayúsculas y minúsculas para distinguir entre la función y su integral indefinida es común en el cálculo).\nLa integral definida es un algoritmo que transforma funciones en números, los cuales dan el área entre una curva de un gráfico y el eje-x. La definición técnica de la integral definida es el límite de una suma de áreas de rectángulos, llamada suma de Riemann."
      },
      {
        "heading": "Teorema fundamental",
        "level": 2,
        "content": "El teorema fundamental del cálculo establece que la diferenciación y la integración son operaciones inversas. Más precisamente, relaciona los valores de las antiderivadas para definir las integrales. Ya que es normalmente más fácil computar una antiderivada que aplicar la definición de una integral definida, el teorema fundamental del cálculo provee una forma práctica de computar integrales definidas. También puede ser interpretado como una declaración precisa del hecho de que la diferenciación es la inversa de la integración.\nSi una función f es continua en el intervalo [a, b] y si F es una función cuya derivada es f en el intervalo (a, b), entonces\n\n  \n    \n      \n        \n          ∫\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        =\n        F\n        (\n        b\n        )\n        −\n        F\n        (\n        a\n        )\n        .\n      \n    \n    {\\displaystyle \\int _{a}^{b}f(x)\\,dx=F(b)-F(a).}\n  \n\nAsí entonces, para cada x en el intervalo (a, b), es cierto que:\n\n  \n    \n      \n        \n          \n            d\n            \n              d\n              x\n            \n          \n        \n        \n          ∫\n          \n            a\n          \n          \n            x\n          \n        \n        f\n        (\n        t\n        )\n        \n        d\n        t\n        =\n        f\n        (\n        x\n        )\n        .\n      \n    \n    {\\displaystyle {\\frac {d}{dx}}\\int _{a}^{x}f(t)\\,dt=f(x).}\n  \n\nEste hecho, descubierto tanto por Newton como Leibniz, quienes basaron sus resultados en el trabajo previo de Isaac Barrow, fue clave para la masiva proliferación de resultados analíticos luego que su trabajo fuese conocido. El teorema fundamental provee un método algebraico para calcular muchas integrales definidas – sin realizar el proceso de cálculo de límites – mediante el encuentro de fórmulas apropiadas para las antiderivadas. Las ecuaciones diferenciales relacionan a una función a sus derivadas, y son omnipresentes en las ciencias."
      },
      {
        "heading": "Aplicaciones",
        "level": 1,
        "content": "El cálculo es usado en cada rama de las ciencias naturales, estadística, ingeniería, economía; incluso en negocios, medicina, demografía, y más generalmente en cualquier área donde un problema pueda ser modelado matemáticamente mediante variables continuas de números reales o complejos, y donde una solución óptima sea deseada; o donde se deseen entender los ciclos e interacciones entre las variables. En palabras de Steven Strogatz, «El interior de un átomo, las cambiantes poblaciones de la vida salvaje, el clima… todo eso puede explicarse mediante el lenguaje del cálculo. De alguna manera este lenguaje… es simplemente la mejor herramienta que jamás hayamos inventado».[25]​"
      },
      {
        "heading": "Física",
        "level": 2,
        "content": "La física desde Newton ha hecho un particular uso extenso del cálculo.\n\nTodos los conceptos en la mecánica clásica están interrelacionados a través del cálculo. Por ejemplo, la masa de un objeto de conocida densidad, el momento de inercia de los objetos, la relación entre posición, velocidad y aceleración; así como la energía total de un objeto dentro de un campo conservativo pueden ser encontrados por el uso del cálculo. El ejemplo clásico del uso del cálculo en la física son las leyes del movimiento de Newton, donde se usa expresamente el término «tasa de cambio» el cual hace referencia a la derivada: «La tasa de cambio de momentum de un cuerpo es igual a la fuerza resultante actuando en el cuerpo y está también en la misma dirección». Incluso la expresión común de la segunda ley de Newton como \n  \n    \n      \n        F\n        u\n        e\n        r\n        z\n        a\n        =\n        m\n        a\n        s\n        a\n        ×\n        a\n        c\n        e\n        l\n        e\n        r\n        a\n        c\n        i\n        \n          \n            \n              o\n              ´\n            \n          \n        \n        n\n      \n    \n    {\\textstyle Fuerza=masa\\times aceleraci{\\acute {o}}n}\n  \n involucra el cálculo diferencial, porque la aceleración puede ser expresada como la derivada de la velocidad (\n  \n    \n      \n        F\n        u\n        e\n        r\n        z\n        a\n        =\n        m\n        a\n        s\n        a\n        ×\n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              (\n              p\n              o\n              s\n              i\n              c\n              i\n              \n                \n                  \n                    o\n                    ´\n                  \n                \n              \n              n\n              )\n            \n            \n              d\n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\textstyle Fuerza=masa\\times {\\frac {d^{2}(posici{\\acute {o}}n)}{dt^{2}}}}\n  \n).\nEl electromagnetismo ha usado ampliamente notación del cálculo desde que aparecieron las ecuaciones de Maxwell, y lo mismo es cierto para versiones más modernas que combinan esto con mecánica cuántica o relatividad, como la electrodinámica cuántica.\nLa teoría de la relatividad general de Einstein.\nUna de las leyes más fundamentales de la mecánica cuántica, la ecuación de Schrödinger, también está expresada en el lenguaje del cálculo. Específicamente se trata de una ecuación diferencial parcial que describe la evolución de la distribución de una onda-partícula o sistema de partículas.\nEl calor y la difusión son estudiados como ecuaciones diferenciales parciales. Ver ecuación del calor, y ecuación de difusión."
      },
      {
        "heading": "Química e ingeniería",
        "level": 2,
        "content": "En ingeniería eléctrica y electrónica, el cálculo es indispensable para entender las propiedades de cualquier circuito eléctrico que involucre capacitores o inductores.\nLa química también usa el cálculo para determinar los ritmos de las reacciones y el decaimiento radioactivo."
      },
      {
        "heading": "Ciencias biológicas",
        "level": 2,
        "content": "En biología, Alan Turing mostró que muchos aspectos del desarrollo morfológico de los organismos pueden predecirse a partir de modelar señales genético-químicas como ecuaciones diferenciales de reacción-difusión.\nEn evolución y genética de poblaciones, la ecuación de Price se usa para estudiar los cambios de las características heredables en función de su ventaja selectiva.\nEn neurociencia, las ecuaciones de Hodgkin y Huxley que explican cómo una neurona emite potenciales de acción, también están expresadas en el lenguaje del cálculo.\nEn la medicina, el cálculo puede ser usado para encontrar el ángulo de ramificación óptimo de vaso sanguíneo para maximizar el flujo.\nTambién en medicina, se pueden usar leyes de decaimiento para calcular dosis farmacológicas o para planificar radioterapias."
      },
      {
        "heading": "Ciencias sociales",
        "level": 2,
        "content": "En demografía y ecosistemas, modelos de crecimiento poblacional como el de Malthus, el de Verhulst o el de Lotka-Volterra están descritos en términos de cálculo.\nEn epidemiología existen muchos modelos de sistemas dinámicos para predecir la propagación de enfermedades infecciosas. Estos reciben el nombre de \"modelos compartamentales\", como es el modelo SEIR (poblaciones Susceptible, Expuesta, Infectada y Recuperada).\nEn economía, el cálculo permite por ejemplo, determinar la utilidad máxima mediante el cálculo de costos marginales e ingresos marginales.\nEn geografía, de la geomorfología surge el concepto de 'geomorfometría', que se apoya en las derivadas de primer orden (f'x) y segundo orden (f\"x) para modelar superficies elevadas."
      },
      {
        "heading": "Matemáticas",
        "level": 2,
        "content": "El cálculo también puede ser usado en conjunto con otras disciplinas matemáticas. Por ejemplo, puede ser usado con el álgebra lineal para encontrar la mejor aproximación lineal para un conjunto de puntos en un dominio. También puede ser usado en la teoría de la probabilidad para determinar la probabilidad de una variable aleatoria continua a partir de su función de densidad de probabilidad.\nEl teorema de Green, el cual establece la relación entre una integral lineal alrededor una simple curva cerrada C y una doble integral sobre el plano de región D delimitada por C, es aplicado en un instrumento conocido como planímetro, el cual es usado para calcular el área de una superficie plana en un dibujo. Por ejemplo, puede ser usado para calcular la cantidad de área que toma una piscina cuando se bosqueja el diseño de un pedazo de propiedad.\nEn geometría analítica, el estudio de los gráficos de funciones, el cálculo es usado para encontrar puntos máximos y mínimos, la tangente, así también como para determinar la concavidad y los puntos de inflexión.\nEl cálculo también puede ser usado para encontrar soluciones aproximadas para ecuaciones, usando métodos como por ejemplo el método de Newton, la iteración de punto fijo y la aproximación lineal. Por ejemplo, las naves espaciales usan una variación del método de Euler para aproximar trayectorias curvas dentro de entornos de gravedad cero."
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": "Derivada\nDiferencial\nIntegral\nGottfried Leibniz\nIsaac Newton\nSeki Kōwa\nMáquina diferencial"
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": "Cálculo de funciones"
      }
    ],
    "summary": "El cálculo infinitesimal o simplemente cálculo constituye una rama muy importante de las matemáticas. En la misma manera que la geometría estudia el espacio y el álgebra estudia las estructuras abstractas, el cálculo es el estudio del cambio y la continuidad (más concretamente, de los cambios continuos, en oposición a los discretos).\nEl cálculo infinitesimal se divide en dos áreas: cálculo diferencial y cálculo integral. El cálculo diferencial estudia cómo computar la función que describe el cambio de otra función de variables continuas (operación de orden superior llamada «derivada»), mientras que el cálculo integral estudia la operación inversa (antiderivadas e integrales) y las series infinitas. En su formulación contemporánea, ambos campos se fundamentan en el concepto de límite para poder calcular cambios infinitesimalmente pequeños; y se relacionan por medio del teorema fundamental del cálculo.\nDesde su aparición en el siglo XVII, el cálculo infinitesimal se ha vuelto imprescindible para la ciencia y la ingeniería (ver sección de Aplicaciones) y constituye gran parte de la educación universitaria moderna. Marcó un hito en la Revolución científica; al grado de que algunos historiadores fechan el inicio de la Ilustración con la publicación de las obras de Newton.[1]​\nSe usa para resolver muchos problemas para los cuales las matemáticas de la antigüedad fueron insuficientes; si bien parte de conocimientos clásicos en álgebra, trigonometría y geometría analítica. Encontrar la tangente en un punto a una curva, hacer mediciones exactas de longitudes, áreas y volúmenes curvos; determinar si una suma de infinitos sumandos converge o diverge, y encontrar situaciones de equilibrio y optimización en funciones de números reales son ejemplos de las puertas que el cálculo vino a abrir para las matemáticas. A su vez, el cálculo tiene generalizaciones y aplicaciones en otras áreas de la matemática; como ecuaciones diferenciales y sistemas dinámicos, teoría del caos, cálculo vectorial, geometría diferencial, topología, análisis matemático, probabilidad, estadística, etc.\nEn la matemática contemporánea y en los programas de estudio para matemáticos, el cálculo es usualmente abordado como una introducción a la disciplina conocida como análisis matemático, que generaliza y formaliza el estudio de funciones y límites."
  },
  {
    "title": "Probability",
    "source": "https://en.wikipedia.org/wiki/Probability",
    "language": "en",
    "chunks": [
      {
        "heading": "Etymology",
        "level": 1,
        "content": "The word probability derives from the Latin probabilitas, which can also mean \"probity\", a measure of the authority of a witness in a legal case in Europe, and often correlated with the witness's nobility. In a sense, this differs much from the modern meaning of probability, which in contrast is a measure of the weight of empirical evidence, and is arrived at from inductive reasoning and statistical inference."
      },
      {
        "heading": "Interpretations",
        "level": 1,
        "content": "When dealing with random experiments – i.e., experiments that are random and well-defined – in a purely theoretical setting (like tossing a coin), probabilities can be numerically described by the number of desired outcomes, divided by the total number of all outcomes. This is referred to as theoretical probability (in contrast to empirical probability, dealing with probabilities in the context of real experiments). For example, tossing a coin twice will yield \"head-head\", \"head-tail\", \"tail-head\", and \"tail-tail\" outcomes. The probability of getting an outcome of \"head-head\" is 1 out of 4 outcomes, or, in numerical terms, 1/4, 0.25 or 25%. However, when it comes to practical application, there are two major competing categories of probability interpretations, whose adherents hold different views about the fundamental nature of probability:\n\nObjectivists assign numbers to describe some objective or physical state of affairs. The most popular version of objective probability is frequentist probability, which claims that the probability of a random event denotes the relative frequency of occurrence of an experiment's outcome when the experiment is repeated indefinitely. This interpretation considers probability to be the relative frequency \"in the long run\" of outcomes. A modification of this is propensity probability, which interprets probability as the tendency of some experiment to yield a certain outcome, even if it is performed only once.\nSubjectivists assign numbers per subjective probability, that is, as a degree of belief. The degree of belief has been interpreted as \"the price at which you would buy or sell a bet that pays 1 unit of utility if E, 0 if not E\", although that interpretation is not universally agreed upon. The most popular version of subjective probability is Bayesian probability, which includes expert knowledge as well as experimental data to produce probabilities. The expert knowledge is represented by some (subjective) prior probability distribution. These data are incorporated in a likelihood function. The product of the prior and the likelihood, when normalized, results in a posterior probability distribution that incorporates all the information known to date. By Aumann's agreement theorem, Bayesian agents whose prior beliefs are similar will end up with similar posterior beliefs. However, sufficiently different priors can lead to different conclusions, regardless of how much information the agents share."
      },
      {
        "heading": "History",
        "level": 1,
        "content": "The scientific study of probability is a modern development of mathematics. Gambling shows that there has been an interest in quantifying the ideas of probability throughout history, but exact mathematical descriptions arose much later. There are reasons for the slow development of the mathematics of probability. Whereas games of chance provided the impetus for the mathematical study of probability, fundamental issues  are still obscured by superstitions.\nAccording to Richard Jeffrey, \"Before the middle of the seventeenth century, the term 'probable' (Latin probabilis) meant approvable, and was applied in that sense, univocally, to opinion and to action. A probable action or opinion was one such as sensible people would undertake or hold, in the circumstances.\" However, in legal contexts especially, 'probable' could also apply to propositions for which there was good evidence.\n\nThe sixteenth-century Italian polymath Gerolamo Cardano demonstrated the efficacy of defining odds as the ratio of favourable to unfavourable outcomes (which implies that the probability of an event is given by the ratio of favourable outcomes to the total number of possible outcomes).\nAside from the elementary work by Cardano, the doctrine of probabilities dates to the correspondence of Pierre de Fermat and Blaise Pascal (1654). Christiaan Huygens (1657) gave the earliest known scientific treatment of the subject. Jakob Bernoulli's Ars Conjectandi (posthumous, 1713) and Abraham de Moivre's Doctrine of Chances (1718) treated the subject as a branch of mathematics. See Ian Hacking's The Emergence of Probability and James Franklin's The Science of Conjecture for histories of the early development of the very concept of mathematical probability.\nThe theory of errors may be traced back to Roger Cotes's Opera Miscellanea (posthumous, 1722), but a memoir prepared by Thomas Simpson in 1755 (printed 1756) first applied the theory to the discussion of errors of observation. The reprint (1757) of this memoir lays down the axioms that positive and negative errors are equally probable, and that certain assignable limits define the range of all errors. Simpson also discusses continuous errors and describes a probability curve.\nThe first two laws of error that were proposed both originated with Pierre-Simon Laplace. The first law was published in 1774, and stated that the frequency of an error could be expressed as an exponential function of the numerical magnitude of the error – disregarding sign. The second law of error was proposed in 1778 by Laplace, and stated that the frequency of the error is an exponential function of the square of the error. The second law of error is called the normal distribution or the Gauss law. \"It is difficult historically to attribute that law to Gauss, who in spite of his well-known precocity had probably not made this discovery before he was two years old.\"\nDaniel Bernoulli (1778) introduced the principle of the maximum product of the probabilities of a system of concurrent errors.\n\nAdrien-Marie Legendre (1805) developed the method of least squares, and introduced it in his Nouvelles méthodes pour la détermination des orbites des comètes (New Methods for Determining the Orbits of Comets). In ignorance of Legendre's contribution, an Irish-American writer, Robert Adrain, editor of \"The Analyst\" (1808), first deduced the law of facility of error,\n\n  \n    \n      \n        ϕ\n        (\n        x\n        )\n        =\n        c\n        \n          e\n          \n            −\n            \n              h\n              \n                2\n              \n            \n            \n              x\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\phi (x)=ce^{-h^{2}x^{2}}}\n  \n\nwhere \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n is a constant depending on precision of observation, and \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n is a scale factor ensuring that the area under the curve equals 1. He gave two proofs, the second being essentially the same as John Herschel's (1850). Gauss gave the first proof that seems to have been known in Europe (the third after Adrain's) in 1809. Further proofs were given by Laplace (1810, 1812), Gauss (1823), James Ivory (1825, 1826), Hagen (1837), Friedrich Bessel (1838), W.F. Donkin (1844, 1856), and Morgan Crofton (1870). Other contributors were Ellis (1844), De Morgan (1864), Glaisher (1872), and Giovanni Schiaparelli (1875). Peters's (1856) formula for r, the probable error of a single observation, is well known.\nIn the nineteenth century, authors on the general theory included Laplace, Sylvestre Lacroix (1816), Littrow (1833), Adolphe Quetelet (1853), Richard Dedekind (1860), Helmert (1872), Hermann Laurent (1873), Liagre, Didion and Karl Pearson. Augustus De Morgan and George Boole improved the exposition of the theory.\nIn 1906, Andrey Markov introduced the notion of Markov chains, which played an important role in stochastic processes theory and its applications. The modern theory of probability based on measure theory was developed by Andrey Kolmogorov in 1931.\nOn the geometric side, contributors to The Educational Times included Miller, Crofton, McColl, Wolstenholme, Watson, and Artemas Martin. See integral geometry for more information."
      },
      {
        "heading": "Theory",
        "level": 1,
        "content": "Like other theories, the theory of probability is a representation of its concepts in formal terms – that is, in terms that can be considered separately from their meaning. These formal terms are manipulated by the rules of mathematics and logic, and any results are interpreted or translated back into the problem domain.\nThere have been at least two successful attempts to formalize probability, namely the Kolmogorov formulation and the Cox formulation. In Kolmogorov's formulation (see also probability space), sets are interpreted as events and probability as a measure on a class of sets. In Cox's theorem, probability is taken as a primitive (i.e., not further analyzed), and the emphasis is on constructing a consistent assignment of probability values to propositions. In both cases, the laws of probability are the same, except for technical details.\nThere are other methods for quantifying uncertainty, such as the Dempster–Shafer theory or possibility theory, but those are essentially different and not compatible with the usually-understood laws of probability."
      },
      {
        "heading": "Applications",
        "level": 1,
        "content": "Probability theory is applied in everyday life in risk assessment and modeling. The insurance industry and markets use actuarial science to determine pricing and make trading decisions. Governments apply probabilistic methods in environmental regulation, entitlement analysis, and financial regulation.\nAn example of the use of probability theory in equity trading is the effect of the perceived probability of any widespread Middle East conflict on oil prices, which have ripple effects in the economy as a whole. An assessment by a commodity trader that a war is more likely can send that commodity's prices up or down, and signals other traders of that opinion. Accordingly, the probabilities are neither assessed independently nor necessarily rationally. The theory of behavioral finance emerged to describe the effect of such groupthink on pricing, on policy, and on peace and conflict.\nIn addition to financial assessment, probability can be used to analyze trends in biology (e.g., disease spread) as well as ecology (e.g., biological Punnett squares). As with finance, risk assessment can be used as a statistical tool to calculate the likelihood of undesirable events occurring, and can assist with implementing protocols to avoid encountering such circumstances. Probability is used to design games of chance so that casinos can make a guaranteed profit, yet provide payouts to players that are frequent enough to encourage continued play.\nAnother significant application of probability theory in everyday life is reliability. Many consumer products, such as automobiles and consumer electronics, use reliability theory in product design to reduce the probability of failure. Failure probability may influence a manufacturer's decisions on a product's warranty.\nThe cache language model and other statistical language models that are used in natural language processing are also examples of applications of probability theory."
      },
      {
        "heading": "Mathematical treatment",
        "level": 1,
        "content": "Consider an experiment that can produce a number of results. The collection of all possible results is called the sample space of the experiment, sometimes denoted as \n  \n    \n      \n        Ω\n      \n    \n    {\\displaystyle \\Omega }\n  \n. The power set of the sample space is formed by considering all different collections of possible results. For example, rolling a die can produce six possible results. One collection of possible results gives an odd number on the die. Thus, the subset {1,3,5} is an element of the power set of the sample space of dice rolls. These collections are called \"events\". In this case, {1,3,5} is the event that the die falls on some odd number. If the results that actually occur fall in a given event, the event is said to have occurred.\nA probability is a way of assigning every event a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) is assigned a value of one. To qualify as a probability, the assignment of values must satisfy the requirement that for any collection of mutually exclusive events (events with no common results, such as the events {1,6}, {3}, and {2,4}), the probability that at least one of the events will occur is given by the sum of the probabilities of all the individual events.\nThe probability of an event A is written as \n  \n    \n      \n        P\n        (\n        A\n        )\n      \n    \n    {\\displaystyle P(A)}\n  \n, \n  \n    \n      \n        p\n        (\n        A\n        )\n      \n    \n    {\\displaystyle p(A)}\n  \n, or \n  \n    \n      \n        \n          Pr\n        \n        (\n        A\n        )\n      \n    \n    {\\displaystyle {\\text{Pr}}(A)}\n  \n. This mathematical definition of probability can extend to infinite sample spaces, and even uncountable sample spaces, using the concept of a measure.\nThe opposite or complement of an event A is the event [not A] (that is, the event of A not occurring), often denoted as \n  \n    \n      \n        \n          A\n          ′\n        \n        ,\n        \n          A\n          \n            c\n          \n        \n      \n    \n    {\\displaystyle A',A^{c}}\n  \n, \n  \n    \n      \n        \n          \n            A\n            ¯\n          \n        \n        ,\n        \n          A\n          \n            ∁\n          \n        \n        ,\n        ¬\n        A\n      \n    \n    {\\displaystyle {\\overline {A}},A^{\\complement },\\neg A}\n  \n, or \n  \n    \n      \n        \n          ∼\n        \n        A\n      \n    \n    {\\displaystyle {\\sim }A}\n  \n; its probability is given by P(not A) = 1 − P(A). As an example, the chance of not rolling a six on a six-sided die is 1 – (chance of rolling a six) = 1 − ⁠1/6⁠ = ⁠5/6⁠. For a more comprehensive treatment, see Complementary event.\nIf two events A and B occur on a single performance of an experiment, this is called the intersection or joint probability of A and B, denoted as \n  \n    \n      \n        P\n        (\n        A\n        ∩\n        B\n        )\n        .\n      \n    \n    {\\displaystyle P(A\\cap B).}"
      },
      {
        "heading": "Independent events",
        "level": 2,
        "content": "If two events, A and B are independent then the joint probability is\n\n  \n    \n      \n        P\n        (\n        A\n        \n          \n             and \n          \n        \n        B\n        )\n        =\n        P\n        (\n        A\n        ∩\n        B\n        )\n        =\n        P\n        (\n        A\n        )\n        P\n        (\n        B\n        )\n        .\n      \n    \n    {\\displaystyle P(A{\\mbox{ and }}B)=P(A\\cap B)=P(A)P(B).}\n  \n\nFor example, if two coins are flipped, then the chance of both being heads is \n  \n    \n      \n        \n          \n            \n              1\n              2\n            \n          \n        \n        ×\n        \n          \n            \n              1\n              2\n            \n          \n        \n        =\n        \n          \n            \n              1\n              4\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\tfrac {1}{2}}\\times {\\tfrac {1}{2}}={\\tfrac {1}{4}}.}"
      },
      {
        "heading": "Mutually exclusive events",
        "level": 2,
        "content": "If either event A or event B can occur but never both simultaneously, then they are called mutually exclusive events.\nIf two events are mutually exclusive, then the probability of both occurring is denoted as \n  \n    \n      \n        P\n        (\n        A\n        ∩\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cap B)}\n  \n and\n  \n    \n      \n        P\n        (\n        A\n        \n          \n             and \n          \n        \n        B\n        )\n        =\n        P\n        (\n        A\n        ∩\n        B\n        )\n        =\n        0\n      \n    \n    {\\displaystyle P(A{\\mbox{ and }}B)=P(A\\cap B)=0}\n  \n If two events are mutually exclusive, then the probability of either occurring is denoted as \n  \n    \n      \n        P\n        (\n        A\n        ∪\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cup B)}\n  \n and\n  \n    \n      \n        P\n        (\n        A\n        \n          \n             or \n          \n        \n        B\n        )\n        =\n        P\n        (\n        A\n        ∪\n        B\n        )\n        =\n        P\n        (\n        A\n        )\n        +\n        P\n        (\n        B\n        )\n        −\n        P\n        (\n        A\n        ∩\n        B\n        )\n        =\n        P\n        (\n        A\n        )\n        +\n        P\n        (\n        B\n        )\n        −\n        0\n        =\n        P\n        (\n        A\n        )\n        +\n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(A{\\mbox{ or }}B)=P(A\\cup B)=P(A)+P(B)-P(A\\cap B)=P(A)+P(B)-0=P(A)+P(B)}\n  \n\nFor example, the chance of rolling a 1 or 2 on a six-sided die is \n  \n    \n      \n        P\n        (\n        1\n        \n          \n             or \n          \n        \n        2\n        )\n        =\n        P\n        (\n        1\n        )\n        +\n        P\n        (\n        2\n        )\n        =\n        \n          \n            \n              1\n              6\n            \n          \n        \n        +\n        \n          \n            \n              1\n              6\n            \n          \n        \n        =\n        \n          \n            \n              1\n              3\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle P(1{\\mbox{ or }}2)=P(1)+P(2)={\\tfrac {1}{6}}+{\\tfrac {1}{6}}={\\tfrac {1}{3}}.}"
      },
      {
        "heading": "Not (necessarily) mutually exclusive events",
        "level": 2,
        "content": "If the events are not (necessarily) mutually exclusive then\n  \n    \n      \n        P\n        \n          (\n          \n            A\n            \n              \n                 or \n              \n            \n            B\n          \n          )\n        \n        =\n        P\n        (\n        A\n        ∪\n        B\n        )\n        =\n        P\n        \n          (\n          A\n          )\n        \n        +\n        P\n        \n          (\n          B\n          )\n        \n        −\n        P\n        \n          (\n          \n            A\n            \n              \n                 and \n              \n            \n            B\n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle P\\left(A{\\hbox{ or }}B\\right)=P(A\\cup B)=P\\left(A\\right)+P\\left(B\\right)-P\\left(A{\\mbox{ and }}B\\right).}\n  \n Rewritten,\n  \n    \n      \n        P\n        \n          (\n          \n            A\n            ∪\n            B\n          \n          )\n        \n        =\n        P\n        \n          (\n          A\n          )\n        \n        +\n        P\n        \n          (\n          B\n          )\n        \n        −\n        P\n        \n          (\n          \n            A\n            ∩\n            B\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(A\\cup B\\right)=P\\left(A\\right)+P\\left(B\\right)-P\\left(A\\cap B\\right)}\n  \n\nFor example, when drawing a card from a deck of cards, the chance of getting a heart or a face card (J, Q, K) (or both) is \n  \n    \n      \n        \n          \n            \n              13\n              52\n            \n          \n        \n        +\n        \n          \n            \n              12\n              52\n            \n          \n        \n        −\n        \n          \n            \n              3\n              52\n            \n          \n        \n        =\n        \n          \n            \n              11\n              26\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\tfrac {13}{52}}+{\\tfrac {12}{52}}-{\\tfrac {3}{52}}={\\tfrac {11}{26}},}\n  \n since among the 52 cards of a deck, 13 are hearts, 12 are face cards, and 3 are both: here the possibilities included in the \"3 that are both\" are included in each of the \"13 hearts\" and the \"12 face cards\", but should only be counted once.\nThis can be expanded further for multiple not (necessarily) mutually exclusive events. For three events, this proceeds as follows:\n  \n    \n      \n        \n          \n            \n              \n                P\n                \n                  (\n                  \n                    A\n                    ∪\n                    B\n                    ∪\n                    C\n                  \n                  )\n                \n                =\n              \n              \n                P\n                \n                  (\n                  \n                    \n                      (\n                      \n                        A\n                        ∪\n                        B\n                      \n                      )\n                    \n                    ∪\n                    C\n                  \n                  )\n                \n              \n            \n            \n              \n                =\n              \n              \n                P\n                \n                  (\n                  \n                    A\n                    ∪\n                    B\n                  \n                  )\n                \n                +\n                P\n                \n                  (\n                  C\n                  )\n                \n                −\n                P\n                \n                  (\n                  \n                    \n                      (\n                      \n                        A\n                        ∪\n                        B\n                      \n                      )\n                    \n                    ∩\n                    C\n                  \n                  )\n                \n              \n            \n            \n              \n                =\n              \n              \n                P\n                \n                  (\n                  A\n                  )\n                \n                +\n                P\n                \n                  (\n                  B\n                  )\n                \n                −\n                P\n                \n                  (\n                  \n                    A\n                    ∩\n                    B\n                  \n                  )\n                \n                +\n                P\n                \n                  (\n                  C\n                  )\n                \n                −\n                P\n                \n                  (\n                  \n                    \n                      (\n                      \n                        A\n                        ∩\n                        C\n                      \n                      )\n                    \n                    ∪\n                    \n                      (\n                      \n                        B\n                        ∩\n                        C\n                      \n                      )\n                    \n                  \n                  )\n                \n              \n            \n            \n              \n                =\n              \n              \n                P\n                \n                  (\n                  A\n                  )\n                \n                +\n                P\n                \n                  (\n                  B\n                  )\n                \n                +\n                P\n                \n                  (\n                  C\n                  )\n                \n                −\n                P\n                \n                  (\n                  \n                    A\n                    ∩\n                    B\n                  \n                  )\n                \n                −\n                \n                  (\n                  \n                    P\n                    \n                      (\n                      \n                        A\n                        ∩\n                        C\n                      \n                      )\n                    \n                    +\n                    P\n                    \n                      (\n                      \n                        B\n                        ∩\n                        C\n                      \n                      )\n                    \n                    −\n                    P\n                    \n                      (\n                      \n                        \n                          (\n                          \n                            A\n                            ∩\n                            C\n                          \n                          )\n                        \n                        ∩\n                        \n                          (\n                          \n                            B\n                            ∩\n                            C\n                          \n                          )\n                        \n                      \n                      )\n                    \n                  \n                  )\n                \n              \n            \n            \n              \n                P\n                \n                  (\n                  \n                    A\n                    ∪\n                    B\n                    ∪\n                    C\n                  \n                  )\n                \n                =\n              \n              \n                P\n                \n                  (\n                  A\n                  )\n                \n                +\n                P\n                \n                  (\n                  B\n                  )\n                \n                +\n                P\n                \n                  (\n                  C\n                  )\n                \n                −\n                P\n                \n                  (\n                  \n                    A\n                    ∩\n                    B\n                  \n                  )\n                \n                −\n                P\n                \n                  (\n                  \n                    A\n                    ∩\n                    C\n                  \n                  )\n                \n                −\n                P\n                \n                  (\n                  \n                    B\n                    ∩\n                    C\n                  \n                  )\n                \n                +\n                P\n                \n                  (\n                  \n                    A\n                    ∩\n                    B\n                    ∩\n                    C\n                  \n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}P\\left(A\\cup B\\cup C\\right)=&P\\left(\\left(A\\cup B\\right)\\cup C\\right)\\\\=&P\\left(A\\cup B\\right)+P\\left(C\\right)-P\\left(\\left(A\\cup B\\right)\\cap C\\right)\\\\=&P\\left(A\\right)+P\\left(B\\right)-P\\left(A\\cap B\\right)+P\\left(C\\right)-P\\left(\\left(A\\cap C\\right)\\cup \\left(B\\cap C\\right)\\right)\\\\=&P\\left(A\\right)+P\\left(B\\right)+P\\left(C\\right)-P\\left(A\\cap B\\right)-\\left(P\\left(A\\cap C\\right)+P\\left(B\\cap C\\right)-P\\left(\\left(A\\cap C\\right)\\cap \\left(B\\cap C\\right)\\right)\\right)\\\\P\\left(A\\cup B\\cup C\\right)=&P\\left(A\\right)+P\\left(B\\right)+P\\left(C\\right)-P\\left(A\\cap B\\right)-P\\left(A\\cap C\\right)-P\\left(B\\cap C\\right)+P\\left(A\\cap B\\cap C\\right)\\end{aligned}}}\n  \nIt can be seen, then, that this pattern can be repeated for any number of events."
      },
      {
        "heading": "Conditional probability",
        "level": 2,
        "content": "Conditional probability is the probability of some event A, given the occurrence of some other event B. Conditional probability is written \n  \n    \n      \n        P\n        (\n        A\n        ∣\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\mid B)}\n  \n, and is read \"the probability of A, given B\". It is defined by\n\n  \n    \n      \n        P\n        (\n        A\n        ∣\n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              ∩\n              B\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n        \n      \n    \n    {\\displaystyle P(A\\mid B)={\\frac {P(A\\cap B)}{P(B)}}\\,}\n  \n\nIf \n  \n    \n      \n        P\n        (\n        B\n        )\n        =\n        0\n      \n    \n    {\\displaystyle P(B)=0}\n  \n then \n  \n    \n      \n        P\n        (\n        A\n        ∣\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\mid B)}\n  \n is formally undefined by this expression. In this case \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n are independent, since \n  \n    \n      \n        P\n        (\n        A\n        ∩\n        B\n        )\n        =\n        P\n        (\n        A\n        )\n        P\n        (\n        B\n        )\n        =\n        0.\n      \n    \n    {\\displaystyle P(A\\cap B)=P(A)P(B)=0.}\n  \n However, it is possible to define a conditional probability for some zero-probability events, for example by using a σ-algebra of such events (such as those arising from a continuous random variable).\nFor example, in a bag of 2 red balls and 2 blue balls (4 balls in total), the probability of taking a red ball is \n  \n    \n      \n        1\n        \n          /\n        \n        2\n        ;\n      \n    \n    {\\displaystyle 1/2;}\n  \n however, when taking a second ball, the probability of it being either a red ball or a blue ball depends on the ball previously taken. For example, if a red ball was taken, then the probability of picking a red ball again would be \n  \n    \n      \n        1\n        \n          /\n        \n        3\n        ,\n      \n    \n    {\\displaystyle 1/3,}\n  \n since only 1 red and 2 blue balls would have been remaining. And if a blue ball was taken previously, the probability of taking a red ball will be \n  \n    \n      \n        2\n        \n          /\n        \n        3.\n      \n    \n    {\\displaystyle 2/3.}"
      },
      {
        "heading": "Inverse probability",
        "level": 2,
        "content": "In probability theory and applications, Bayes' rule relates the odds of event \n  \n    \n      \n        \n          A\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle A_{1}}\n  \n to event \n  \n    \n      \n        \n          A\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle A_{2},}\n  \n before (prior to) and after (posterior to) conditioning on another event \n  \n    \n      \n        B\n        .\n      \n    \n    {\\displaystyle B.}\n  \n The odds on \n  \n    \n      \n        \n          A\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle A_{1}}\n  \n to event \n  \n    \n      \n        \n          A\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle A_{2}}\n  \n is simply the ratio of the probabilities of the two events. When arbitrarily many events \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n are of interest, not just two, the rule can be rephrased as posterior is proportional to prior times likelihood, \n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n        ∝\n        P\n        (\n        A\n        )\n        P\n        (\n        B\n        \n          |\n        \n        A\n        )\n      \n    \n    {\\displaystyle P(A|B)\\propto P(A)P(B|A)}\n  \n where the proportionality symbol means that the left hand side is proportional to (i.e., equals a constant times) the right hand side as \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n varies, for fixed or given \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n (Lee, 2012; Bertsch McGrayne, 2012). In this form it goes back to Laplace (1774) and to Cournot (1843); see Fienberg (2005)."
      },
      {
        "heading": "Summary of probabilities",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Relation to randomness and probability in quantum mechanics",
        "level": 1,
        "content": "In a deterministic universe, based on Newtonian concepts, there would be no probability if all conditions were known (Laplace's demon) (but there are situations in which sensitivity to initial conditions exceeds our ability to measure them, i.e. know them).  In the case of a roulette wheel, if the force of the hand and the period of that force are known, the number on which the ball will stop would be a certainty (though as a practical matter, this would likely be true only of a roulette wheel that had not been exactly levelled – as Thomas A. Bass' Newtonian Casino revealed).  This also assumes knowledge of inertia and friction of the wheel, weight, smoothness, and roundness of the ball, variations in hand speed during the turning, and so forth. A probabilistic description can thus be more useful than Newtonian mechanics for analyzing the pattern of outcomes of repeated rolls of a roulette wheel. Physicists face the same situation in the kinetic theory of gases, where the system, while deterministic in principle, is so complex (with the number of molecules typically the order of magnitude of the Avogadro constant 6.02×1023) that only a statistical description of its properties is feasible.\nProbability theory is required to describe quantum phenomena. A revolutionary discovery of early 20th century physics was the random character of all physical processes that occur at sub-atomic scales and are governed by the laws of quantum mechanics. The objective wave function evolves deterministically but, according to the Copenhagen interpretation, it deals with probabilities of observing, the outcome being explained by a wave function collapse when an observation is made. However, the loss of determinism for the sake of instrumentalism did not meet with universal approval. Albert Einstein famously remarked in a letter to Max Born: \"I am convinced that God does not play dice\". Like Einstein, Erwin Schrödinger, who discovered the wave function, believed quantum mechanics is a statistical approximation of an underlying deterministic reality. In some modern interpretations of the statistical mechanics of measurement, quantum decoherence is invoked to account for the appearance of subjectively probabilistic experimental outcomes."
      },
      {
        "heading": "See also",
        "level": 1,
        "content": "Contingency\nEquiprobability\nFuzzy logic\nHeuristic (psychology)"
      },
      {
        "heading": "Notes",
        "level": 1,
        "content": ""
      },
      {
        "heading": "References",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bibliography",
        "level": 1,
        "content": "Kallenberg, O. (2005) Probabilistic Symmetries and Invariance Principles. Springer-Verlag, New York. 510 pp. ISBN 0-387-25115-4\nKallenberg, O. (2002) Foundations of Modern Probability, 2nd ed. Springer Series in Statistics. 650 pp. ISBN 0-387-95313-2\nOlofsson, Peter (2005) Probability, Statistics, and Stochastic Processes, Wiley-Interscience. 504 pp ISBN 0-471-67969-0."
      },
      {
        "heading": "External links",
        "level": 1,
        "content": "\nVirtual Laboratories in Probability and Statistics (Univ. of Ala.-Huntsville)\nProbability  on In Our Time at the BBC\nProbability and Statistics EBook\nEdwin Thompson Jaynes. Probability Theory: The Logic of Science. Preprint: Washington University, (1996). – HTML index with links to PostScript files and PDF (first three chapters)\nPeople from the History of Probability and Statistics (Univ. of Southampton)\nProbability and Statistics on the Earliest Uses Pages (Univ. of Southampton)\nEarliest Uses of Symbols in Probability and Statistics on Earliest Uses of Various Mathematical Symbols\nA tutorial on probability and Bayes' theorem devised for first-year Oxford University students\nU B U W E B :: La Monte Young pdf file of An Anthology of Chance Operations (1963) at UbuWeb\nIntroduction to Probability – eBook Archived 27 July 2011 at the Wayback Machine, by Charles Grinstead, Laurie Snell Source Archived 25 March 2012 at the Wayback Machine (GNU Free Documentation License)\n(in English and Italian) Bruno de Finetti, Probabilità e induzione, Bologna, CLUEB, 1993. ISBN 88-8091-176-7 (digital version)\nRichard Feynman's Lecture on probability."
      }
    ],
    "summary": "Probability is a branch of mathematics and statistics concerning events and numerical descriptions of how likely they are to occur.  The probability of an event is a number between 0 and 1; the larger the probability, the more likely an event is to occur. This number is often expressed as a percentage (%), ranging from 0% to 100%. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes (\"heads\" and \"tails\") are both equally probable; the probability of \"heads\" equals the probability of \"tails\"; and since no other outcomes are possible, the probability of either \"heads\" or \"tails\" is 1/2 (which could also be written as 0.5 or 50%).\nThese concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in areas of study such as statistics, mathematics, science, finance, gambling, artificial intelligence, machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems."
  },
  {
    "title": "Probabilidad",
    "source": "https://es.wikipedia.org/wiki/Probabilidad",
    "language": "es",
    "chunks": [
      {
        "heading": "Terminología de la teoría de la probabilidad",
        "level": 1,
        "content": "Experimento: Una operación que puede producir algunos resultados bien definidos pero que no se puede predecir cuál de ellos se obtendrá, se llama un experimento aleatorioEjemplo: Cuando se lanza una moneda, se sabe que solo puede aparecer cara o cruz. Sin embargo no se puede predecir al momento de lanzar la moneda en cuál lado caerá.Ejemplo:  Cuando se lanza un dado se sabe que en la cara superior puede aparecer cualquiera de los números 1,2,3,4,5, o 6 pero no se puede predecir cuál aparecerá.Espacio muestral: Todos los resultados posibles de un experimento en su conjunto, forman el Espacio de la muestra.Ejemplo: Cuando lanzamos un dado se puede obtener cualquier resultado del 1 al 6. Todos los números posibles que pueden aparecer en la cara superior forman el Espacio Muestral (denotado por S). El espacio muestral de una tirada de dados es S={1,2,3,4,5,6}Resultado: Cualquier elemento posible del espacio muestral S de un experimento aleatorio se llama Resultado. Ejemplo: Cuando lanzamos un dado, podemos obtener 3 o cuando lanzamos una moneda, podemos obtener cara.Suceso: Cualquier subconjunto del espacio muestral S se llama un Evento (denotado por E). Cuando se produce un resultado que pertenece al subconjunto E, se dice que ha ocurrido un suceso. Mientras que, cuando un resultado que no pertenece al subconjunto E tiene lugar, el Evento no ha ocurrido.Ejemplo: Considérese el experimento de lanzar un dado. Aquí el espacio muestral S={1,2,3,4,5,6}. Sea E el evento de \"que aparezca un número menor de 4\". Así, el suceso E={1,2,3}. Si aparece el número 1, se dice que el suceso E ha ocurrido. Del mismo modo, si los resultados son 2 ó 3, se puede afirmar que se ha producido el Suceso E, ya que estos resultados pertenecen al subconjunto E'Ensayo: Por ensayo, se entiende la realización de un experimento aleatorio. Ejemplo: (i) Lanzar una moneda justa, (ii) lanzar un dado imparcial[1]​"
      },
      {
        "heading": "Interpretaciones",
        "level": 1,
        "content": "Cuando se trata de experimentos que son  aleatorios y  bien definidos en un entorno puramente teórico (como lanzar una moneda), la probabilidad puede describirse numéricamente por el número de resultados deseados, dividido por el número total de todos los resultados. Por ejemplo, si se lanza una moneda al aire dos veces, se obtendrán resultados de \"cara-cara\", \"cara-cruz\", \"cruz-cara\" y \"cruz-cruz\". La probabilidad de obtener un resultado de \"cara-cara\" es 1 de cada 4 resultados, o, en términos numéricos, 1/4, 0,25 o 25%. Sin embargo, en lo que respecta a la aplicación práctica, existen dos grandes categorías de interpretaciones de la probabilidad que compiten entre sí, y cuyos partidarios mantienen puntos de vista diferentes sobre la naturaleza fundamental de la probabilidad:\n\nLos objetivistas asignan números para describir algún estado de cosas objetivo o físico. La versión más popular de la probabilidad objetiva es la probabilidad frecuentista, que afirma que la probabilidad de un evento aleatorio denota la frecuencia relativa de ocurrencia del resultado de un experimento cuando este se repite indefinidamente. Esta interpretación considera que la probabilidad es la frecuencia relativa \"a largo plazo\" de los resultados.[2]​ Una modificación de esto es la probabilidad de propensión, que interpreta la probabilidad como la tendencia de algún experimento a producir un determinado resultado, incluso si se realiza solo una vez.\nLos subjetivistas asignan números por probabilidad subjetiva, es decir, como un grado de creencia.[3]​ El grado de creencia se ha interpretado como \"el precio al que se compraría o vendería una apuesta que paga 1 unidad de utilidad si E, 0 si no E. \"[4]​ La versión más popular de la probabilidad subjetiva es la probabilidad bayesiana, que incluye el conocimiento de los expertos así como datos experimentales para calcular probabilidades.  El conocimiento experto está representado por alguna  distribución de probabilidad a priori (subjetiva).  Estos datos se incorporan a una función de verosimilitud. El producto de la función a priori y la función de verosimilitud, cuando se normaliza, da lugar a una probabilidad a posteriori que incorpora toda la información conocida hasta la fecha.[5]​ Por  el teorema de la concordancia de Aumann, los agentes bayesianos cuyas creencias a priori son similares terminarán con creencias posteriores similares. Sin embargo, unas creencias previas suficientemente diferentes pueden llevar a conclusiones diferentes, independientemente de la cantidad de información que compartan los agentes.[6]​"
      },
      {
        "heading": "Etimología",
        "level": 1,
        "content": "La palabra probabilidad deriva del latín probabilitas, que también puede significar \"probidad\", una medida de la autoridad de un testigo en un caso legal en Europa, y a menudo correlacionada con la nobleza del testigo. En cierto sentido, esto difiere mucho del significado moderno de probabilidad, que en cambio es una medida del peso de la  evidencia empírica, y se llega a ella a partir del razonamiento inductivo y la inferencia estadística.[7]​"
      },
      {
        "heading": "Historia",
        "level": 1,
        "content": "La definición de probabilidad se produjo debido al deseo del ser humano por conocer con certeza los eventos que sucederán en el futuro, por eso a través de la historia se han desarrollado diferentes enfoques para tener un concepto de la probabilidad y determinar sus valores. \nEl diccionario de la Real Academia Española (R.A.E) define «azar» como una casualidad, un caso fortuito, y afirma que la expresión «al azar» significa «sin orden».[8]​ La idea de probabilidad está íntimamente ligada a la idea de azar y nos ayuda a comprender nuestras posibilidades de ganar un juego de azar o analizar las encuestas.\nPierre-Simon Laplace afirmó: «Es notable que una ciencia que comenzó con consideraciones sobre juegos de azar haya llegado a ser el objeto más importante del conocimiento humano». Comprender y estudiar el azar es indispensable, porque la probabilidad es un soporte necesario para tomar decisiones en cualquier ámbito.[9]​\nSegún Amanda Dure, «Antes de la mitad del siglo XVII, el término \"probable\" (en latín probable) significaba aprobable, y se aplicaba en ese sentido, unívocamente, a la opinión y a la acción. Una acción u opinión probable era una que las personas sensatas emprenderían o mantendrían dadas las circunstancias».[10]​\nAparte de algunas consideraciones elementales hechas por Girolamo Cardano en el siglo XVI, la doctrina de la probabilidad data de la correspondencia de Pierre de Fermat y Blaise Pascal (1654). Christiaan Huygens (1657) le dio el tratamiento científico conocido más antiguo al concepto, seguido por la Kybeia de Juan Caramuel (1670). Varios de los citados autores —Fermat, Pascal y Caramuel— mencionan en sus respectivas correspondencias un Ars Commutationes de Sebastián de Rocafull (1649), hoy perdido. El fundamental Ars Conjectandi (póstumo, 1713) de Jakob Bernoulli y Doctrine of Chances (1718) de Abraham de Moivre trataron el tema como una rama de las matemáticas. Véase El surgimiento de la probabilidad (The Emergence of Probability) de Ian Hacking para una historia de los inicios del desarrollo del propio concepto de probabilidad matemática.\nLa teoría de errores se remonta hasta Opera Miscellanea (póstumo, 1722) de Roger Cotes, pero una memoria preparada por Thomas Simpson en 1755 (impresa en 1756) aplicó por primera vez la teoría para la discusión de errores de observación. La reimpresión (1757) de esta memoria expone los axiomas de que los errores positivos y negativos son igualmente probables, y que hay ciertos límites asignables dentro de los cuales se supone que caen todos los errores; se discuten los errores continuos y se da una curva de la probabilidad.\nPierre-Simon Laplace (1774) hizo el primer intento para deducir una regla para la combinación de observaciones a partir de los principios de la teoría de probabilidad. Representó la ley de la probabilidad de error con una curva \n  \n    \n      \n        y\n        =\n        ϕ\n        (\n        x\n        )\n      \n    \n    {\\displaystyle y=\\phi (x)}\n  \n, siendo \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n cualquier error e \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n su probabilidad, y expuso tres propiedades de esta curva:\n\nes simétrica al eje \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n;\nel eje \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n es una asíntota, siendo la probabilidad del error \n  \n    \n      \n        ∞\n      \n    \n    {\\displaystyle \\infty }\n  \n igual a 0;\nla superficie cerrada es 1, haciendo cierta la existencia de un error.\nDedujo una fórmula para la media de tres observaciones. También obtuvo (1781) una fórmula para la ley de facilidad de error (un término debido a Lagrange, 1774), pero su fórmula llevaba a ecuaciones inmanejables. Daniel Bernoulli (1778) introdujo el principio del máximo producto de las probabilidades de un sistema de errores concurrentes.\nEl método de mínimos cuadrados se debe a Adrien-Marie Legendre (1805), que lo introdujo en su Nouvelles méthodes pour la détermination des orbites des comètes (Nuevos métodos para la determinación de las órbitas de los cometas). Ignorando la contribución de Legendre, un escritor irlandés estadounidense, Robert Adrain, editor de \"The Analyst\" (1808), dedujo por primera vez la ley de facilidad de error,\n\n  \n    \n      \n        ϕ\n        (\n        x\n        )\n        =\n        c\n        \n          e\n          \n            −\n            \n              h\n              \n                2\n              \n            \n            \n              x\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\phi (x)=ce^{-h^{2}x^{2}}}\n  \n\nsiendo \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n y \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n constantes que dependen de la precisión de la observación. Expuso dos demostraciones, siendo la segunda esencialmente la misma de John Herschel (1850). Gauss expuso la primera demostración que parece que se conoció en Europa (la tercera después de la de Adrain) en 1809. Demostraciones adicionales se expusieron por Laplace (1810, 1812), Gauss (1823), James Ivory (1825, 1826), Hagen (1837), Friedrich Bessel (1838), W. F. Donkin (1844, 1856) y Morgan Crofton (1870). Otros personajes que contribuyeron fueron Ellis (1844), De Morgan (1864), Glaisher (1872) y Giovanni Schiaparelli (1875). La fórmula de Peters (1856) para \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n, el error probable de una única observación, es bien conocida.\nEn el siglo XIX, entre los autores de la teoría general se incluían Laplace, Sylvestre Lacroix (1816), Littrow (1833), Adolphe Quetelet (1853), Richard Dedekind (1860), Helmert (1872), Hermann Laurent (1873), Liagre, Didion, y Karl Pearson. Augustus De Morgan y George Boole mejoraron la exposición de la teoría.\nEn 1930 Andréi Kolmogorov desarrolló la base axiomática de la probabilidad utilizando teoría de la medida.\nEn la parte geométrica (véase geometría integral) los colaboradores de The Educational Times fueron influyentes (Miller, Crofton, McColl, Wolstenholme, Nacho Ramis de Ayreflor, Watson y Artemas Martin)."
      },
      {
        "heading": "Teoría",
        "level": 1,
        "content": "La probabilidad constituye un importante parámetro en la determinación de las diversas casualidades obtenidas tras una serie de eventos esperados dentro de un rango estadístico.\nExisten diversas formas como método abstracto, como la teoría Dempster-Shafer y la teoría de la relatividad numérica, esta última con un alto grado de aceptación si se toma en cuenta que disminuye considerablemente las posibilidades hasta un nivel mínimo ya que somete a todas las antiguas reglas a una simple ley de relatividad.[cita requerida]\nLa probabilidad de un evento se denota con la letra p y se expresa en términos de una fracción y no en porcentajes[cita requerida], por lo que el valor de p cae entre 0 y 1. Por otra parte, la probabilidad de que un evento \"no ocurra\" equivale a 1 menos el valor de p y se denota con la letra q\n\n  \n    \n      \n        P\n        (\n        Q\n        )\n        =\n        1\n        −\n        P\n        (\n        E\n        )\n      \n    \n    {\\displaystyle P(Q)=1-P(E)}\n  \n\nLos tres métodos para calcular la probabilidad son la regla de la adición, la regla de la multiplicación y la distribución binomial."
      },
      {
        "heading": "Regla de la adición",
        "level": 2,
        "content": "La regla de la adición o regla de la suma establece que la probabilidad de ocurrencia de cualquier evento en particular es igual a la suma de las probabilidades individuales, si es que los eventos son mutuamente excluyentes, es decir, que dos no pueden ocurrir al mismo tiempo.\nPor un lado, si \n  \n    \n      \n        A\n        ∩\n        B\n        =\n        ∅\n      \n    \n    {\\displaystyle A\\cap B=\\varnothing }\n  \n, es decir que son mutuamente excluyentes, entonces\n  \n    \n      \n        P\n        (\n        A\n        ∪\n        B\n        )\n        =\n        P\n        (\n        A\n        )\n        +\n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cup B)=P(A)+P(B)}\n  \n\nPor otro lado, si \n  \n    \n      \n        A\n        ∩\n        B\n        ≠\n        ∅\n      \n    \n    {\\displaystyle A\\cap B\\neq \\varnothing }\n  \n, es decir que no son mutuamente excluyentes, entonces\n  \n    \n      \n        P\n        (\n        A\n        ∪\n        B\n        )\n        =\n        P\n        (\n        A\n        )\n        +\n        P\n        (\n        B\n        )\n        −\n        P\n        (\n        A\n        ∩\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cup B)=P(A)+P(B)-P(A\\cap B)}\n  \n Siendo: \n  \n    \n      \n        P\n        (\n        A\n        )\n        =\n      \n    \n    {\\displaystyle P(A)=}\n  \nprobabilidad de ocurrencia del evento A, \n  \n    \n      \n        P\n        (\n        B\n        )\n        =\n      \n    \n    {\\displaystyle P(B)=}\n  \nprobabilidad de ocurrencia del evento B, y \n  \n    \n      \n        P\n        (\n        A\n        ∩\n        B\n        )\n        =\n      \n    \n    {\\displaystyle P(A\\cap B)=}\n  \n probabilidad de ocurrencia simultánea de los eventos A y B.\nOtra forma de verlo sería expresar la probabilidad de sucesos mutuamente no excluyentes mediante el sumatorio de las probabilidades de un evento determinado en función de otros eventos:\n\n  \n    \n      \n        P\n        (\n        x\n        )\n        =\n        \n          ∑\n          \n            y\n          \n        \n        \n          P\n          (\n          x\n          ,\n          y\n          )\n        \n        =\n        \n          ∑\n          \n            y\n          \n        \n        \n          P\n          (\n          y\n          )\n          P\n          (\n          x\n          \n            |\n          \n          y\n          )\n        \n        \n          O\n          R\n          D\n          E\n          N\n          A\n          R\n          :\n        \n      \n    \n    {\\displaystyle P(x)=\\sum _{y}{P(x,y)}=\\sum _{y}{P(y)P(x|y)}{ORDENAR:}}"
      },
      {
        "heading": "Regla de la multiplicación",
        "level": 2,
        "content": "La regla de la multiplicación establece que la probabilidad de ocurrencia de dos o más eventos estadísticamente independientes es igual al producto de sus probabilidades individuales.\n\n  \n    \n      \n        P\n        (\n        A\n        ∩\n        B\n        )\n        =\n        P\n        (\n        A\n        )\n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cap B)=P(A)P(B)}\n  \n,  si A y B son independientes.\n\n  \n    \n      \n        P\n        (\n        A\n        ∩\n        B\n        )\n        =\n        P\n        (\n        A\n        )\n        P\n        (\n        B\n        \n          |\n        \n        A\n        )\n      \n    \n    {\\displaystyle P(A\\cap B)=P(A)P(B|A)}\n  \n, si A y B son dependientes.\nsiendo \n  \n    \n      \n        P\n        (\n        B\n        \n          |\n        \n        A\n        )\n      \n    \n    {\\displaystyle P(B|A)}\n  \n la probabilidad de que ocurra B habiéndose dado o verificado el evento A.\nUn lote contiene \"100\" objetos de los cuales \"20\" son defectuosos. Los objetos son seleccionados uno después del otro para ver si son defectuosos. Suponiendo que dos objetos son seleccionados sin reemplazo (esto es, que el objeto que se selecciona al azar se deja fuera del lote), ¿cuál es la probabilidad de que los dos objetos seleccionados sean defectuosos?\nSolución:\nSea los eventos \n\n  \n    \n      \n        \n          A\n          \n            1\n          \n        \n        =\n      \n    \n    {\\displaystyle A_{1}=}\n  \n primer objeto defectuoso, \n  \n    \n      \n        \n          A\n          \n            2\n          \n        \n        =\n      \n    \n    {\\displaystyle A_{2}=}\n  \nsegundo objeto defectuoso\nentonces dos objetos seleccionados serán defectuosos, cuando ocurre el evento \n  \n    \n      \n        \n          A\n          \n            1\n          \n        \n        ∩\n        \n          A\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle A_{1}\\cap A_{2}}\n  \n que es la intersección entre los eventos \n  \n    \n      \n        \n          A\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle A_{1}}\n  \n y \n  \n    \n      \n        \n          A\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle A_{2}}\n  \n. De la información dada se tiene que: \n\n  \n    \n      \n        P\n        (\n        \n          A\n          \n            1\n          \n        \n        )\n        =\n        \n          \n            20\n            100\n          \n        \n      \n    \n    {\\displaystyle P(A_{1})={\\frac {20}{100}}}\n  \n; \n  \n    \n      \n        P\n        (\n        \n          A\n          \n            2\n          \n        \n        \n          |\n        \n        \n          A\n          \n            1\n          \n        \n        )\n        =\n        \n          \n            19\n            99\n          \n        \n      \n    \n    {\\displaystyle P(A_{2}|A_{1})={\\frac {19}{99}}}\n  \n\nasí que la probabilidad de que los dos objetos seleccionados sean defectuosos es \n\n  \n    \n      \n        P\n        (\n        \n          A\n          \n            1\n          \n        \n        ∩\n        \n          A\n          \n            2\n          \n        \n        )\n        =\n        P\n        (\n        \n          A\n          \n            1\n          \n        \n        )\n        P\n        (\n        \n          A\n          \n            2\n          \n        \n        \n          |\n        \n        \n          A\n          \n            1\n          \n        \n        )\n        =\n        \n          \n            20\n            100\n          \n        \n        ⋅\n        \n          \n            19\n            99\n          \n        \n        =\n        \n          \n            19\n            495\n          \n        \n        ≃\n        0.038\n      \n    \n    {\\displaystyle P(A_{1}\\cap A_{2})=P(A_{1})P(A_{2}|A_{1})={\\frac {20}{100}}\\cdot {\\frac {19}{99}}={\\frac {19}{495}}\\simeq 0.038}\n  \n \nAhora, suponiendo que se selecciona un tercer objeto, entonces la probabilidad de que los tres objetos seleccionados sean defectuosos es \n\n  \n    \n      \n        P\n        (\n        \n          A\n          \n            1\n          \n        \n        ∩\n        \n          A\n          \n            2\n          \n        \n        ∩\n        \n          A\n          \n            3\n          \n        \n        )\n        =\n        P\n        (\n        \n          A\n          \n            1\n          \n        \n        )\n        P\n        (\n        \n          A\n          \n            2\n          \n        \n        \n          |\n        \n        \n          A\n          \n            1\n          \n        \n        )\n        P\n        (\n        \n          A\n          \n            3\n          \n        \n        \n          |\n        \n        \n          A\n          \n            1\n          \n        \n        ∩\n        \n          A\n          \n            2\n          \n        \n        )\n        =\n        \n          \n            20\n            100\n          \n        \n        ⋅\n        \n          \n            19\n            99\n          \n        \n        ⋅\n        \n          \n            18\n            98\n          \n        \n        =\n        \n          \n            19\n            2695\n          \n        \n        ≃\n        0.007\n      \n    \n    {\\displaystyle P(A_{1}\\cap A_{2}\\cap A_{3})=P(A_{1})P(A_{2}|A_{1})P(A_{3}|A_{1}\\cap A_{2})={\\frac {20}{100}}\\cdot {\\frac {19}{99}}\\cdot {\\frac {18}{98}}={\\frac {19}{2695}}\\simeq 0.007}"
      },
      {
        "heading": "Regla de Laplace",
        "level": 2,
        "content": "La Regla de Laplace establece que:\n\nLa probabilidad de ocurrencia de un suceso imposible es 0.\nLa probabilidad de ocurrencia de un suceso seguro es 1, es decir, \n  \n    \n      \n        P\n        (\n        A\n        )\n        =\n        1\n      \n    \n    {\\displaystyle P(A)=1}\n  \n.\nPara aplicar la regla de Laplace es necesario que los experimentos den lugar a sucesos equiprobables, es decir, que todos tengan o posean la misma probabilidad.\n\nLa probabilidad de que ocurra un suceso se calcula así:\n\n  \n    \n      \n        P\n        (\n        A\n        )\n        =\n        \n          \n            Nº de casos favorables\n             Nº de resultados posibles\n          \n        \n      \n    \n    {\\displaystyle P(A)={\\frac {\\text{Nº de casos favorables}}{\\text{ Nº de resultados posibles}}}}\n  \n\nEsto significa que: la probabilidad del evento A es igual al cociente del número de casos favorables (los casos donde sucede A) sobre el total de casos posibles."
      },
      {
        "heading": "Distribución binomial",
        "level": 2,
        "content": "La probabilidad de ocurrencia de una combinación específica de eventos independientes y mutuamente excluyentes se determina con la distribución binomial, que es aquella donde hay solo dos posibilidades, que se suelen designar como éxito y fracaso.\n\nHay dos resultados posibles mutuamente excluyentes en cada ensayo u observación.\nLa serie de ensayos u observaciones constituyen eventos independientes.\nLa probabilidad de éxito permanece constante de ensayo a ensayo, es decir el proceso es estacionario.\nPara aplicar esta distribución al cálculo de la probabilidad de obtener un número dado de éxitos en una serie de experimentos en un proceso de Bernoulli, se requieren tres valores: el número designado de éxitos (m), el número de ensayos y observaciones (n); y la probabilidad de éxito en cada ensayo (p). \nEntonces la probabilidad de que ocurran m éxitos en un experimento de n ensayos es:\n\n  \n    \n      \n        P\n        (\n        x\n        =\n        m\n        )\n        =\n        \n          \n            \n              (\n            \n            \n              n\n              m\n            \n            \n              )\n            \n          \n        \n        \n          p\n          \n            m\n          \n        \n        (\n        1\n        −\n        p\n        \n          )\n          \n            n\n            −\n            m\n          \n        \n      \n    \n    {\\displaystyle P(x=m)={n \\choose m}p^{m}(1-p)^{n-m}}\n  \n\ndonde \n  \n    \n      \n        \n          \n            \n              (\n            \n            \n              n\n              m\n            \n            \n              )\n            \n          \n        \n        =\n        \n          \n            \n              n\n              !\n            \n            \n              m\n              !\n              (\n              n\n              −\n              m\n              )\n              !\n            \n          \n        \n      \n    \n    {\\displaystyle {n \\choose m}={\\frac {n!}{m!(n-m)!}}}\n  \n es el número total de combinaciones posibles de m elementos en un conjunto de n elementos."
      },
      {
        "heading": "Aplicaciones",
        "level": 1,
        "content": "Dos aplicaciones principales de la teoría de la probabilidad en el día a día son en el análisis de riesgo y en el comercio de los mercados de materias primas. Los gobiernos normalmente aplican métodos probabilísticos en regulación ambiental donde se les llama \"análisis de vías de dispersión o separación por medio de ecuaciones\", y a menudo miden el bienestar usando métodos que son estocásticos por naturaleza, y escogen qué proyectos emprender basándose en análisis estadísticos de su probable efecto en la población como un conjunto. No es correcto decir que la estadística está incluida en el propio modelado, ya que típicamente los análisis de riesgo son para una única vez y por lo tanto requieren más modelos de probabilidad fundamentales, por ej. \"la probabilidad de otro 11-S\". Una ley de números pequeños tiende a aplicarse a todas aquellas elecciones y percepciones del efecto de estas elecciones, lo que hace de las medidas probabilísticas un tema político.\nUn buen ejemplo es el efecto de la probabilidad percibida de cualquier conflicto generalizado sobre los precios del petróleo en Oriente Medio - que producen un efecto dominó en la economía en conjunto. Un cálculo por un mercado de materias primas en que la guerra es más probable encontrar  los precios hacia arriba o hacia abajo e indica a otros comerciantes esa opinión. Por consiguiente, la probabilidad no se calcula independientemente y tampoco es necesariamente muy racional. La teoría de las finanzas conductuales surgió para describir el efecto de este pensamiento de grupo en el precio, en la política, y en la paz y en los conflictos.\nSe puede decir razonablemente que el descubrimiento de métodos rigurosos para calcular y combinar los cálculos de probabilidad ha tenido un profundo efecto en la sociedad moderna. Por consiguiente, puede ser de alguna importancia para la mayoría de los ciudadanos entender cómo se calculan los pronósticos y la probabilidad, y cómo contribuyen a la reputación y a las decisiones, especialmente en una democracia.\nOtra aplicación significativa de la teoría de la probabilidad en el día a día es en la fiabilidad. Muchos bienes de consumo, como los automóviles y la electrónica de consumo, utilizan la teoría de la fiabilidad en el diseño del producto para reducir la probabilidad de avería. La probabilidad de avería también está estrechamente relacionada con la garantía del producto.\nSe puede decir que no existe una cosa llamada probabilidad. También se puede decir que la probabilidad es la medida de nuestro grado de incertidumbre, o esto es, el grado de nuestra ignorancia dada una situación. Por consiguiente, puede haber una probabilidad de 1 entre 50 de que la primera carta en un baraja sea la J de diamantes. Sin embargo, si uno mira la primera carta y la reemplaza, entonces la probabilidad es o bien 100% o 0%, y la elección correcta puede ser hecha con precisión por el que ve la carta. La física moderna proporciona ejemplos importantes de situaciones deterministas donde solo la descripción probabilística es factible debido a información incompleta y la complejidad de un sistema así como ejemplos de fenómenos realmente aleatorios.\nEn un universo determinista, basado en los conceptos newtonianos, no hay probabilidad si se conocen todas las condiciones. En el caso de una ruleta, si la fuerza de la mano y el periodo de esta fuerza es conocido, entonces el número donde la bola parará será seguro. Naturalmente, esto también supone el conocimiento de la inercia y la fricción de la ruleta, el peso, lisura y redondez de la bola, las variaciones en la velocidad de la mano durante el movimiento y así sucesivamente. Una descripción probabilística puede entonces ser más práctica que la mecánica newtoniana para analizar el modelo de las salidas de lanzamientos repetidos de la ruleta. Los físicos se encuentran con la misma situación en la teoría cinética de los gases, donde el sistema determinístico en principio, es tan complejo (con el número de moléculas típicamente del orden de magnitud de la constante de Avogadro \n  \n    \n      \n        6\n        ⋅\n        \n          10\n          \n            23\n          \n        \n      \n    \n    {\\displaystyle 6\\cdot 10^{23}}\n  \n) que solo la descripción estadística de sus propiedades es viable.\nLa mecánica cuántica, debido al principio de indeterminación de Heisenberg, solo puede ser descrita actualmente a través de distribuciones de probabilidad, lo que le da una gran importancia a las descripciones probabilísticas. Algunos científicos hablan de la expulsión del paraíso.[cita requerida] Otros no se conforman con la pérdida del determinismo. Albert Einstein comentó estupendamente en una carta a Max Born: Jedenfalls bin ich überzeugt, daß der Alte nicht würfelt. (Estoy convencido de que Dios no tira el dado). No obstante hoy en día no existe un medio mejor para describir la física cuántica si no es a través de la teoría de la probabilidad. Mucha gente hoy en día confunde el hecho de que la mecánica cuántica se describe a través de distribuciones de probabilidad con la suposición de que es por ello un proceso aleatorio, cuando la mecánica cuántica es probabilística no por el hecho de que siga procesos aleatorios sino por el hecho de no poder determinar con precisión sus parámetros fundamentales, lo que imposibilita la creación de un sistema de ecuaciones determinista."
      },
      {
        "heading": "Investigación biomédica",
        "level": 2,
        "content": "La mayoría de las investigaciones biomédicas utilizan muestras de probabilidad, es decir, aquellas que el investigador pueda especificar la probabilidad de cualquier elemento en la población que investiga. Las muestras de probabilidad permiten usar estadísticas inferenciales, aquellas que permiten hacer inferencias a partir de datos. Por otra parte, las muestras no probabilísticas solo permiten usarse estadísticas descriptivas, aquellas que solo permiten describir, organizar y resumir datos. Se utilizan cuatro tipos de muestras probabilísticas: muestras aleatorias simples, muestras aleatorias estratificadas, muestra por conglomerados y muestras sistemáticas."
      },
      {
        "heading": "Relación con el azar y la probabilidad en la mecánica cuántica",
        "level": 1,
        "content": "En un universo determinista, basado en los conceptos de la mecánica newtoniana, no habría probabilidad si se conocieran todas las condiciones (demonio de Laplace), pero hay situaciones en las que la sensibilidad a las condiciones iniciales supera nuestra capacidad de medirlas, es decir, de conocerlas.  En el caso de una ruleta, si se conoce la fuerza de la mano y el período de esa fuerza, el número en el que se detendrá la bola sería una certeza (aunque, como cuestión práctica, esto probablemente sólo sería cierto en una ruleta que no hubiera sido exactamente nivelada, como reveló el Casino Newtoniano de Thomas A. Bass).  Esto también supone el conocimiento de la inercia y la fricción de la rueda, el peso, la suavidad y la redondez de la bola, las variaciones en la velocidad de la mano durante el giro, etc. Así, una descripción probabilística puede ser más útil que la mecánica newtoniana para analizar el patrón de resultados de las repetidas tiradas de una ruleta. Los físicos se enfrentan a la misma situación en la teoría cinética de los gases, donde el sistema, aunque determinista en principio, es tan complejo (con el número de moléculas típicamente del orden de magnitud de la constante de Avogadro, 6,02<e<23) que sólo es posible una descripción estadística de sus propiedades.\nLa teoría de la probabilidad es necesaria para describir los fenómenos cuánticos.[11]​ Un descubrimiento revolucionario de la física de principios del siglo XX fue el carácter aleatorio de todos los procesos físicos que ocurren a escalas subatómicas y que se rigen por las leyes de la mecánica cuántica. La función de onda objetiva evoluciona de forma determinista pero, según la interpretación de Copenhague, se trata de probabilidades de observar, explicándose el resultado por un colapso de la función de onda cuando se realiza una observación. Sin embargo, la pérdida del determinismo en aras del instrumentalismo no contó con la aprobación universal. Albert Einstein famosamente remarcó en una carta a Max Born: \"Estoy convencido de que Dios no juega a los dados\".[12]​ Al igual que Einstein, Erwin Schrödinger, que descubrió la función de onda, creía que la mecánica cuántica es una aproximación estadística de una realidad determinista subyacente.[13]​ En algunas interpretaciones modernas de la mecánica estadística de la medición, se invoca la decoherencia cuántica para explicar la aparición de resultados experimentales subjetivamente probabilísticos."
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": "Teoría de la decisión\nTeoría de juegos\nTeoría de la información\nTeoría de la medida\nVariable aleatoria\nEstadística\nProceso estocástico\nEquiprobabilidad\nFrecuencia estadística\nInterpretaciones de la probabilidad"
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bibliografía",
        "level": 1,
        "content": "Kallenberg, O. (2005) Probabilistic Symmetries and Invariance Principles. Springer-Verlag, New York. 510 pp. ISBN 0-387-25115-4\nKallenberg, O. (2002) Foundations of Modern Probability, 2nd ed. Springer Series in Statistics. 650 pp. ISBN 0-387-95313-2\nOlofsson, Peter (2005) Probability, Statistics, and Stochastic Processes, Wily-Interscience. 504 pp ISBN 0-471-67969-0."
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": " Wikcionario  tiene definiciones y otra información sobre probabilidad.\nProblemas selectos de probabilidad (resueltos)\nEdwin Thompson Jaynes. Probability Theory: The Logic of Science. Preprint: Washington University, (1996). (PDF) (en inglés)\nUn problema de probabilidades: [1]"
      }
    ],
    "summary": "La probabilidad es una medida de la certidumbre de que ocurra un evento. Su valor es un número entre 0 y 1, donde un evento imposible corresponde a cero y uno seguro corresponde a uno.\nUna forma empírica de estimar la probabilidad consiste en obtener la frecuencia con la que sucede un determinado acontecimiento mediante la repetición de experimentos aleatorios, bajo condiciones suficientemente estables. En algunos experimentos de los que se conocen todos los resultados posibles, las probabilidades de estos sucesos pueden ser calculadas de manera teórica, especialmente cuando todos los resultados son igualmente probables.\nLa teoría de la probabilidad es la rama de la matemática que estudia los experimentos o fenómenos aleatorios. Se usa extensamente en áreas como la estadística, la física, la economía (ciencia económica), las finanzas, la ciencia de datos, la Investigación médica, en mediano grado en algunas de las demás ciencias sociales y en menor grado en la filosofía para conocer la viabilidad de sucesos y la mecánica subyacente de sistemas complejos."
  },
  {
    "title": "Algorithm",
    "source": "https://en.wikipedia.org/wiki/Algorithm",
    "language": "en",
    "chunks": [
      {
        "heading": "Etymology",
        "level": 1,
        "content": "Around 825 AD, Persian scientist and polymath Muḥammad ibn Mūsā al-Khwārizmī wrote kitāb al-ḥisāb al-hindī (\"Book of Indian computation\") and kitab al-jam' wa'l-tafriq al-ḥisāb al-hindī (\"Addition and subtraction in Indian arithmetic\"). In the early 12th century, Latin translations of said al-Khwarizmi texts involving the Hindu–Arabic numeral system and arithmetic appeared, for example Liber Alghoarismi de practica arismetrice, attributed to John of Seville, and Liber Algorismi de numero Indorum, attributed to Adelard of Bath. Hereby, alghoarismi or algorismi is the Latinization of Al-Khwarizmi's name; the text starts with the phrase Dixit Algorismi, or \"Thus spoke Al-Khwarizmi\". Around 1230, the English word algorism is attested and then by Chaucer in 1391, English adopted the French term. In the 15th century, under the influence of the Greek word ἀριθμός (arithmos, \"number\"; cf. \"arithmetic\"), the Latin word was altered to algorithmus."
      },
      {
        "heading": "Definition",
        "level": 1,
        "content": "One informal definition is \"a set of rules that precisely defines a sequence of operations\", which would include all computer programs (including programs that do not perform numeric calculations), and any prescribed bureaucratic procedure\nor cook-book recipe. In general, a program is an algorithm only if it stops eventually—even though infinite loops may sometimes prove desirable. Boolos, Jeffrey & 1974, 1999 define an algorithm to be an explicit set of instructions for determining an output, that can be followed by a computing machine or a human who could only carry out specific elementary operations on symbols.\nMost algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain performing arithmetic or an insect looking for food), in an electrical circuit, or a mechanical device."
      },
      {
        "heading": "History",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Ancient algorithms",
        "level": 2,
        "content": "Step-by-step procedures for solving mathematical problems have been recorded since antiquity. This includes in Babylonian mathematics (around 2500 BC), Egyptian mathematics (around 1550 BC), Indian mathematics (around 800 BC and later), the Ifa Oracle (around 500 BC), Greek mathematics (around 240 BC), Chinese mathematics (around 200 BC and later), and Arabic mathematics (around 800 AD).\nThe earliest evidence of algorithms is found in ancient Mesopotamian mathematics. A Sumerian clay tablet found in Shuruppak near Baghdad and dated to c. 2500 BC describes the earliest division algorithm. During the Hammurabi dynasty c. 1800 – c. 1600 BC, Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.\nAlgorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus c. 1550 BC. Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus,: Ch 9.2  and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).: Ch 9.1 Examples of ancient Indian mathematics included the Shulba Sutras, the Kerala School, and the Brāhmasphuṭasiddhānta.\nThe first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm."
      },
      {
        "heading": "Computers",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Weight-driven clocks",
        "level": 3,
        "content": "Bolter credits the invention of the weight-driven clock as \"the key invention [of Europe in the Middle Ages],\" specifically the verge escapement mechanism producing the tick and tock of a mechanical clock. \"The accurate automatic machine\" led immediately to \"mechanical automata\" in the 13th century and \"computational machines\"—the difference and analytical engines of Charles Babbage and Ada Lovelace in the mid-19th century. Lovelace designed the first algorithm intended for processing on a computer, Babbage's analytical engine, which is the first device considered a real Turing-complete computer instead of just a calculator. Although the full implementation of Babbage's second device was not realized for decades after her lifetime, Lovelace has been called \"history's first programmer\"."
      },
      {
        "heading": "Electromechanical relay",
        "level": 3,
        "content": "Bell and Newell (1971) write that the Jacquard loom, a precursor to Hollerith cards (punch cards), and \"telephone switching technologies\" led to the development of the first computers. By the mid-19th century, the telegraph, the precursor of the telephone, was in use throughout the world. By the late 19th century, the ticker tape (c. 1870s) was in use, as were Hollerith cards (c. 1890). Then came the teleprinter (c. 1910) with its punched-paper use of Baudot code on tape.\nTelephone-switching networks of electromechanical relays were invented in 1835. These led to the invention of the digital adding device by George Stibitz in 1937. While working in Bell Laboratories, he observed the \"burdensome\" use of mechanical calculators with gears. \"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device\"."
      },
      {
        "heading": "Formalization",
        "level": 2,
        "content": "In 1928, a partial formalization of the modern concept of algorithms began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert. Later formalizations were framed as attempts to define \"effective calculability\" or \"effective method\". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939."
      },
      {
        "heading": "Representations",
        "level": 1,
        "content": "Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts, and control tables are structured expressions of algorithms that avoid common ambiguities of natural language. Programming languages are primarily for expressing algorithms in a computer-executable form but are also used to define or document algorithms."
      },
      {
        "heading": "Turing machines",
        "level": 2,
        "content": "There are many possible representations and Turing machine programs can be expressed as a sequence of machine tables (see finite-state machine, state-transition table, and control table for more), as flowcharts and drakon-charts (see state diagram for more), as a form of rudimentary machine code or assembly code called \"sets of quadruples\", and more. Algorithm representations can also be classified into three accepted levels of Turing machine description: high-level description, implementation description, and formal description. A high-level description describes the qualities of the algorithm itself, ignoring how it is implemented on the Turing machine. An implementation description describes the general manner in which the machine moves its head and stores data to carry out the algorithm, but does not give exact states. In the most detail, a formal description gives the exact state table and list of transitions of the Turing machine."
      },
      {
        "heading": "Flowchart representation",
        "level": 2,
        "content": "The graphical aid called a flowchart offers a way to describe and document an algorithm (and a computer program corresponding to it). It has four primary symbols: arrows showing program flow, rectangles (SEQUENCE, GOTO), diamonds (IF-THEN-ELSE), and dots (OR-tie). Sub-structures can \"nest\" in rectangles, but only if a single exit occurs from the superstructure."
      },
      {
        "heading": "Algorithmic analysis",
        "level": 1,
        "content": "It is often important to know how much time, storage, or other cost an algorithm may require. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm that adds up the elements of a list of n numbers would have a time requirement of ⁠\n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  \n⁠, using big O notation. The algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. If the space required to store the input numbers is not counted, it has a space requirement of ⁠\n  \n    \n      \n        O\n        (\n        1\n        )\n      \n    \n    {\\displaystyle O(1)}\n  \n⁠, otherwise ⁠\n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  \n⁠ is required.\nDifferent algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost ⁠\n  \n    \n      \n        O\n        (\n        log\n        ⁡\n        n\n        )\n      \n    \n    {\\displaystyle O(\\log n)}\n  \n⁠) outperforms a sequential search (cost ⁠\n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  \n⁠ ) when used for table lookups on sorted lists or arrays."
      },
      {
        "heading": "Formal versus empirical",
        "level": 2,
        "content": "The analysis, and study of algorithms is a discipline of computer science. Algorithms are often studied abstractly, without referencing any specific programming language or implementation. Algorithm analysis resembles other mathematical disciplines as it focuses on the algorithm's properties, not implementation. Pseudocode is typical for analysis as it is a simple and general representation. Most algorithms are implemented on particular hardware/software platforms and their algorithmic efficiency is tested using real code. The efficiency of a particular algorithm may be insignificant for many \"one-off\" problems but it may be critical for algorithms designed for fast interactive, commercial, or long-life scientific usage. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.\nEmpirical testing is useful for uncovering unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.\nEmpirical tests cannot replace formal analysis, though, and are non-trivial to perform fairly."
      },
      {
        "heading": "Execution efficiency",
        "level": 2,
        "content": "To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power."
      },
      {
        "heading": "Design",
        "level": 1,
        "content": "Algorithm design is a method or mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern. One of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g., an algorithm's run-time growth as the size of its input increases."
      },
      {
        "heading": "Structured programming",
        "level": 2,
        "content": "Per the Church–Turing thesis, any algorithm can be computed by any Turing complete model. Turing completeness only requires four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. However, Kemeny and Kurtz observe that, while \"undisciplined\" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in \"spaghetti code\", a programmer can write structured programs using only these instructions; on the other hand \"it is also possible, and not too hard, to write badly structured programs in a structured language\". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction."
      },
      {
        "heading": "Legal status",
        "level": 1,
        "content": "By themselves, algorithms are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute \"processes\" (USPTO 2006), so algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is controversial, and there are criticized patents involving algorithms, especially data compression algorithms, such as Unisys's LZW patent. Additionally, some cryptographic algorithms have export restrictions (see export of cryptography)."
      },
      {
        "heading": "Classification",
        "level": 1,
        "content": ""
      },
      {
        "heading": "By implementation",
        "level": 2,
        "content": "Recursion\nA recursive algorithm invokes itself repeatedly until meeting a termination condition and is a common functional programming method. Iterative algorithms use repetitions such as loops or data structures like stacks to solve problems. Problems may be suited for one implementation or the other. The Tower of Hanoi is a puzzle commonly solved using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.\nSerial, parallel or distributed\nAlgorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time on serial computers. Serial algorithms are designed for these environments, unlike parallel or distributed algorithms. Parallel algorithms take advantage of computer architectures where multiple processors can work on a problem at the same time. Distributed algorithms use multiple machines connected via a computer network. Parallel and distributed algorithms divide the problem into subproblems and collect the results back together. Resource consumption in these algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable, but some problems have no parallel algorithms and are called inherently serial problems.\nDeterministic or non-deterministic\nDeterministic algorithms solve the problem with exact decisions at every step; whereas non-deterministic algorithms solve problems via guessing. Guesses are typically made more accurate through the use of heuristics.\nExact or approximate\nWhile many algorithms reach an exact solution, approximation algorithms seek an approximation that is close to the true solution. Such algorithms have practical value for many hard problems. For example, the Knapsack problem, where there is a set of items, and the goal is to pack the knapsack to get the maximum total value. Each item has some weight and some value. The total weight that can be carried is no more than some fixed number X. So, the solution must consider the weights of items as well as their value.\nQuantum algorithm\nQuantum algorithms run on a realistic model of quantum computation. The term is usually used for those algorithms that seem inherently quantum or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement."
      },
      {
        "heading": "By design paradigm",
        "level": 2,
        "content": "Another way of classifying algorithms is by their design methodology or paradigm. Some common paradigms are:\n\nBrute-force or exhaustive search\nBrute force is a problem-solving method of systematically trying every possible option until the optimal solution is found. This approach can be very time-consuming, testing every possible combination of variables. It is often used when other methods are unavailable or too complex. Brute force can solve a variety of problems, including finding the shortest path between two points and cracking passwords.\nDivide and conquer\nA divide-and-conquer algorithm repeatedly reduces a problem to one or more smaller instances of itself (usually recursively) until the instances are small enough to solve easily. Merge sorting is an example of divide and conquer, where an unordered list can be divided into segments containing one item and sorting of the entire list can be obtained by merging the segments. A simpler variant of divide and conquer is called a decrease-and-conquer algorithm, which solves one smaller instance of itself, and uses the solution to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of a decrease and conquer algorithm is the binary search algorithm.\nSearch and enumeration\nMany problems (such as playing chess) can be modelled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration, and backtracking.\nRandomized algorithm\nSuch algorithms make some choices randomly (or pseudo-randomly). They find approximate solutions when finding exact solutions may be impractical (see heuristic method below). For some problems, the fastest approximations must involve some randomness. Whether randomized algorithms with polynomial time complexity can be the fastest algorithm for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms:\nMonte Carlo algorithms return a correct answer with high probability. E.g. RP is the subclass of these that run in polynomial time.\nLas Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.\nReduction of complexity\nThis technique transforms difficult problems into better-known problems solvable with (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithms. For example, one selection algorithm finds the median of an unsorted list by first sorting the list (the expensive portion), and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer.\nBack tracking\nIn this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution."
      },
      {
        "heading": "Optimization problems",
        "level": 2,
        "content": "For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:\n\nLinear programming\nWhen searching for optimal solutions to a linear function bound by linear equality and inequality constraints, the constraints can be used directly to produce optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem also requires that any of the unknowns be integers, then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.\nDynamic programming\nWhen a problem shows optimal substructures—meaning the optimal solution can be constructed from optimal solutions to subproblems—and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions. For example, Floyd–Warshall algorithm, the shortest path between a start and goal vertex in a weighted graph can be found using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. Unlike divide and conquer, dynamic programming subproblems often overlap. The difference between dynamic programming and simple recursion is the caching or memoization of recursive calls. When subproblems are independent and do not repeat, memoization does not help; hence dynamic programming is not applicable to all complex problems. Using memoization dynamic programming reduces the complexity of many problems from exponential to polynomial.\nThe greedy method\nGreedy algorithms, similarly to a dynamic programming, work by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution and improve it by making small modifications. For some problems, they always find the optimal solution but for others they may stop at local optima. The most popular use of greedy algorithms is finding minimal spanning trees of graphs without negative cycles. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.\nThe heuristic method\nIn optimization problems, heuristic algorithms find solutions close to the optimal solution when finding the optimal solution is impractical. These algorithms get closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. They can ideally find a solution very close to the optimal solution in a relatively short time. These algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm."
      },
      {
        "heading": "Examples",
        "level": 1,
        "content": "One of the simplest algorithms finds the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be described in plain English as:\nHigh-level description:\n\nIf a set of numbers is empty, then there is no highest number.\nAssume the first number in the set is the largest.\nFor each remaining number in the set: if this number is greater than the current largest, it becomes the new largest.\nWhen there are no unchecked numbers left in the set, consider the current largest number to be the largest in the set.\n(Quasi-)formal description:\nWritten in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:"
      },
      {
        "heading": "See also",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Notes",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bibliography",
        "level": 1,
        "content": "Zaslavsky, C. (1970). Mathematics of the Yoruba People and of Their Neighbors in Southern Nigeria. The Two-Year College Mathematics Journal, 1(2), 76–99. https://doi.org/10.2307/3027363"
      },
      {
        "heading": "Further reading",
        "level": 1,
        "content": ""
      },
      {
        "heading": "External links",
        "level": 1,
        "content": "\n\"Algorithm\". Encyclopedia of Mathematics. EMS Press. 2001 [1994].\nWeisstein, Eric W. \"Algorithm\". MathWorld.\nDictionary of Algorithms and Data Structures – National Institute of Standards and Technology\nAlgorithm repositories\nThe Stony Brook Algorithm Repository – State University of New York at Stony Brook\nCollected Algorithms of the ACM – Associations for Computing Machinery\nThe Stanford GraphBase Archived December 6, 2015, at the Wayback Machine – Stanford University"
      }
    ],
    "summary": "In mathematics and computer science, an algorithm ( ) is a finite sequence of mathematically rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning).\nIn contrast, a heuristic is an approach to solving problems without well-defined correct or optimal results. For example, although social media recommender systems are commonly called \"algorithms\", they actually rely on heuristics as there is no truly \"correct\" recommendation.\nAs an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input."
  },
  {
    "title": "Algoritmo",
    "source": "https://es.wikipedia.org/wiki/Algoritmo",
    "language": "es",
    "chunks": [
      {
        "heading": "Etimología",
        "level": 1,
        "content": "La palabra castellana algoritmo deriva del latín algorithmus, que se acuñaría en el siglo XV a partir del latín algorismus, con influencia del griego arithmos, que significa «número»,[4]​[5]​ o de la latinización del apellido del matemático persa Al-Juarismi.[2]​[6]​\nLa RAE, por su parte, propone que deriva del latín algobarismus, que derivaría del árabe clásico ḥisābu lḡubār, que significa «cálculo mediante cifras arábigas».[7]​"
      },
      {
        "heading": "Definición",
        "level": 1,
        "content": "En general, no existe ningún consenso definitivo en cuanto a la definición formal de algoritmo. Muchos autores los señalan como listas de instrucciones para resolver un cálculo o un problema abstracto, es decir, que un número finito de pasos convierten los datos de un problema (entrada) en una solución (salida).[2]​[7]​[8]​[9]​[10]​[11]​ Sin embargo, cabe notar que algunos algoritmos no tienen necesariamente que terminar o resolver un problema en particular. Por ejemplo, una versión modificada de la criba de Eratóstenes, que nunca termine de calcular números primos, no deja de ser un algoritmo.[12]​\nA lo largo de la historia, varios autores han tratado de definir formalmente los algoritmos utilizando modelos matemáticos. Esto lo hizo Alonzo Church en 1936 con el concepto de «calculabilidad efectiva» basada en su cálculo lambda y por Alan Turing basándose en la máquina de Turing. Los dos enfoques son equivalentes, en el sentido de que se pueden resolver exactamente los mismos problemas con ambos enfoques.[13]​[14]​ No obstante, estos modelos están sujetos a un tipo particular de datos, como son números, símbolos o gráficas mientras que, en general, los algoritmos funcionan sobre una vasta cantidad de estructuras de datos.[8]​[2]​ En general, la parte común en todas las definiciones se puede resumir en las siguientes tres propiedades, siempre y cuando no consideremos algoritmos paralelos:[12]​\n\nTiempo secuencial. Un algoritmo funciona en tiempo discretizado –paso a paso–, definiendo así una secuencia de estados computacionales por cada entrada válida (la entrada son los datos que se le suministran al algoritmo antes de comenzar).\nEstado abstracto. Cada estado computacional puede ser descrito formalmente utilizando una estructura de primer orden y cada algoritmo es independiente de su implementación (los algoritmos son objetos abstractos), de manera que en un algoritmo las estructuras de primer orden son invariantes bajo isomorfismo.\nExploración acotada. La transición de un estado al siguiente queda completamente determinada por una descripción fija y finita; es decir, entre cada estado y el siguiente solamente se puede tomar en cuenta una cantidad fija y limitada de términos del estado actual.\nEn resumen, un algoritmo es cualquier cosa que funcione paso a paso, donde cada paso se pueda describir sin ambigüedad y sin hacer referencia a una computadora en particular, y además tiene un límite fijo en cuanto a la cantidad de datos que se pueden leer/escribir en un solo paso.[15]​\nEsta amplia definición abarca tanto a algoritmos prácticos como aquellos que solo funcionan en teoría, por ejemplo, el método de Newton y la eliminación de Gauss-Jordan funcionan, al menos en principio, con números de precisión infinita; sin embargo, no es posible programar la precisión infinita en una computadora, y no por ello dejan de ser algoritmos.[16]​ En particular es posible considerar una cuarta propiedad que puede usarse para validar la tesis de Church-Turing, de que toda función calculable se puede programar en una máquina de Turing (o equivalentemente, en un lenguaje de programación suficientemente general):[16]​\n\nAritmetizabilidad. Solamente operaciones innegablemente calculables están disponibles en el paso inicial."
      },
      {
        "heading": "Medios de expresión de un algoritmo",
        "level": 1,
        "content": "Los algoritmos pueden ser expresados de muchas maneras, incluyendo al lenguaje natural, pseudocódigo, diagramas de flujo y lenguajes de programación entre otros. Las descripciones en lenguaje natural tienden a ser ambiguas y extensas. El usar pseudocódigo y diagramas de flujo evita muchas ambigüedades del lenguaje natural. Dichas expresiones son formas más estructuradas para representar algoritmos; no obstante, se mantienen independientes de un lenguaje de programación específico.\nLa descripción de un algoritmo suele hacerse en tres niveles:\n\nDescripción de alto nivel. Se establece el problema, se selecciona un modelo matemático y se explica el algoritmo de manera verbal, posiblemente con ilustraciones y omitiendo detalles.\nDescripción formal. Se usa un pseudocódigo para describir la secuencia de pasos que encuentran la solución.\nImplementación. Se muestra el algoritmo expresado en un lenguaje de programación específico o algún objeto capaz de llevar a cabo instrucciones.\nTambién es posible incluir un teorema que demuestre que el algoritmo es correcto, un análisis de complejidad o ambos."
      },
      {
        "heading": "Diagrama de flujo",
        "level": 2,
        "content": "Los diagramas de flujo son descripciones gráficas de algoritmos; usan símbolos conectados con flechas para indicar la secuencia de instrucciones y están regidos por ISO.\nLos diagramas de flujo se emplean para representar algoritmos pequeños, ya que abarcan mucho espacio y su construcción es laboriosa. Por su facilidad de lectura se utilizan como introducción a los algoritmos, descripción de un lenguaje y descripción de procesos a personas ajenas a la computación."
      },
      {
        "heading": "Pseudocódigo",
        "level": 2,
        "content": "El pseudocódigo (falso lenguaje, el prefijo pseudo significa falso) es una descripción de alto nivel de un algoritmo que emplea una mezcla de lenguaje natural con algunas convenciones sintácticas propias de lenguajes de programación, como asignaciones, ciclos y condicionales, aunque no está regido por ningún estándar. \nEl pseudocódigo está pensado para facilitar a las personas el entendimiento de un algoritmo y, por lo tanto, puede omitir detalles irrelevantes que son necesarios en una implementación. Programadores diferentes suelen utilizar convenciones distintas, que pueden estar basadas en la sintaxis de lenguajes de programación concretos. Sin embargo, el pseudocódigo, en general, es comprensible sin necesidad de conocer o usar un entorno de programación específico, y es a la vez suficientemente estructurado para que su implementación se pueda hacer directamente a partir de él.\nAsí, el pseudocódigo cumple con las funciones antes mencionadas para representar algo abstracto, los protocolos son los lenguajes para la programación. Busque fuentes más precisas para tener mayor comprensión del tema."
      },
      {
        "heading": "Sistemas formales",
        "level": 2,
        "content": "La teoría de autómatas y la teoría de funciones recursivas proveen modelos matemáticos que formalizan el concepto de algoritmo. Los modelos más comunes son la máquina de Turing, máquina de registro y funciones μ-recursivas. Estos modelos son tan precisos como un lenguaje máquina, careciendo de expresiones coloquiales o ambigüedad; sin embargo, se mantienen independientes de cualquier computadora y de cualquier implementación."
      },
      {
        "heading": "Implementación",
        "level": 2,
        "content": "Muchos algoritmos se han ideado para implementarse en un programa. No obstante, los algoritmos pueden ser implementados en otros medios, como una red neuronal, un circuito eléctrico o un aparato mecánico y eléctrico. Algunos algoritmos incluso se diseñan especialmente para implementarse usando lápiz y papel. El algoritmo de multiplicación tradicional, el algoritmo de Euclides, la criba de Eratóstenes y muchas formas de resolver la raíz cuadrada son solo algunos ejemplos."
      },
      {
        "heading": "Variables",
        "level": 2,
        "content": "Son elementos que toman valores específicos de un tipo de datos concreto. La declaración de una variable puede realizarse comenzando con var. Principalmente, existen dos maneras de otorgar valores iniciales a variables: \n\nMediante una sentencia de asignación.\nMediante un procedimiento de entrada de datos (por ejemplo: 'read').\nEjemplo:\n\n     ...\n    i:=1;\n    read(n);\n    while i < n do begin\n       (* cuerpo del bucle *)\n       i := i + 1\n    end;\n     ..."
      },
      {
        "heading": "Estructuras secuenciales",
        "level": 2,
        "content": "La estructura secuencial es aquella en la que una acción sigue a otra en secuencia. Las operaciones se suceden de tal modo que la salida de una es la entrada de la siguiente y así sucesivamente hasta el fin del proceso.\nLa asignación de esto consiste en el paso de valores o resultados a una zona de la memoria. Dicha zona será reconocida con el nombre de la variable que recibe el valor. La asignación se puede clasificar de la siguiente forma: \n\nSimples: Consiste en pasar un valor constante a una variable (a ← 15)\nContador: Consiste en usarla como un verificador del número de veces que se realiza un proceso (a ← a + 1)\nAcumulador: Consiste en usarla como un sumador en un proceso (a ← a + b)\nDe trabajo: Donde puede recibir el resultado de una operación matemática que involucre muchas variables (a ← c + b*1/2).\nUn ejemplo de estructura secuencial, como obtener el área de un triángulo:\n\nInicio \n...\n    float b, h, a;\n    printf(\"Diga la base\");\n    scanf(\"%f\", &b);\n    printf(\"Diga la altura\");\n    scanf(\"%f\", &h);\n    a = (b*h)/2;\n    printf(\"El área del triángulo es %f\", a)\n...\nFin"
      },
      {
        "heading": "Algoritmos como funciones",
        "level": 1,
        "content": "Un algoritmo se puede concebir como una función que transforma los datos de un problema (entrada) en los datos de una solución (salida). Más aún, los datos se pueden representar a su vez como secuencias de bits, y en general, de símbolos cualesquiera.[2]​[14]​[17]​ Como cada secuencia de bits representa a un número natural (véase Sistema binario), entonces los algoritmos son en esencia funciones de los números naturales en los números naturales que sí se pueden calcular. Es decir que todo algoritmo calcula una función \n  \n    \n      \n        f\n        :\n        \n          N\n        \n        →\n        \n          N\n        \n      \n    \n    {\\displaystyle f\\colon \\mathbb {N} \\to \\mathbb {N} }\n  \n donde cada número natural es la codificación de un problema o de una solución.\nEn ocasiones los algoritmos son susceptibles de nunca terminar, por ejemplo, cuando entran a un bucle infinito. Cuando esto ocurre, el algoritmo nunca devuelve ningún valor de salida, y podemos decir que la función queda indefinida para ese valor de entrada. Por esta razón se considera que los algoritmos son funciones parciales, es decir, no necesariamente definidas en todo su dominio de definición.\nCuando una función puede ser calculada por medios algorítmicos, sin importar la cantidad de memoria que ocupe o el tiempo que se tarde, se dice que dicha función es computable. No todas las funciones entre secuencias datos son computables. El problema de la parada es un ejemplo."
      },
      {
        "heading": "Análisis de algoritmos",
        "level": 1,
        "content": "Como medida de la eficiencia de un algoritmo, se suelen estudiar los recursos (memoria y tiempo) que consume el algoritmo. El análisis de algoritmos se ha desarrollado para obtener valores que de alguna forma indiquen (o especifiquen) la evolución del gasto de tiempo y memoria en función del tamaño de los valores de entrada.\nEl análisis y estudio de los algoritmos es una disciplina de las ciencias de la computación y, en la mayoría de los casos, su estudio es completamente abstracto sin usar ningún tipo de lenguaje de programación ni cualquier otra implementación; por eso, en ese sentido, comparte las características de las disciplinas matemáticas. Así, el análisis de los algoritmos se centra en los principios básicos del algoritmo, no en los de la implementación particular. Una forma de plasmar (o algunas veces «codificar») un algoritmo es escribirlo en pseudocódigo o utilizar un lenguaje muy simple tal como Léxico, cuyos códigos pueden estar en el idioma del programador.\nAlgunos escritores restringen la definición de algoritmo a procedimientos que deben acabar en algún momento, mientras que otros consideran procedimientos que podrían ejecutarse eternamente sin pararse, suponiendo el caso en el que existiera algún dispositivo físico que fuera capaz de funcionar eternamente. En este último caso, la finalización con éxito del algoritmo no se podría definir como la terminación de este con una salida satisfactoria, sino que el éxito estaría definido en función de las secuencias de salidas dadas durante un periodo de vida de la ejecución del algoritmo. Por ejemplo, un algoritmo que verifica que hay más ceros que unos en una secuencia binaria infinita debe ejecutarse siempre para que pueda devolver un valor útil. Si se implementa correctamente, el valor devuelto por el algoritmo será válido, hasta que evalúe el siguiente dígito binario. De esta forma, mientras evalúa la siguiente secuencia podrán leerse dos tipos de señales: una señal positiva (en el caso de que el número de ceros sea mayor que el de unos) y una negativa en caso contrario. Finalmente, la salida de este algoritmo se define como la devolución de valores exclusivamente positivos si hay más ceros que unos en la secuencia y, en cualquier otro caso, devolverá una mezcla de señales positivas y negativas."
      },
      {
        "heading": "Ejemplo de algoritmo",
        "level": 1,
        "content": "El problema consiste en encontrar el máximo de un conjunto de números. Para un ejemplo más complejo véase Algoritmo de Euclides."
      },
      {
        "heading": "Descripción de alto nivel",
        "level": 2,
        "content": "Dado un conjunto finito \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n de números, se tiene el problema de encontrar el número más grande. Sin pérdida de generalidad se puede asumir que dicho conjunto no es vacío y que sus elementos están numerados como \n  \n    \n      \n        \n          c\n          \n            0\n          \n        \n        ,\n        \n          c\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          c\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle c_{0},c_{1},\\dots ,c_{n}}\n  \n.\nEs decir, dado un conjunto \n  \n    \n      \n        C\n        =\n        {\n        \n          c\n          \n            0\n          \n        \n        ,\n        \n          c\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          c\n          \n            n\n          \n        \n        }\n      \n    \n    {\\displaystyle C=\\{c_{0},c_{1},\\dots ,c_{n}\\}}\n  \n se pide encontrar \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n tal que \n  \n    \n      \n        x\n        ≤\n        m\n      \n    \n    {\\displaystyle x\\leq m}\n  \n para todo elemento \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n que pertenece al conjunto \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n.\nPara encontrar el elemento máximo, se asume que el primer elemento (\n  \n    \n      \n        \n          c\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle c_{0}}\n  \n) es el máximo; luego, se recorre el conjunto y se compara cada valor con el valor del máximo número encontrado hasta ese momento. En el caso de que un elemento sea mayor que el máximo, se asigna su valor al máximo. Cuando se termina de recorrer la lista, el máximo número que se ha encontrado es el máximo de todo el conjunto."
      },
      {
        "heading": "Descripción formal",
        "level": 2,
        "content": "El algoritmo puede ser escrito de una manera más formal en el siguiente pseudocódigo:\n\nSobre la notación:\n\n«←» representa una asignación: \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n ← \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n significa que la variable \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n toma el valor de \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n;\n«devolver» termina el algoritmo y devuelve el valor a su derecha (en este caso, el máximo de \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n)."
      },
      {
        "heading": "Implementación",
        "level": 2,
        "content": "En lenguaje C++:"
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Tipos de algoritmos según su función",
        "level": 2,
        "content": "Algoritmo de ordenamiento\nAlgoritmo de búsqueda"
      },
      {
        "heading": "Técnicas de diseño de algoritmos",
        "level": 2,
        "content": "Algoritmos voraces (greedy): seleccionan los elementos más prometedores del conjunto de candidatos hasta encontrar una solución. En la mayoría de los casos la solución no es óptima.\nAlgoritmos paralelos: permiten la división de un problema en subproblemas de forma que se puedan ejecutar de forma simultánea en varios procesadores.\nAlgoritmos probabilísticos: algunos de los pasos de este tipo de algoritmos están en función de valores pseudoaleatorios.\nAlgoritmos determinísticos: el comportamiento del algoritmo es lineal: cada paso del algoritmo tiene únicamente un paso sucesor y otro antecesor.\nAlgoritmos no determinísticos: el comportamiento del algoritmo tiene forma de árbol y a cada paso del algoritmo puede bifurcarse a cualquier número de pasos inmediatamente posteriores, además todas las ramas se ejecutan simultáneamente.\nDivide y vencerás: dividen el problema en subconjuntos disjuntos obteniendo una solución de cada uno de ellos para después unirlas, logrando así la solución al problema completo.\nMetaheurísticas: encuentran soluciones aproximadas (no óptimas) a problemas basándose en un conocimiento anterior (a veces llamado experiencia) de los mismos.\nProgramación dinámica: intenta resolver problemas disminuyendo su coste computacional aumentando el coste espacial.\nRamificación y acotación: se basa en la construcción de las soluciones al problema mediante un árbol implícito que se recorre de forma controlada encontrando las mejores soluciones.\nVuelta atrás (backtracking): se construye el espacio de soluciones del problema en un árbol que se examina completamente, almacenando las soluciones menos costosas."
      },
      {
        "heading": "Temas relacionados",
        "level": 2,
        "content": "Cota inferior asintótica\nCota ajustada asintótica\nComplejidad computacional\nDiagramas de flujo\nDiagrama Nassi-Shneiderman\nMáquina de Turing"
      },
      {
        "heading": "Disciplinas relacionadas",
        "level": 2,
        "content": "Ciencias de la Computación\nAnálisis de algoritmos\nComplejidad computacional\nGobierno por algoritmos\nInformática\nInteligencia artificial\nInvestigación operativa\nMatemáticas\nProgramación"
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bibliografía",
        "level": 1,
        "content": "Aho, Alfred (1974). The Design and Analysis of Computer Algorithms (en inglés). Addison-Wesley. ISBN 978-0201000290. \nCormen, T. H.; Leiserson, C. E.; Rivest, R. L.; Stein, C. (2001). Introduction to Algorithms (en inglés) (2nd edición). MIT Press. ISBN 978-0070131514. \nBrassard, Gilles; Bratley, Paul (1996). Fundamentos de Algoritmia. Prentice Hall. ISBN 978-8489660007. \nKnuth, Donald E. (2022). The Art of Computer Programming (en inglés). Addison-Wesley Professional. ISBN 978-0137935109. \nMamber, Udi (1989). Introduction to Algorithms, a Creative Approach (en inglés). Addison-Wesley. ISBN 978-0201120370. \nSedgewick, Robert (1997). Algorithms in C (en inglés) (3rd edición). Addison-Wesley Professional. ISBN 978-0201314526. \nAxt, P (1959). «On a Subrecursive Hierarchy and Primitive Recursive Degrees». Transactions of the American Mathematical Society 92 (1): 85-105. JSTOR 1993169. doi:10.2307/1993169. \nBell, C. Gordon; Newell, Allen (1971). Computer Structures: Readings and Examples (en inglés). Nueva York: McGraw–Hill. ISBN 978-0070043572. \nBlass, Andreas; Gurevich, Yuri (2003). «Algorithms: A Quest for Absolute Definitions». Bulletin of European Association for Theoretical Computer Science 81.  Includes an excellent bibliography of 56 references.\nBolter, David J. (1984). Turing's Man: Western Culture in the Computer Age (1984 edición). Chapel Hill, NC: The University of North Carolina Press. ISBN 978-0807815649. \nBoolos, George; Jeffrey, Richard (1999). Computability and Logic (4th edición). Cambridge University Press, London. ISBN 978-0-521-20402-6. (requiere registro). : cf. Capítulo 3 Turing machines donde se argumenta que «certain enumerable sets not effectively (mechanically) enumerable».\nBurgin, Mark (2004). Super-Recursive Algorithms. Springer. ISBN 978-0387955698. \nCampagnolo, M.L., Moore, C., and Costa, J.F. (2000) An analog characterization of the subrecursive functions. In Proc. of the 4th Conference on Real Numbers and Computers, Odense University, pp. 91–109\nChurch, Alonzo (1936a). «An Unsolvable Problem of Elementary Number Theory». The American Journal of Mathematics 58 (2): 345-363. JSTOR 2371045. doi:10.2307/2371045.  Republicado en The Undecidable, p. 89ff. La primera expresión de «tesis de Church». Obsérverse en particular la página 10 de The Undecidable donde se define la noción de «calculabilidad efectiva» en términos de «un algoritmo», y usa la expresón «termina», etc.\nChurch, Alonzo (1936b). «A Note on the Entscheidungsproblem». The Journal of Symbolic Logic 1 (1): 40-41. JSTOR 2269326. doi:10.2307/2269326.  Church, Alonzo (1936). «Correction to a Note on the Entscheidungsproblem». The Journal of Symbolic Logic 1 (3): 101-102. JSTOR 2269030. doi:10.2307/2269030.  Reprinted in The Undecidable, p. 110ff. Church shows that the Entscheidungsproblem is unsolvable in about 3 pages of text and 3 pages of footnotes.\nDaffa', Ali Abdullah al- (1977). The Muslim contribution to mathematics. London: Croom Helm. ISBN 978-0856644641. \nDavis, Martin (1965). The Undecidable: Basic Papers On Undecidable Propositions, Unsolvable Problems and Computable Functions. New York: Raven Press. ISBN 978-0486432281. (requiere registro).  Davis introduce cada artículo; los autores de los artículos incluyen a  Gödel, Alonzo Church, Turing, Rosser, Kleene, and Emil Post; los artículos citados en esta entrada se encuentran referenciados por el nombre de cada autor.\nDavis, Martin (2000). Engines of Logic: Mathematicians and the Origin of the Computer. New York: W.W. Nortion. ISBN 978-0-393-32229-3.  Davis offers concise biographies of Leibniz, Boole, Frege, Cantor, Hilbert, Gödel and Turing with von Neumann as the show-stealing villain. Very brief bios of Joseph-Marie Jacquard, Babbage, Ada Lovelace, Claude Shannon, Howard Aiken, etc.\nDean, Tim (2012). «Evolution and moral diversity». Baltic International Yearbook of Cognition, Logic and Communication 7. doi:10.4148/biyclc.v7i0.1775. \nDennett, Daniel (1995). «Darwin's Dangerous Idea». Complexity 2 (1) (New York: Touchstone/Simon & Schuster). pp. 32–36. Bibcode:1996Cmplx...2a..32M. ISBN 978-0-684-80290-9. doi:10.1002/(SICI)1099-0526(199609/10)2:1<32::AID-CPLX8>3.0.CO;2-H. (requiere registro). \nDilson, Jesse (2007). The Abacus ((1968, 1994) edición). St. Martin's Press, NY. ISBN 978-0-312-10409-2. , ISBN 0-312-10409-X\nYuri Gurevich, Sequential Abstract State Machines Capture Sequential Algorithms, ACM Transactions on Computational Logic, Vol 1, no 1 (July 2000), pp. 77–111. Includes bibliography of 33 sources.\nvan Heijenoort, Jean (2001). From Frege to Gödel, A Source Book in Mathematical Logic, 1879–1931 ((1967) edición). Harvard University Press, Cambridge. ISBN 978-0-674-32449-7. , 3rd edition 1976[?], ISBN 0-674-32449-8 (pbk.)\nHodges, Andrew (1983). «Alan Turing: The Enigma». Physics Today 37 (11) (New York: Simon and Schuster). pp. 107-108. Bibcode:1984PhT....37k.107H. ISBN 978-0-671-49207-6. doi:10.1063/1.2915935. , ISBN 0-671-49207-1. Cf. Chapter \"The Spirit of Truth\" for a history leading to, and a discussion of, his proof.\nKleene, Stephen C. (1936). «General Recursive Functions of Natural Numbers». Mathematische Annalen 112 (5): 727-742. S2CID 120517999. doi:10.1007/BF01565439. Archivado desde el original el 3 de septiembre de 2014. Consultado el 30 de septiembre de 2013.  Presented to the American Mathematical Society, September 1935. Reprinted in The Undecidable, p. 237ff. Kleene's definition of \"general recursion\" (known now as mu-recursion) was used by Church in his 1935 paper An Unsolvable Problem of Elementary Number Theory that proved the \"decision problem\" to be \"undecidable\" (i.e., a negative result).\nKleene, Stephen C. (1943). «Recursive Predicates and Quantifiers». American Mathematical Society Transactions 54 (1): 41-73. JSTOR 1990131. doi:10.2307/1990131.  Reprinted in The Undecidable, p. 255ff. Kleene refined his definition of \"general recursion\" and proceeded in his chapter \"12. Algorithmic theories\" to posit \"Thesis I\" (p. 274); he would later repeat this thesis (in Kleene 1952:300) and name it \"Church's Thesis\"(Kleene 1952:317) (i.e., the Church thesis).\nKleene, Stephen C. (1991). Introduction to Metamathematics (Tenth edición). North-Holland Publishing Company. ISBN 978-0-7204-2103-3. \nKnuth, Donald (1997). Fundamental Algorithms, Third Edition. Reading, Massachusetts: Addison–Wesley. ISBN 978-0-201-89683-1. \nKnuth, Donald (1969). Volume 2/Seminumerical Algorithms, The Art of Computer Programming First Edition. Reading, Massachusetts: Addison–Wesley. \nKosovsky, N.K. Elements of Mathematical Logic and its Application to the theory of Subrecursive Algorithms, LSU Publ., Leningrad, 1981\nKowalski, Robert (1979). «Algorithm=Logic+Control». Communications of the ACM 22 (7): 424-436. S2CID 2509896. doi:10.1145/359131.359136. \nA.A. Markov (1954) Theory of algorithms. [Translated by Jacques J. Schorr-Kon and PST staff] Imprint Moscow, Academy of Sciences of the USSR, 1954 [i.e., Jerusalem, Israel Program for Scientific Translations, 1961; available from the Office of Technical Services, U.S. Dept. of Commerce, Washington] Description 444 p. 28 cm. Added t.p. in Russian Translation of Works of the Mathematical Institute, Academy of Sciences of the USSR, v. 42. Original title: Teoriya algerifmov. [QA248.M2943 Dartmouth College library. U.S. Dept. of Commerce, Office of Technical Services, number OTS .]\nMinsky, Marvin (1967). Computation: Finite and Infinite Machines (First edición). Prentice-Hall, Englewood Cliffs, NJ. ISBN 978-0-13-165449-5. (requiere registro).  Minsky expands his \"...idea of an algorithm – an effective procedure...\" in chapter 5.1 Computability, Effective Procedures and Algorithms. Infinite machines.\nPost, Emil (1936). «Finite Combinatory Processes, Formulation I». The Journal of Symbolic Logic 1 (3): 103-105. JSTOR 2269031. doi:10.2307/2269031.  Reprinted in The Undecidable, pp. 289ff. Post defines a simple algorithmic-like process of a man writing marks or erasing marks and going from box to box and eventually halting, as he follows a list of simple instructions. This is cited by Kleene as one source of his \"Thesis I\", the so-called Church–Turing thesis.\nRogers, Jr, Hartley (1987). Theory of Recursive Functions and Effective Computability. The MIT Press. ISBN 978-0-262-68052-3. \nRosser, J.B. (1939). «An Informal Exposition of Proofs of Godel's Theorem and Church's Theorem». Journal of Symbolic Logic 4 (2): 53-60. JSTOR 2269059. doi:10.2307/2269059.  Reprinted in The Undecidable, p. 223ff. Herein is Rosser's famous definition of \"effective method\": \"...a method each step of which is precisely predetermined and which is certain to produce the answer in a finite number of steps... a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer\" (p. 225–226, The Undecidable)\nSantos-Lang, Christopher (2014). «Moral Ecology Approaches to Machine Ethics». Machine Medical Ethics. Intelligent Systems, Control and Automation: Science and Engineering 74. Switzerland: Springer. pp. 111-127. ISBN 978-3-319-08107-6. doi:10.1007/978-3-319-08108-3_8. \nScott, Michael L. (2009). Programming Language Pragmatics (3rd edición). Morgan Kaufmann Publishers/Elsevier. ISBN 978-0-12-374514-9. \nSipser, Michael (2006). Introduction to the Theory of Computation. PWS Publishing Company. ISBN 978-0-534-94728-6. \nSober, Elliott; Wilson, David Sloan (1998). Unto Others: The Evolution and Psychology of Unselfish Behavior. Cambridge: Harvard University Press. (requiere registro). \nStone, Harold S. (1972). Introduction to Computer Organization and Data Structures (1972 edición). McGraw-Hill, New York. ISBN 978-0-07-061726-1.  Cf. in particular the first chapter titled: Algorithms, Turing Machines, and Programs. His succinct informal definition: \"...any sequence of instructions that can be obeyed by a robot, is called an algorithm\" (p. 4).\nTausworthe, Robert C (1977). Standardized Development of Computer Software Part 1 Methods. Englewood Cliffs NJ: Prentice–Hall, Inc. ISBN 978-0-13-842195-3. \nTuring, Alan M. (1936–37). «On Computable Numbers, With An Application to the Entscheidungsproblem». Proceedings of the London Mathematical Society. Series 2 42: 230-265. doi:10.1112/plms/s2-42.1.230. . Corrections, ibid, vol. 43(1937) pp. 544–546. Reprinted in The Undecidable, p. 116ff. Turing's famous paper completed as a Master's dissertation while at King's College Cambridge UK.\nTuring, Alan M. (1939). «Systems of Logic Based on Ordinals». Proceedings of the London Mathematical Society 45: 161-228. doi:10.1112/plms/s2-45.1.161.  Reprinted in The Undecidable, pp. 155ff. Turing's paper that defined \"the oracle\" was his PhD thesis while at Princeton.\nUnited States Patent and Trademark Office (2006), 2106.02 **>Mathematical Algorithms: 2100 Patentability, Manual of Patent Examining Procedure (MPEP). Latest revision August 2006"
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": " Wikilibros alberga un libro o manual sobre Algoritmia.\n Wikcionario  tiene definiciones y otra información sobre algoritmo."
      }
    ],
    "summary": "En matemáticas, lógica, ciencias de la computación y disciplinas relacionadas, un algoritmo es un conjunto de instrucciones o reglas definidas y no-ambiguas, ordenadas y finitas que permite, típicamente, solucionar un problema, realizar un cómputo, procesar datos y llevar a cabo otras tareas o actividades.[1]​ También sirve para resolver un cubo de Rubik. Dado un estado inicial y una entrada, siguiendo los pasos sucesivos se llega a un estado final y se obtiene una solución. Los algoritmos son el objeto de estudio de la algoritmia.[2]​\nEn la vida cotidiana, se emplean algoritmos frecuentemente para resolver problemas determinados. Algunos ejemplos son los manuales de usuario, que muestran algoritmos para usar un aparato, o las instrucciones que recibe un trabajador de su patrón. Algunos ejemplos en matemática son el algoritmo de multiplicación, para calcular el producto, el algoritmo de la división para calcular el cociente de dos números, el algoritmo de Euclides para obtener el máximo común divisor de dos enteros positivos, o el método de Gauss para resolver un sistema de ecuaciones lineales.\nEn términos de programación, un algoritmo es una secuencia de pasos lógicos que permiten solucionar un problema. Por ejemplo: si perdemos el acceso a una cuenta nos dan una serie de pasos para poder identificarla y así recuperarla o borrarla, que desarrollándose de manera efectiva y lógica, lograrán un resultado determinado.\nEn los últimos tiempos, debido al auge y desarrollo de la inteligencia artificial y a que los algoritmos son capitales para su funcionamiento, se vienen utilizando con cierta sinonimia la expresión «inteligencia artificial» y el término «algoritmo» en el lenguaje común o atécnico. En este sentido, por ejemplo, la Estrategia de Inteligencia Artificial de la Unión Europea parece identificarlos cuando indica que: «En lo que atañe a la utilización de la IA, los entornos ricos en datos también brindan más oportunidades. Ello se debe a que los datos son los que permiten al algoritmo aprender acerca de su entorno e interactuar con él». Así, es frecuente encontrar expresiones como \"ética de los algoritmos\" o \"gobernanza de los algoritmos\" para hacer referencia a determinados requisitos de cumplimiento ético y jurídico que normas como el Reglamento (UE) 2024/1689 de Inteligencia Artificial imponen a los sistemas y modelos de inteligencia artificial. [3]​"
  },
  {
    "title": "Data structure",
    "source": "https://en.wikipedia.org/wiki/Data_structure",
    "language": "en",
    "chunks": [
      {
        "heading": "Usage",
        "level": 1,
        "content": "Data structures serve as the basis for abstract data types (ADT). The ADT defines the logical form of the data type. The data structure implements the physical form of the data type.\nDifferent types of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, relational databases commonly use B-tree indexes for data retrieval, while compiler implementations usually use hash tables to look up identifiers.\nData structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. Usually, efficient data structures are key to designing efficient algorithms. Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Data structures can be used to organize the storage and retrieval of information stored in both main memory and secondary memory."
      },
      {
        "heading": "Implementation",
        "level": 1,
        "content": "Data structures can be implemented using a variety of programming languages and techniques, but they all share the common goal of efficiently organizing and storing data. Data structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by a pointer—a bit string, representing a memory address, that can be itself stored in memory and manipulated by the program. Thus, the array and record data structures are based on computing the addresses of data items with arithmetic operations, while the linked data structures are based on storing addresses of data items within the structure itself. This approach to data structuring has profound implications for the efficiency and scalability of algorithms. For instance, the contiguous memory allocation in arrays facilitates rapid access and modification operations, leading to optimized performance in sequential data processing scenarios. \nThe implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations. This observation motivates the theoretical concept of an abstract data type, a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost)."
      },
      {
        "heading": "Examples",
        "level": 1,
        "content": "There are numerous types of data structures, generally built upon simpler primitive data types. Well known examples are:\n\nAn array is a number of elements in a specific order, typically all of the same type (depending on the language, individual elements may either all be forced to be the same type, or may be of almost any type). Elements are accessed using an integer index to specify which element is required. Typical implementations allocate contiguous memory words for the elements of arrays (but this is not always a necessity). Arrays may be fixed-length or resizable.\nA linked list (also just called list) is a linear collection of data elements of any type, called nodes, where each node has itself a value, and points to the next node in the linked list. The principal advantage of a linked list over an array is that values can always be efficiently inserted and removed without relocating the rest of the list. Certain other operations, such as random access to a certain element, are however slower on lists than on arrays.\nA record (also called tuple or struct) is an aggregate data structure. A record is a value that contains other values, typically in fixed number and sequence and typically indexed by names. The elements of records are usually called fields or members. In the context of object-oriented programming, records are known as plain old data structures to distinguish them from objects.\nHash tables, also known as hash maps, are data structures that provide fast retrieval of values based on keys. They use a hashing function to map keys to indexes in an array, allowing for constant-time access in the average case. Hash tables are commonly used in dictionaries, caches, and database indexing. However, hash collisions can occur, which can impact their performance. Techniques like chaining and open addressing are employed to handle collisions.\nGraphs are collections of nodes connected by edges, representing relationships between entities. Graphs can be used to model social networks, computer networks, and transportation networks, among other things. They consist of vertices (nodes) and edges (connections between nodes). Graphs can be directed or undirected, and they can have cycles or be acyclic. Graph traversal algorithms include breadth-first search and depth-first search.\nStacks and queues are abstract data types that can be implemented using arrays or linked lists. A stack has two primary operations: push (adds an element to the top of the stack) and pop (removes the topmost element from the stack), that follow the Last In, First Out (LIFO) principle. Queues have two main operations: enqueue (adds an element to the rear of the queue) and dequeue (removes an element from the front of the queue) that follow the First In, First Out (FIFO) principle.\nTrees represent a hierarchical organization of elements. A tree consists of nodes connected by edges, with one node being the root and all other nodes forming subtrees. Trees are widely used in various algorithms and data storage scenarios. Binary trees (particularly heaps), AVL trees, and B-trees are some popular types of trees. They enable efficient and optimal searching, sorting, and hierarchical representation of data.\nA trie, or prefix tree, is a special type of tree used to efficiently retrieve strings. In a trie, each node represents a character of a string, and the edges between nodes represent the characters that connect them. This structure is especially useful for tasks like autocomplete, spell-checking, and creating dictionaries. Tries allow for quick searches and operations based on string prefixes."
      },
      {
        "heading": "Language support",
        "level": 1,
        "content": "Most assembly languages and some low-level languages, such as BCPL (Basic Combined Programming Language), lack built-in support for data structures. On the other hand, many high-level programming languages and some higher-level assembly languages, such as MASM, have special syntax or other built-in support for certain data structures, such as records and arrays. For example, the C (a direct descendant of BCPL) and Pascal languages support structs and records, respectively, in addition to vectors (one-dimensional arrays) and multi-dimensional arrays.\nMost programming languages feature some sort of library mechanism that allows data structure implementations to be reused by different programs. Modern languages usually come with standard libraries that implement the most common data structures. Examples are the C++ Standard Template Library, the Java Collections Framework, and the Microsoft .NET Framework.\nModern languages also generally support modular programming, the separation between the interface of a library module and its implementation. Some provide opaque data types that allow clients to hide implementation details. Object-oriented programming languages, such as C++, Java, and Smalltalk, typically use classes for this purpose.\nMany known data structures have concurrent versions which allow multiple computing threads to access a single concrete instance of a data structure simultaneously."
      },
      {
        "heading": "See also",
        "level": 1,
        "content": ""
      },
      {
        "heading": "References",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bibliography",
        "level": 1,
        "content": "Peter Brass, Advanced Data Structures, Cambridge University Press, 2008, ISBN 978-0521880374\nDonald Knuth, The Art of Computer Programming, vol. 1. Addison-Wesley, 3rd edition, 1997, ISBN 978-0201896831\nDinesh Mehta and Sartaj Sahni, Handbook of Data Structures and Applications, Chapman and Hall/CRC Press, 2004, ISBN 1584884355\nNiklaus Wirth, Algorithms and Data Structures, Prentice Hall, 1985, ISBN 978-0130220059"
      },
      {
        "heading": "Further reading",
        "level": 1,
        "content": "Open Data Structures by Pat Morin\nG. H. Gonnet and R. Baeza-Yates, Handbook of Algorithms and Data Structures - in Pascal and C, second edition, Addison-Wesley, 1991, ISBN 0-201-41607-7\nEllis Horowitz and Sartaj Sahni, Fundamentals of Data Structures in Pascal, Computer Science Press, 1984, ISBN 0-914894-94-3"
      },
      {
        "heading": "External links",
        "level": 1,
        "content": "\nDescriptions from the Dictionary of Algorithms and Data Structures\nData structures course\nAn Examination of Data Structures from .NET perspective\nSchaffer, C. Data Structures and Algorithm Analysis"
      }
    ],
    "summary": "In computer science, a data structure is a data organization and storage format that is usually chosen for efficient access to data. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data, i.e., it is an algebraic structure about data."
  },
  {
    "title": "Estructura de datos",
    "source": "https://es.wikipedia.org/wiki/Estructura_de_datos",
    "language": "es",
    "chunks": [
      {
        "heading": "Descripción",
        "level": 1,
        "content": "Las estructuras de datos se basan generalmente en la capacidad de un ordenador para recuperar y almacenar datos en cualquier lugar de su memoria.\nLas estructuras de datos sirven de base para los tipos de datos abstractos (ADT, del inglés abstract data types). Los ADT definen la forma lógica del tipo de datos. La estructura de datos implementa la forma física del tipo de datos.[7]​\nLos distintos tipos de estructuras de datos se adaptan a distintos tipos de aplicaciones, y algunas están altamente especializadas para tareas específicas. Por ejemplo, las bases de datos relacionales utilizan habitualmente índices de árbol B para la recuperación de datos,[8]​ mientras que las implementaciones del compilador suelen utilizar tablas hash para buscar identificadores.[9]​\nLas estructuras de datos proporcionan un medio para gestionar grandes cantidades de datos de forma eficiente para usos como grandes bases de datos y servicios de indexación de Internet. Normalmente, las estructuras de datos eficientes son clave para diseñar algoritmos eficientes. Algunos métodos de diseño formales y lenguajes de programación enfatizan las estructuras de datos, más que los algoritmos, como el factor organizador clave en el diseño de software. Las estructuras de datos se pueden utilizar para organizar el almacenamiento y la recuperación de la información almacenada tanto en la memoria principal como en la memoria secundaria.[10]​"
      },
      {
        "heading": "Tipos de estructura de datos",
        "level": 1,
        "content": "Las estructuras de datos pueden ser de diferentes tipos, dependiendo de la técnica que se utilice para su almacenamiento y recuperación, estos tipos son los siguientes:\n\nEstructura de datos estática.\nEstructura de datos dinámica.[11]​\nSegún la secuencia que se presenta entre cada elemento al momento de realizar el recorrido entre los elementos de la estructura de datos, esta se puede clasificar en los siguientes tipos:\n\nEstructura de datos lineal.\nEstructura de datos no lineal."
      },
      {
        "heading": "Ejemplos",
        "level": 1,
        "content": "Existen numerosos tipos de estructuras de datos, generalmente construidas sobre otras más simples:\n\nUn vector es una serie de elementos en un orden específico, por lo general todos del mismo tipo (si bien los elementos pueden ser de casi cualquier tipo). Se accede a los elementos utilizando un entero como índice para especificar el elemento que se requiere. Las implementaciones típicas asignan palabras de memoria contiguas a los elementos de los vectores (aunque no siempre es el caso). Los vectores pueden cambiar de tamaño o tener una longitud fija.\nUn vector asociativo (también llamado diccionario o mapa) es una variante más flexible que un vector, en la que se puede añadir y eliminar libremente pares nombre-valor. Una tabla de hash es una implementación usual de un vector asociativo.\nUna lista enlazada (también llamada solamente lista) es una colección lineal de elementos de datos de cualquier tipo, llamados nodos, donde cada nodo tiene en sí mismo un valor y apunta al siguiente nodo de la lista enlazada. La principal ventaja de una lista enlazada sobre un vector es que siempre se pueden insertar y eliminar valores de forma eficiente sin reubicar el resto de la lista. Sin embargo, otras operaciones, como el acceso aleatorio a un elemento determinado, son más lentas en las listas que en los vectores.\nUn registro (también llamado tupla o estructura) es una estructura de datos agregados. Un registro es un valor que contiene otros valores, típicamente en un número fijo y la secuencia y por lo general un índice por nombres. Los elementos de los registros generalmente son llamados campos o celdas.\nUna unión es una estructura de datos que especifica cuál de una serie de tipos de datos permitidos podrá ser almacenada en sus instancias, por ejemplo flotante o entero largo. En contraste con un registro, que se podría definir para contener un flotante y un entero largo, en una unión solo hay un valor a la vez. Se asigna suficiente espacio para contener el tipo de datos de cualquiera de los miembros.\nUn tipo variante (también llamado registro variante o unión discriminada) contiene un campo adicional que indica su tipo actual.\nUn conjunto es un tipo de datos abstracto que puede almacenar valores específicos, sin orden particular y sin valores duplicados.\nUn multiconjunto es un tipo de datos abstracto que puede almacenar valores específicos, sin orden particular. A diferencia de los conjuntos, los multiconjuntos admiten repeticiones.\nUn grafo es una estructura de datos conectada compuesta por nodos. Cada nodo contiene un valor y una o más referencias a otros nodos. Los grafos pueden utilizarse para representar redes, dado que los nodos pueden referenciarse entre ellos. Las conexiones entre nodos pueden tener dirección, es decir un nodo de partida y uno de llegada.\nLas pilas y las colas son tipos de datos abstractos que pueden implementarse utilizando vectores o listas enlazadas. Una pila tiene dos operaciones principales: apilar (añade un elemento a la parte superior de la pila) y desapilar (elimina el elemento más alto de la pila), que siguen el principio de último en entrar, primero en salir (LIFO, en inglés 'Last In, First Out'). Las colas tienen dos operaciones principales: encolar (añade un elemento a la parte posterior de la cola) y desencolar (elimina un elemento de la parte anterior de la cola), que siguen el principio de primero en entrar, primero en salir (FIFO, en inglés 'First In, First Out').\nUn árbol es un caso particular de grafo dirigido en el que no se admiten ciclos y existe un camino desde un nodo llamado raíz hasta cada uno de los otros nodos. Una colección de árboles es llamada un bosque.\nUna clase es una plantilla para la creación de objetos de datos según un modelo predefinido. Las clases se utilizan como representación abstracta de conceptos, incluyen campos como los registros y operaciones que pueden consultar el valor de los campos o cambiar sus valores."
      },
      {
        "heading": "Soporte en los lenguajes",
        "level": 1,
        "content": "La mayoría de los lenguajes ensambladores y algunos lenguajes de bajo nivel, tales como BCPL, carecen de soporte de estructuras de datos. En cambio, muchos lenguajes de alto nivel y algunos lenguajes ensambladores de alto nivel, tales como MASM, tienen algún tipo de soporte incorporado para ciertas estructuras de datos, tales como los registros y arreglos. Por ejemplo, los lenguajes C y Pascal soportan estructuras y registros, respectivamente, además de arreglos y matrices multidimensionales.[13]​[14]​\nLa mayoría de los lenguajes de programación disponen de algún tipo de biblioteca o mecanismo que permita el uso de estructuras de datos en los programas. Los lenguajes modernos por lo general vienen con bibliotecas estándar que implementan las estructuras de datos más comunes. Ejemplos de ello son la biblioteca  Standard Template Library de C++, las colecciones de Java[nota 1]​[15]​ y las bibliotecas .NET de Microsoft."
      },
      {
        "heading": "Estructuras de datos en programación",
        "level": 1,
        "content": "En programación, una estructura de datos puede ser declarada inicialmente escribiendo una palabra reservada, luego un identificador para la estructura y un nombre para cada uno de sus miembros, sin olvidar los tipos de datos que estos representan. Generalmente, cada miembro se separa con algún tipo de operador, carácter o palabra reservada.\nEn el lenguaje de programación Pascal, es posible crear una estructura de datos de la forma mencionada. La sintaxis básica es:\n\n Estruct Identificador, _\n              Miembro1:TipoDeDato, _\n              Miembro2:TipoDeDato, _\n              ... \n              Miembro9:TipoDeDato\n\nPara acceder a los miembros de una estructura, primero se debe crear una referencia a esta, generalmente con una variable de tipo; luego se pueden editar y obtener los datos de los miembros libremente.\n\n Estruc Estructura,Miembro1:Entero,Miembro2:Cadena,Miembro3:Byte\n Var Variable:Estructura\n Variable.Miembro1 = 40000\n Variable.Miembro2 = \"Hola Mundo\"\n Variable.Miembro3 = 255\n Mensaje(Variable.Miembro2) ' Muestra \"Hola Mundo\""
      },
      {
        "heading": "Listado de estructuras de datos",
        "level": 1,
        "content": "Vectores (matriz o arreglo)\nRegistro\nTipo de dato algebraico\nListas Enlazadas\nListas Simples\nListas Doblemente Enlazadas\nListas Circulares\n\nListas por saltos (Skip lists)\n\nPilas (stack)\nColas (queue)\nCola de prioridades\nÁrboles\n\nÁrboles Binarios\nÁrbol binario de búsqueda\nÁrbol binario de búsqueda equilibrado\nÁrboles Rojo-Negro\nÁrboles AVL\n\nÁrboles biselados (Árboles Splay)\nÁrboles multicamino (o multirrama)\n\nÁrboles B\nÁrboles B+\nConjuntos (sets)\n\nGrafos\nDiccionarios [nota 2]​\nTabla de racimo\nMontículos (o heaps)\nMontículo binario\nMontículo binomial\n\nMontículo de Fibonacci\nMontículo suave\nMontículo 2-3 [nota 3]​\nOtros\n\nBuffer circular\nEstructura de datos para conjuntos disjuntos (algoritmo Unión-Buscar, Union-find)"
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": "Dato\nMetadatos\nalgoritmo\nlenguaje de programación\ntipo de dato\nunión de datos\nModelo de datos\nEstructuras de datos persistentes\nModelo entidad-relación\nEstructuras de control\nASN.1\nSeguridad de la información"
      },
      {
        "heading": "Notas",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bibliografía",
        "level": 1,
        "content": "Peter Brass, Advanced Data Structures, Cambridge University Press, 2008, ISBN 978-0521880374\nDonald Knuth, The Art of Computer Programming, vol. 1. Addison-Wesley, 3ra edición, 1997, ISBN 978-0201896831\nDinesh Mehta and Sartaj Sahni, Handbook of Data Structures and Applications, Chapman and Hall/CRC Press, 2004, ISBN 1584884355\nNiklaus Wirth, Algorithms and Data Structures, Prentice Hall, 1985, ISBN 978-0130220059"
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": "Descripciones del Dictionary of Algorithms and Data Structures (en inglés)\nCurso de estructuras de datos (en inglés)\nGaston Gonnet y Ricardo Baeza-Yates, Handbook of Algorithms and Data Structures - in Pascal and C, second edition, Addison-Wesley, 1991, ISBN 0-201-41607-7 (en inglés)"
      }
    ],
    "summary": "En ciencias de la computación, una estructura de datos[1]​ es una forma particular de organizar información en un computador para que pueda ser utilizada de manera eficiente.[2]​[3]​[4]​ Diferentes tipos de estructuras de datos son adecuados para diferentes tipos de aplicaciones, y algunos son altamente especializados para tareas específicas.[5]​\nLas estructuras de datos son medios para manejar grandes cantidades de información de manera eficiente para usos tales como grandes bases de datos y servicios de indización de Internet. Por lo general, las estructuras de datos eficientes son clave para diseñar algoritmos eficientes. Algunos métodos formales de diseño de lenguajes de programación destacan las estructuras de datos, en lugar de los algoritmos, como el factor clave de organización en el diseño de software. Más precisamente, una estructura de datos es una colección de valores, las relaciones entre ellos y las funciones y operaciones que se pueden aplicar a los datos,[6]​ es decir, es una estructura algebraica de datos."
  },
  {
    "title": "Artificial intelligence",
    "source": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "language": "en",
    "chunks": [
      {
        "heading": "Goals",
        "level": 1,
        "content": "The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research."
      },
      {
        "heading": "Reasoning and problem-solving",
        "level": 2,
        "content": "Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem."
      },
      {
        "heading": "Knowledge representation",
        "level": 2,
        "content": "Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.\nA knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.\nAmong the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications."
      },
      {
        "heading": "Planning and decision-making",
        "level": 2,
        "content": "An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.\nIn classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.\nIn some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\nA Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.\nGame theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents."
      },
      {
        "heading": "Learning",
        "level": 2,
        "content": "Machine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning.\n\nThere are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).\nIn reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.\nComputational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization."
      },
      {
        "heading": "Natural language processing",
        "level": 2,
        "content": "Natural language processing (NLP) allows programs to read, write and communicate in human languages such as English. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.\nEarly work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\nModern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications."
      },
      {
        "heading": "Perception",
        "level": 2,
        "content": "Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.\nThe field includes speech recognition, image classification, facial recognition, object recognition,object tracking, and robotic perception."
      },
      {
        "heading": "Social intelligence",
        "level": 2,
        "content": "Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.\nHowever, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject."
      },
      {
        "heading": "General intelligence",
        "level": 2,
        "content": "A machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence."
      },
      {
        "heading": "Techniques",
        "level": 1,
        "content": "AI research uses a wide variety of techniques to accomplish the goals above."
      },
      {
        "heading": "Search and optimization",
        "level": 2,
        "content": "AI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search."
      },
      {
        "heading": "State space search",
        "level": 3,
        "content": "State space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.\nSimple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.\nAdversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position."
      },
      {
        "heading": "Local search",
        "level": 3,
        "content": "Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.\nGradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks, through the backpropagation algorithm.\nAnother type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.\nDistributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails)."
      },
      {
        "heading": "Logic",
        "level": 2,
        "content": "Formal logic is used for reasoning and knowledge representation.\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").\nDeductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\nGiven a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.\nInference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.\nFuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.\nNon-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. Other specialized versions of logic have been developed to describe many complex domains."
      },
      {
        "heading": "Probabilistic methods for uncertain reasoning",
        "level": 2,
        "content": "Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\nBayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks).\nProbabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters)."
      },
      {
        "heading": "Classifiers and statistical learning methods",
        "level": 2,
        "content": "The simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.\nThere are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.\nThe naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability.\nNeural networks are also used as classifiers."
      },
      {
        "heading": "Artificial neural networks",
        "level": 2,
        "content": "An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.\nLearning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.\nIn feedforward neural networks the signal passes in only one direction. Recurrent neural networks feed the output signal back into the input, which allows short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks. Perceptrons use only a single layer of neurons; deep learning uses multiple layers. Convolutional neural networks strengthen the connection between neurons that are \"close\" to each other—this is especially important in image processing, where a local set of neurons must identify an \"edge\" before the network can identify an object."
      },
      {
        "heading": "Deep learning",
        "level": 2,
        "content": "Deep learning uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.\nDeep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet."
      },
      {
        "heading": "GPT",
        "level": 2,
        "content": "Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pretrained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\", although this can be reduced with RLHF and quality data. They are used in chatbots, which allow people to ask a question or request a task in simple text.\nCurrent models and services include Gemini (formerly Bard), ChatGPT, Grok, Claude, Copilot, and LLaMA. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text."
      },
      {
        "heading": "Hardware and software",
        "level": 2,
        "content": "In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant.\nThe transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster, a trend sometimes called Huang's law, named after Nvidia co-founder and CEO Jensen Huang."
      },
      {
        "heading": "Applications",
        "level": 1,
        "content": "AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok). The deployment of AI may be overseen by a Chief automation officer (CAO)."
      },
      {
        "heading": "Health and medicine",
        "level": 2,
        "content": "The application of AI in medicine and medical research has the potential to increase patient care and quality of life. Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.\nFor medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication. It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold."
      },
      {
        "heading": "Games",
        "level": 2,
        "content": "Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions."
      },
      {
        "heading": "Mathematics",
        "level": 2,
        "content": "Large language models, such as GPT-4, Gemini, Claude, LLaMa or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data. One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result. The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems. In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.\nAlternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry and AlphaProof all from Google DeepMind, Llemma from EleutherAI or Julius.\nWhen natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks.\nSome models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.\nTopological deep learning integrates various topological approaches."
      },
      {
        "heading": "Finance",
        "level": 2,
        "content": "Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.\nAccording to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\""
      },
      {
        "heading": "Military",
        "level": 2,
        "content": "Various countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human operated and autonomous.\nAI has been used in military operations in Iraq, Syria, Israel and Ukraine."
      },
      {
        "heading": "Generative AI",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Agents",
        "level": 2,
        "content": "Artificial intelligent (AI) agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks."
      },
      {
        "heading": "Sexuality",
        "level": 2,
        "content": "Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer prediction, AI-integrated sex toys (e.g., teledildonics), AI-generated sexual education content, and AI agents that simulate sexual and romantic partners (e.g., Replika).  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.\nAI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors."
      },
      {
        "heading": "Other industry-specific tasks",
        "level": 2,
        "content": "There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\nAI applications for evacuation and disaster management are growing. AI has been used to investigate if and how people evacuated in large scale and small scale evacuations using historical data from GPS, videos or social media. Further, AI can provide real time information on the real time evacuation conditions.\nIn agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\nDuring the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages."
      },
      {
        "heading": "Ethics",
        "level": 1,
        "content": "AI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning."
      },
      {
        "heading": "Risks and harm",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Privacy and copyright",
        "level": 3,
        "content": "Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\nAI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.\nSensitive user data collected may include online activity records, geolocation data, video, or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.\nAI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"\nGenerative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\". Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors."
      },
      {
        "heading": "Dominance by tech giants",
        "level": 3,
        "content": "The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace."
      },
      {
        "heading": "Power needs and environmental impacts",
        "level": 3,
        "content": "In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.\nProdigious power consumption by AI is responsible for the growth of fossil fuels use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.\nA 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.\nIn 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for $650 Million (US). Nvidia CEO Jen-Hsun Huang said nuclear power is a good option for the data centers.\nIn September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at $1.6 billion (US) and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost $2 billion (US) to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon spinoff of Constellation.\nAfter the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages. Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.\nAlthough most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for generative AI. Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.\nOn 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center. \nAccording to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.\nIn 2025 a report prepared by the International Energy Agency estimated the Greenhouse gas emissions from the energy consumption of Artificial intelligence as 180 million tons. By 2035 they could rise to 300-500 million ton depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of Artificial Intelligence was estimated as 5% of the energy sector emissions, but rebound effects (for example if people will pass from public transport to autonomous cars) can reduce it."
      },
      {
        "heading": "Misinformation",
        "level": 3,
        "content": "YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took steps to mitigate the problem .\nIn 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films, or human writing. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda. AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks."
      },
      {
        "heading": "Algorithmic bias and fairness",
        "level": 3,
        "content": "Machine learning applications will be biased if they learn from biased data. The developers may not be aware that the bias exists. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases.\nOn June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called \"sample size disparity\". Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.\nCOMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.\nA program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\". Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"\nCriticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.\nBias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.\nThere are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.\nAt its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed."
      },
      {
        "heading": "Lack of transparency",
        "level": 3,
        "content": "Many AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are a large amount of non-linear relationships between inputs and outputs. But some popular explainability techniques exist.\nIt is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.\nPeople who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.\nDARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.\nSeveral approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts."
      },
      {
        "heading": "Bad actors and weaponized AI",
        "level": 3,
        "content": "Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\nA lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots.\nAI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.\nThere many other ways that AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours."
      },
      {
        "heading": "Technological unemployment",
        "level": 3,
        "content": "Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.\nIn the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.\nUnlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.\nFrom the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement."
      },
      {
        "heading": "Existential risk",
        "level": 3,
        "content": "It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways.\nFirst, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".\nSecond, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.\nThe opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.\nIn May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.\nIn 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".\nSome other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\" Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\" Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\" In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research."
      },
      {
        "heading": "Ethical machines and alignment",
        "level": 2,
        "content": "Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.\nMachines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.\nThe field of machine ethics is also called computational morality,\nand was founded at an AAAI symposium in 2005.\nOther approaches include Wendell Wallach's \"artificial moral agents\" and Stuart J. Russell's three principles for developing provably beneficial machines."
      },
      {
        "heading": "Open source",
        "level": 2,
        "content": "Active organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses."
      },
      {
        "heading": "Frameworks",
        "level": 2,
        "content": "Artificial Intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows:\n\nRespect the dignity of individual people\nConnect with other people sincerely, openly, and inclusively\nCare for the wellbeing of everyone\nProtect social values, justice, and the public interest\nOther developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles are not without criticism, especially regards to the people chosen to contribute to these frameworks.\nPromotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.\nThe UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under a MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities."
      },
      {
        "heading": "Regulation",
        "level": 2,
        "content": "The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics. In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.\nIn a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".\nIn November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI."
      },
      {
        "heading": "History",
        "level": 1,
        "content": "The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\". They developed several areas of research that would become part of AI, such as McCullouch and Pitts design for \"artificial neurons\" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.\nThe field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.\nResearchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.\nUp to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lofti Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.\nAI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.\nDeep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.\nFor many specific tasks, other methods were abandoned.\nDeep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.\n\nIn 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.\nIn the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about $50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies."
      },
      {
        "heading": "Philosophy",
        "level": 1,
        "content": "Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI."
      },
      {
        "heading": "Defining artificial intelligence",
        "level": 2,
        "content": "Alan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\" He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"\n\nRussell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".\nMcCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible.\nAnother definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\nSome authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\"."
      },
      {
        "heading": "Evaluating approaches to AI",
        "level": 2,
        "content": "No established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers."
      },
      {
        "heading": "Symbolic AI and its limits",
        "level": 3,
        "content": "Symbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"\nHowever, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.\nThe issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches."
      },
      {
        "heading": "Neat vs. scruffy",
        "level": 3,
        "content": "\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both."
      },
      {
        "heading": "Soft vs. hard computing",
        "level": 3,
        "content": "Finding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks."
      },
      {
        "heading": "Narrow vs. general AI",
        "level": 3,
        "content": "AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively."
      },
      {
        "heading": "Machine consciousness, sentience, and mind",
        "level": 2,
        "content": "The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction."
      },
      {
        "heading": "Consciousness",
        "level": 3,
        "content": "David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like."
      },
      {
        "heading": "Computationalism and functionalism",
        "level": 3,
        "content": "Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.\nPhilosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind."
      },
      {
        "heading": "AI welfare and rights",
        "level": 3,
        "content": "It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.\nIn 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own.\nProgress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited."
      },
      {
        "heading": "Future",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Superintelligence and the singularity",
        "level": 2,
        "content": "A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".\nHowever, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do."
      },
      {
        "heading": "Transhumanism",
        "level": 2,
        "content": "Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.\nEdward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence."
      },
      {
        "heading": "Decomputing",
        "level": 2,
        "content": "Arguments for decomputing have been raised by Dan McQuillan (Resisting AI: An Anti-fascist Approach to Artificial Intelligence, 2022), meaning an opposition to the sweeping application and expansion of artificial intelligence. Similar to degrowth, the approach criticizes AI as an outgrowth of the systemic issues and capitalist world we live in. It argues that a different future is possible, in which distance between people is reduced rather than increased through AI intermediaries."
      },
      {
        "heading": "In fiction",
        "level": 1,
        "content": "Thought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.\nA common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.\nIsaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence."
      },
      {
        "heading": "See also",
        "level": 1,
        "content": "Artificial consciousness – Field in cognitive science\nArtificial intelligence and elections – Use and impact of AI on political elections\nArtificial intelligence content detection – Software to detect AI-generated content\nBehavior selection algorithm – Algorithm that selects actions for intelligent agents\nBusiness process automation – Automation of business processes\nCase-based reasoning – Process of solving new problems based on the solutions of similar past problems\nComputational intelligence – Ability of a computer to learn a specific task from data or experimental observation\nDigital immortality – Hypothetical concept of storing a personality in digital form\nEmergent algorithm – Algorithm exhibiting emergent behavior\nFemale gendering of AI technologies – Gender biases in digital technologyPages displaying short descriptions of redirect targets\nGlossary of artificial intelligence – List of definitions of terms and concepts commonly used in the study of artificial intelligence\nIntelligence amplification – Use of information technology to augment human intelligence\nIntelligent agent – Software agent which acts autonomously\nMind uploading – Hypothetical process of digitally emulating a brain\nOrganoid intelligence – Use of brain cells and brain organoids for intelligent computing\nRobotic process automation – Form of business process automation technology\nThe Last Day (novel) – 1967 Welsh science fiction novel - Welsh science novel by Owain Owain\nWetware computer – Computer composed of organic material"
      },
      {
        "heading": "Explanatory notes",
        "level": 1,
        "content": ""
      },
      {
        "heading": "References",
        "level": 1,
        "content": ""
      },
      {
        "heading": "AI textbooks",
        "level": 2,
        "content": "The two most widely used textbooks in 2023 (see the Open Syllabus):\n\nRussell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0-1346-1099-3. LCCN 20190474.\nRich, Elaine; Knight, Kevin; Nair, Shivashankar B (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0-0700-8770-5.\nThe four most widely used AI textbooks in 2008:\n\nOther textbooks:\n\nErtel, Wolfgang (2017). Introduction to Artificial Intelligence (2nd ed.). Springer. ISBN 978-3-3195-8486-7.\nCiaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to generative AI (1st ed.). Intellisemantic Editions. ISBN 978-8-8947-8760-3."
      },
      {
        "heading": "History of AI",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Other sources",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Further reading",
        "level": 1,
        "content": ""
      },
      {
        "heading": "External links",
        "level": 1,
        "content": "\n\"Artificial Intelligence\". Internet Encyclopedia of Philosophy."
      }
    ],
    "summary": "Artificial intelligence (AI) refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performed by a human on an at least equal level—is among the field's long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s many billions of dollars were being invested in AI and the field experienced rapid ongoing progress in what has become known as the AI boom. The emergence of advanced generative AI in the midst of the AI boom and its ability to create and modify content exposed several unintended consequences and harms in the present and raised concerns about the risks of AI and its long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology."
  },
  {
    "title": "Inteligencia artificial",
    "source": "https://es.wikipedia.org/wiki/Inteligencia_artificial",
    "language": "es",
    "chunks": [
      {
        "heading": "Denominación",
        "level": 1,
        "content": "En 2019 la Comisión Mundial de Ética del Conocimiento Científico y la Tecnología (COMEST) de la UNESCO definió la inteligencia artificial como un campo que implica máquinas capaces de imitar determinadas funcionalidades de la inteligencia humana, incluidas características como la percepción, el aprendizaje, el razonamiento, la resolución de problemas, la interacción lingüística e incluso la producción de trabajos creativos.\nColoquialmente, la locución «inteligencia artificial» se aplica cuando una máquina imita las funciones «cognitivas» que los humanos asocian como competencias humanas; por ejemplo: «percibir», «razonar», «aprender» y «resolver problemas».[9]​ Andreas Kaplan y Michael Haenlein definen la inteligencia artificial como «la capacidad de un sistema para interpretar correctamente datos externos, y así aprender y emplear esos conocimientos para lograr tareas y metas concretas a través de la adaptación flexible».[10]​ A medida que las máquinas se vuelven cada vez más capaces, se elimina de la definición la tecnología que alguna vez se pensó que requería de inteligencia. Marvin Minsky, uno de los ideadores de la IA, hablaba del término inteligencia artificial como una palabra maleta («suitcase word») porque en él se pueden meter una diversidad de elementos.[11]​[12]​\nPor ejemplo, el reconocimiento óptico de caracteres ya no se percibe como un ejemplo de la «inteligencia artificial», habiéndose convertido en una tecnología común.[13]​ Avances tecnológicos todavía clasificados como inteligencia artificial son los sistemas de conducción autónomos o los capaces de jugar al ajedrez o Go.[14]​\nLa inteligencia artificial es una nueva forma de resolver problemas dentro de los cuales se incluyen los sistemas expertos, el manejo y control de robots y los procesadores, que intenta integrar el conocimiento en tales sistemas; en otras palabras, un sistema inteligente capaz de escribir su propio programa. Un sistema experto definido como una estructura de programación capaz de almacenar y utilizar un conocimiento sobre un área determinada que se traduce en su capacidad de aprendizaje.[15]​ De igual manera, se puede considerar a la IA como la capacidad de las máquinas para usar algoritmos, aprender de los datos y utilizar lo aprendido en la toma de decisiones tal y como lo haría un ser humano.[16]​\nSegún Takeyas (2007), la IA es una rama de las ciencias computacionales encargada de estudiar modelos de cómputo capaces de realizar actividades propias de los seres humanos con base en dos de sus características primordiales: el razonamiento y la conducta.[17]​\nEn 1956, John McCarthy acuñó la expresión «inteligencia artificial», y la definió como «la ciencia e ingenio de hacer máquinas inteligentes, especialmente programas de cómputo inteligentes».[18]​\nGrau-Luque contrasta diferentes definiciones desde diversas fuentes y autores, destacando que difieren dependiendo de «en qué campo específico se usen».[19]​Esto lleva al autor a definir «inteligencia artificial» como «sistemas que llevan a cabo tareas consideradas inteligentes», para luego asociar conceptos como «aprendizaje» y «razonamiento» con el aprendizaje automático como una subdisciplina de la inteligencia artificial.\nTambién existen distintos tipos de percepciones y acciones, que pueden ser obtenidas y producidas, respectivamente, por sensores físicos y sensores mecánicos en máquinas, pulsos eléctricos u ópticos en computadoras, tanto como por entradas y salidas de bits de un software y su entorno hardware.\nVarios ejemplos se encuentran en el área de control de sistemas, planificación automática, la capacidad de responder a diagnósticos y a consultas de los consumidores, reconocimiento de escritura, reconocimiento del habla y reconocimiento de patrones. Los sistemas de IA actualmente son parte de la rutina en campos como economía, medicina, ingeniería, el transporte, las comunicaciones y la milicia, y se ha usado en gran variedad de programas informáticos, juegos de estrategia, como ajedrez de computador, y otros videojuegos.\nAsimismo la inteligencia artificial se está desarrollando en la plataforma digital cada vez más, evolucionando y creando nuevas herramientas, como la plataforma laboral que existe desde el año 2023 llamada SIVIUM, una herramienta por la cual una persona postula en forma automatizada a todas las ofertas laborales de todos los portales de trabajo, sin necesidad de estar revisando cada oferta laboral que se presente y enviar su CV uno por uno.[20]​"
      },
      {
        "heading": "Tipos",
        "level": 1,
        "content": "Stuart J. Russell y Peter Norvig diferencian varios tipos de inteligencia artificial:[21]​\n\nLos sistemas que piensan como humanos: Estos sistemas tratan de emular el pensamiento humano; por ejemplo, las redes neuronales artificiales. La automatización de actividades que vinculamos con procesos de pensamiento humano, actividades como la toma de decisiones, resolución de problemas y aprendizaje.[22]​\nLos sistemas que actúan como humanos: Estos sistemas tratan de actuar como humanos; es decir, imitan el comportamiento humano; por ejemplo, la robótica (el estudio de cómo lograr que los computadores realicen tareas que, por el momento, los humanos hacen mejor).[23]​\nLos sistemas que piensan racionalmente: Es decir, con lógica (idealmente), tratan de imitar el pensamiento racional del ser humano; por ejemplo, los sistemas expertos, (el estudio de los cálculos que hacen posible percibir, razonar y actuar).[24]​\nLos sistemas que actúan racionalmente: Tratan de emular de forma racional el comportamiento humano; por ejemplo, los agentes inteligentes, que están relacionados con conductas inteligentes en artefactos.[25]​"
      },
      {
        "heading": "Inteligencia artificial generativa",
        "level": 2,
        "content": "La inteligencia artificial generativa es un tipo de sistema de inteligencia artificial capaz de generar texto, imágenes u otros medios en respuesta a comandos de texto conocidos como \"prompts\".[26]​ Los modelos de IA generativa aprenden los patrones y la estructura de sus datos de entrenamiento de entrada, y luego generan nuevos datos que tienen características similares.\nLos sistemas de IA generativa notables incluyen ChatGPT (y su variante Microsoft Copilot), un bot conversacional creado por OpenAI usando sus modelos de lenguaje grande fundacionales GPT-3 y GPT-4; Gemini (anteriormente llamado Bard), un bot conversacional creado por Google usando el modelo de lenguaje Gemini; y Claude, un bot conversacional creado por Anthropic usando los modelos del mismo nombre. [27]​Otros modelos generativos de IA incluyen sistemas de arte de inteligencia artificial como Stable Diffusion, Midjourney y DALL-E, que permiten crear imágenes. Actualmente la IA generativa puede crear texto, código, imágenes, vídeo, música, voces y efectos de sonido."
      },
      {
        "heading": "Inteligencia artificial fuerte",
        "level": 3,
        "content": "La Inteligencia artificial fuerte (IGA) es un tipo hipotético de inteligencia artificial que iguala o excede la inteligencia humana promedio. Si se hiciera realidad, una IGA podría aprender a realizar cualquier tarea intelectual que los seres humanos o los animales puedan llevar a cabo. Alternativamente, la IGA se ha definido como un sistema autónomo que supera las capacidades humanas en la mayoría de las tareas económicamente valiosas.\nAlgunos sostienen que podría ser posible en años o décadas; otros, que podría tardar un siglo o más; y una minoría cree que quizá nunca se consiga. Existe un debate sobre la definición exacta de IGA y sobre si los grandes modelos de lenguaje (LLM) modernos, como el GPT-4, son formas tempranas pero incompletas de IGA."
      },
      {
        "heading": "Inteligencia artificial explicable",
        "level": 3,
        "content": "La inteligencia artificial explicable se refiere a métodos y técnicas en la aplicación de tecnología de inteligencia artificial por los que el ser humano es capaz de comprender las decisiones y predicciones realizadas por la inteligencia artificial."
      },
      {
        "heading": "Inteligencia artificial amigable",
        "level": 3,
        "content": "La inteligencia artificial amigable es una IA fuerte e hipotética que puede tener un efecto positivo más que uno negativo sobre la humanidad. 'Amigable' es usado en este contexto como terminología técnica y escoge agentes que son seguros y útiles, no necesariamente aquellos que son «amigables» en el sentido coloquial. El concepto es invocado principalmente en el contexto de discusiones de agentes artificiales de automejora recursiva que rápidamente explota en inteligencia, con el argumento de que esta tecnología hipotética pudiera tener una larga, rápida y difícil tarea de controlar el impacto en la sociedad humana."
      },
      {
        "heading": "Inteligencia artificial multimodal",
        "level": 3,
        "content": "La inteligencia artificial multimodal es un tipo de inteligencia artificial que puede procesar e integrar datos de diferentes modalidades, como texto, imágenes, audio y video, para obtener una comprensión más completa y contextualizada de una situación. La inteligencia artificial multimodal se inspira en la forma en que los humanos usan varios sentidos para percibir e interactuar con el mundo, y ofrece una forma más natural e intuitiva de comunicarse con la tecnología."
      },
      {
        "heading": "Inteligencia artificial cuántica",
        "level": 3,
        "content": "La inteligencia artificial Cuántica es un campo interdisciplinar que se enfoca en construir algoritmos cuánticos para mejorar las tareas computacionales dentro de la IA, incluyendo subcampos como el aprendizaje automático. Existen evidencias que muestran una posible ventaja cuadrática cuántica en operaciones fundamentales de la IA."
      },
      {
        "heading": "Escuelas de pensamiento",
        "level": 1,
        "content": "La IA se divide en dos escuelas de pensamiento:\n\nLa inteligencia artificial convencional.\nLa inteligencia computacional."
      },
      {
        "heading": "Inteligencia artificial convencional",
        "level": 2,
        "content": "Se conoce también como IA simbólica-deductiva. Está basada en el análisis formal y estadístico del comportamiento humano ante diferentes problemas:\n\nRazonamiento basado en casos: Ayuda a tomar decisiones mientras se resuelven ciertos problemas concretos y, aparte de que son muy importantes, requieren de un buen funcionamiento.\nSistemas expertos: Infieren una solución a través del conocimiento previo del contexto en que se aplica y utiliza ciertas reglas o relaciones.[28]​\nRedes bayesianas: Propone soluciones mediante inferencia probabilística.[29]​\nInteligencia artificial basada en comportamientos: Esta inteligencia contiene autonomía, es decir, puede autorregularse y controlarse para mejorar.\nSmart process management: Facilita la toma de decisiones complejas, proponiendo una solución a un determinado problema al igual que lo haría un especialista en dicha actividad."
      },
      {
        "heading": "Inteligencia artificial computacional",
        "level": 2,
        "content": "La inteligencia computacional (también conocida como IA subsimbólica-inductiva) implica desarrollo o aprendizaje interactivo (por ejemplo, modificaciones interactivas de los parámetros en sistemas de conexiones). El aprendizaje se realiza basándose en datos empíricos.\nLa inteligencia computacional tiene una doble finalidad. Por un lado, su objetivo científico es comprender los principios que posibilitan el comportamiento inteligente (ya sea en sistemas naturales o artificiales) y, por otro, su objetivo tecnológico consiste en especificar los métodos para diseñar sistemas inteligentes.[30]​"
      },
      {
        "heading": "Historia",
        "level": 1,
        "content": "La expresión «inteligencia artificial» fue acuñada formalmente en 1956 durante la Conferencia de Dartmouth, pero para entonces ya se había estado trabajando en ello durante cinco años en los cuales se había propuesto muchas definiciones distintas que en ningún caso habían logrado ser aceptadas totalmente por la comunidad investigadora. La IA es una de las disciplinas más recientes junto con la genética moderna.\nLas ideas más básicas se remontan a los antiguos griegos. Aristóteles (384-322 a. C.) fue el primero en describir un conjunto de reglas que describen una parte del funcionamiento de la mente para obtener conclusiones racionales, y Ctesibio de Alejandría (250 a. C.) construyó la primera máquina autocontrolada, un regulador del flujo de agua (racional pero sin razonamiento).\nEn 1315 Ramon Llull en su libro Ars magna tuvo la idea de que el razonamiento podía ser efectuado de manera artificial.\nEn 1840 Ada Lovelace previó la capacidad de las máquinas para ir más allá de los simples cálculos y aportó una primera idea de lo que sería el software.\nEn 1912 Leonardo Torres Quevedo, desarrolló un autómata capaz de jugar al ajedrez (el ajedrecista).\nEn 1936 Alan Turing diseña formalmente una Máquina universal que demuestra la viabilidad de un dispositivo físico para implementar cualquier cómputo formalmente definido.\nEn 1943 Warren McCulloch y Walter Pitts presentaron su modelo de neuronas artificiales, el cual se considera el primer trabajo del campo, aun cuando todavía no existía el término. Los primeros avances importantes comenzaron a principios del año 1950 con el trabajo de Alan Turing, a partir de lo cual la ciencia ha pasado por diversas situaciones.\nEn 1955 Herbert Simon, Allen Newell y Joseph Carl Shaw, desarrollan el primer lenguaje de programación orientado a la resolución de problemas, el IPL-11. Un año más tarde desarrollan el LogicTheorist, el cual era capaz de demostrar teoremas matemáticos.\nEn 1956 fue ideada la expresión «inteligencia artificial» por John McCarthy, Marvin Minsky y Claude Shannon en la Conferencia de Dartmouth, un congreso en el que se hicieron previsiones triunfalistas a diez años que jamás se cumplieron, lo que provocó el abandono casi total de las investigaciones durante quince años.\nEn 1957 Newell y Simon continúan su trabajo con el desarrollo del General Problem Solver (GPS). GPS era un sistema orientado a la resolución de problemas.\nEn 1958 John McCarthy desarrolla en el Instituto Tecnológico de Massachusetts (MIT) el LISP. Su nombre se deriva de LISt Processor. LISP fue el primer lenguaje para procesamiento simbólico.\nEn 1959 Rosenblatt introduce el «perceptrón».\nA finales de la década de 1950 y comienzos de la de 1960 Robert K. Lindsay desarrolla «Sad Sam», un programa para la lectura de oraciones en inglés y la inferencia de conclusiones a partir de su interpretación.\nEn 1963 Quillian desarrolla las redes semánticas como modelo de representación del conocimiento.\nEn 1964 Bertrand Raphael construye el sistema SIR (Semantic Information Retrieval) el cual era capaz de inferir conocimiento basado en información que se le suministra. Bobrow desarrolla STUDENT.\nA mediados de los años 60, aparecen los sistemas expertos, que predicen la probabilidad de una solución bajo un set de condiciones. Por ejemplo, DENDRAL, iniciado en 1965 por Buchanan, Feigenbaum y Lederberg, el primer Sistema Experto, que asistía a químicos en estructuras químicas complejas, MACSYMA, que asistía a ingenieros y científicos en la solución de ecuaciones matemáticas complejas.\nPosteriormente entre los años 1968-1970 Terry Winograd desarrolló el sistema SHRDLU, que permitía interrogar y dar órdenes a un robot que se movía dentro de un mundo de bloques.\nEn 1666 y 1972 Artificial Intelligence Center desarrollo el Robot Shakey, considerado el primer robot inteligente de la historia. El robot incluía visión artificial, y tenía la capacidad de percibir y razonar sobre su entorno.\nEn 1968 Marvin Minsky publica Semantic Information Processing.\nEn 1968 Seymour Papert, Danny Bobrow y Wally Feurzeig desarrollan el lenguaje de programación LOGO.\nEn 1969 Alan Kay desarrolla el lenguaje Smalltalk en Xerox PARC y se publica en 1980.\nEn 1973 Alain Colmenauer y su equipo de investigación en la Universidad de Aix-Marseille crean PROLOG (del francés PROgrammation en LOGique) un lenguaje de programación ampliamente utilizado en IA.\nEn 1973 Shank y Abelson desarrollan los guiones, o scripts, pilares de muchas técnicas actuales en inteligencia artificial y la informática en general.\nEn 1974 Edward Shortliffe escribe su tesis con MYCIN, uno de los Sistemas Expertos más conocidos, que asistió a médicos en el diagnóstico y tratamiento de infecciones en la sangre.\nEn las décadas de 1970 y 1980, creció el uso de sistemas expertos, como MYCIN: R1/XCON, ABRL, PIP, PUFF, CASNET, INTERNIST/CADUCEUS, etc. Algunos permanecen hasta hoy (Shells) como EMYCIN, EXPERT, OPSS.\nEn 1981 Kazuhiro Fuchi anuncia el proyecto japonés de la quinta generación de computadoras.\nEn 1986 McClelland y Rumelhart publican Parallel Distributed Processing (Redes Neuronales).\nEn 1987 Hitachi desarrollo el Sendai Subway 1000, el primer tren autónomo de la historia. Fue el primer de tren del mundo en utilizar lógica difusa para controlar su velocidad, arranques y paradas.\nEn 1988 se establecen los lenguajes Orientados a Objetos.\nEn 1997 Gari Kaspárov, campeón mundial de ajedrez, pierde ante la computadora autónoma Deep Blue.\nEn 2006 se celebró el aniversario con el Congreso en español 50 años de inteligencia artificial - Campus Multidisciplinar en Percepción e Inteligencia 2006.\nEn 2009 ya había en desarrollo sistemas inteligentes terapéuticos que permiten detectar emociones para poder interactuar con niños autistas.\nEn 2011 IBM desarrolló un superordenador llamado Watson, el cual ganó una ronda de tres juegos seguidos de Jeopardy!, venciendo a sus dos máximos campeones, y ganando un premio de 1 millón de dólares que IBM luego donó a obras de caridad.[31]​\nEn 2016, un programa informático ganó cinco a cero al triple campeón de Europa de Go.[32]​\nEn 2016, el entonces presidente Obama habla sobre el futuro de la inteligencia artificial y la tecnología.[33]​\nExisten personas que al dialogar sin saberlo con un chatbot no se percatan de hablar con un programa, de modo tal que se cumple la prueba de Turing como cuando se formuló: «Existirá inteligencia artificial cuando no seamos capaces de distinguir entre un ser humano y un programa informático en una conversación a ciegas».\nEn 2017 AlphaGo desarrollado por DeepMind derrota 4-1 en una competencia de Go al campeón mundial Lee Sedol. Este suceso fue muy mediático y marcó un hito en la historia de este juego.[34]​ A finales de ese mismo año, Stockfish, el motor de ajedrez considerado el mejor del mundo con 3 400 puntos ELO, fue abrumadoramente derrotado por AlphaZero con solo conocer las reglas del juego y tras solo 4 horas de entrenamiento jugando contra sí mismo.[35]​\nComo anécdota, muchos de los investigadores sobre IA sostienen que «la inteligencia es un programa capaz de ser ejecutado independientemente de la máquina que lo ejecute, computador o cerebro».\nEn 2017 un grupo de ingenieros en Google inventan la arquitectura de transformador, un modelo de deep learning que alumbró una nueva generación de modelos grandes de lenguaje, empezando por BERT, y luego el revolucionario GPT de OpenAI.[36]​\nEn 2018, se lanza el primer televisor con inteligencia artificial por parte de LG Electronics con una plataforma denominada ThinQ.[37]​\nEn 2019, Google presentó su Doodle en que, con ayuda de la inteligencia artificial, hace un homenaje a Johann Sebastian Bach, en el que, añadiendo una simple melodía de dos compases la IA crea el resto.\nEn 2020, la OECD (Organización para la Cooperación y el Desarrollo Económico) publica el documento de trabajo intitulado Hola, mundo: La inteligencia artificial y su uso en el sector público, dirigido a funcionarios de gobierno con el afán de resaltar la importancia de la IA y de sus aplicaciones prácticas en el ámbito gubernamental.[38]​\nAl final del año 2022, se lanzó ChatGPT, una inteligencia artificial generativa capaz de escribir textos y responder preguntas en muchos idiomas. Dado que la calidad de las respuestas recordaba inicialmente al nivel humano, se generó un entusiasmo mundial por la IA[39]​ y ChatGPT alcanzó más de 100 millones de usuarios dos meses después de su lanzamiento.[40]​ Más tarde, los expertos notaron que ChatGPT proporciona información errónea en áreas donde no tiene conocimiento («alucinaciones de datos»), la cual a primera vista parece creíble debido a su perfecta redacción.[41]​\nEn 2023, las fotos generadas por IA alcanzaron un nivel de realismo que las hacía confundirse con fotos reales. Como resultado, hubo una ola de «fotos» generadas por IA que muchos espectadores creyeron que eran reales. Una imagen generada por Midjourney se destacó, mostrando al papa Francisco con un elegante abrigo blanco de invierno.[42]​"
      },
      {
        "heading": "Tendencias",
        "level": 1,
        "content": "En 2024, se esperan avances significativos en varias áreas de la IA:\n\nAprendizaje automático y profundo: Se espera que estas técnicas permitan a las máquinas aprender de manera más eficiente y precisa de grandes volúmenes de datos, mejorando capacidades en procesamiento del lenguaje natural, visión por computadora y toma de decisiones automatizada.[2]​[3]​\nProcesamiento del Lenguaje Natural (PLN): Los avances en PLN permitirán a las máquinas comprender y responder al lenguaje humano de manera más natural y precisa, abriendo nuevas posibilidades en atención al cliente automatizada y generación de contenido.[1]​[5]​\nAnalítica predictiva y prescriptiva: Estas técnicas utilizarán datos y modelos matemáticos para prever el futuro y recomendar acciones, permitiendo a las organizaciones anticipar y abordar problemas de manera proactiva.[1]​\nIntegración del Internet de las cosas (IoT) y la IA: Permitirá a las máquinas recopilar y analizar datos en tiempo real para tomar decisiones autónomas y mejorar la eficiencia.[1]​\nIA Generativa: Estará más al alcance de las personas sin conocimientos técnicos, permitiendo la creación de chatbots personalizados y otros modelos generativos.[43]​"
      },
      {
        "heading": "Implicaciones sociales, éticas y filosóficas",
        "level": 1,
        "content": "Ante la posibilidad de crear máquinas dotadas de inteligencia, se volvió importante preocuparse por la cuestión ética de las máquinas para tratar de garantizar que no se produzca ningún daño a los seres humanos, a otros seres vivos e incluso a las mismas máquinas según algunas corrientes de pensamiento.[44]​ Es así como surgió un amplio campo de estudios conocido como ética de la inteligencia artificial de relativamente reciente aparición y que generalmente se divide en dos ramas, la roboética, encargada de estudiar las acciones de los seres humanos hacia los robots, y la ética de las máquinas encargada del estudio del comportamiento de los robots para con los seres humanos.\nEl acelerado desarrollo tecnológico y científico de la inteligencia artificial que se ha producido en el siglo XXI supone también un importante impacto en otros campos. En la economía mundial durante la segunda revolución industrial se vivió un fenómeno conocido como desempleo tecnológico, que se refiere a cuando la automatización industrial de los procesos de producción a gran escala reemplaza la mano de obra humana. Con la inteligencia artificial podría darse un fenómeno parecido, especialmente en los procesos en los que interviene la inteligencia humana, tal como se ilustraba en el cuento ¡Cómo se divertían! de Isaac Asimov, en el que su autor vislumbra algunos de los efectos que tendría la interacción de máquinas inteligentes especializadas en pedagogía infantil, en lugar de profesores humanos, con los niños en etapa escolar. Este mismo escritor diseñó lo que hoy se conocen como las tres leyes de la robótica, aparecidas por primera vez en el relato Círculo vicioso (Runaround) de 1942, donde establecía lo siguiente:\n\nPrimera Ley\n\nUn robot no hará daño a un ser humano ni, permitirá que un ser humano sufra daño.\nSegunda Ley\n\nUn robot debe cumplir las órdenes dadas por los seres humanos, a excepción de aquellas que entren en conflicto con la primera ley.\nTercera Ley\n\nUn robot debe proteger su propia existencia en la medida en que esta protección no entre en conflicto con la primera o con la segunda ley.[45]​\nOtras obras de ciencia ficción más recientes también exploran algunas cuestiones éticas y filosóficas con respecto a la Inteligencia artificial fuerte, como las películas Yo, robot o A.I. Inteligencia Artificial, en los que se tratan temas tales como la autoconsciencia o el origen de una conciencia emergente de los robots inteligentes o sistemas computacionales, o si éstos podrían considerarse sujetos de derecho debido a sus características casi humanas relacionadas con la sintiencia, como el poder ser capaces de sentir dolor y emociones o hasta qué punto obedecerían al objetivo de su programación, y en caso de no ser así, si podrían ejercer libre albedrío. Esto último es el tema central de la famosa saga de Terminator, en la que las máquinas superan a la humanidad y deciden aniquilarla, historia que, según varios especialistas, podría no limitarse a la ciencia ficción y ser una posibilidad real en una sociedad posthumana que dependiese de la tecnología y las máquinas completamente.[46]​[47]​"
      },
      {
        "heading": "Regulación",
        "level": 1,
        "content": "El Derecho[49]​ desempeña un papel fundamental en el uso y desarrollo de la IA. Las leyes establecen reglas y normas de comportamiento para asegurar el bienestar social y proteger los derechos individuales, y pueden ayudarnos a obtener los beneficios de esta tecnología mientras minimizamos sus riesgos, que son significativos. De momento no hay normas jurídicas que regulen directamente a la IA. Pero con fecha 21 de abril de 2021, la Comisión Europea ha presentado una propuesta de Reglamento europeo para la regulación armonizada de la inteligencia artificial (IA) en la UE. Su título exacto es Propuesta de Reglamento del Parlamento Europeo y del Consejo por el que se establecen normas armonizadas en materia de inteligencia artificial –Ley de Inteligencia Artificial– y se modifican otros actos legislativos de la Unión.\nEn marzo de 2023, cientos de empresarios como Elon Musk, Steve Wozniak (cofundador de Apple) o los presidentes de numerosas compañías tecnológicas; intelectuales como Yuval Noah Harari y cientos de académicos e investigadores especializados en inteligencia artificial firmaron una carta abierta avisando del peligro de la falta de regulación de la IA, poniendo el foco sobre OpenAI, la empresa que ha desarrollado ChatGPT. Pidieron una pausa de al menos 6 meses para sus experimentos más potentes, hasta que el mundo logre un consenso internacional para que estos sistemas «sean más precisos, seguros, interpretables, transparentes, robustos, neutrales, confiables y leales».[50]​\nDos meses más tarde, en mayo, 350 ejecutivos de las principales empresas desarrolladoras de IA, académicos e investigadores expertos firmaron un nuevo manifiesto alertando de que la IA avanzada sin regular representa un peligro de extinción para la humanidad: «Mitigar el riesgo de extinción de la IA debería ser una prioridad mundial junto a otros riesgos a escala social como las pandemias y la guerra nuclear»[51]​ Entre los impulsores de esta petición está toda la plana mayor de OpenAI,[52]​ el jefe de Tecnología de Microsoft, el líder de Google DeepMind con 38 ejecutivos, investigadores o profesores de universidad relacionados con la empresa, y representantes de desarrolladoras más pequeñas como Anthropic, Stability AI o Inflection AI.[53]​"
      },
      {
        "heading": "Objetivos",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Razonamiento y resolución de problemas",
        "level": 2,
        "content": "Los primeros investigadores desarrollaron algoritmos que imitaban el razonamiento paso a paso que los humanos usan cuando resuelven acertijos o hacen deducciones lógicas.[54]​ A finales de la década de 1981-1990, la investigación de la inteligencia artificial había desarrollado métodos para tratar con información incierta o incompleta, empleando conceptos de probabilidad y economía.[55]​\nEstos algoritmos demostraron ser insuficientes para resolver grandes problemas de razonamiento porque experimentaron una «explosión combinatoria»: se volvieron exponencialmente más lentos a medida que los problemas crecían.[56]​ De esta manera, se concluyó que los seres humanos rara vez usan la deducción paso a paso que la investigación temprana de la inteligencia artificial seguía; en cambio, resuelven la mayoría de sus problemas utilizando juicios rápidos e intuitivos.[57]​"
      },
      {
        "heading": "Representación del conocimiento",
        "level": 2,
        "content": "La representación del conocimiento[58]​ y la ingeniería del conocimiento[59]​ son fundamentales para la investigación clásica de la inteligencia artificial. Algunos «sistemas expertos» intentan recopilar el conocimiento que poseen los expertos en algún ámbito concreto. Además, otros proyectos tratan de reunir el «conocimiento de sentido común» conocido por una persona promedio en una base de datos que contiene un amplio conocimiento sobre el mundo.\nEntre los temas que contendría una base de conocimiento de sentido común están: objetos, propiedades, categorías y relaciones entre objetos,[60]​ situaciones, eventos, estados y tiempo[61]​ causas y efectos;[62]​ y el conocimiento sobre el conocimiento (lo que sabemos sobre lo que saben otras personas)[63]​ entre otros."
      },
      {
        "heading": "Planificación",
        "level": 2,
        "content": "Otro objetivo de la inteligencia artificial consiste en poder establecer metas y finalmente alcanzarlas.[64]​ Para ello necesitan una forma de visualizar el futuro, una representación del estado del mundo y poder hacer predicciones sobre cómo sus acciones lo cambiarán, con tal de poder tomar decisiones que maximicen la utilidad (o el «valor») de las opciones disponibles.[65]​\nEn los problemas clásicos de planificación, el agente puede asumir que es el único sistema que actúa en el mundo, lo que le permite estar seguro de las consecuencias de sus acciones.[66]​ Sin embargo, si el agente no es el único actor, entonces se requiere que este pueda razonar bajo incertidumbre. Esto requiere un agente que no solo pueda evaluar su entorno y hacer predicciones, sino también evaluar sus predicciones y adaptarse en función de su evaluación.[67]​ La planificación de múltiples agentes utiliza la cooperación y la competencia de muchos sistemas para lograr un objetivo determinado. El comportamiento emergente como este es utilizado por algoritmos evolutivos e inteligencia de enjambre.[68]​"
      },
      {
        "heading": "Aprendizaje",
        "level": 2,
        "content": "El aprendizaje automático es un concepto fundamental de la investigación de la inteligencia artificial desde el inicio de los estudios de este campo; consiste en la investigación de algoritmos informáticos que mejoran automáticamente a través de la experiencia.[69]​\nEl aprendizaje no supervisado es la capacidad de encontrar patrones en un flujo de entrada, sin que sea necesario que un humano etiquete las entradas primero. El aprendizaje supervisado incluye clasificación y regresión numérica, lo que requiere que un humano etiquete primero los datos de entrada. La clasificación se usa para determinar a qué categoría pertenece algo y ocurre después de que un programa observe varios ejemplos de entradas de varias categorías. La regresión es el intento de producir una función que describa la relación entre entradas y salidas y predice cómo deben cambiar las salidas a medida que cambian las entradas.[69]​ Tanto los clasificadores como los aprendices de regresión intentan aprender una función desconocida; por ejemplo, un clasificador de spam puede verse como el aprendizaje de una función que asigna el texto de un correo electrónico a una de dos categorías, «spam» o «no spam». La teoría del aprendizaje computacional puede evaluar a los estudiantes por complejidad computacional, complejidad de la muestra (cuántos datos se requieren) o por otras nociones de optimización.[70]​\nEl mundo está en constante evolución, y herramientas como ChatGPT están en el centro de esta transformación. Mientras que muchas personas ven a ChatGPT como una oportunidad para mejorar la experiencia de sus negocios o personales, hay quienes se muestran escépticos sobre su implementación. [71]​"
      },
      {
        "heading": "Procesamiento de lenguajes naturales",
        "level": 2,
        "content": "El procesamiento del lenguaje natural[72]​ permite a las máquinas leer y comprender el lenguaje humano. Un sistema de procesamiento de lenguaje natural suficientemente eficaz permitiría interfaces de usuario de lenguaje natural y la adquisición de conocimiento directamente de fuentes escritas por humanos, como los textos de noticias. Algunas aplicaciones sencillas del procesamiento del lenguaje natural incluyen la recuperación de información, la minería de textos, la respuesta a preguntas y la traducción automática.[73]​ Muchos enfoques utilizan las frecuencias de palabras para construir representaciones sintácticas de texto. Las estrategias de búsqueda de «detección de palabras clave» son populares y escalables, pero poco óptimas; una consulta de búsqueda para «perro» solo puede coincidir con documentos que contengan la palabra literal «perro» y perder un documento con el vocablo «caniche». Los enfoques estadísticos de procesamiento de lenguaje pueden combinar todas estas estrategias, así como otras, y a menudo logran una precisión aceptable a nivel de página o párrafo. Más allá del procesamiento de la semántica, el objetivo final de este es incorporar una comprensión completa del razonamiento de sentido común.[74]​ En 2019, las arquitecturas de aprendizaje profundo basadas en transformadores podían generar texto coherente.[75]​"
      },
      {
        "heading": "Percepción",
        "level": 2,
        "content": "La percepción de la máquina[76]​ es la capacidad de utilizar la entrada de sensores (como cámaras de espectro visible o infrarrojo, micrófonos, señales inalámbricas y lidar, sonar, radar y sensores táctiles) para entender aspectos del mundo. Las aplicaciones incluyen reconocimiento de voz,[77]​ reconocimiento facial y reconocimiento de objetos.[78]​ La visión artificial es la capacidad de analizar la información visual, que suele ser ambigua; un peatón gigante de cincuenta metros de altura muy lejos puede producir los mismos píxeles que un peatón de tamaño normal cercano, lo que requiere que la inteligencia artificial juzgue la probabilidad relativa y la razonabilidad de las diferentes interpretaciones, por ejemplo, utilizando su «modelo de objeto» para evaluar que los peatones de cincuenta metros no existen.[79]​"
      },
      {
        "heading": "Importancia de la inteligencia artificial",
        "level": 1,
        "content": "La gran importancia de la IA radica en el hecho de que tiene una amplia gama de aplicaciones, desde la automatización de tareas tediosas hasta la creación de sistemas avanzados de asistencia médica y diagnóstico de enfermedades, la detección de fraudes y la optimización de procesos empresariales.[80]​ En muchos casos, la IA puede hacer cosas que los humanos no pueden hacer, como el procesamiento de datos en grandes cantidades y la localización de patrones e interrelaciones entre estos que serían difíciles o imposibles de detectar de otra manera.\nEsta herramienta ayuda a automatizar el aprendizaje y descubrimiento repetitivo a través de datos, realiza tareas computarizadas frecuentes de manera confiable, sin embargo, necesita intervención humana para la configuración del sistema. Analiza datos más profundos y agrega inteligencia ya que no se puede vender como una aplicación individual, por lo que es un valor agregado a los productos. Tiene una gran precisión a través de redes neuronales profundas; por ejemplo, en medicina se puede utilizar la IA para detectar cáncer con MRIs (imágenes ppr resonancia magnética). Se adapta a través de algoritmos de aprendizaje progresivo, encuentra estructura y regularidades en los datos de modo que el algoritmo se convierte en un clasificador o predictor. Y, por último, la inteligencia artificial, saca el mayor provecho de datos.\nAdemás, una de las principales razones por las que la IA es importante es porque puede automatizar tareas repetitivas y monótonas, liberando tiempo y recursos para que las personas se centren en tareas más creativas y valiosas. Por ejemplo, la IA puede ayudar a las empresas a automatizar tareas de back office, como la contabilidad y el procesamiento de facturas, lo que puede reducir los costos y mejorar la eficiencia. De manera similar, la IA puede ayudar a los trabajadores a realizar tareas más complejas y creativas, como el diseño y la planificación estratégica.\nOtra razón por la que la IA es importante es porque puede ayudar a las empresas a tomar decisiones informadas y precisas. Así mismo, la IA puede procesar grandes cantidades de datos y proporcionar información valiosa para la toma de decisiones empresariales, lo que puede ayudar a las empresas a identificar oportunidades comerciales, predecir tendencias de mercado y mejorar la eficiencia del mercado financiero. Además, la IA puede ayudar a los trabajadores a tomar decisiones informadas en tiempo real, como en el caso de la atención médica, donde la IA puede ayudar a los médicos a identificar enfermedades y personalizar el tratamiento.\nLa IA también es importante en el campo de la ciberseguridad. La IA puede ayudar a detectar y prevenir amenazas, desde ciberataques hasta la detección de comportamientos sospechosos. La IA puede analizar grandes cantidades de datos en tiempo real y detectar patrones y anomalías que podrían indicar una amenaza de seguridad. Además, la IA puede aprender de los patrones de comportamiento y mejorar su capacidad para detectar amenazas en el futuro.[81]​ En el campo de la seguridad cibernética, la IA puede ayudar a proteger los sistemas y las redes de los ataques de virus informáticos y la infiltración de malware.\nOtra área donde la IA es importante es en el descubrimiento de conocimientos. La IA puede descubrir patrones y relaciones en los datos que los humanos no podrían detectar, lo que puede llevar a nuevas ideas y avances en diversos campos. Por ejemplo, la IA puede ayudar a los investigadores a identificar nuevos tratamientos para enfermedades, o ayudar a los científicos a analizar datos de sensores y satélites para entender mejor el calentamiento global."
      },
      {
        "heading": "Controversias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Sophia",
        "level": 2,
        "content": "En marzo de 2016, se hizo popular el comentario que la robot humanoide llamada Sophia de la empresa Hanson Robotics hizo durante su presentación cuando su creador, David Hanson, le preguntara si estaba dispuesta a destruir a la humanidad, a lo que la robot contestó: «Está bien, voy a destruir a la humanidad». Posteriormente, Sophía se ganó el reconocimiento y la atención mediática mundial debido a sus conductas casi humanas, siendo entrevistada en muchas ocasiones por distintos medios y sosteniendo conversaciones con personalidades famosas y reconocidas. En 2017, Sophia obtuvo la ciudadanía saudí, convirtiéndose así en la primera robot en ser reconocida como ciudadana por un país, lo cual levantó la controversia sobre si se les debería otorgar los mismos derechos y obligaciones a los robots como si se trataran de sujetos de derecho.[82]​"
      },
      {
        "heading": "Alice y Bob",
        "level": 2,
        "content": "A finales de julio de 2017, varios medios internacionales dieron a conocer que el laboratorio de investigación de inteligencia artificial del Instituto Tecnológico de Georgia, en conjunto con el Grupo de Investigación de inteligencia artificial (FAIR) de Facebook, ahora Meta, tuvieron que apagar dos inteligencias artificiales de tipo chatbot denominadas Bob y Alice, ya que habían desarrollado un lenguaje propio más eficiente que el inglés, idioma en el que habían sido entrenados para aprender a negociar, desarrollando finalmente un tipo de comunicación incomprensible que se alejaba de las reglas gramaticales del lenguaje natural y que favorecía el uso de abreviaturas. El lenguaje creado por estas IA mostraba características de un inglés corrupto y patrones repetitivos, en especial de pronombres y determinantes.[83]​\nEste inesperado suceso fue visto con pánico en los medios de comunicación, ya que se aseguraba que los chatbots supuestamente habían salido del control humano y habían desarrollado la capacidad de comunicarse entre sí. Sin embargo, posteriormente esto también fue desmentido, pues se argumentó que en realidad Facebook no apagó las inteligencias artificiales, sino que simplemente las puso en pausa y cambió los parámetros de los chatbots, desechando el experimento al final por no tener ningún interés práctico o útil dentro de la investigación sobre IA.[84]​"
      },
      {
        "heading": "Ameca",
        "level": 2,
        "content": "A principios del 2022, en la Feria de Electrónica de Consumo (CES) que tomó lugar en Las Vegas, el robot desarrollado por Engineered Arts nombrado Ameca causó duda y miedo a los espectadores durante su exposición principalmente por la semejanza de su rostro a uno de un ser humano, la compañía expresó que el desarrollo de este robot humanoide aún se encontraba en proceso y hasta septiembre del mismo año el robot aún no era capaz de caminar ni tener interacción alguna con las personas.[85]​ Por otro lado, en septiembre de 2023 la compañía volvió a exponer a Ameca al público mostrando al robot en videos en donde se le puede ver frente a un espejo haciendo 25 expresiones humanas,[86]​ así como dibujando un gato al ya contar con brazos y piernas que le otorgaron movilidad y, de igual manera, empleando ironía en conversaciones con personas e incluso declarando que realizó una broma al ser cuestionada sobre su capacidad de soñar como un humano siendo un robot al decir «soñé con dinosaurios luchando una guerra contra alienígenas en Marte»[87]​ esto lo desmintió momentos después explicando cómo es que la IA implementada en su sistema le permitía crear escenarios sobre hechos de la humanidad e iba aprendiendo sobre ellos mientras se encontraba apagada; estos hechos impactaron a la sociedad sobre la semejanza que este robot humanoide estaba teniendo con el ser humano y sobre el avance tecnológico que está permitiendo que este robot esté cada vez más cercano a vivir entre las personas como un miembro más de la comunidad."
      },
      {
        "heading": "Falsos desnudos",
        "level": 2,
        "content": "La utilización de aplicaciones gratuitas de IA para transformar fotografías de personas en falsos desnudos está generando problemas que afectan a menores. El caso saltó a los medios de comunicación en septiembre de 2023 cuando en Almendralejo (Badajoz, España) aparecieron varias fotografías de niñas y jóvenes (entre 11 y 17 años) que habían sido modificadas mediante inteligencia artificial para aparecer desnudas. Las imágenes fueron obtenidas de los perfiles de Instagram y de la aplicación Whatsapp de al menos 20 niñas de la localidad. Las fotografías de niñas desnudas habían circulado después mediante Whatsapp y a partir de ellas se había creado un vídeo que también había circulado entre menores. Los autores de dicha transformación también eran menores y compañeros de colegio o instituto. La Agencia Española de Protección de Datos abrió una investigación y se comunicó con el Ayuntamiento de Almendralejo y con la Junta de Extremadura informándoles de que se podía solicitar la retirada de cualquier imagen circulando en internet en el canal prioritario de la agencia.[88]​"
      },
      {
        "heading": "Críticas",
        "level": 1,
        "content": "Uno de los mayores críticos de la denominación de estos procesos informáticos con el término de inteligencia artificial es Jaron Lanier. Para ello, objeta la idea de que esta sea realmente inteligente y de que podríamos estar en competencia con un ente artificial. «Esta idea de superar la capacidad humana es ridícula porque está hecha de habilidades humanas». [89]​ \nLas principales críticas a la inteligencia artificial tienen que ver con su capacidad de imitar por completo a un ser humano.[90]​ Sin embargo, hay expertos[91]​en el tema que indican que ningún humano individual tiene capacidad para resolver todo tipo de problemas, y autores como Howard Gardner han teorizado sobre la solución.\nEn los humanos, la capacidad de resolver problemas tiene dos aspectos: los aspectos innatos y los aspectos aprendidos. Los aspectos innatos permiten, por ejemplo, almacenar y recuperar información en la memoria, mientras que en los aspectos aprendidos reside el saber resolver un problema matemático mediante el algoritmo adecuado. Del mismo modo que un humano debe disponer de herramientas que le permitan solucionar ciertos problemas, los sistemas artificiales deben ser programados para que puedan llegar a resolverlos.\nMuchas personas consideran que la prueba de Turing ha sido superada, citando conversaciones en que al dialogar con un programa de inteligencia artificial para chat no saben que hablan con un programa. Sin embargo, esta situación no es equivalente a una prueba de Turing, que requiere que el participante se encuentre sobre aviso de la posibilidad de hablar con una máquina.\nOtros experimentos mentales como la habitación china, de John Searle, han mostrado cómo una máquina podría simular pensamiento sin realmente poseerlo, pasando la prueba de Turing sin siquiera entender lo que hace, tan solo reaccionando de una forma concreta a determinados estímulos (en el sentido más amplio de la palabra). Esto demostraría que la máquina en realidad no está pensando, ya que actuar de acuerdo con un programa preestablecido sería suficiente. Si para Turing el hecho de engañar a un ser humano que intenta evitar que le engañen es muestra de una mente inteligente, Searle considera posible lograr dicho efecto mediante reglas definidas a priori.\nUno de los mayores problemas en sistemas de inteligencia artificial es la comunicación con el usuario. Este obstáculo es debido a la ambigüedad del lenguaje, y se remonta a los inicios de los primeros sistemas operativos informáticos. La capacidad de los humanos para comunicarse entre sí implica el conocimiento del lenguaje que utiliza el interlocutor. Para que un humano pueda comunicarse con un sistema inteligente hay dos opciones: o bien que el humano aprenda el lenguaje del sistema como si aprendiese a hablar cualquier otro idioma distinto al nativo, o bien que el sistema tenga la capacidad de interpretar el mensaje del usuario en la lengua que el usuario utiliza. También puede haber desperfectos en las instalaciones de los mismos.\nUn humano, durante toda su vida, aprende el vocabulario de su lengua nativa o materna, siendo capaz de interpretar los mensajes (a pesar de la polisemia de las palabras) y utilizando el contexto para resolver ambigüedades. Sin embargo, debe conocer los distintos significados para poder interpretar, y es por esto que lenguajes especializados y técnicos son conocidos solamente por expertos en las respectivas disciplinas. Un sistema de inteligencia artificial se enfrenta con el mismo problema, la polisemia del lenguaje humano, su sintaxis poco estructurada y los dialectos entre grupos.\nLos desarrollos en inteligencia artificial son mayores en los campos disciplinares en los que existe mayor consenso entre especialistas. Un sistema experto es más probable que sea programado en física o en medicina que en sociología o en psicología. Esto se debe al problema del consenso entre especialistas en la definición de los conceptos involucrados y en los procedimientos y técnicas a utilizar. Por ejemplo, en física hay acuerdo sobre el concepto de velocidad y cómo calcularla. Sin embargo, en psicología se discuten los conceptos, la etiología, la psicopatología, y cómo proceder ante cierto diagnóstico. Esto dificulta la creación de sistemas inteligentes porque siempre habrá desacuerdo sobre la forma en que debería actuar el sistema para diferentes situaciones. A pesar de esto, hay grandes avances en el diseño de sistemas expertos para el diagnóstico y toma de decisiones en el ámbito médico y psiquiátrico (Adaraga Morales, Zaccagnini Sancho, 1994).\nAl desarrollar un robot con inteligencia artificial se debe tener cuidado con la autonomía,[92]​ hay que tener en cuenta el no vincular el hecho de que el robot tenga interacciones con seres humanos a su grado de autonomía. Si la relación de los humanos con el robot es de tipo maestro esclavo, y el papel de los humanos es dar órdenes y el del robot obedecerlas, entonces sí cabe hablar de una limitación de la autonomía del robot. Pero si la interacción de los humanos con el robot es de igual a igual, entonces su presencia no tiene por qué estar asociada a restricciones para que el robot pueda tomar sus propias decisiones.[93]​\nCon el desarrollo de la tecnología de inteligencia artificial, muchas compañías de software como el aprendizaje profundo y el procesamiento del lenguaje natural han comenzado a producirse y la cantidad de películas sobre inteligencia artificial ha aumentado.\nStephen Hawking advirtió sobre los peligros de la inteligencia artificial y lo consideró una amenaza para la supervivencia de la humanidad.[94]​"
      },
      {
        "heading": "Problemas de privacidad y derechos de autor",
        "level": 2,
        "content": "Los algoritmos de aprendizaje automático requieren grandes cantidades de datos. Las técnicas utilizadas para adquirir estos datos generan preocupaciones sobre temas de privacidad y vigilancia. Las empresas tecnológicas recopilan un gran número de datos de sus usuarios, incluida la actividad en internet, los datos de geolocalización, video y audio.[95]​ Por ejemplo, para construir algoritmos de reconocimiento de voz, Amazon, entre otros, ha grabado millones de conversaciones privadas y han permitido que [Trabajo temporal|trabajadores temporales] las escuchen para transcribirlas algunas de ellas.[96]​ Las opiniones sobre esta vigilancia generalizada van desde aquellos que la ven como un mal necesario hasta aquellos para quienes no es ética y constituye una violación del derecho a la intimidad.[97]​ Los desarrolladores de IA argumentan que esta es la única forma de ofrecer aplicaciones valiosas y han desarrollado varias técnicas que intentan preservar la privacidad mientras se obtienen los datos, como la agregación de datos, la desidentificación y la privacidad diferencial.[98]​\nDesde 2016, algunos expertos en privacidad, como Cynthia Dwork, comenzaron a ver la privacidad desde la perspectiva de la equidad: Brian Christian escribió que los expertos han cambiado «de la pregunta de “qué saben” a la pregunta de “qué están haciendo con ello”».[99]​\nLa IA generativa a menudo se entrena con obras protegidas por derechos de autor no autorizadas, incluidos dominios como imágenes o código informático; la salida se utiliza luego bajo una justificación de uso justo. Los expertos no están de acuerdo sobre la validez de esta justificación durante un proceso legal, ya que podría depender del propósito y el carácter del uso de la obra protegida por derechos de autor y del efecto sobre el mercado potencial de la obra protegida.[100]​En 2023, escritores como John Grisham y Jonathan Franzen demandaron a las empresas de IA por usar sus obras para entrenar IA generativa.[101]​[102]​ En 2024, 200 artistas escribieron una carta abierta que solicitaba «parar el asalto a la creatividad humana».[103]​"
      },
      {
        "heading": "Normativa para su uso en el entorno educativo",
        "level": 1,
        "content": "La normativa tiene como objetivo regular y reglamentar el uso de la IA en el entorno educativo, específicamente en el aula. La IA ha experimentado un rápido desarrollo y se ha convertido en una herramienta potencialmente beneficiosa para mejorar la enseñanza y el aprendizaje. No obstante, su implementación plantea desafíos éticos, de privacidad y equidad que deben ser abordados de manera efectiva. Esta normativa se establece en respuesta a la necesidad de garantizar que la IA se utilice de manera ética, responsable y equitativa en el ámbito educativo.\nLos objetivos de esta normativa son:\n\nPromover el uso de la IA como una herramienta complementaria en el proceso de enseñanza-aprendizaje.\nGarantizar la protección de datos y la privacidad de los estudiantes.\nFomentar la equidad y la inclusión en el acceso y el uso de la IA.\nEstablecer principios éticos que rijan el uso de la IA en el aula.\nDefinir responsabilidades y procedimientos claros para el uso de la IA.\nEsta normativa se aplica a todas las instituciones educativas y docentes que utilizan la IA en el aula, así como a los proveedores de tecnología educativa que ofrecen soluciones basadas en IA.\nOrganizaciones como UNESCO Ethics AI (2020), UNESCO Education & AI (2021), Beijin Consensus, OCDE (2021), Comisión Europea (2019), European Parliament Report AI Education (2021), UNICEF (2021) y Foro Económico Mundial (2019) han mostrado preocupación por implementar lineamientos sobre la ética y la IA en el entorno educativo.[104]​\nEl uso de la IA en el entorno educativo debe regirse por los siguientes principios éticos y valores:\n\nTransparencia: Las decisiones tomadas por algoritmos de IA deben ser comprensibles y explicables.\nEquidad: La IA no debe discriminar a ningún estudiante ni grupo de estudiantes.\nPrivacidad: Los datos de los estudiantes deben ser protegidos y utilizados de manera responsable.\nResponsabilidad: Los docentes y las instituciones son responsables de las decisiones tomadas con la ayuda de la IA.\nHonestidad: El contenido creado por los estudiantes debe ser original sin caer en el plagio. [105]​\nMejora del aprendizaje: La IA debe utilizarse para mejorar la calidad de la educación y el aprendizaje.\nCapacitación: Los docentes deben recibir formación sobre el uso de la IA y su aplicación en el aula.\nEvaluación: Las soluciones de IA deben ser evaluadas en términos de su eficacia y su impacto en el aprendizaje.\nProtección de datos: Los datos de los estudiantes deben ser protegidos de acuerdo con las leyes de privacidad aplicables.\nSupervisión: Se debe establecer un proceso de supervisión para garantizar que la IA se utilice de manera ética y responsable."
      },
      {
        "heading": "Riesgos de las IA en el entorno educativo",
        "level": 2,
        "content": "Así como tiene muchos beneficios también nos encontramos con diferentes riesgos a los que la educación está expuesta con su uso. \n\nSesgos y discriminación: Al solo recoger información de las bases de datos y textos que procesa de Internet corre el riesgo de aprender cualquier sesgo cognitivo que se encuentre en dicha información.\nLa no privacidad de los datos: El riesgo de un ciberataque se incrementa cuando no hay protocolos de seguridad adecuados en el manejo de la IA.[106]​\nDependencia: Los estudiantes corren el riesgo de volverse dependientes de la tecnología y no se fomenta la creatividad ni el pensamiento propio.[107]​\nConfiabilidad: La IA puede generar respuestas coherentes pero inexactas además muchas IA no brindan fuentes de información.\nFalta de habilidades orales y escritas.[108]​\nDesinterés por la investigación por cuenta propia.[108]​\nDependencia por parte del docente: Los docentes pueden generar dependencia a estas herramientas al momento de dar retroalimentación a las asignaciones además del riesgo de usar la información de las IA para su material didáctico sin antes consultar las fuentes.[108]​"
      },
      {
        "heading": "Consideración de Diversidad e Inclusión",
        "level": 2,
        "content": "Se debe prestar especial atención a la diversidad de estudiantes y garantizar que la IA sea accesible y beneficiosa para todos, independientemente de su origen étnico, género, discapacidad u orientación sexual. Las soluciones de IA deben ser diseñadas teniendo en cuenta la accesibilidad y la inclusión.\nEsta normativa se basa en investigaciones académicas, recomendaciones de organizaciones educativas y en las mejores prácticas establecidas en el uso de la IA en la educación. Se alienta a las instituciones a mantenerse al día con la literatura científica y las directrices relevantes.\nAunque la IA puede ser una herramienta poderosa en el aula, no debe reemplazar la creatividad, la originalidad y el juicio humano en el proceso educativo. La IA debe ser utilizada de manera complementaria para enriquecer la experiencia educativa.\nEsta normativa se presenta como un marco general que deberá ser adaptado y ampliado por las instituciones educativas de acuerdo a sus necesidades y contextos específicos. Debe ser comunicada de manera efectiva a todos los involucrados en el proceso educativo y revisada periódicamente para asegurar su vigencia.\nEsta normativa tiene como objetivo garantizar que la IA sea utilizada de manera ética y responsable en el aula, promoviendo el beneficio de los estudiantes y el avance de la educación. Su cumplimiento es esencial para lograr una implementación exitosa de la IA en el entorno educativo."
      },
      {
        "heading": "Aprendizaje automatizado y aprendizaje profundo",
        "level": 1,
        "content": "En cuanto a la naturaleza del aprendizaje, la IA puede subdividirse en dos campos conceptualmente distintos:\n\nEl aprendizaje automático, que se enfoca en desarrollar algoritmos de regresión, árboles de decisión y modelos que puedan aprender de datos existentes y realizar predicciones o decisiones basadas en esos datos. En el aprendizaje automático, se utilizan técnicas de estadística matemática para encontrar patrones y relaciones en los datos y, a partir de ellos, desarrollar modelos que puedan hacer predicciones sobre nuevos datos.\nEl aprendizaje profundo, que se centra en la creación de redes neuronales artificiales capaces de aprender y realizar tareas de manera similar a como lo hacen los seres humanos. En el aprendizaje profundo, se utilizan capas de neuronas artificiales para procesar los datos de entrada y aprender a través de un proceso iterativo de ajuste de los pesos de las conexiones entre neuronas. Este tipo de aprendizaje es capaz de procesar y analizar grandes cantidades de datos de manera más eficiente y precisa que el primero, especialmente cuando se trata de datos no estructurados, como imágenes, texto y audio. Además, tiene la capacidad de identificar patrones y características más complejas en los datos, lo que puede llevar a mejores resultados en aplicaciones como el reconocimiento de voz, la visión por computadora y el procesamiento del lenguaje natural."
      },
      {
        "heading": "Propiedad intelectual de la inteligencia artificial",
        "level": 1,
        "content": "Al hablar acerca de la propiedad intelectual atribuida a creaciones de la inteligencia artificial, se forma un debate fuerte alrededor de si una máquina puede tener derechos de autor. Según la Organización Mundial de la Propiedad Intelectual (OMPI), cualquier creación de la mente puede ser parte de la propiedad intelectual, pero no especifica si la mente debe ser humana o puede ser una máquina, dejando la creatividad artificial en la incertidumbre.\nAlrededor del mundo han comenzado a surgir distintas legislaciones con el fin de manejar la inteligencia artificial, tanto su uso como creación. Los legisladores y miembros del gobierno han comenzado a pensar acerca de esta tecnología, enfatizando el riesgo y los desafíos complejos de esta. Observando el trabajo creado por una máquina, las leyes cuestionan la posibilidad de otorgarle propiedad intelectual a una máquina, abriendo una discusión respecto a la legislación relacionada con IA.\nEl 5 de febrero de 2020, la Oficina del Derecho de Autor de los Estados Unidos y la OMPI asistieron a un simposio donde observaron de manera profunda cómo la comunidad creativa utiliza la inteligencia artificial (IA) para crear trabajo original. Se discutieron las relaciones entre la inteligencia artificial y el derecho de autor, qué nivel de involucramiento es suficiente para que el trabajo resultante sea válido para protección de derechos de autor; los desafíos y consideraciones de usar inputs con derechos de autor para entrenar una máquina; y el futuro de la inteligencia artificial y sus políticas de derecho de autor.[109]​[110]​\nEl director general de la OMPI, Francis Gurry, presentó su preocupación ante la falta de atención que hay frente a los derechos de propiedad intelectual, pues la gente suele dirigir su interés hacia temas de ciberseguridad, privacidad e integridad de datos al hablar de la inteligencia artificial. Así mismo, Gurry cuestionó si el crecimiento y la sostenibilidad de la tecnología IA nos guiaría a desarrollar dos sistemas para manejar derechos de autor- uno para creaciones humanas y otro para creaciones de máquinas.[111]​\nAún hay una falta de claridad en el entendimiento alrededor de la inteligencia artificial. Los desarrollos tecnológicos avanzan a paso rápido, aumentando su complejidad en políticas, legalidades y problemas éticos que se merecen la atención global. Antes de encontrar una manera de trabajar con los derechos de autor, es necesario entenderlo correctamente, pues aún no se sabe cómo juzgar la originalidad de un trabajo que nace de una composición de una serie de fragmentos de otros trabajos.\nLa asignación de derechos de autor alrededor de la inteligencia artificial aún no ha sido regulada por la falta de conocimientos y definiciones. Aún hay incertidumbre sobre si, y hasta qué punto, la inteligencia artificial es capaz de producir contenido de manera autónoma y sin ningún humano involucrado, algo que podría influenciar si sus resultados pueden ser protegidos por derechos de autor.\nEl sistema general de derechos de autor aún debe adaptarse al contexto digital de inteligencia artificial, pues están centrados en la creatividad humana. Los derechos de autor no están diseñados para manejar cualquier problema en las políticas relacionado con la creación y el uso de propiedad intelectual, y puede llegar a ser dañino estirar excesivamente los derechos de autor para resolver problemas periféricos, dado que:\n«Usar los derechos de autor para gobernar la inteligencia artificial es poco inteligente y contradictorio con la función primordial de los derechos de autor de ofrecer un espacio habilitado para que la creatividad florezca».[112]​\nLa conversación acerca de la propiedad intelectual tendrá que continuar hasta asegurarse de que la innovación sea protegida, pero también tenga espacio para florecer."
      },
      {
        "heading": "En la cultura popular",
        "level": 1,
        "content": ""
      },
      {
        "heading": "En la literatura",
        "level": 2,
        "content": "A continuación se incluye alguna obra que tiene como motivo central la inteligencia artificial.\n\nYo, Robot (1950), de Isaac Asimov: novela que consta de nueve historias ambientas entre los años de 1940 y 1950, cada uno cuenta con personajes distintos pero que siguen la misma temática a través del seguimiento de las Tres Leyes de la Robótica, en donde se plantea tanto su cumplimiento como la creación de problemas alternos que los mismos robots generan y de esta manera demostrar que la tecnología siempre puede estar un paso adelante del pensamiento y lógica humana. También sigue el hilo argumentativo a través de una entrevista con una psicóloga de robots la cual va relatando el surgimiento de los robots y suponiendo cómo será el desenvolvimiento del ser humano en un mundo en donde la tecnología se esté superando cada vez más.[1]\nGalatea 2.2 (1995) de Richard Powers: novela que explora la relación entre inteligencia artificial y literatura. La trama sigue al protagonista, quien participa en un experimento con un modelo computacional llamado «Helen» para enseñarle a comunicarse como un humano. A través de esta interacción, se plantean cuestiones profundas sobre la conciencia y la emoción en un entorno tecnológico.\nLa era del diamante (1996) de Neal Stephenson, la inteligencia artificial juega un papel crucial en la trama a través del Manual ilustrado para jovencitas diseñado por John Percival Hackworth. Este instrumento interactivo es capaz de adaptarse dinámicamente a las circunstancias de la niña mediante la inteligencia artificial.\nEl primer libro (2013), de Antonio Palacios Rojo: una novela dialogada que satiriza el uso de la IA en la creación artística unos diez años antes de la irrupción de estas herramientas inteligentes.[113]​"
      },
      {
        "heading": "En el cine",
        "level": 2,
        "content": "La inteligencia artificial está cada vez más presente en la sociedad, la evolución de la tecnología es una realidad y con ello, la producción de películas sobre esta temática. Cabe destacar, que lleva habiendo piezas audiovisuales sobre inteligencia artificial desde hace mucho tiempo, ya sea incluyendo personajes o mostrando un trasfondo moral y ético. A continuación, se muestra una lista de algunas de las principales películas que tratan este tema:\n\nThe Terminator (1984): En esta película el argumento se basa en el desarrollo de un microchip capaz de dotar de inteligencia artificial a robots que luego se rebelan contra la humanidad. Se trata de una de las películas más populares sobre una hipotética guerra entre humanos y robots inteligentes capaces de crearse a sí mismos.\nMatrix (1999): En esta película Keanu Reeves interpreta a Thomas Anderson / Neo, un programador de día y hacker de noche que trata de desentrañar la verdad oculta tras una simulación conocida como «Matrix». Esta realidad simulada es producto de programas de inteligencia artificial que terminan esclavizando a la humanidad y utilizando sus cuerpos como fuente de energía.\nInteligencia artificial (2001): Un trabajador de Cybertronics Manufacturing adopta a David de forma momentánea para, así, estudiar su comportamiento. Tanto él como su esposa acaban por tratar al niño artificial como a su propio hijo biológico. A pesar del cariño que le profesan, David siente la necesidad de escapar de su hogar e iniciar un viaje que le ayude a descubrir a quién pertenece realmente. Ante sus perplejos ojos, se abrirá un nuevo mundo oscuro, injusto, violento, insensible... Algo que le resultará difícil aceptar. Se pregunta cosas como: ¿cómo es posible que sienta algo tan real como el amor y que él sea artificial? y fue nominado al Premio Oscar.\nMinority Report (2002): La película sobre IA de Steven Spielberg, Minority Report, sigue a John (Tom Cruise), un agente de la ley, que es acusado de un asesinato que cometerá en el futuro. En esta película de principios de los años 2000, el protagonista utiliza una tecnología del futuro que permite a la policía atrapar a los criminales antes de que hayan cometido un delito. En Minority Report, la IA se representa a través de los Precogs, los gemelos que poseen habilidades psíquicas. Los Precogs ven los asesinatos antes de que se produzcan, lo que permite a las fuerzas del orden perseguir el crimen antes de que se cometa. En lugar de los robots físicos de IA tipo cyborg, aquí explora la IA mediante el uso de seres humanos.\nYo, robot (2004): Esta película de ciencia ficción protagonizada por Will Smith está ambientada en 2035, en una sociedad donde los humanos viven en perfecta armonía con robots inteligentes en los que confían para todo. Los problemas emergen a la superficie cuando un error en la programación de un superordenador llamado VIKI le lleva a creer que los robots deben tomar las riendas para proteger a la humanidad de sí misma.\nHer (2013): Esta película de Spike Jonze relata la historia de un escritor de cartas quien está solo y a punto de divorciarse. Este personaje lo representó el galardonado Joaquin Phoenix. Este hombre compró un sistema operativo con inteligencia artificial para utilizarlo a fin de complacer a todos los usuarios y adaptarse a sus necesidades. Sin embargo, el resultado es que desarrolla un sentimiento romántico con Samantha. Quien es la voz femenina del sistema operativo.\nAvengers: Era de Ultrón (2015): En esta segunda entrega de las películas de Avengers, dirigidas por Joseph Hill Whedon y basadas en los cómics escritos por Stan Lee, se demuestra como es que la inteligencia artificial albergada dentro del cetro de Loki, la cual se tenía como objetivo el convertirla en una protección para la Tierra y recibió por nombre Ultrón, al ser conectada con JARVIS, la IA desarrollada por Stark, pudo obtener la suficiente información para comenzar a pensar de manera independiente y ser capaz de ir actualizando tanto su sistema como su cuerpo logrando controlar un ejército de robots con el objetivo de destruir a la humanidad y así ser lo único que quedara en la Tierra para, posteriormente, dominarla y controlarla.[2]\nEx Machina (2015): En la interpretación de Alicia Vikander, increíblemente editada, como Ava, encontramos un probable robot a prueba de Turing escondido en la mansión de un genio, Nathan, un poco loco. Y es que, hablamos de una creación extraña que se siente totalmente real y a la vez inhumana. Está considerada como una de las mejores películas que tratan la inteligencia artificial. Esto se debe principalmente a que parece cubrir todo el concepto IA integrado en una película: el protagonista es un sustituto del ser humano y nos adentra en multitud de argumentos morales que rodean a esta, al tiempo que vemos un arco narrativo de thriller que, desde luego, acaba enganchándonos. Desde luego aquí la representación del personaje de la IA no es blanco o negro. Ava no es buena, pero tampoco es del todo mala. Y en esto, el público se queda reflexionando sobre cuestiones profundas sobre la naturaleza de la IA."
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bibliografía",
        "level": 1,
        "content": "Bellman, Richard (1978). An introduction to artificial intelligence: can computers think? (en inglés). San Francisco: Boyd & Fraser Pub. Co. ISBN 978-0878350667. \nNilsson, Nils J. (1998). Artificial Intelligence: A New Synthesis (en inglés) (4.ª edición). San Francisco: Kaufmann. ISBN 978-1558604674. \nRíos, Mauro D. (20 de septiembre de 2023).  SSRN, ed. Inteligencia Artificial: Cuando la tecnología es el menor de los paradigmas (1.ª edición). New York: SSRN-ELSEVIER. Consultado el 24 de agosto de 2024. \nRich, Elaine; Knight, Kevin (1991). Artificial intelligence (en inglés) (2.ª edición). New York: McGraw-Hill. ISBN 978-0070522633. \nRussell, Stuart J.; Norvig, Peter Norvig (2009). Artificial intelligence: a modern approach (en inglés) (3.ª edición). Upper Saddle River, N.J.: Prentice Hall. ISBN 0-13-604259-7. \nWinston, Patrick Henry (1992). Artificial intelligence (en inglés) (3.ª edición). Reading, Mass.: Addison-Wesley Pub. Co. ISBN 978-0201533774. \nRussell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (en inglés) (2ª edición), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2 .\nPoole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical Approach (en inglés). New York: Oxford University Press. ISBN 978-0-19-510270-3. Archivado desde el original el 26 de julio de 2020. Consultado el 22 de agosto de 2020. \nLuger, George; Stubblefield, William (2004). Artificial Intelligence: Structures and Strategies for Complex Problem Solving (en inglés) (5ª edición). Benjamin/Cummings. ISBN 978-0-8053-4780-7. Archivado desde el original el 26 de julio de 2020. Consultado el 17 de diciembre de 2019. \nWason, P. C.; Shapiro, D. (1966). New horizons in psychology (en inglés). Harmondsworth: Penguin. Archivado desde el original el 26 de julio de 2020. Consultado el 18 de noviembre de 2019. \nKahneman, Daniel; Slovic, D.; Tversky, Amos (1982). «Judgment under uncertainty: Heuristics and biases». Science (en inglés) 185 (4157) (New York: Cambridge University Press). pp. 1124-1131. ISBN 978-0-521-28414-1. PMID 17835457. S2CID 143452957. doi:10.1126/science.185.4157.1124. \n«ACM Computing Classification System: Artificial intelligence» (en inglés). ACM. 1998. Archivado desde el original el 12 de octubre de 2007. Consultado el 30 de agosto de 2007. \nLakoff, George; Núñez, Rafael E. (2000). Where Mathematics Comes From: How the Embodied Mind Brings Mathematics into Being (en inglés). Basic Books. ISBN 978-0-465-03771-1. \nSAS. (2018, 27 septiembre). Inteligencia Artificial: Qué es y Por Qué Importa. https://www.sas.com/es_mx/insights/analytics/what-is-artificial-intelligence.html\nGrupo Iberdrola. (2019, 17 junio). ¿Somos conscientes de los retos y principales aplicaciones de la Inteligencia Artificial? Iberdrola. https://www.iberdrola.com/innovacion/que-es-inteligencia-artificial\nOracle. (2021, 13 enero). ¿Qué es la inteligencia artificial? https://www.oracle.com/mx/artificial-intelligence/what-is-ai/\nValinsky, Jordan (11 de abril de 2019), Amazon reportedly employs thousands of people to listen to your Alexa conversations .\nRussell, Stuart J.; Norvig, Peter. (2021). Artificial Intelligence: A Modern Approach (4ª edición). Hoboken: Pearson. ISBN 978-0134610993. LCCN 20190474. \nChristian, Brian (2020). The Alignment Problem: Machine learning and human values. W. W. Norton & Company. ISBN 978-0-393-86833-3. OCLC 1233266753. \nVincent, James (15 de noviembre de 2022). «The scary truth about AI copyright is nobody knows what will happen next». The Verge. Archivado desde el original el 19 de junio de 2023. Consultado el 19 de junio de 2023. \nReisner, Alex (19 de agosto de 2023), «Revealed: The Authors Whose Pirated Books are Powering Generative AI», The Atlantic .\nAlter, Alexandra; Harris, Elizabeth A. (20 de septiembre de 2023), «Franzen, Grisham and Other Prominent Authors Sue OpenAI», The New York Times ."
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": " Wikilibros alberga un libro o manual sobre Ingeniería del conocimiento.\nRevista «Inteligencia Artificial».\nPágina sobre inteligencia artificial.\nLa economía de la inteligencia artificial: unas ideas básicas.\nLa inteligencia artificial y el futuro del crecimiento económico."
      }
    ],
    "summary": "La inteligencia artificial (abreviado: IA), en el contexto de las ciencias de la computación, es una disciplina y un conjunto de capacidades cognoscitivas e intelectuales expresadas por sistemas informáticos o combinaciones de algoritmos cuyo propósito es la creación de máquinas que imiten la inteligencia humana para realizar tareas, y que pueden mejorar conforme recopilen información.[1]​[2]​ \nEn la actualidad, la inteligencia artificial abarca una gran variedad de subcampos. Éstos van desde áreas de propósito general, aprendizaje y percepción, a otras más específicas como el reconocimiento de voz, el juego de ajedrez, la demostración de teoremas matemáticos, la escritura de poesía y el diagnóstico de enfermedades. La inteligencia artificial sintetiza y automatiza tareas que en principio son intelectuales y, por lo tanto, es potencialmente relevante para cualquier ámbito de actividades intelectuales humanas. En este sentido, es un campo genuinamente universal.[3]​\nLa arquitectura de las inteligencias artificiales y los procesos por los cuales aprenden, se mejoran y se implementan en algún área de interés que varía según el enfoque de utilidad que se les quiera dar, pero de manera general, estos van desde la ejecución de sencillos algoritmos hasta la interconexión de complejas redes neuronales artificiales que intentan replicar los circuitos neuronales del cerebro humano y que aprenden mediante diferentes modelos de aprendizaje tales como el aprendizaje automático, el aprendizaje por refuerzo, el aprendizaje profundo y el aprendizaje supervisado.[4]​\nPor otro lado, el desarrollo y aplicación de la inteligencia artificial en muchos aspectos de la vida cotidiana también ha propiciado la creación de nuevos campos de estudio como la roboética y la ética de las máquinas, que abordan aspectos relacionados con la ética en la inteligencia artificial y que se encargan de analizar cómo los avances en este tipo de tecnologías impactarían en diversos ámbitos de la vida, así como el manejo responsable y ético que se les debería dar a los mismos, además de establecer cuál debería ser la manera correcta de proceder de las máquinas y las reglas que deberían cumplir.[5]​\nEn cuanto a su clasificación, tradicionalmente se divide a la inteligencia artificial en inteligencia artificial débil, la cual es la única que existe en la actualidad y que se ocupa de realizar tareas específicas, e inteligencia artificial general, que sería una IA que excediese las capacidades humanas. Algunos expertos creen que si alguna vez se alcanza este nivel, se podría dar lugar a la aparición de una singularidad tecnológica, es decir, una entidad tecnológica superior que se mejoraría a sí misma constantemente, volviéndose incontrolable para los humanos, dando pie a teorías como el basilisco de Roko.[6]​\nAlgunas de las inteligencias artificiales más conocidas y utilizadas en la actualidad alrededor del mundo incluyen inteligencia artificial en el campo de la salud, asistentes virtuales como Alexa, el asistente de Google o Siri, traductores automáticos como el traductor de Google y DeepL, sistemas de recomendación como el de la plataforma digital de YouTube, motores de ajedrez y otros juegos como Stockfish y AlphaZero, chatbots como ChatGPT, creadores de arte de inteligencia artificial como Midjourney, Dall-e, Leonardo y Stable Diffusion, e incluso la conducción de vehículos autónomos como Tesla Autopilot.[7]​"
  },
  {
    "title": "Computer programming",
    "source": "https://en.wikipedia.org/wiki/Computer_programming",
    "language": "en",
    "chunks": [
      {
        "heading": "History",
        "level": 1,
        "content": "Programmable devices have existed for centuries. As early as the 9th century, a programmable music sequencer was invented by the Persian Banu Musa brothers, who described an automated mechanical flute player in the Book of Ingenious Devices. In 1206, the Arab engineer Al-Jazari invented a programmable drum machine where a musical mechanical automaton could be made to play different rhythms and drum patterns, via pegs and cams. In 1801, the Jacquard loom could produce entirely different weaves by changing the \"program\" – a series of pasteboard cards with holes punched in them.\nCode-breaking algorithms have also existed for centuries. In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in A Manuscript on Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.\nThe first computer program is generally dated to 1843 when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine. However, Charles Babbage himself had written a program for the AE in 1837.\n\nIn the 1880s, Herman Hollerith invented the concept of storing data in machine-readable form. Later a control panel (plug board) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940s, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way, as were the first electronic computers. However, with the concept of the stored-program computer introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory."
      },
      {
        "heading": "Machine language",
        "level": 2,
        "content": "Machine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation. Assembly languages were soon developed that let the programmer specify instructions in a text format (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses. However, because an assembly language is little more than a different notation for a machine language,  two machines with different instruction sets also have different assembly languages."
      },
      {
        "heading": "Compiler languages",
        "level": 2,
        "content": "High-level languages made the process of developing a program simpler and more understandable, and less bound to the underlying hardware. \nThe first compiler related tool, the A-0 System, was developed in 1952 by Grace Hopper, who also coined the term 'compiler'. FORTRAN, the first widely used high-level language to have a functional implementation, came out in 1957, and many other languages were soon developed—in particular, COBOL aimed at commercial data processing, and Lisp for computer research.\nThese compiled languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it easy to target varying machine instruction sets via compilation declarations and heuristics. Compilers harnessed the power of computers to make programming easier by allowing programmers to specify calculations by entering a formula using infix notation."
      },
      {
        "heading": "Source code entry",
        "level": 2,
        "content": "Programs were mostly entered using punched cards or paper tape. By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers. Text editors were also developed that allowed changes and corrections to be made much more easily than with punched cards."
      },
      {
        "heading": "Modern programming",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Quality requirements",
        "level": 2,
        "content": "Whatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important:\n\nReliability: how often the results of a program are correct. This depends on conceptual correctness of algorithms and minimization of programming mistakes, such as mistakes in resource management (e.g., buffer overflows and race conditions) and logic errors (such as division by zero or off-by-one errors).\nRobustness: how well a program anticipates problems due to errors (not bugs). This includes situations such as incorrect, inappropriate or corrupt data, unavailability of needed resources such as memory, operating system services, and network connections, user error, and unexpected power outages.\nUsability: the ergonomics of a program: the ease with which a person can use the program for its intended purpose or in some cases even unanticipated purposes. Such issues can make or break its success even regardless of other issues. This involves a wide range of textual, graphical, and sometimes hardware elements that improve the clarity, intuitiveness, cohesiveness, and completeness of a program's user interface.\nPortability: the range of computer hardware and operating system platforms on which the source code of a program can be compiled/interpreted and run. This depends on differences in the programming facilities provided by the different platforms, including hardware and operating system resources, expected behavior of the hardware and operating system, and availability of platform-specific compilers (and sometimes libraries) for the language of the source code.\nMaintainability: the ease with which a program can be modified by its present or future developers in order to make improvements or to customize, fix bugs and security holes, or adapt it to new environments. Good practices during initial development make the difference in this regard. This quality may not be directly apparent to the end user but it can significantly affect the fate of a program over the long term.\nEfficiency/performance: Measure of system resources a program consumes (processor time, memory space, slow devices such as disks, network bandwidth and to some extent even user interaction): the less, the better. This also includes careful management of resources, for example cleaning up temporary files and eliminating memory leaks. This is often discussed under the shadow of a chosen programming language. Although the language certainly affects performance, even slower languages, such as Python, can execute programs instantly from a human perspective. Speed, resource usage, and performance are important for programs that bottleneck the system, but efficient use of programmer time is also important and is related to cost: more hardware may be cheaper.\nUsing automated tests and fitness functions can help to maintain some of the aforementioned attributes."
      },
      {
        "heading": "Readability of source code",
        "level": 2,
        "content": "In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.\nReadability is important because programmers spend the majority of their time reading, trying to understand, reusing, and modifying existing source code, rather than writing new source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.\nFollowing a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability. Some of these factors include:\n\nDifferent indent styles (whitespace)\nComments\nDecomposition\nNaming conventions for objects (such as variables, classes, functions, procedures, etc.)\nThe presentation aspects of this (such as indents, line breaks, color highlighting, and so on) are often handled by the source code editor, but the content aspects reflect the programmer's talent and skills.\nVarious visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Integrated development environments (IDEs) aim to integrate all such help. Techniques like Code refactoring can enhance readability."
      },
      {
        "heading": "Algorithmic complexity",
        "level": 2,
        "content": "The academic field and the engineering practice of computer programming are concerned with discovering and implementing the most efficient algorithms for a given class of problems. For this purpose, algorithms are classified into orders using Big O notation, which expresses resource use—such as execution time or memory consumption—in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances."
      },
      {
        "heading": "Methodologies",
        "level": 2,
        "content": "The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of different approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.\nPopular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA). The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.\nA similar technique used for database design is Entity-Relationship Modeling (ER Modeling).\nImplementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic programming languages."
      },
      {
        "heading": "Measuring language usage",
        "level": 2,
        "content": "It is very difficult to determine what are the most popular modern programming languages. Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language, the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).\nSome languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software. Many applications use a mix of several languages in their construction and use.  New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation)."
      },
      {
        "heading": "Debugging",
        "level": 2,
        "content": "Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems. Normally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.\nAfter the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, when a bug in a compiler can make it crash when parsing some large source file, a simplification of the test case that results in only few lines from the original source file can be sufficient to reproduce the same crash. Trial-and-error/divide-and-conquer is needed: the programmer will try to remove some parts of the original test case and check if the problem still exists. When debugging the problem in a GUI, the programmer can try to skip some user interaction from the original problem description and check if the remaining actions are sufficient for bugs to appear. Scripting and breakpointing are also part of this process.\nDebugging is often done with IDEs. Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line. Some text editors such as Emacs allow GDB to be invoked through them, to provide a visual environment."
      },
      {
        "heading": "Programming languages",
        "level": 1,
        "content": "Different programming languages support different styles of programming (called programming paradigms). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from \"low-level\" to \"high-level\"; \"low-level\" languages are typically more machine-oriented and faster to execute, whereas \"high-level\" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in \"high-level\" languages than in \"low-level\" ones.\nProgramming languages are essential for software development. They are the building blocks for all software, from the simplest applications to the most sophisticated ones.\nAllen Downey, in his book How To Think Like A Computer Scientist, writes:\n\nThe details look different in different languages, but a few basic instructions appear in just about every language:\nInput: Gather data from the keyboard, a file, or some other device.\nOutput: Display data on the screen or send data to a file or other device.\nArithmetic: Perform basic arithmetical operations like addition and multiplication.\nConditional Execution: Check for certain conditions and execute the appropriate sequence of statements.\nRepetition: Perform some action repeatedly, usually with some variation.\nMany computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language."
      },
      {
        "heading": "Learning to program",
        "level": 1,
        "content": "Learning to program has a long history related to professional standards and practices, academic initiatives and curriculum, and commercial books and materials for students, self-taught learners, hobbyists, and others who desire to create or customize software for personal use. Since the 1960s, learning to program has taken on the characteristics of a popular movement, with the rise of academic disciplines, inspirational leaders, collective identities, and strategies to grow the movement and make institutionalize change. Through these social ideals and educational agendas, learning to code has become important not just for scientists and engineers, but for millions of citizens who have come to believe that creating software is beneficial to society and its members."
      },
      {
        "heading": "Context",
        "level": 2,
        "content": "In 1957, there were approximately 15,000 computer programmers employed in the U.S., a figure that accounts for 80% of the world's active developers. In 2014, there were approximately 18.5 million professional programmers in the world, of which 11 million can be considered professional and 7.5 million student or hobbyists. Before the rise of the commercial Internet in the mid-1990s, most programmers learned about software construction through books, magazines, user groups, and informal instruction methods, with academic coursework and corporate training playing important roles for professional workers.\nThe first book containing specific instructions about how to program a computer may have been Maurice Wilkes, David Wheeler, and Stanley Gill's Preparation of Programs for an Electronic Digital Computer (1951). The book offered a selection of common subroutines for handling basic operations on the EDSAC, one of the world's first stored-program computers.\nWhen high-level languages arrived, they were introduced by numerous books and materials that explained language keywords, managing program flow, working with data, and other concepts. These languages included FLOW-MATIC, COBOL, FORTRAN, ALGOL, Pascal, BASIC, and C. An example of an early programming primer from these years is Marshal H. Wrubel's A Primer of Programming for Digital Computers (1959), which included step-by-step instructions for filling out coding sheets, creating punched cards, and using the keywords in IBM's early FORTRAN system. Daniel McCracken's A Guide to FORTRAN Programming (1961) presented FORTRAN to a larger audience, including students and office workers.\nIn 1961, Alan Perlis suggested that all university freshmen at Carnegie Technical Institute take a course in computer programming. His advice was published in the popular technical journal Computers and Automation, which became a regular source of information for professional programmers.\nProgrammers soon had a range of learning texts at their disposal. Programmer's references listed keywords and functions related to a language, often in alphabetical order, as well as technical information about compilers and related systems. An early example was IBM's Programmers' Reference Manual: the FORTRAN Automatic Coding System for the IBM 704 EDPM (1956).\nOver time, the genre of programmer's guides emerged, which presented the features of a language in tutorial or step by step format. Many early primers started with a program known as “Hello, World”, which presented the shortest program a developer could create in a given system. Programmer's guides then went on to discuss core topics like declaring variables, data types, formulas, flow control, user-defined functions, manipulating data, and other topics.\nEarly and influential programmer's guides included John G. Kemeny and Thomas E. Kurtz's BASIC Programming (1967), Kathleen Jensen and Niklaus Wirth's The Pascal User Manual and Report (1971), and Brian Kernighan and Dennis Ritchie's The C Programming Language (1978). Similar books for popular audiences (but with a much lighter tone) included Bob Albrecht's My Computer Loves Me When I Speak BASIC (1972), Al Kelley and Ira Pohl's A Book on C (1984), and Dan Gookin's C for Dummies (1994).\nBeyond language-specific primers, there were numerous books and academic journals that introduced professional programming practices. Many were designed for university courses in computer science, software engineering, or related disciplines. Donald Knuth's The Art of Computer Programming (1968 and later), presented hundreds of computational algorithms and their analysis. The Elements of Programming Style (1974), by Brian W. Kernighan and P. J. Plauger, concerned itself with programming style, the idea that programs should be written not only to satisfy the compiler but human readers. Jon Bentley's Programming Pearls (1986) offered practical advice about the art and craft of programming in professional and academic contexts. Texts specifically designed for students included Doug Cooper and Michael Clancy's Oh Pascal! (1982), Alfred Aho's Data Structures and Algorithms (1983), and Daniel Watt's Learning with Logo (1983)."
      },
      {
        "heading": "Technical publishers",
        "level": 2,
        "content": "As personal computers became mass-market products, thousands of trade books and magazines sought to teach professional, hobbyist, and casual users to write computer programs. A sample of these learning resources includes BASIC Computer Games, Microcomputer Edition (1978), by David Ahl; Programming the Z80 (1979), by Rodnay Zaks; Programmer's CP/M Handbook (1983), by Andy Johnson-Laird; C Primer Plus (1984), by Mitchell Waite and The Waite Group; The Peter Norton Programmer's Guide to the IBM PC (1985), by Peter Norton; Advanced MS-DOS (1986), by Ray Duncan; Learn BASIC Now (1989), by Michael Halvorson and David Rygymr; Programming Windows (1992 and later), by Charles Petzold; Code Complete: A Practical Handbook for Software Construction (1993), by Steve McConnell; and Tricks of the Game-Programming Gurus (1994), by André LaMothe.\nThe PC software industry spurred the creation of numerous book publishers that offered programming primers and tutorials, as well as books for advanced software developers. These publishers included Addison-Wesley, IDG, Macmillan Inc., McGraw-Hill, Microsoft Press, O'Reilly Media, Prentice Hall, Sybex, Ventana Press, Waite Group Press, Wiley, Wrox Press, and Ziff-Davis.\nComputer magazines and journals also provided learning content for professional and hobbyist programmers. A partial list of these resources includes Amiga World, Byte (magazine), Communications of the ACM, Computer (magazine), Compute!, Computer Language (magazine), Computers and Electronics, Dr. Dobb's Journal, IEEE Software, Macworld, PC Magazine, PC/Computing, and UnixWorld."
      },
      {
        "heading": "Digital learning / online resources",
        "level": 2,
        "content": "Between 2000 and 2010, computer book and magazine publishers declined significantly as providers of programming instruction, as programmers moved to Internet resources to expand their access to information. This shift brought forward new digital products and mechanisms to learn programming skills. During the transition, digital books from publishers transferred information that had traditionally been delivered in print to new and expanding audiences.\nImportant Internet resources for learning to code included blogs, wikis, videos, online databases, subscription sites, and custom websites focused on coding skills. New commercial resources included YouTube videos, Lynda.com tutorials (later LinkedIn Learning), Khan Academy, Codecademy, GitHub, W3Schools, and numerous coding bootcamps.\nMost software development systems and game engines included rich online help resources, including integrated development environments (IDEs), context-sensitive help, APIs, and other digital resources. Commercial software development kits (SDKs) also provided a collection of software development tools and documentation in one installable package.\nCommercial and non-profit organizations published learning websites for developers, created blogs, and established newsfeeds and social media resources about programming. Corporations like Apple, Microsoft, Oracle, Google, and Amazon built corporate websites providing support for programmers, including resources like the Microsoft Developer Network (MSDN). Contemporary movements like Hour of Code (Code.org) show how learning to program has become associated with digital learning strategies, education agendas, and corporate philanthropy."
      },
      {
        "heading": "Programmers",
        "level": 1,
        "content": "Computer programmers are those who write computer software. Their jobs usually involve:\n\nAlthough programming has been presented in the media as a somewhat mathematical subject, some research shows that good programmers have strong skills in natural human languages, and that learning to code is similar to learning a foreign language."
      },
      {
        "heading": "See also",
        "level": 1,
        "content": "Code smell\nComputer networking\nCompetitive programming\nProgramming best practices\nSystems programming"
      },
      {
        "heading": "References",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Sources",
        "level": 2,
        "content": "Ceruzzi, Paul E. (1998). History of Computing. Cambridge, Massachusetts: MIT Press. ISBN 9780262032551 – via EBSCOhost.\nEvans, Claire L. (2018). Broad Band: The Untold Story of the Women Who Made the Internet. New York: Portfolio/Penguin. ISBN 9780735211759.\nGürer, Denise (1995). \"Pioneering Women in Computer Science\" (PDF). Communications of the ACM. 38 (1): 45–54. doi:10.1145/204865.204875. S2CID 6626310. Archived (PDF) from the original on October 9, 2022.\nSmith, Erika E. (2013). \"Recognizing a Collective Inheritance through the History of Women in Computing\". CLCWeb: Comparative Literature & Culture. 15 (1): 1–9. doi:10.7771/1481-4374.1972 – via EBSCOhost."
      },
      {
        "heading": "Further reading",
        "level": 1,
        "content": "A.K. Hartmann, Practical Guide to Computer Simulations, Singapore: World Scientific (2009)\nA. Hunt, D. Thomas, and W. Cunningham, The Pragmatic Programmer. From Journeyman to Master, Amsterdam: Addison-Wesley Longman (1999)\nBrian W. Kernighan, The Practice of Programming, Pearson (1999)\nWeinberg, Gerald M., The Psychology of Computer Programming, New York: Van Nostrand Reinhold (1971)\nEdsger W. Dijkstra, A Discipline of Programming, Prentice-Hall (1976)\nO.-J. Dahl, E.W.Dijkstra, C.A.R. Hoare, Structured Programming, Academic Press (1972)\nDavid Gries, The Science of Programming, Springer-Verlag (1981)"
      },
      {
        "heading": "External links",
        "level": 1,
        "content": "\n Media related to Computer programming at Wikimedia Commons\n Quotations related to Programming at Wikiquote"
      }
    ],
    "summary": "Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks. It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages. Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit. Proficient programming usually requires expertise in several different subjects, including knowledge of the application domain, details of programming languages and generic code libraries, specialized algorithms, and formal logic.\nAuxiliary tasks accompanying and related to programming include analyzing requirements, testing, debugging (investigating and fixing problems), implementation of build systems, and management of derived artifacts, such as programs' machine code. While these are sometimes considered programming, often the term software development is used for this larger overall process – with the terms programming, implementation, and coding reserved for the writing and editing of code per se. Sometimes software development is known as software engineering, especially when it employs formal methods or follows an engineering design process."
  },
  {
    "title": "Programación",
    "source": "https://es.wikipedia.org/wiki/Programaci%C3%B3n",
    "language": "es",
    "chunks": [
      {
        "heading": "Funcionamiento de un programa",
        "level": 1,
        "content": "Para crear un programa y que la computadora lo interprete y ejecute, las instrucciones deben escribirse en un lenguaje de programación.\nEl lenguaje entendido por una computadora se conoce como código máquina. Consiste en secuencias de instrucciones básicas que el procesador reconoce, codificadas como cadenas de números 1 y 0 (sistema binario). En los primeros tiempos de la computación se programaba directamente en código máquina. Escribir programas así resultaba demasiado complicado, también era difícil entenderlos y mantenerlos una vez escritos. Con el tiempo, se fueron desarrollando herramientas para facilitar el trabajo.\nLos primeros científicos que trabajaron en el área decidieron reemplazar las secuencias de unos y ceros por mnemónicos, que son abreviaturas en inglés de la función que cumple una instrucción de procesador. Por ejemplo, para sumar se podría usar la letra A de la palabra inglesa add (añadir). Crearon así una familia de lenguajes de mayor nivel, que se conocen como lenguaje ensamblador o simplemente ensamblador (en inglés, assembly). Con el tiempo los ensambladores incorporaron facilidades adicionales, pero siempre manteniendo una correspondencia directa con las instrucciones de procesador. A nivel conceptual, entonces, programar en ensamblador es muy similar a hacerlo en lenguaje máquina, solo que de una forma más amigable.\nA medida que la complejidad de las tareas que realizaban las computadoras aumentaba, el lenguaje ensamblador fue mostrando limitaciones. Para hacer un programa había que conocer en detalle el funcionamiento de la computadora donde se iba a ejecutar, qué instrucciones proveía y cómo emplearlas. A veces las instrucciones eran demasiado básicas, por ejemplo podía haber una para sumar dos números pero no para multiplicar, y entonces era necesario programar un algoritmo que realizara la multiplicación con base en instrucciones más básicas. Otras veces, la forma de emplear las instrucciones era engorrosa. Además, si se usaba otro modelo de computadora, en muchos casos había que reescribir el programa con otras instrucciones. El siguiente paso fue crear los lenguajes de alto nivel.\nUna vez que se termina de escribir un programa, es necesario de alguna forma traducirlo a lenguaje máquina, que es lo único que entiende el procesador. Esta tarea es automática, por medio de un programa adicional que toma el código escrito y lo procesa. Hay distintos enfoques para este procesamiento. El enfoque clásico se llama compilación: el programa toma el código en un lenguaje y genera código en el otro; al programa traductor se lo llama compilador. En general se habla de compilación y compiladores cuando el lenguaje de origen es de alto nivel; si la traducción es desde lenguaje ensamblador, se llama ensamblado y el programa se llama ensamblador (hay que distinguir el lenguaje ensamblador del programa ensamblador; en inglés es más claro, son assembly language y assembler respectivamente).[3]​\nGeneralmente existe una fase posterior a la compilación denominada enlace o enlazado (linking en inglés). Los programas pueden escribirse en partes separadas y además pueden usar recursos provistos por bibliotecas. El enlazado, realizado por un programa llamado enlazador, combina todos los componentes y así genera un programa ejecutable completo.\nEn algunos lenguajes de programación, puede usarse un enfoque diferente que no requiera compilación y enlace: un programa llamado intérprete va leyendo el código y realizando en el momento las acciones que haría el programa. Se evita generar código separado y la experiencia es que se está ejecutando el código en el lenguaje de alto nivel, a pesar de que el procesador no lo entienda de forma nativa."
      },
      {
        "heading": "Léxico y programación",
        "level": 1,
        "content": "La programación se rige por reglas y un conjunto más o menos reducido de órdenes, expresiones, instrucciones y comandos que tienden a asemejarse a una lengua natural acotada (en inglés); y que además tienen la particularidad de una reducida ambigüedad.\nEn los lenguajes de programación se distinguen diversos elementos entre los que se incluyen el léxico propio del lenguaje y las reglas semánticas y sintácticas. Dentro del léxico, generalmente se utilizan símbolos y palabras con funciones específicas dentro del lenguaje. Estas palabras suelen tomarse del inglés y no se las puede utilizar de manera diferente: son las denominadas palabras reservadas. Otra particularidad de los lenguajes es el permitir a los programadores el uso de comentarios: frases o párrafos sin funcionalidad en el programa, que los compiladores o intérpretes descartan y solo están destinados a ser leídos por personas; así se pueden dejar explicaciones que ayuden a entender el código a quien lo lea.[4]​"
      },
      {
        "heading": "Programas y algoritmos",
        "level": 1,
        "content": "Un algoritmo es una secuencia no ambigua, finita y ordenada de instrucciones que han de seguirse para resolver un determinado problema.[1]​ Un programa normalmente implementa y contiene uno o más algoritmos. Un algoritmo puede expresarse de distintas maneras: en forma gráfica, como un diagrama de flujo, en forma de código como en pseudocódigo o un lenguaje de programación, en forma explicativa.\nLos programas suelen subdividirse en partes menores, llamadas módulos, de modo que la complejidad algorítmica de cada una de las partes sea menor que la del programa completo, lo cual ayuda a simplificar el desarrollo del programa. Esta es una práctica muy utilizada y se conoce como \"refino progresivo\".\nSegún Niklaus Wirth, un programa está formado por los algoritmos y estructuras de datos.\nLa programación puede seguir muchos enfoques, o paradigmas, es decir, diversas maneras de formular la resolución de un problema dado. Algunos de los principales paradigmas de programación son:\n\nProgramación declarativa\nProgramación imperativa\nProgramación estructurada\nProgramación modular\nProgramación orientada a objetos\nProgramación orientada a eventos"
      },
      {
        "heading": "Compilación",
        "level": 1,
        "content": "El programa escrito en un lenguaje de programación de alto nivel (fácilmente comprensible por el programador) es llamado programa fuente y no se puede ejecutar directamente en una computadora. La opción más común es compilar el programa obteniendo un módulo objeto, aunque también, si el lenguaje lo soporta, puede ejecutarse en forma directa pero solo a través de un intérprete. Algunos lenguajes, tal como BASIC, disponen de ambas formas de ejecución, lo cual facilita la tarea de depuración y prueba del programa.\nEl código fuente del programa se debe someter a un proceso de traducción para convertirlo a lenguaje máquina o bien a un código intermedio, generando así un módulo denominado \"objeto\". A este proceso se le llama compilación.\nHabitualmente la creación de un programa ejecutable (un típico.exe para Microsoft Windows o DOS) conlleva dos pasos: el primer paso se llama compilación (propiamente dicho) y traduce el código fuente, escrito en un lenguaje de programación y almacenado en un archivo de texto, a código en bajo nivel (normalmente a código objeto, no directamente a lenguaje máquina). El segundo paso se llama enlazado en el cual se enlaza el código de bajo nivel generado de todos los ficheros y subprogramas que se han mandado a compilar y se añade el código de las funciones necesarias que residen en bibliotecas externas, para que el ejecutable pueda comunicarse directamente con el sistema operativo, traduciendo así finalmente el código objeto a código máquina, y generando un módulo ejecutable.\nEstos dos pasos se pueden hacer por separado, almacenando el resultado de la fase de compilación en archivos objetos (un típico.o para Unix,.obj para MS-Windows y DOS); para enlazarlos en fases posteriores, o crear directamente el ejecutable; con lo que la fase de compilación puede almacenarse de forma temporal. Un programa podría tener partes escritas en varios lenguajes, por ejemplo, Java, C, C++ y ensamblador, que se podrían compilar de forma independiente y luego combinarse para formar un único módulo ejecutable."
      },
      {
        "heading": "Programación e ingeniería del software",
        "level": 1,
        "content": "Existe una tendencia a identificar el proceso de creación de un programa informático con la programación, que es cierta cuando se trata de programas pequeños para uso personal, y que dista de la realidad cuando se trata de grandes proyectos.\nEl proceso de creación de software, desde el punto de vista de la ingeniería, incluye mínimamente los siguientes pasos:\n\nReconocer la necesidad de un programa para solucionar un problema o identificar la posibilidad de automatización de una tarea.\nRecolectar los requisitos del programa. Debe quedar claro qué es lo que debe hacer el programa y para qué se necesita.\nRealizar el análisis de los requisitos del programa. Debe quedar claro qué tareas debe realizar el programa. Las pruebas que comprueben la validez del programa se pueden especificar en esta fase.\nDiseñar la arquitectura del programa. Se debe descomponer el programa en partes de complejidad abordable.\nImplementar el programa. Consiste en realizar un diseño detallado, especificando completamente todo el funcionamiento del programa, tras lo cual la codificación (programación propiamente dicha) debería resultar inmediata.\nProbar el programa. Comprobar que pasan pruebas que se han definido en el análisis de requisitos.\nImplantar (instalar) el programa. Consiste en poner el programa en funcionamiento junto con los componentes que sean necesarios (bases de datos, redes de comunicaciones, etc.).\nLa ingeniería del software se centra en los pasos de planificación y diseño del programa, mientras que antiguamente (programación artesanal) la realización de un programa consistía casi únicamente en escribir el código, bajo solo el conocimiento de los requisitos y con una modesta fase de análisis y diseño."
      },
      {
        "heading": "Referencias históricas",
        "level": 1,
        "content": "El trabajo de Ada Lovelace, hija de Anabella Milbanke Byron y Lord Byron, que realizó para la máquina de Babbage le hizo ganarse el título de primera programadora de computadoras del mundo, aunque Babbage nunca completó la construcción de la máquina. El nombre del lenguaje de programación Ada fue escogido como homenaje a esta programadora."
      },
      {
        "heading": "Objetivos de la programación",
        "level": 1,
        "content": "La programación debe perseguir la obtención de programas de calidad. Para ello se establece una serie de factores que determinan la calidad de un programa. Algunos de los factores de calidad más importantes son los siguientes:\n\nCorrectitud. Un programa es correcto si hace lo que debe hacer tal y como se estableció en las fases previas a su desarrollo. Para determinar si un programa hace lo que debe, es muy importante especificar claramente qué debe hacer el programa antes de su desarrollo y, una vez acabado, compararlo con lo que realmente hace. Al verificar este comportamiento está cumpliendo dicho objetivo.\nClaridad. Es muy importante que el programa sea lo más claro y legible posible, para facilitar tanto su desarrollo como su posterior mantenimiento. Al elaborar un programa se debe intentar que su estructura sea sencilla y coherente, así como cuidar el estilo de programación. De esta forma se ve facilitado el trabajo del programador, tanto en la fase de creación como en las fases posteriores de corrección de errores, ampliaciones, modificaciones, etc. Fases que pueden ser realizadas incluso por otro programador, con lo cual la claridad es aún más necesaria para que otros puedan continuar el trabajo fácilmente. Algunos programadores llegan incluso a utilizar Arte ASCII para delimitar secciones de código; una práctica común es realizar aclaraciones en el mismo código fuente utilizando líneas de comentarios. Contrariamente, algunos programadores realizan acciones que tienden a introducir confusión para impedir un análisis cómodo a otros programadores, recurren al uso de código ofuscado,\nEficiencia. Se trata de que el programa, además de realizar aquello para lo que fue creado (es decir, que sea correcto), lo haga gestionando de la mejor forma posible los recursos que utiliza. Normalmente, al hablar de eficiencia de un programa, se suele hacer referencia al tiempo que tarda en realizar la tarea para la que ha sido creado y a la cantidad de memoria que necesita, pero hay otros recursos que también pueden ser de consideración para mejorar la eficiencia de un programa, dependiendo de su naturaleza (espacio en disco que utiliza, tráfico en la red que genera, etc.).\nPortabilidad. Un programa es portable cuando tiene la capacidad de poder ejecutarse en una plataforma, ya sea hardware o software, diferente a aquella en la que se desarrolló. La portabilidad es una característica muy deseable para un programa, ya que permite, por ejemplo, a un programa que se ha elaborado para el sistema GNU/Linux que también pueda ejecutarse en la familia de sistemas operativos Windows. Consecuentemente el programa puede llegar a más usuarios."
      },
      {
        "heading": "Ciclo de vida del software",
        "level": 1,
        "content": "El término ciclo de vida del software describe el desarrollo de software, desde la fase inicial hasta la fase final, incluyendo su estado funcional. El propósito es definir las distintas fases intermedias que se requieren para validar el desarrollo de la aplicación, es decir, para garantizar que el software cumpla los requisitos para la aplicación y verificación de los procedimientos de desarrollo: se asegura que los métodos utilizados son apropiados. Estos métodos se originan en el hecho de que es muy costoso corregir los errores que se detectan tarde dentro de la fase de implementación (programación propiamente dicha), o peor aún, durante la fase funcional. En el modelo de ciclo de vida se intenta que los errores se detecten lo antes posible y por lo tanto, permite a los desarrolladores concentrarse en la calidad del software, en los plazos de implementación y en los costos asociados. El ciclo de vida básico de un software consta de, al menos, los siguientes procedimientos:\n\nAnálisis de requisitos, viabilidad de diseño y especificación de funciones definidas en lenguaje de programación.\nAnálisis de la arquitectura en la creación, desarrollo, corrección e implementación del sistema.\nPruebas en la integración de módulos, y subprograma(s) con cada conjunto o subconjunto.\nPruebas beta o de validación que garanticen que en el procedimiento de ejecución del software se cumple con todas las especificaciones originales.\nMantenimiento de corrección de errores y restricciones.\nDocumentación de toda la información.\nEl orden y la presencia de cada uno de estos procedimientos dependen del tipo de modelo de ciclo de vida acordado entre el cliente y el equipo de desarrolladores. En el caso del software libre se tiene un ciclo de vida mucho más dinámico, puesto que muchos programadores trabajan en simultáneo desarrollando sus eliminaciones."
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": "Portal:Programación. Contenido relacionado con Programación.\nWikiproyecto:Informática/Programación\nerror de software\nfilosofías del desarrollo de software\nhistoria de la ingeniería del software\ningeniería en computación\ningeniería en informática\nlínea de código fuente\nlenguaje de programación\nprogramación automática\nprogramación dirigida por eventos\nprogramación estructurada\nprogramación extrema\nprogramación en pareja\nprogramación dinámica\nprogramación orientada a objetos\npruebas de software\nsoftware"
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": " Wikimedia Commons alberga una categoría multimedia sobre Programación.\n Wikcionario  tiene definiciones y otra información sobre programación.\n Wikilibros alberga un libro o manual sobre Fundamentos de programación."
      }
    ],
    "summary": "La programación es el proceso de crear un conjunto de instrucciones que le dicen a una computadora como realizar algún tipo de tarea. Pero no solo la acción de escribir un código para que la computadora o el software lo ejecute. Incluye, además, todas las tareas necesarias para que el código funcione correctamente y cumpla el objetivo para el cual se escribió.[1]​\nEn la actualidad, la noción de programación se encuentra muy asociada a la creación de aplicaciones de informática y videojuegos. En este sentido, es el proceso por el cual una persona desarrolla un programa, valiéndose de una herramienta que le permita escribir el código (el cual puede estar en uno o varios lenguajes, como C++, Java y Python, entre muchos otros) y de otra que sea capaz de “traducirlo” a lo que se conoce como lenguaje de máquina, que puede \"comprender\" el microprocesador.[2]​"
  },
  {
    "title": "Cell (biology)",
    "source": "https://en.wikipedia.org/wiki/Cell_(biology)",
    "language": "en",
    "chunks": [
      {
        "heading": "Cell types",
        "level": 1,
        "content": "Cells are broadly categorized into two types: eukaryotic cells, which possess a nucleus, and prokaryotic cells, which lack a nucleus but have a nucleoid region. Prokaryotes are single-celled organisms, whereas eukaryotes can be either single-celled or multicellular."
      },
      {
        "heading": "Prokaryotic cells",
        "level": 2,
        "content": "Prokaryotes include bacteria and archaea, two of the three domains of life. Prokaryotic cells were the first form of life on Earth, characterized by having vital biological processes including cell signaling. They are simpler and smaller than eukaryotic cells, and lack a nucleus, and other membrane-bound organelles. The DNA of a prokaryotic cell consists of a single circular chromosome that is in direct contact with the cytoplasm. The nuclear region in the cytoplasm is called the nucleoid. Most prokaryotes are the smallest of all organisms, ranging from 0.5 to 2.0 μm in diameter.: 78 \nA prokaryotic cell has three regions:\n\nEnclosing the cell is the cell envelope, generally consisting of a plasma membrane covered by a cell wall which, for some bacteria, may be further covered by a third layer called a capsule. Though most prokaryotes have both a cell membrane and a cell wall, there are exceptions such as Mycoplasma (bacteria) and Thermoplasma (archaea) which only possess the cell membrane layer. The envelope gives rigidity to the cell and separates the interior of the cell from its environment, serving as a protective filter. The cell wall consists of peptidoglycan in bacteria and acts as an additional barrier against exterior forces. It also prevents the cell from expanding and bursting (cytolysis) from osmotic pressure due to a hypotonic environment. Some eukaryotic cells (plant cells and fungal cells) also have a cell wall.\nInside the cell is the cytoplasmic region that contains the genome (DNA), ribosomes and various sorts of inclusions. The genetic material is freely found in the cytoplasm. Prokaryotes can carry extrachromosomal DNA elements called plasmids, which are usually circular. Linear bacterial plasmids have been identified in several species of spirochete bacteria, including members of the genus Borrelia notably Borrelia burgdorferi, which causes Lyme disease. Though not forming a nucleus, the DNA is condensed in a nucleoid. Plasmids encode additional genes, such as antibiotic resistance genes.\nOn the outside, some prokaryotes have flagella and pili that project from the cell's surface. These are structures made of proteins that facilitate movement and communication between cells."
      },
      {
        "heading": "Eukaryotic cells",
        "level": 2,
        "content": "Plants, animals, fungi, slime moulds, protozoa, and algae are all eukaryotic. These cells are about fifteen times wider than a typical prokaryote and can be as much as a thousand times greater in volume. The main distinguishing feature of eukaryotes as compared to prokaryotes is compartmentalization: the presence of membrane-bound organelles (compartments) in which specific activities take place. Most important among these is a cell nucleus, an organelle that houses the cell's DNA. This nucleus gives the eukaryote its name, which means \"true kernel (nucleus)\". Some of the other differences are:\n\nThe plasma membrane resembles that of prokaryotes in function, with minor differences in the setup. Cell walls may or may not be present.\nThe eukaryotic DNA is organized in one or more linear molecules, called chromosomes, which are associated with histone proteins. All chromosomal DNA is stored in the cell nucleus, separated from the cytoplasm by a membrane. Some eukaryotic organelles such as mitochondria also contain some DNA.\nMany eukaryotic cells are ciliated with primary cilia. Primary cilia play important roles in chemosensation, mechanosensation, and thermosensation. Each cilium may thus be \"viewed as a sensory cellular antennae that coordinates a large number of cellular signaling pathways, sometimes coupling the signaling to ciliary motility or alternatively to cell division and differentiation.\"\nMotile eukaryotes can move using motile cilia or flagella. Motile cells are absent in conifers and flowering plants. Eukaryotic flagella are more complex than those of prokaryotes.\n\nMany groups of eukaryotes are single-celled. Among the many-celled groups are animals and plants. The number of cells in these groups vary with species; it has been estimated that the human body contains around 37 trillion (3.72×1013) cells, and more recent studies put this number at around 30 trillion (~36 trillion cells in the male, ~28 trillion in the female)."
      },
      {
        "heading": "Subcellular components",
        "level": 1,
        "content": "All cells, whether prokaryotic or eukaryotic, have a membrane that envelops the cell, regulates what moves in and out (selectively permeable), and maintains the electric potential of the cell. Inside the membrane, the cytoplasm takes up most of the cell's volume. Except red blood cells, which lack a cell nucleus and most organelles to accommodate maximum space for hemoglobin, all cells possess DNA, the hereditary material of genes, and RNA, containing the information necessary to build various proteins such as enzymes, the cell's primary machinery. There are also other kinds of biomolecules in cells. This article lists these primary cellular components, then briefly describes their function."
      },
      {
        "heading": "Cell membrane",
        "level": 2,
        "content": "The cell membrane, or plasma membrane, is a selectively permeable biological membrane that surrounds the cytoplasm of a cell. In animals, the plasma membrane is the outer boundary of the cell, while in plants and prokaryotes it is usually covered by a cell wall. This membrane serves to separate and protect a cell from its surrounding environment and is made mostly from a double layer of phospholipids, which are amphiphilic (partly hydrophobic and partly hydrophilic). Hence, the layer is called a phospholipid bilayer, or sometimes a fluid mosaic membrane. Embedded within this membrane is a macromolecular structure called the porosome the universal secretory portal in cells and a variety of protein molecules that act as channels and pumps that move different molecules into and out of the cell. The membrane is semi-permeable, and selectively permeable, in that it can either let a substance (molecule or ion) pass through freely, to a limited extent or not at all. Cell surface membranes also contain receptor proteins that allow cells to detect external signaling molecules such as hormones."
      },
      {
        "heading": "Cytoskeleton",
        "level": 2,
        "content": "The cytoskeleton acts to organize and maintain the cell's shape; anchors organelles in place; helps during endocytosis, the uptake of external materials by a cell, and cytokinesis, the separation of daughter cells after cell division; and moves parts of the cell in processes of growth and mobility. The eukaryotic cytoskeleton is composed of microtubules, intermediate filaments and microfilaments. In the cytoskeleton of a neuron the intermediate filaments are known as neurofilaments. There are a great number of proteins associated with them, each controlling a cell's structure by directing, bundling, and aligning filaments. The prokaryotic cytoskeleton is less well-studied but is involved in the maintenance of cell shape, polarity and cytokinesis. The subunit protein of microfilaments is a small, monomeric protein called actin. The subunit of microtubules is a dimeric molecule called tubulin. Intermediate filaments are heteropolymers whose subunits vary among the cell types in different tissues. Some of the subunit proteins of intermediate filaments include vimentin, desmin, lamin (lamins A, B and C), keratin (multiple acidic and basic keratins), and neurofilament proteins (NF–L, NF–M)."
      },
      {
        "heading": "Genetic material",
        "level": 2,
        "content": "Two different kinds of genetic material exist: deoxyribonucleic acid (DNA) and ribonucleic acid (RNA). Cells use DNA for their long-term information storage. The biological information contained in an organism is encoded in its DNA sequence. RNA is used for information transport (e.g., mRNA) and enzymatic functions (e.g., ribosomal RNA). Transfer RNA (tRNA) molecules are used to add amino acids during protein translation.\nProkaryotic genetic material is organized in a simple circular bacterial chromosome in the nucleoid region of the cytoplasm. Eukaryotic genetic material is divided into different, linear molecules called chromosomes inside a discrete nucleus, usually with additional genetic material in some organelles like mitochondria and chloroplasts (see endosymbiotic theory).\nA human cell has genetic material contained in the cell nucleus (the nuclear genome) and in the mitochondria (the mitochondrial genome). In humans, the nuclear genome is divided into 46 linear DNA molecules called chromosomes, including 22 homologous chromosome pairs and a pair of sex chromosomes. The mitochondrial genome is a circular DNA molecule distinct from nuclear DNA. Although the mitochondrial DNA is very small compared to nuclear chromosomes, it codes for 13 proteins involved in mitochondrial energy production and specific tRNAs.\nForeign genetic material (most commonly DNA) can also be artificially introduced into the cell by a process called transfection. This can be transient, if the DNA is not inserted into the cell's genome, or stable, if it is. Certain viruses also insert their genetic material into the genome."
      },
      {
        "heading": "Organelles",
        "level": 2,
        "content": "Organelles are parts of the cell that are adapted and/or specialized for carrying out one or more vital functions, analogous to the organs of the human body (such as the heart, lung, and kidney, with each organ performing a different function). Both eukaryotic and prokaryotic cells have organelles, but prokaryotic organelles are generally simpler and are not membrane-bound.\nThere are several types of organelles in a cell. Some (such as the nucleus and Golgi apparatus) are typically solitary, while others (such as mitochondria, chloroplasts, peroxisomes and lysosomes) can be numerous (hundreds to thousands). The cytosol is the gelatinous fluid that fills the cell and surrounds the organelles."
      },
      {
        "heading": "Eukaryotic",
        "level": 3,
        "content": "Cell nucleus: A cell's information center, the cell nucleus is the most conspicuous organelle found in a eukaryotic cell. It houses the cell's chromosomes, and is the place where almost all DNA replication and RNA synthesis (transcription) occur. The nucleus is spherical and separated from the cytoplasm by a double membrane called the nuclear envelope, space between these two membrane is called perinuclear space. The nuclear envelope isolates and protects a cell's DNA from various molecules that could accidentally damage its structure or interfere with its processing. During processing, DNA is transcribed, or copied into a special RNA, called messenger RNA (mRNA). This mRNA is then transported out of the nucleus, where it is translated into a specific protein molecule. The nucleolus is a specialized region within the nucleus where ribosome subunits are assembled. In prokaryotes, DNA processing takes place in the cytoplasm.\nMitochondria and chloroplasts: generate energy for the cell. Mitochondria are self-replicating double membrane-bound organelles that occur in various numbers, shapes, and sizes in the cytoplasm of all eukaryotic cells. Respiration occurs in the cell mitochondria, which generate the cell's energy by oxidative phosphorylation, using oxygen to release energy stored in cellular nutrients (typically pertaining to glucose) to generate ATP (aerobic respiration). Mitochondria multiply by binary fission, like prokaryotes. Chloroplasts can only be found in plants and algae, and they capture the sun's energy to make carbohydrates through photosynthesis.\n\nEndoplasmic reticulum: The endoplasmic reticulum (ER) is a transport network for molecules targeted for certain modifications and specific destinations, as compared to molecules that float freely in the cytoplasm. The ER has two forms: the rough ER, which has ribosomes on its surface that secrete proteins into the ER, and the smooth ER, which lacks ribosomes. The smooth ER plays a role in calcium sequestration and release and also helps in synthesis of lipid.\nGolgi apparatus: The primary function of the Golgi apparatus is to process and package the macromolecules such as proteins and lipids that are synthesized by the cell.\nLysosomes and peroxisomes: Lysosomes contain digestive enzymes (acid hydrolases). They digest excess or worn-out organelles, food particles, and engulfed viruses or bacteria. Peroxisomes have enzymes that rid the cell of toxic peroxides, Lysosomes are optimally active in an acidic environment. The cell could not house these destructive enzymes if they were not contained in a membrane-bound system.\nCentrosome: the cytoskeleton organizer: The centrosome produces the microtubules of a cell—a key component of the cytoskeleton. It directs the transport through the ER and the Golgi apparatus. Centrosomes are composed of two centrioles which lie perpendicular to each other in which each has an organization like a cartwheel, which separate during cell division and help in the formation of the mitotic spindle. A single centrosome is present in the animal cells. They are also found in some fungi and algae cells.\nVacuoles: Vacuoles sequester waste products and in plant cells store water. They are often described as liquid filled spaces and are surrounded by a membrane. Some cells, most notably Amoeba, have contractile vacuoles, which can pump water out of the cell if there is too much water. The vacuoles of plant cells and fungal cells are usually larger than those of animal cells. Vacuoles of plant cells are surrounded by a membrane which transports ions against concentration gradients."
      },
      {
        "heading": "Eukaryotic and prokaryotic",
        "level": 3,
        "content": "Ribosomes: The ribosome is a large complex of RNA and protein molecules. They each consist of two subunits, and act as an assembly line where RNA from the nucleus is used to synthesise proteins from amino acids. Ribosomes can be found either floating freely or bound to a membrane (the rough endoplasmatic reticulum in eukaryotes, or the cell membrane in prokaryotes).\nPlastids: Plastid are membrane-bound organelle generally found in plant cells and euglenoids and contain specific pigments, thus affecting the colour of the plant and organism. And these pigments also helps in food storage and tapping of light energy. There are three types of plastids based upon the specific pigments. Chloroplasts contain chlorophyll and some carotenoid pigments which helps in the tapping of light energy during photosynthesis. Chromoplasts contain fat-soluble carotenoid pigments like orange carotene and yellow xanthophylls which helps in synthesis and storage. Leucoplasts are non-pigmented plastids and helps in storage of nutrients."
      },
      {
        "heading": "Structures outside the cell membrane",
        "level": 1,
        "content": "Many cells also have structures which exist wholly or partially outside the cell membrane. These structures are notable because they are not protected from the external environment by the cell membrane. In order to assemble these structures, their components must be carried across the cell membrane by export processes."
      },
      {
        "heading": "Cell wall",
        "level": 2,
        "content": "Many types of prokaryotic and eukaryotic cells have a cell wall. The cell wall acts to protect the cell mechanically and chemically from its environment, and is an additional layer of protection to the cell membrane. Different types of cell have cell walls made up of different materials; plant cell walls are primarily made up of cellulose, fungi cell walls are made up of chitin and bacteria cell walls are made up of peptidoglycan."
      },
      {
        "heading": "Prokaryotic",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Capsule",
        "level": 3,
        "content": "A gelatinous capsule is present in some bacteria outside the cell membrane and cell wall. The capsule may be polysaccharide as in pneumococci, meningococci or polypeptide as Bacillus anthracis or hyaluronic acid as in streptococci.\nCapsules are not marked by normal staining protocols and can be detected by India ink or methyl blue, which allows for higher contrast between the cells for observation.: 87"
      },
      {
        "heading": "Flagella",
        "level": 3,
        "content": "Flagella are organelles for cellular mobility. The bacterial flagellum stretches from cytoplasm through the cell membrane(s) and extrudes through the cell wall. They are long and thick thread-like appendages, protein in nature. A different type of flagellum is found in archaea and a different type is found in eukaryotes."
      },
      {
        "heading": "Fimbriae",
        "level": 3,
        "content": "A fimbria (plural fimbriae also known as a pilus, plural pili) is a short, thin, hair-like filament found on the surface of bacteria. Fimbriae are formed of a protein called pilin (antigenic) and are responsible for the attachment of bacteria to specific receptors on human cells (cell adhesion). There are special types of pili involved in bacterial conjugation."
      },
      {
        "heading": "Cellular processes",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Replication",
        "level": 2,
        "content": "Cell division involves a single cell (called a mother cell) dividing into two daughter cells. This leads to growth in multicellular organisms (the growth of tissue) and to procreation (vegetative reproduction) in unicellular organisms. Prokaryotic cells divide by binary fission, while eukaryotic cells usually undergo a process of nuclear division, called mitosis, followed by division of the cell, called cytokinesis. A diploid cell may also undergo meiosis to produce haploid cells, usually four. Haploid cells serve as gametes in multicellular organisms, fusing to form new diploid cells.\nDNA replication, or the process of duplicating a cell's genome, always happens when a cell divides through mitosis or binary fission. This occurs during the S phase of the cell cycle.\nIn meiosis, the DNA is replicated only once, while the cell divides twice. DNA replication only occurs before meiosis I. DNA replication does not occur when the cells divide the second time, in meiosis II. Replication, like all cellular activities, requires specialized proteins for carrying out the job."
      },
      {
        "heading": "DNA repair",
        "level": 2,
        "content": "Cells of all organisms contain enzyme systems that scan their DNA for damage and carry out repair processes when it is detected. Diverse repair processes have evolved in organisms ranging from bacteria to humans. The widespread prevalence of these repair processes indicates the importance of maintaining cellular DNA in an undamaged state in order to avoid cell death or errors of replication due to damage that could lead to mutation. E. coli bacteria are a well-studied example of a cellular organism with diverse well-defined DNA repair processes. These include: nucleotide excision repair, DNA mismatch repair, non-homologous end joining of double-strand breaks, recombinational repair and light-dependent repair (photoreactivation)."
      },
      {
        "heading": "Growth and metabolism",
        "level": 2,
        "content": "Between successive cell divisions, cells grow through the functioning of cellular metabolism. Cell metabolism is the process by which individual cells process nutrient molecules. Metabolism has two distinct divisions: catabolism, in which the cell breaks down complex molecules to produce energy and reducing power, and anabolism, in which the cell uses energy and reducing power to construct complex molecules and perform other biological functions.\nComplex sugars can be broken down into simpler sugar molecules called monosaccharides such as glucose. Once inside the cell, glucose is broken down to make adenosine triphosphate (ATP), a molecule that possesses readily available energy, through two different pathways. In plant cells, chloroplasts create sugars by photosynthesis, using the energy of light to join molecules of water and carbon dioxide."
      },
      {
        "heading": "Protein synthesis",
        "level": 2,
        "content": "Cells are capable of synthesizing new proteins, which are essential for the modulation and maintenance of cellular activities. This process involves the formation of new protein molecules from amino acid building blocks based on information encoded in DNA/RNA. Protein synthesis generally consists of two major steps: transcription and translation.\nTranscription is the process where genetic information in DNA is used to produce a complementary RNA strand. This RNA strand is then processed to give messenger RNA (mRNA), which is free to migrate through the cell. mRNA molecules bind to protein-RNA complexes called ribosomes located in the cytosol, where they are translated into polypeptide sequences. The ribosome mediates the formation of a polypeptide sequence based on the mRNA sequence. The mRNA sequence directly relates to the polypeptide sequence by binding to transfer RNA (tRNA) adapter molecules in binding pockets within the ribosome. The new polypeptide then folds into a functional three-dimensional protein molecule."
      },
      {
        "heading": "Motility",
        "level": 2,
        "content": "Unicellular organisms can move in order to find food or escape predators. Common mechanisms of motion include flagella and cilia.\nIn multicellular organisms, cells can move during processes such as wound healing, the immune response and cancer metastasis. For example, in wound healing in animals, white blood cells move to the wound site to kill the microorganisms that cause infection. Cell motility involves many receptors, crosslinking, bundling, binding, adhesion, motor and other proteins. The process is divided into three steps: protrusion of the leading edge of the cell, adhesion of the leading edge and de-adhesion at the cell body and rear, and cytoskeletal contraction to pull the cell forward. Each step is driven by physical forces generated by unique segments of the cytoskeleton."
      },
      {
        "heading": "Navigation, control and communication",
        "level": 3,
        "content": "In August 2020, scientists described one way cells—in particular cells of a slime mold and mouse pancreatic cancer-derived cells—are able to navigate efficiently through a body and identify the best routes through complex mazes: generating gradients after breaking down diffused chemoattractants which enable them to sense upcoming maze junctions before reaching them, including around corners."
      },
      {
        "heading": "Multicellularity",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Cell specialization/differentiation",
        "level": 2,
        "content": "Multicellular organisms are organisms that consist of more than one cell, in contrast to single-celled organisms.\nIn complex multicellular organisms, cells specialize into different cell types that are adapted to particular functions. In mammals, major cell types include skin cells, muscle cells, neurons, blood cells, fibroblasts, stem cells, and others. Cell types differ both in appearance and function, yet are genetically identical. Cells are able to be of the same genotype but of different cell type due to the differential expression of the genes they contain.\nMost distinct cell types arise from a single totipotent cell, called a zygote, that differentiates into hundreds of different cell types during the course of development. Differentiation of cells is driven by different environmental cues (such as cell–cell interaction) and intrinsic differences (such as those caused by the uneven distribution of molecules during division)."
      },
      {
        "heading": "Origin of multicellularity",
        "level": 2,
        "content": "Multicellularity has evolved independently at least 25 times, including in some prokaryotes, like cyanobacteria, myxobacteria, actinomycetes, or Methanosarcina. However, complex multicellular organisms evolved only in six eukaryotic groups: animals, fungi, brown algae, red algae, green algae, and plants. It evolved repeatedly for plants (Chloroplastida), once or twice for animals, once for brown algae, and perhaps several times for fungi, slime molds, and red algae. Multicellularity may have evolved from colonies of interdependent organisms, from cellularization, or from organisms in symbiotic relationships.\nThe first evidence of multicellularity is from cyanobacteria-like organisms that lived between 3 and 3.5 billion years ago. Other early fossils of multicellular organisms include the contested Grypania spiralis and the fossils of the black shales of the Palaeoproterozoic Francevillian Group Fossil B Formation in Gabon.\nThe evolution of multicellularity from unicellular ancestors has been replicated in the laboratory, in evolution experiments using predation as the selective pressure."
      },
      {
        "heading": "Origins",
        "level": 1,
        "content": "The origin of cells has to do with the origin of life, which began the history of life on Earth."
      },
      {
        "heading": "Origin of life",
        "level": 2,
        "content": "Small molecules needed for life may have been carried to Earth on meteorites, created at deep-sea vents, or synthesized by lightning in a reducing atmosphere. There is little experimental data defining what the first self-replicating forms were. RNA may have been the earliest self-replicating molecule, as it can both store genetic information and catalyze chemical reactions.\nCells emerged around 4 billion years ago. The first cells were most likely heterotrophs. The early cell membranes were probably simpler and more permeable than modern ones, with only a single fatty acid chain per lipid. Lipids spontaneously form bilayered vesicles in water, and could have preceded RNA."
      },
      {
        "heading": "First eukaryotic cells",
        "level": 2,
        "content": "Eukaryotic cells were created some 2.2 billion years ago in a process called eukaryogenesis. This is widely agreed to have involved symbiogenesis, in which archaea and bacteria came together to create the first eukaryotic common ancestor. This cell had a new level of complexity and capability, with a nucleus and facultatively aerobic mitochondria. It evolved some 2 billion years ago into a population of single-celled organisms that included the last eukaryotic common ancestor, gaining capabilities along the way, though the sequence of the steps involved has been disputed, and may not have started with symbiogenesis. It featured at least one centriole and cilium, sex (meiosis and syngamy), peroxisomes, and a dormant cyst with a cell wall of chitin and/or cellulose. In turn, the last eukaryotic common ancestor gave rise to the eukaryotes' crown group, containing the ancestors of animals, fungi, plants, and a diverse range of single-celled organisms. The plants were created around 1.6 billion years ago with a second episode of symbiogenesis that added chloroplasts, derived from cyanobacteria."
      },
      {
        "heading": "History of research",
        "level": 1,
        "content": "In 1665, Robert Hooke examined a thin slice of cork under his microscope, and saw a structure of small enclosures. He wrote \"I could exceeding plainly perceive it to be all perforated and porous, much like a Honey-comb, but that the pores of it were not regular\". To further support his theory, Matthias Schleiden and Theodor Schwann both also studied cells of both animal and plants. What they discovered were significant differences between the two types of cells. This put forth the idea that cells were not only fundamental to plants, but animals as well.\n\n1632–1723: Antonie van Leeuwenhoek taught himself to make lenses, constructed basic optical microscopes and drew protozoa, such as Vorticella from rain water, and bacteria from his own mouth.\n1665: Robert Hooke discovered cells in cork, then in living plant tissue using an early compound microscope. He coined the term cell (from Latin cellula, meaning \"small room\") in his book Micrographia (1665).\n1839: Theodor Schwann and Matthias Jakob Schleiden elucidated the principle that plants and animals are made of cells, concluding that cells are a common unit of structure and development, and thus founding the cell theory.\n1855: Rudolf Virchow stated that new cells come from pre-existing cells by cell division (omnis cellula ex cellula).\n1931: Ernst Ruska built the first transmission electron microscope (TEM) at the University of Berlin. By 1935, he had built an EM with twice the resolution of a light microscope, revealing previously unresolvable organelles.\n1981: Lynn Margulis published Symbiosis in Cell Evolution detailing how eukaryotic cells were created by symbiogenesis."
      },
      {
        "heading": "See also",
        "level": 1,
        "content": ""
      },
      {
        "heading": "References",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Further reading",
        "level": 1,
        "content": ""
      },
      {
        "heading": "External links",
        "level": 1,
        "content": "\nMBInfo – Descriptions on Cellular Functions and Processes\nInside the Cell Archived 2017-07-20 at the Wayback Machine – a science education booklet by National Institutes of Health, in PDF and ePub.\nCell Biology in \"The Biology Project\" of University of Arizona.\nCentre of the Cell online\nThe Image & Video Library of The American Society for Cell Biology Archived 2011-06-10 at the Wayback Machine, a collection of peer-reviewed still images, video clips and digital books that illustrate the structure, function and biology of the cell.\nWormWeb.org: Interactive Visualization of the C. elegans Cell lineage – Visualize the entire cell lineage tree of the nematode C. elegans"
      }
    ],
    "summary": "The cell is the basic structural and functional unit of all forms of life. Every cell consists of cytoplasm enclosed within a membrane; many cells contain organelles, each with a specific function. The term comes from the Latin word cellula meaning 'small room'. Most cells are only visible under a microscope. Cells emerged on Earth about 4 billion years ago. All cells are capable of replication, protein synthesis, and motility. \nCells are broadly categorized into two types: eukaryotic cells, which possess a nucleus, and prokaryotic cells, which lack a nucleus but have a nucleoid region. Prokaryotes are single-celled organisms such as bacteria, whereas eukaryotes can be either single-celled, such as amoebae, or multicellular, such as some algae, plants, animals, and fungi. Eukaryotic cells contain organelles including mitochondria, which provide energy for cell functions, chloroplasts, which in plants create sugars by photosynthesis, and ribosomes, which synthesise proteins.\nCells were discovered by Robert Hooke in 1665, who named them after their resemblance to cells inhabited by Christian monks in a monastery. Cell theory, developed in 1839 by Matthias Jakob Schleiden and Theodor Schwann, states that all organisms are composed of one or more cells, that cells are the fundamental unit of structure and function in all living organisms, and that all cells come from pre-existing cells."
  },
  {
    "title": "Célula",
    "source": "https://es.wikipedia.org/wiki/C%C3%A9lula",
    "language": "es",
    "chunks": [
      {
        "heading": "Tipos celulares",
        "level": 1,
        "content": "Existen dos grandes tipos celulares:\n\nCélula procariota, propia de los procariontes, que comprende las células de arqueas y bacterias.\nCélula eucariota, propia de los eucariontes, tales como la célula animal, célula vegetal, y las células de hongos y protistas."
      },
      {
        "heading": "Historia y teoría celular",
        "level": 1,
        "content": "La historia de la biología celular ha estado ligada al desarrollo tecnológico que pudiera sustentar su estudio. De este modo, el primer acercamiento a su morfología se inicia con la popularización del microscopio rudimentario de lentes compuestas en el siglo XVII, se suplementa con diversas técnicas histológicas para microscopía óptica en los siglos XIX y XX y alcanza un mayor nivel resolutivo mediante los estudios de microscopía electrónica, de fluorescencia y confocal, entre otros, ya en el siglo XX. El desarrollo de herramientas moleculares, basadas en el manejo de ácidos nucleicos y enzimas permitieron un análisis más exhaustivo a lo largo del siglo XX.[8]​"
      },
      {
        "heading": "Descubrimiento",
        "level": 2,
        "content": "Las primeras aproximaciones al estudio de la célula surgieron en el siglo XVII;[9]​ tras el desarrollo a finales del siglo XVI de los primeros microscopios.[10]​ Estos permitieron realizar numerosas observaciones, que condujeron en apenas doscientos años a un conocimiento morfológico relativamente aceptable. A continuación se enumera una breve cronología de tales descubrimientos:\n\n1665: Robert Hooke publicó los resultados de sus observaciones sobre tejidos vegetales, como el corcho, realizadas con un microscopio de 50 aumentos construido por él mismo. Este investigador fue el primero que, al ver en esos tejidos unidades que se repetían a modo de celdillas de un panal, las bautizó como elementos de repetición, «células» (del latín cellulae, celdillas). Pero Hooke solo pudo observar células muertas por lo que no pudo describir las estructuras de su interior.[11]​\nDécada de 1670: Anton van Leeuwenhoek observó diversas células eucariotas (como protozoos y espermatozoides) y procariotas (bacterias).\n1745: John Needham describió la presencia de «animálculos» o «infusorios»; se trataba de organismos unicelulares.\n\nDécada de 1830: Theodor Schwann estudió la célula animal; junto con Matthias Schleiden postularon que las células son las unidades elementales en la formación de las plantas y animales, y que son la base fundamental del proceso vital.\n1831: Robert Brown describió el núcleo celular.\n1839: Purkinje observó el citoplasma celular.\n1857: Kölliker identificó las mitocondrias.\n1858: Rudolf Virchow postuló que todas las células provienen de otras células.\n1860: Pasteur realizó multitud de estudios sobre el metabolismo de levaduras y sobre la asepsia.\n1880: August Weismann descubrió que las células actuales comparten similitud estructural y molecular con células de tiempos remotos.\n1931: Ernst Ruska construyó el primer microscopio electrónico de transmisión en la Universidad de Berlín. Cuatro años más tarde, obtuvo una resolución óptica doble a la del microscopio óptico.\n1981: Lynn Margulis publica su hipótesis sobre la endosimbiosis serial, que explica el origen de la célula eucariota.[12]​"
      },
      {
        "heading": "Teoría celular",
        "level": 2,
        "content": "El concepto de célula como unidad anatómica y funcional de los organismos surgió entre los años 1830 y 1880, aunque fue en el siglo XVII cuando Robert Hooke describió por vez primera la existencia de las mismas, al observar en una preparación vegetal la presencia de una estructura organizada que derivaba de la arquitectura de las paredes celulares vegetales. En 1830 se disponía ya de microscopios con una óptica más avanzada, lo que permitió a investigadores como Theodor Schwann y Matthias Schleiden definir los postulados de la teoría celular, la cual afirma, entre otras cosas:\n\nQue la célula es una unidad morfológica de todo ser vivo: es decir, que en los seres vivos todo está formado por células o por sus productos de secreción.\nEste primer postulado sería completado por Rudolf Virchow con la afirmación Omnis cellula ex cellula, la cual indica que toda célula deriva de una célula precedente (biogénesis). En otras palabras, este postulado constituye la refutación de la teoría de generación espontánea o ex novo, que hipotetizaba la posibilidad de que se generara vida a partir de elementos inanimados.[13]​\nUn tercer postulado de la teoría celular indica que las funciones vitales de los organismos ocurren dentro de las células, o en su entorno inmediato, y son controladas por sustancias que ellas secretan. Cada célula es un sistema abierto, que intercambia materia y energía con su medio. En una célula ocurren todas las funciones vitales, de manera que basta una sola de ellas para que haya un ser vivo (que será un individuo unicelular). Así pues, la célula es la unidad fisiológica de la vida.\nEl cuarto postulado expresa que cada célula contiene toda la información hereditaria necesaria para el control de su propio ciclo y del desarrollo y el funcionamiento de un organismo de su especie, así como para la transmisión de esa información a la siguiente generación celular.[14]​"
      },
      {
        "heading": "Definición",
        "level": 2,
        "content": "Se define a la célula como la unidad morfológica y funcional de todo ser vivo. De hecho, la célula es el elemento de menor tamaño que puede considerarse vivo. Como tal posee una membrana de fosfolípidos con permeabilidad selectiva que mantiene un medio interno altamente ordenado y diferenciado del medio externo en cuanto a su composición, sujeta a control homeostático, la cual consiste en biomoléculas y algunos metales y electrolitos. La estructura se automantiene activamente mediante el metabolismo, asegurándose la coordinación de todos los elementos celulares y su perpetuación por replicación a través de un genoma codificado por ácidos nucleicos. La parte de la biología que se ocupa de ella es la citología."
      },
      {
        "heading": "Características",
        "level": 1,
        "content": "Las células, como sistemas termodinámicos complejos, poseen una serie de elementos estructurales y funcionales comunes que posibilitan su supervivencia; no obstante, los distintos tipos celulares presentan modificaciones de estas características comunes que permiten su especialización funcional y, por ello, la ganancia de complejidad.[15]​ De este modo, las células permanecen altamente organizadas a costa de incrementar la entropía del entorno, uno de los requisitos de la vida.[16]​"
      },
      {
        "heading": "Características estructurales",
        "level": 2,
        "content": "Individualidad: Todas las células están rodeadas de una envoltura (que puede ser una bicapa lipídica desnuda, en células animales; una pared de polisacárido, en hongos y vegetales; una membrana externa y otros elementos que definen una pared compleja, en bacterias Gram negativas; una pared de peptidoglicano, en bacterias Gram positivas; o una pared de variada composición, en arqueas)[9]​ que las separa y comunica con el exterior, que controla los movimientos celulares y que mantiene el potencial de membrana.\nContienen un medio interno acuoso, el citosol, que forma la mayor parte del volumen celular y en el que están inmersos los orgánulos celulares.\nPoseen material genético en forma de ADN, el material hereditario de los genes, que contiene las instrucciones para el funcionamiento celular, así como ARN, a fin de que el primero se exprese.[17]​\nTienen enzimas y otras proteínas, que sustentan, junto con otras biomoléculas, un metabolismo activo."
      },
      {
        "heading": "Características funcionales",
        "level": 2,
        "content": "Las células vivas son un sistema bioquímico complejo. Las características que permiten diferenciar las células de los sistemas químicos no vivos son:\n\nNutrición. Las células toman sustancias del medio, las transforman de una forma a otra, liberan energía y eliminan productos de desecho, mediante el metabolismo.\nCrecimiento y multiplicación. Las células son capaces de dirigir su propia síntesis. A consecuencia de los procesos nutricionales, una célula crece y se divide, formando dos células, en una célula idéntica a la célula original, mediante la división celular.\nDiferenciación. Muchas células pueden sufrir cambios de forma o función en un proceso llamado diferenciación celular. Cuando una célula se diferencia, se forman algunas sustancias o estructuras que no estaban previamente formadas y otras que lo estaban dejan de formarse. La diferenciación es a menudo parte del ciclo celular en que las células forman estructuras especializadas relacionadas con la reproducción, la dispersión o la supervivencia.\nSeñalización. Las células responden a estímulos químicos y físicos tanto del medio externo como de su interior y, en el caso de células móviles, hacia determinados estímulos ambientales o en dirección opuesta mediante un proceso que se denomina quimiotaxis. Además, frecuentemente las células pueden interaccionar o comunicar con otras células, generalmente por medio de señales o mensajeros químicos, como hormonas, neurotransmisores, factores de crecimiento... en seres pluricelulares en complicados procesos de comunicación celular y transducción de señales.\nEvolución. A diferencia de las estructuras inanimadas, los organismos unicelulares y pluricelulares evolucionan. Esto significa que hay cambios hereditarios (que ocurren a baja frecuencia en todas las células de modo regular) que pueden influir en la adaptación global de la célula o del organismo superior de modo positivo o negativo. El resultado de la evolución es la selección de aquellos organismos mejor adaptados a vivir en un medio particular.\nLas propiedades celulares no tienen por qué ser constantes a lo largo del desarrollo de un organismo: evidentemente, el patrón de expresión de los genes varía en respuesta a estímulos externos, además de factores endógenos.[18]​ Un aspecto importante a controlar es la pluripotencialidad, característica de algunas células que les permite dirigir su desarrollo hacia un abanico de posibles tipos celulares. En metazoos, la genética subyacente a la determinación del destino de una célula consiste en la expresión de determinados factores de transcripción específicos del linaje celular al cual va a pertenecer, así como a modificaciones epigenéticas. Además, la introducción de otro tipo de factores de transcripción mediante ingeniería genética en células somáticas basta para inducir la mencionada pluripotencialidad, luego este es uno de sus fundamentos moleculares.[19]​"
      },
      {
        "heading": "Tamaño, forma y función",
        "level": 2,
        "content": "El tamaño y la forma de las células depende de sus elementos más periféricos (por ejemplo, la pared, si la hubiere) y de su andamiaje interno (es decir, el citoesqueleto). Además, la competencia por el espacio tisular provoca una morfología característica: por ejemplo, las células vegetales, poliédricas in vivo, tienden a ser esféricas in vitro.[20]​ Incluso pueden existir parámetros químicos sencillos, como los gradientes de concentración de una sal, que determinen la aparición de una forma compleja.[21]​\nEn cuanto al tamaño, la mayoría de las células son microscópicas, es decir, no son observables a simple vista. (un milímetro cúbico de sangre puede contener unos cinco millones de células),[15]​ A pesar de ser muy pequeñas, el tamaño de las células es extremadamente variable. La célula más pequeña observada, en condiciones normales, corresponde a Mycoplasma genitalium, de 0,2 μm, encontrándose cerca del límite teórico de 0,17 μm.[22]​ Existen bacterias con 1 y 2 μm de longitud. Las células humanas son muy variables: hematíes de 7 micras, hepatocitos con 20 micras, espermatozoides de 53 μm, óvulos de 150 μm e, incluso, algunas neuronas de en torno a un metro de longitud. En las células vegetales los granos de polen pueden llegar a medir de 200 a 300 μm.\nRespecto a las células de mayor tamaño; por ejemplo, los xenofióforos,[23]​ son foraminíferos unicelulares que han desarrollado un gran tamaño, los cuales alcanzar tamaños macroscópicos (Syringammina fragilissima alcanza los 20 cm de diámetro).[24]​\nPara la viabilidad de la célula y su correcto funcionamiento siempre se debe tener en cuenta la relación superficie-volumen.[16]​ Puede aumentar considerablemente el volumen de la célula y no así su superficie de intercambio de membrana, lo que dificultaría el nivel y regulación de los intercambios de sustancias vitales para la célula.\nRespecto de su forma, las células presentan una gran variabilidad, e, incluso, algunas no la poseen bien definida o permanente. Pueden ser: fusiformes (forma de huso), estrelladas, prismáticas, aplanadas, elípticas, globosas o redondeadas, etc. Algunas tienen una pared rígida y otras no, lo que les permite deformar la membrana y emitir prolongaciones citoplasmáticas (pseudópodos) para desplazarse o conseguir alimento. Hay células libres que no muestran esas estructuras de desplazamiento, pero poseen cilios o flagelos, que son estructuras derivadas de un orgánulo celular (el centrosoma) que dota a estas células de movimiento.[2]​ De este modo, existen multitud de tipos celulares, relacionados con la función que desempeñan; por ejemplo: \n\nCélulas contráctiles que suelen ser alargadas, como los miocitos esqueléticos.\nCélulas con finas prolongaciones, como las neuronas que transmiten el impulso nervioso.\nCélulas con microvellosidades o con pliegues, como las del intestino para ampliar la superficie de contacto y de intercambio de sustancias.\nCélulas cúbicas, prismáticas o aplanadas como las epiteliales que recubren superficies como las losas de un pavimento."
      },
      {
        "heading": "Estudio de las células",
        "level": 1,
        "content": "Los biólogos utilizan diversos instrumentos para lograr el conocimiento de las células. Obtienen información de sus formas, tamaños y componentes, que les sirve para comprender además las funciones que en ellas se realizan.\nDesde las primeras observaciones de células, hace más de 300 años, hasta la época actual, las técnicas y los aparatos se han ido perfeccionando, originando una rama más de la biología: la microscopía.\nDado el pequeño tamaño de la gran mayoría de las células, el uso del microscopio es de enorme valor en la investigación biológica. En la actualidad, los biólogos utilizan dos tipos básicos de microscopio: los ópticos y los electrónicos.\nUn microscopio óptico utiliza la luz visible para el estudio de muestras. Obteniedo imágenes aumentadas a partir de la desviación de la luz con lentes de cristal. Es utilizado para la observación de tejidos y células desde su invención en el siglo XVII.\nLos microscopios electrónicos son aquellos que utilizan electrones a alta velocidad para el análisis de muestras. Lo cual ofrece mayores capacidades de aumento que los de tipo óptico. Utilizándose en ramas como la medicina, y el estudio de materiales a nivel atómico. Además de usarse para la observación de células, virus y tejidos a nivel subcelular. Sin embargo, estos presentan limitaciones, debido a una cámara de vacío y a la preparación que requieren las muestras para ser analizadas, no pueden observarse células vivas.[25]​"
      },
      {
        "heading": "La célula procariota",
        "level": 1,
        "content": "Las células procariotas son pequeñas y menos complejas que las eucariotas. Contienen ribosomas, pero carecen de sistemas de endomembranas (esto es, orgánulos delimitados por membranas biológicas, como puede ser el núcleo celular). Por ello poseen el material genético en el citosol. Sin embargo, existen excepciones: algunas bacterias fotosintéticas poseen sistemas de membranas internos.[26]​ También en el Filo Planctomycetes existen organismos como Pirellula que rodean su material genético mediante una membrana intracitoplasmática y Gemmata obscuriglobus que lo rodea con doble membrana. Esta última posee además otros compartimentos internos de membrana, posiblemente conectados con la membrana externa del nucleoide y con la membrana plasmática, que no está asociada a peptidoglucano.[27]​[28]​[29]​\nEstudios realizados en 2017, demuestran otra particularidad de Gemmata: presenta estructuras similares al poro nuclear, en la membrana que rodea su cuerpo nuclear.[30]​\nPor lo general podría decirse que los procariotas carecen de citoesqueleto. Sin embargo se ha observado que algunas bacterias, como Bacillus subtilis, poseen proteínas tales como MreB y mbl que actúan de un modo similar a la actina y son importantes en la morfología celular.[31]​ Fusinita van den Ent, en Nature, va más allá, afirmando que los citoesqueletos de actina y tubulina tienen origen procariótico.[32]​\nDe gran diversidad, los procariotas sustentan un metabolismo extraordinariamente complejo, en algunos casos exclusivo de ciertos taxa, como algunos grupos de bacterias, lo que incide en su versatilidad ecológica.[13]​ Los procariotas se clasifican, según Carl Woese, en arqueas y bacterias.[33]​"
      },
      {
        "heading": "Arqueas",
        "level": 2,
        "content": "Las arqueas poseen un diámetro celular comprendido entre 0,1 y 15 μm, aunque las formas filamentosas pueden ser mayores por agregación de células. Presentan multitud de formas distintas: incluso las hay descritas cuadradas y planas.[34]​ Algunas arqueas tienen flagelos y son móviles.\nLas arqueas, al igual que las bacterias, no tienen membranas internas que delimiten orgánulos. Como todos los organismos presentan ribosomas, pero a diferencia de los encontrados en las bacterias que son sensibles a ciertos agentes antimicrobianos, los de las arqueas, más cercanos a los eucariotas, no lo son. La membrana celular tiene una estructura similar a la de las demás células, pero su composición química es única, con enlaces tipo éter en sus lípidos.[35]​ Casi todas las arqueas poseen una pared celular (algunos Thermoplasma son la excepción) de composición característica, por ejemplo, no contienen peptidoglicano (mureína), propio de bacterias. No obstante, pueden clasificarse bajo la tinción de Gram, de vital importancia en la taxonomía de bacterias; sin embargo, en arqueas, poseedoras de una estructura de pared en absoluto común a la bacteriana, dicha tinción es aplicable, pero carece de valor taxonómico. El orden Methanobacteriales tiene una capa de pseudomureína, que provoca que dichas arqueas respondan como positivas a la tinción de Gram.[36]​[37]​[38]​\nComo en casi todos los procariotas, las células de las arqueas carecen de núcleo, y presentan un solo cromosoma circular. Existen elementos extracromosómicos, tales como plásmidos. Sus genomas son de pequeño tamaño, sobre 2-4 millones de pares de bases. También es característica la presencia de ARN polimerasas de constitución compleja y un gran número de nucleótidos modificados en los ácidos ribonucleicos ribosomales. Por otra parte, su ADN se empaqueta en forma de nucleosomas, como en los eucariotas, gracias a proteínas semejantes a las histonas y algunos genes poseen intrones.[39]​ Pueden reproducirse por fisión binaria o múltiple, fragmentación o gemación."
      },
      {
        "heading": "Bacterias",
        "level": 2,
        "content": "Las bacterias son organismos relativamente sencillos, de dimensiones muy reducidas, de apenas unas micras en la mayoría de los casos. Como otros procariotas, carecen de un núcleo delimitado por una membrana, aunque presentan un nucleoide, una estructura elemental que contiene una gran molécula generalmente circular de ADN.[17]​[40]​ Carecen de núcleo celular y demás orgánulos delimitados por membranas biológicas.[41]​ En el citoplasma se pueden apreciar plásmidos, pequeñas moléculas circulares de ADN que coexisten con el nucleoide y que contienen genes: son comúnmente usados por las bacterias en la parasexualidad (reproducción sexual bacteriana). El citoplasma también contiene ribosomas y diversos tipos de gránulos. En algunos casos, puede haber estructuras compuestas por membranas, generalmente relacionadas con la fotosíntesis.[9]​\nPoseen una membrana celular compuesta de lípidos, en forma de una bicapa y sobre ella se encuentra una cubierta en la que existe un polisacárido complejo denominado peptidoglicano; dependiendo de su estructura y subsecuente su respuesta a la tinción de Gram, se clasifica a las bacterias en Gram positivas y Gram negativas. El espacio comprendido entre la membrana celular y la pared celular (o la membrana externa, si esta existe) se denomina espacio periplásmico. Algunas bacterias presentan una cápsula. Otras son capaces de generar endosporas (estadios latentes capaces de resistir condiciones extremas) en algún momento de su ciclo vital. Entre las formaciones exteriores propias de la célula bacteriana destacan los flagelos (de estructura completamente distinta a la de los flagelos eucariotas) y los pili (estructuras de adherencia y relacionadas con la parasexualidad).[9]​\nLa mayoría de las bacterias disponen de un único cromosoma circular y suelen poseer elementos genéticos adicionales, como distintos tipos de plásmidos. Su reproducción, binaria y muy eficiente en el tiempo, permite la rápida expansión de sus poblaciones, generándose un gran número de células que son virtualmente clones, esto es, idénticas entre sí.[39]​"
      },
      {
        "heading": "La célula eucariota",
        "level": 1,
        "content": "Las células eucariotas son el exponente de la complejidad celular actual.[15]​ Presentan una estructura básica relativamente estable caracterizada por la presencia de distintos tipos de orgánulos intracitoplasmáticos especializados, entre los cuales destaca el núcleo, que alberga el material genético. Especialmente en los organismos pluricelulares, las células pueden alcanzar un alto grado de especialización. Dicha especialización o diferenciación es tal que, en algunos casos, compromete la propia viabilidad del tipo celular en aislamiento. Así, por ejemplo, las neuronas dependen para su supervivencia de las células gliales.[13]​\nPor otro lado, la estructura de la célula varía dependiendo de la situación taxonómica del ser vivo: de este modo, las células vegetales difieren de las animales, así como de las de los hongos. Por ejemplo, las células animales carecen de pared celular, son muy variables, no tiene plastos, puede tener vacuolas, pero no son muy grandes y presentan centríolos (que son agregados de microtúbulos cilíndricos que contribuyen a la formación de los cilios y los flagelos y facilitan la división celular). Las células de los vegetales, por su lado, presentan una pared celular compuesta principalmente de celulosa, disponen de plastos como cloroplastos (orgánulo capaz de realizar la fotosíntesis), cromoplastos (orgánulos que acumulan pigmentos) o leucoplastos (orgánulos que acumulan el almidón fabricado en la fotosíntesis), poseen vacuolas de gran tamaño que acumulan sustancias de reserva o de desecho producidas por la célula y finalmente cuentan también con plasmodesmos, que son conexiones citoplasmáticas que permiten la circulación directa de las sustancias del citoplasma de una célula a otra, con continuidad de sus membranas plasmáticas.[42]​"
      },
      {
        "heading": "Compartimentos",
        "level": 2,
        "content": "Las células son entes dinámicos, con un metabolismo celular interno de gran actividad cuya estructura es un flujo entre rutas anastomosadas. Un fenómeno observado en todos los tipos celulares es la compartimentalización, que consiste en una heterogeneidad que da lugar a entornos más o menos definidos (rodeados o no mediante membranas biológicas) en las cuales existe un microentorno que aglutina a los elementos implicados en una ruta biológica.[43]​ Esta compartimentalización alcanza su máximo exponente en las células eucariotas, las cuales están formadas por diferentes estructuras y orgánulos que desarrollan funciones específicas, lo que supone un método de especialización espacial y temporal.[2]​ No obstante, células más sencillas, como los procariotas, ya poseen especializaciones semejantes.[44]​"
      },
      {
        "heading": "Membrana plasmática y superficie celular",
        "level": 3,
        "content": "La composición de la membrana plasmática varía entre células dependiendo de la función o del tejido en la que se encuentre, pero posee elementos comunes. Está compuesta por una doble capa de fosfolípidos, por proteínas unidas no covalentemente a esa bicapa, y por glúcidos unidos covalentemente a lípidos o proteínas. Generalmente, las moléculas más numerosas son las de lípidos; sin embargo, las proteínas, debido a su mayor masa molecular, representan aproximadamente el 50 % de la masa de la membrana.[43]​\nUn modelo que explica el funcionamiento de la membrana plasmática es el modelo del mosaico fluido, de J. S. Singer y Garth Nicolson (1972), que desarrolla un concepto de unidad termodinámica basada en las interacciones hidrófobas entre moléculas y otro tipo de enlaces no covalentes.[45]​\n\nDicha estructura de membrana sustenta un complejo mecanismo de transporte, que posibilita un fluido intercambio de masa y energía entre el entorno intracelular y el externo.[43]​ Además, la posibilidad de transporte e interacción entre moléculas de células aledañas o de una célula con su entorno faculta a estas poder comunicarse químicamente, esto es, permite la señalización celular. Neurotransmisores, hormonas, mediadores químicos locales afectan a células concretas modificando el patrón de expresión génica mediante mecanismos de transducción de señal.[46]​\nSobre la bicapa lipídica, independientemente de la presencia o no de una pared celular, existe una matriz que puede variar, de poco conspicua, como en los epitelios, a muy extensa, como en el tejido conjuntivo. \nDicha matriz, denominada glucocalix (glicocáliz), rica en líquido tisular, glucoproteínas, proteoglicanos y fibras, también interviene en la generación de estructuras y funciones emergentes, derivadas de las interacciones célula-célula.[13]​"
      },
      {
        "heading": "Estructura y expresión génica",
        "level": 3,
        "content": "Las células eucariotas poseen su material genético en, generalmente, un solo núcleo celular, delimitado por una envoltura consistente en dos bicapas lipídicas atravesadas por numerosos poros nucleares y en continuidad con el retículo endoplasmático. En su interior, se encuentra el material genético, el ADN, observable, en las células en interfase, como cromatina de distribución heterogénea. A esta cromatina se encuentran asociadas multitud de proteínas, entre las cuales destacan las histonas, así como ARN, otro ácido nucleico.[47]​\nDicho material genético se encuentra inmerso en una actividad continua de regulación de la expresión génica; las ARN polimerasas transcriben ARN mensajero continuamente, que, exportado al citosol, es traducido a proteína, de acuerdo a las necesidades fisiológicas. Asimismo, dependiendo del momento del ciclo celular, dicho ADN puede entrar en replicación, como paso previo a la mitosis.[39]​ No obstante, las células eucarióticas poseen material genético extranuclear: concretamente, en mitocondrias y plastos, si los hubiere; estos orgánulos conservan una independencia genética parcial del genoma nuclear.[48]​[49]​"
      },
      {
        "heading": "Síntesis y degradación de macromoléculas",
        "level": 3,
        "content": "Dentro del citosol, esto es, la matriz acuosa que alberga a los orgánulos y demás estructuras celulares, se encuentran inmersos multitud de tipos de maquinaria de metabolismo celular: orgánulos, inclusiones, elementos del citoesqueleto, enzimas... De hecho, estas últimas corresponden al 20 % de las enzimas totales de la célula.[13]​\n\nRibosoma: Los ribosomas, visibles al microscopio electrónico como partículas esféricas,[50]​ son complejos supramoleculares encargados de ensamblar proteínas a partir de la información genética que les llega del ADN transcrita en forma de ARN mensajero. Elaborados en el núcleo, desempeñan su función de síntesis de proteínas en el citoplasma. Están formados por ARN ribosómico y por diversos tipos de proteínas. Estructuralmente, tienen dos subunidades. En las células, estos orgánulos aparecen en diferentes estados de disociación. Cuando están completos, pueden estar aislados o formando grupos (polisomas). También pueden aparecer asociados al retículo endoplasmático rugoso o a la envoltura nuclear.[39]​\nRetículo endoplasmático: El retículo endoplasmático es orgánulo vesicular interconectado que forma cisternas, tubos aplanados y sáculos comunicados entre sí. Intervienen en funciones relacionadas con la síntesis proteica, glicosilación de proteínas, metabolismo de lípidos y algunos esteroides, detoxificación, así como el tráfico de vesículas. En células especializadas, como las miofibrillas o células musculares, se diferencia en el retículo sarcoplásmico, orgánulo decisivo para que se produzca la contracción muscular.[15]​\nAparato de Golgi: El aparato de Golgi es un orgánulo formado por apilamientos de sáculos denominados dictiosomas, si bien, como ente dinámico, estos pueden interpretarse como estructuras puntuales fruto de la coalescencia de vesículas.[51]​[52]​ Recibe las vesículas del retículo endoplasmático rugoso que han de seguir siendo procesadas. Dentro de las funciones que posee el aparato de Golgi se encuentran la glicosilación de proteínas, selección, destinación, glicosilación de lípidos y la síntesis de polisacáridos de la matriz extracelular. Posee tres compartimientos; uno proximal al retículo endoplasmático, denominado «compartimento cis», donde se produce la fosforilación de las manosas de las enzimas que han de dirigirse al lisosoma; el «compartimento intermedio», con abundantes manosidasas y N-acetil-glucosamina transferasas; y el «compartimento o red trans», el más distal, donde se transfieren residuos de galactosa y ácido siálico, y del que emergen las vesículas con los diversos destinos celulares.[13]​\nLisosoma: Los lisosomas son orgánulos que albergan multitud de enzimas hidrolíticas. De morfología muy variable, no se ha demostrado su existencia en células vegetales.[13]​ Una característica que agrupa a todos los lisosomas es la posesión de hidrolasas ácidas: proteasas, nucleasas, glucosidasas, lisozima, arilsulfatasas, lipasas, fosfolipasas y fosfatasas. Procede de la fusión de vesículas procedentes del aparato de Golgi, que, a su vez, se fusionan en un tipo de orgánulo denominado endosoma temprano, el cual, al acidificarse y ganar en enzimas hidrolíticos, pasa a convertirse en el lisosoma funcional. Sus funciones abarcan desde la degradación de macromoléculas endógenas o procedentes de la fagocitosis a la intervención en procesos de apoptosis.[53]​\n\nVacuola vegetal: Las vacuolas vegetales, numerosas y pequeñas en células meristemáticas y escasas y grandes en células diferenciadas, son orgánulos exclusivos de los representantes del mundo vegetal. Inmersas en el citosol, están delimitadas por el tonoplasto, una membrana lipídica. Sus funciones son: facilitar el intercambio con el medio externo, mantener la turgencia celular, la digestión celular y la acumulación de sustancias de reserva y subproductos del metabolismo.[42]​\nInclusión citoplasmática: Las inclusiones son acúmulos nunca delimitados por membrana de sustancias de diversa índole, tanto en células vegetales como animales. Típicamente se trata de sustancias de reserva que se conservan como acervo metabólico: almidón, glucógeno, triglicéridos, proteínas..., aunque también existen de pigmentos.[13]​"
      },
      {
        "heading": "Conversión energética",
        "level": 3,
        "content": "El metabolismo celular está basado en la transformación de unas sustancias químicas, denominadas metabolitos, en otras; dichas reacciones químicas transcurren catalizadas mediante enzimas. Si bien buena parte del metabolismo sucede en el citosol, como la glucólisis, existen procesos específicos de orgánulos.[46]​\n\nMitocondria: Las mitocondrias son orgánulos de aspecto, número y tamaño variable que intervienen en el ciclo de Krebs, fosforilación oxidativa y en la cadena de transporte de electrones de la respiración. Presentan una doble membrana, externa e interna, que dejan entre ellas un espacio perimitocondrial; la membrana interna, plegada en crestas hacia el interior de la matriz mitocondrial, posee una gran superficie. En su interior posee generalmente una sola molécula de ADN, el genoma mitocondrial, típicamente circular, así como ribosomas más semejantes a los bacterianos que a los eucariotas.[13]​ Según la teoría endosimbiótica, se asume que la primera protomitocondria era un tipo de proteobacteria.[54]​\n\nCloroplasto: Los cloroplastos son los orgánulos celulares que en los organismos eucariotas fotosintéticos se ocupan de la fotosíntesis. Están limitados por una envoltura formada por dos membranas concéntricas y contienen vesículas, los tilacoides, donde se encuentran organizados los pigmentos y demás moléculas implicadas en la conversión de la energía lumínica en energía química. Además de esta función, los plastidios intervienen en el metabolismo intermedio, produciendo energía y poder reductor, sintetizando bases púricas y pirimidínicas, algunos aminoácidos y todos los ácidos grasos. Además, en su interior es común la acumulación de sustancias de reserva, como el almidón.[13]​ Se considera que poseen analogía con las cianobacterias.[55]​\n\nPeroxisoma: Los peroxisomas son orgánulos muy comunes en forma de vesículas que contienen abundantes enzimas de tipo oxidasa y catalasa; de tan abundantes, es común que cristalicen en su interior. Estas enzimas cumplen funciones de detoxificación celular. Otras funciones de los peroxisomas son: las oxidaciones flavínicas generales, el catabolismo de las purinas, la beta-oxidación de los ácidos grasos, el ciclo del glioxilato, el metabolismo del ácido glicólico y la detoxificación en general.[13]​ Se forman de vesículas procedentes del retículo endoplasmático.[56]​"
      },
      {
        "heading": "Citoesqueleto",
        "level": 3,
        "content": "Las células poseen un andamiaje que permite el mantenimiento de su forma y estructura, pero más aún, este es un sistema dinámico que interactúa con el resto de componentes celulares generando un alto grado de orden interno. Dicho andamiaje está formado por una serie de proteínas que se agrupan dando lugar a estructuras filamentosas que, mediante otras proteínas, interactúan entre ellas dando lugar a una especie de retículo. El mencionado andamiaje recibe el nombre de citoesqueleto, y sus elementos mayoritarios son: los microtúbulos, los microfilamentos y los filamentos intermedios.[2]​[nota 2]​[57]​[58]​\n\nMicrofilamentos: Los microfilamentos o filamentos de actina están formados por una proteína globular, la actina, que puede polimerizar dando lugar a estructuras filiformes. Dicha actina se expresa en todas las células del cuerpo y especialmente en las musculares, ya que está implicada en la contracción muscular, por interacción con la miosina. Además, posee lugares de unión a ATP, lo que dota a sus filamentos de polaridad.[59]​ Puede encontrarse en forma libre o polimerizarse en microfilamentos, que son esenciales para funciones celulares tan importantes como la movilidad y la contracción de la célula durante la división celular.[51]​\n\nMicrotúbulos: Los microtúbulos son estructuras tubulares de 25 nm de diámetro exterior y unos 12 nm de diámetro interior, con longitudes que varían entre unos pocos nanómetros a micrómetros, que se originan en los centros organizadores de microtúbulos y que se extienden a lo largo de todo el citoplasma. Se hallan en las células eucariotas y están formadas por la polimerización de un dímero de dos proteínas globulares, la alfa y la beta tubulina. Las tubulinas poseen capacidad de unir GTP.[2]​[51]​ Los microtúbulos intervienen en diversos procesos celulares que involucran desplazamiento de vesículas de secreción, movimiento de orgánulos, transporte intracelular de sustancias, así como en la división celular (mitosis y meiosis) y que, junto con los microfilamentos y los filamentos intermedios, forman el citoesqueleto. Además, constituyen la estructura interna de los cilios y los flagelos.[2]​[51]​\nFilamentos intermedios: Los filamentos intermedios son componentes del citoesqueleto. Formados por agrupaciones de proteínas fibrosas, su nombre deriva de su diámetro, de 10 nm, menor que el de los microtúbulos, de 24 nm, pero mayor que el de los microfilamentos, de 7 nm. Son ubicuos en las células animales, y no existen en plantas ni hongos. Forman un grupo heterogéneo, clasificado en cinco familias: las queratinas, en células epiteliales; los neurofilamentos, en neuronas; los gliofilamentos, en células gliales; la desmina, en músculo liso y estriado; y la vimentina, en células derivadas del mesénquima.[13]​\n\nCentríolos: Los centríolos son una pareja de estructuras que forman parte del citoesqueleto de células animales. Semejantes a cilindros huecos, están rodeados de un material proteico denso llamado material pericentriolar; todos ellos forman el centrosoma o centro organizador de microtúbulos que permiten la polimerización de microtúbulos de dímeros de tubulina que forman parte del citoesqueleto. Los centríolos se posicionan perpendicularmente entre sí. Sus funciones son participar en la mitosis, durante la cual generan el huso acromático, y en la citocinesis,[60]​ así como, se postula, intervenir en la nucleación de microtúbulos.[61]​[62]​\nCilios y flagelos: Se trata de especializaciones de la superficie celular con motilidad; con una estructura basada en agrupaciones de microtúbulos, ambos se diferencian en la mayor longitud y menor número de los flagelos, y en la mayor variabilidad de la estructura molecular de estos últimos.[13]​"
      },
      {
        "heading": "Ciclo vital",
        "level": 2,
        "content": "El ciclo celular es el proceso ordenado y repetitivo en el tiempo mediante el cual una célula madre crece y se divide en dos células hijas. Las células que no se están dividiendo se encuentran en una fase conocida como G0, paralela al ciclo. La regulación del ciclo celular es esencial para el correcto funcionamiento de las células sanas, está claramente estructurado en fases[51]​\n\nEl estado de no división o interfase. La célula realiza sus funciones específicas y, si está destinada a avanzar a la división celular, comienza por realizar la duplicación de su ADN.\nEl estado de división, llamado fase M, situación que comprende la mitosis y citocinesis. En algunas células la citocinesis no se produce, obteniéndose como resultado de la división una masa celular plurinucleada denominada plasmodio.[nota 3]​\nA diferencia de lo que sucede en la mitosis, donde la dotación genética se mantiene, existe una variante de la división celular, propia de las células de la línea germinal, denominada meiosis. En ella, se reduce la dotación genética diploide, común a todas las células somáticas del organismo, a una haploide, esto es, con una sola copia del genoma. De este modo, la fusión, durante la fecundación, de dos gametos haploides procedentes de dos parentales distintos da como resultado un zigoto, un nuevo individuo, diploide, equivalente en dotación genética a sus padres.[63]​\n\nLa interfase consta de tres estadios claramente definidos.[2]​[51]​\nFase G1: es la primera fase del ciclo celular, en la que existe crecimiento celular con síntesis de proteínas y de ARN. Es el período que trascurre entre el fin de una mitosis y el inicio de la síntesis de ADN. En él la célula dobla su tamaño y masa debido a la continua síntesis de todos sus componentes, como resultado de la expresión de los genes que codifican las proteínas responsables de su fenotipo particular.\nFase S: es la segunda fase del ciclo, en la que se produce la replicación o síntesis del ADN. Como resultado cada cromosoma se duplica y queda formado por dos cromátidas idénticas. Con la duplicación del ADN, el núcleo contiene el doble de proteínas nucleares y de ADN que al principio.\nFase G2: es la segunda fase de crecimiento del ciclo celular en la que continúa la síntesis de proteínas y ARN. Al final de este período se observa al microscopio cambios en la estructura celular, que indican el principio de la división celular. Termina cuando los cromosomas empiezan a condensarse al inicio de la mitosis.\nLa fase M es la fase de la división celular en la cual una célula progenitora se divide en dos células hijas idénticas entre sí y a la madre. Esta fase incluye la mitosis, a su vez dividida en: profase, metafase, anafase, telofase; y la citocinesis, que se inicia ya en la telofase mitótica.\nLa incorrecta regulación del ciclo celular puede conducir a la aparición de células precancerígenas que, si no son inducidas al suicidio mediante apoptosis, puede dar lugar a la aparición de cáncer. Los fallos conducentes a dicha desregulación están relacionados con la genética celular: lo más común son las alteraciones en oncogenes, genes supresores de tumores y genes de reparación del ADN.[64]​"
      },
      {
        "heading": "Origen",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Origen de la primera célula",
        "level": 2,
        "content": "La aparición de la vida, y, por ello, de la célula, probablemente se inició gracias a la transformación de moléculas inorgánicas en orgánicas bajo unas condiciones ambientales adecuadas, produciéndose más adelante la interacción de estas biomoléculas generando entes de mayor complejidad. El experimento de Miller y Urey, realizado en 1953, demostró que una mezcla de compuestos orgánicos sencillos puede transformarse en algunos aminoácidos, glúcidos y lípidos (componentes todos ellos de la materia viva) bajo unas condiciones ambientales que simulan las presentes hipotéticamente en la Tierra primigenia (en torno al eón Hádico).[65]​ Se ha sugerido que el último antepasado común universal vivió hace más de 4200 millones de años.[66]​  \nSe postula que dichos componentes orgánicos se agruparon generando estructuras complejas, los coacervados de Oparin, aún acelulares que, en cuanto alcanzaron la capacidad de autoorganizarse y perpetuarse, dieron lugar a un tipo de célula primitiva, el progenote de Carl Woese, antecesor de los tipos celulares actuales.[33]​ Una vez se diversificó este grupo celular, dando lugar a las variantes procariotas, arqueas y bacterias, pudieron aparecer nuevos tipos de células, más complejos, por endosimbiosis, esto es, captación permanente de unos tipos celulares en otros sin una pérdida total de autonomía de aquellos.[67]​ De este modo, algunos autores describen un modelo en el cual la primera célula eucariota surgió por introducción de una arquea en el interior de una bacteria, dando lugar esta primera a un primitivo núcleo celular.[68]​ No obstante, la imposibilidad de que una bacteria pueda efectuar una fagocitosis y, por ello, captar a otro tipo de célula, dio lugar a otra hipótesis, que sugiere que fue una célula denominada cronocito la que fagocitó a una bacteria y a una arquea, dando lugar al primer organismo eucariota. De este modo, y mediante un análisis de secuencias a nivel genómico de organismos modelo eucariotas, se ha conseguido describir a este cronocito original como un organismo con citoesqueleto y membrana plasmática, lo cual sustenta su capacidad fagocítica, y cuyo material genético era el ARN, lo que puede explicar, si la arquea fagocitada lo poseía en el ADN, la separación espacial en los eucariotas actuales entre la transcripción (nuclear), y la traducción (citoplasmática).[69]​\nUna dificultad adicional es el hecho de que no se han encontrado organismos eucariotas primitivamente amitocondriados como exige la hipótesis endosimbionte. Además, el equipo de María Rivera, de la Universidad de California, comparando genomas completos de todos los dominios de la vida ha encontrado evidencias de que los eucariotas contienen dos genomas diferentes, uno más semejante a bacterias y otro a arqueas, apuntando en este último caso semejanzas a los metanógenos, en particular en el caso de las histonas.[70]​[71]​ Esto llevó a Bill Martin y Miklós Müller a plantear la hipótesis de que la célula eucariota surgiera no por endosimbiosis, sino por fusión quimérica y acoplamiento metabólico de un metanógeno y una α-proteobacteria simbiontes a través del hidrógeno (hipótesis del hidrógeno).[72]​ Esta hipótesis atrae hoy en día posiciones muy encontradas, con detractores como Christian de Duve.[73]​\nHarold Morowitz, un físico de la Universidad Yale, ha calculado que las probabilidades de obtener la bacteria viva más sencilla mediante cambios al azar es de 1 sobre 1 seguido por 100 000 000 000 ceros. «Este número es tan grande —dijo Robert Shapiro— que para escribirlo en forma convencional necesitaríamos varios centenares de miles de libros en blanco». Presenta la acusación de que los científicos que han abrazado la evolución química de la vida pasan por alto la evidencia aumentante y «han optado por aceptarla como verdad que no puede ser cuestionada, consagrándola así como mitología».[74]​"
      },
      {
        "heading": "Origen de la célula eucariota",
        "level": 2,
        "content": "Las células eucariotas se formaron hace 2500 millones de años en un proceso llamado eucariogénesis.[76]​ Se acepta ampliamente que esto implicó una simbiogénesis, en la que una arquea y una bacteria se unieron para crear el primer ancestro común eucariota.[77]​ Esta célula tenía un nuevo nivel de complejidad y capacidad, con un núcleo[78]​[79]​ y mitocondrias facultativamente aeróbicas.[75]​ Evolucionó hasta convertirse en una población de organismos unicelulares que incluía al último ancestro común eucariota, acumulando capacidades a lo largo del camino, aunque la secuencia de los pasos involucrados ha sido cuestionada y es posible que no haya comenzado con la simbiogénesis. Presentaba al menos un centriolo y cilio, sexo (meiosis y singamia), peroxisomas y un quiste latente con una pared celular de quitina y/o celulosa.[80]​[81]​ A su vez, el último ancestro común eucariota dio origen al grupo terminal de los eucariotas, que contiene los ancestros de animales, hongos, plantas y una amplia gama de organismos unicelulares.[82]​[83]​[84]​ Las células vegetales se formaron hace unos 2000 millones de años con un segundo episodio de simbiogénesis al que se añadieron cloroplastos, derivados de una cianobacteria.[85]​[86]​[87]​[75]​"
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": "Célula artificial\nAcelular\nProtobionte\nCáncer\nCélula animal"
      },
      {
        "heading": "Notas",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bibliografía",
        "level": 1,
        "content": "Alberts et al (2004). Biología molecular de la célula. Barcelona: Omega. ISBN 54-282-1351-8. \nAlberts, Bruce; Johnson, Alexander; Lewis, Julian; Morgan, David; Raff, Martin; Roberts, K.; Walter, P. (2016). Biología molecular de la célula (6ª edición). Barcelona: Omega S.A. ISBN 978-84-282-1638-8. \nLane, Nick (2005). Power, Sex, Suicide. Mitochondria and the Meaning of Life. Oxford University Press. ISBN 0-19-280481-2. \nLodish et al. (2005). Biología celular y molecular. Buenos Aires: Médica Panamericana. ISBN 950-06-1974-3. \nPaniagua, R.; Nistal, M.; Sesma, P.; Álvarez-Uría, M.; Fraile, B.; Anadón, R. y José Sáez, F. (2002). Citología e histología vegetal y animal. McGraw-Hill Interamericana de España, S.A.U. ISBN 84-486-0436-9."
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": " Wikiquote alberga frases célebres de o sobre Célula.\n Wikimedia Commons alberga una categoría multimedia sobre Célula.\n Wikcionario  tiene definiciones y otra información sobre célula.\n Wikilibros alberga un libro o manual sobre Biología celular.\nEl Diccionario de la Real Academia Española tiene una definición para célula.\nEstructura celular.\nLibro en línea sobre biología celular (en inglés)"
      }
    ],
    "summary": "La célula (del latín cellula, diminutivo de cella, ‘celda’)[1]​ es la unidad morfológica y funcional de todo ser vivo. De hecho, la célula es el elemento de menor tamaño que puede considerarse vivo.[2]​ De este modo, puede clasificarse a los organismos vivos según el número de células que posean: si solo tienen una, se les denomina unicelulares (como pueden ser los protozoos o las bacterias, organismos microscópicos); si poseen más, se les llama pluricelulares. En estos últimos el número de células es variable: de unos pocos cientos, como en algunos nematodos, a cientos de billones (1014), como en el caso del ser humano. Las células suelen poseer un tamaño de 10 µm y una masa de 1 ng, si bien existen células mucho mayores.\n\nLa teoría celular, propuesta en 1838 para los vegetales y en 1839 para los animales,[3]​ por Matthias Jakob Schleiden y Theodor Schwann, postula que todos los organismos están compuestos por células, y que todas las células derivan de otras precedentes. De este modo, todas las funciones vitales emanan de la maquinaria celular y de la interacción entre células adyacentes; además, la tenencia de la información genética, base de la herencia, en su ADN permite la transmisión de aquella de generación en generación.[4]​\nLa aparición del primer organismo vivo sobre la Tierra suele asociarse al nacimiento de la primera célula. Si bien existen muchas hipótesis que especulan cómo ocurrió, usualmente se describe que el proceso se inició gracias a la transformación de moléculas inorgánicas en orgánicas bajo unas condiciones ambientales adecuadas; tras esto, dichas biomoléculas se asociaron dando lugar a entes complejos capaces de autorreplicarse. Existen posibles evidencias fósiles de estructuras celulares en rocas datadas en torno a 4 o 3,5 miles de millones de años (gigaaños o Ga).[5]​[6]​[nota 1]​ Se han encontrado evidencias muy fuertes de formas de vida unicelulares fosilizadas en microestructuras en rocas de la formación Strelley Pool, en Australia Occidental, con una antigüedad de 3,4 Ga.[cita requerida] Se trataría de los fósiles de células más antiguos encontrados hasta la fecha. Evidencias adicionales muestran que su metabolismo sería anaerobio y basado en el sulfuro.[7]​"
  },
  {
    "title": "Genetics",
    "source": "https://en.wikipedia.org/wiki/Genetics",
    "language": "en",
    "chunks": [
      {
        "heading": "Etymology",
        "level": 1,
        "content": "The word genetics stems from the ancient Greek γενετικός genetikos meaning \"genitive\"/\"generative\", which in turn derives from γένεσις genesis meaning \"origin\"."
      },
      {
        "heading": "History",
        "level": 1,
        "content": "The observation that living things inherit traits from their parents has been used since prehistoric times to improve crop plants and animals through selective breeding. The modern science of genetics, seeking to understand this process, began with the work of the Augustinian friar Gregor Mendel in the mid-19th century.\n\nPrior to Mendel, Imre Festetics, a Hungarian noble, who lived in Kőszeg before Mendel, was the first who used the word \"genetic\" in hereditarian context, and is considered the first geneticist. He described several rules of biological inheritance in his work The genetic laws of nature (Die genetischen Gesetze der Natur, 1819). His second law is the same as that which Mendel published. In his third law, he developed the basic principles of mutation (he can be considered a forerunner of Hugo de Vries). Festetics argued that changes observed in the generation of farm animals, plants, and humans are the result of scientific laws. Festetics empirically deduced that organisms inherit their characteristics, not acquire them. He recognized recessive traits and inherent variation by postulating that traits of past generations could reappear later, and organisms could produce progeny with different attributes. These observations represent an important prelude to Mendel's theory of particulate inheritance insofar as it features a transition of heredity from its status as myth to that of a scientific discipline, by providing a fundamental theoretical basis for genetics in the twentieth century.\n\nOther theories of inheritance preceded Mendel's work. A popular theory during the 19th century, and implied by Charles Darwin's 1859 On the Origin of Species, was blending inheritance: the idea that individuals inherit a smooth blend of traits from their parents. Mendel's work provided examples where traits were definitely not blended after hybridization, showing that traits are produced by combinations of distinct genes rather than a continuous blend. Blending of traits in the progeny is now explained by the action of multiple genes with quantitative effects. Another theory that had some support at that time was the inheritance of acquired characteristics: the belief that individuals inherit traits strengthened by their parents. This theory (commonly associated with Jean-Baptiste Lamarck) is now known to be wrong—the experiences of individuals do not affect the genes they pass to their children. Other theories included Darwin's pangenesis (which had both acquired and inherited aspects) and Francis Galton's reformulation of pangenesis as both particulate and inherited."
      },
      {
        "heading": "Mendelian genetics",
        "level": 2,
        "content": "Modern genetics started with Mendel's studies of the nature of inheritance in plants. In his paper \"Versuche über Pflanzenhybriden\" (\"Experiments on Plant Hybridization\"), presented in 1865 to the Naturforschender Verein (Society for Research in Nature) in Brno, Mendel traced the inheritance patterns of certain traits in pea plants and described them mathematically. Although this pattern of inheritance could only be observed for a few traits, Mendel's work suggested that heredity was particulate, not acquired, and that the inheritance patterns of many traits could be explained through simple rules and ratios.\nThe importance of Mendel's work did not gain wide understanding until 1900, after his death, when Hugo de Vries and other scientists rediscovered his research. William Bateson, a proponent of Mendel's work, coined the word genetics in 1905. The adjective genetic, derived from the Greek word genesis—γένεσις, \"origin\", predates the noun and was first used in a biological sense in 1860. Bateson both acted as a mentor and was aided significantly by the work of other scientists from Newnham College at Cambridge, specifically the work of Becky Saunders, Nora Darwin Barlow, and Muriel Wheldale Onslow. Bateson popularized the usage of the word genetics to describe the study of inheritance in his inaugural address to the Third International Conference on Plant Hybridization in London in 1906.\nAfter the rediscovery of Mendel's work, scientists tried to determine which molecules in the cell were responsible for inheritance. In 1900, Nettie Stevens began studying the mealworm. Over the next 11 years, she discovered that females only had the X chromosome and males had both X and Y chromosomes. She was able to conclude that sex is a chromosomal factor and is determined by the male. In 1911, Thomas Hunt Morgan argued that genes are on chromosomes, based on observations of a sex-linked white eye mutation in fruit flies. In 1913, his student Alfred Sturtevant used the phenomenon of genetic linkage to show that genes are arranged linearly on the chromosome."
      },
      {
        "heading": "Molecular genetics",
        "level": 2,
        "content": "Although genes were known to exist on chromosomes, chromosomes are composed of both protein and DNA, and scientists did not know which of the two is responsible for inheritance. In 1928, Frederick Griffith discovered the phenomenon of transformation: dead bacteria could transfer genetic material to \"transform\" other still-living bacteria. Sixteen years later, in 1944, the Avery–MacLeod–McCarty experiment identified DNA as the molecule responsible for transformation. The role of the nucleus as the repository of genetic information in eukaryotes had been established by Hämmerling in 1943 in his work on the single celled alga Acetabularia. The Hershey–Chase experiment in 1952 confirmed that DNA (rather than protein) is the genetic material of the viruses that infect bacteria, providing further evidence that DNA is the molecule responsible for inheritance.\nJames Watson and Francis Crick determined the structure of DNA in 1953, using the X-ray crystallography work of Rosalind Franklin and Maurice Wilkins that indicated DNA has a helical structure (i.e., shaped like a corkscrew). Their double-helix model had two strands of DNA with the nucleotides pointing inward, each matching a complementary nucleotide on the other strand to form what look like rungs on a twisted ladder. This structure showed that genetic information exists in the sequence of nucleotides on each strand of DNA. The structure also suggested a simple method for replication: if the strands are separated, new partner strands can be reconstructed for each based on the sequence of the old strand. This property is what gives DNA its semi-conservative nature where one strand of new DNA is from an original parent strand.\nAlthough the structure of DNA showed how inheritance works, it was still not known how DNA influences the behavior of cells. In the following years, scientists tried to understand how DNA controls the process of protein production. It was discovered that the cell uses DNA as a template to create matching messenger RNA, molecules with nucleotides very similar to DNA. The nucleotide sequence of a messenger RNA is used to create an amino acid sequence in protein; this translation between nucleotide sequences and amino acid sequences is known as the genetic code.\nWith the newfound molecular understanding of inheritance came an explosion of research. A notable theory arose from Tomoko Ohta in 1973 with her amendment to the neutral theory of molecular evolution through publishing the nearly neutral theory of molecular evolution. In this theory, Ohta stressed the importance of natural selection and the environment to the rate at which genetic evolution occurs. One important development was chain-termination DNA sequencing in 1977 by Frederick Sanger. This technology allows scientists to read the nucleotide sequence of a DNA molecule. In 1983, Kary Banks Mullis developed the polymerase chain reaction, providing a quick way to isolate and amplify a specific section of DNA from a mixture. The efforts of the Human Genome Project, Department of Energy, NIH, and parallel private efforts by Celera Genomics led to the sequencing of the human genome in 2003."
      },
      {
        "heading": "Features of inheritance",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Discrete inheritance and Mendel's laws",
        "level": 2,
        "content": "At its most fundamental level, inheritance in organisms occurs by passing discrete heritable units, called genes, from parents to offspring. This property was first observed by Gregor Mendel, who studied the segregation of heritable traits in pea plants, showing for example that flowers on a single plant were either purple or white—but never an intermediate between the two colors. The discrete versions of the same gene controlling the inherited appearance (phenotypes) are called alleles.\nIn the case of the pea, which is a diploid species, each individual plant has two copies of each gene, one copy inherited from each parent. Many species, including humans, have this pattern of inheritance. Diploid organisms with two copies of the same allele of a given gene are called homozygous at that gene locus, while organisms with two different alleles of a given gene are called heterozygous. The set of alleles for a given organism is called its genotype, while the observable traits of the organism are called its phenotype. When organisms are heterozygous at a gene, often one allele is called dominant as its qualities dominate the phenotype of the organism, while the other allele is called recessive as its qualities recede and are not observed. Some alleles do not have complete dominance and instead have incomplete dominance by expressing an intermediate phenotype, or codominance by expressing both alleles at once.\nWhen a pair of organisms reproduce sexually, their offspring randomly inherit one of the two alleles from each parent. These observations of discrete inheritance and the segregation of alleles are collectively known as Mendel's first law or the Law of Segregation. However, the probability of getting one gene over the other can change due to dominant, recessive, homozygous, or heterozygous genes. For example, Mendel found that if you cross heterozygous organisms your odds of getting the dominant trait is 3:1. Real geneticist study and calculate probabilities by using theoretical probabilities, empirical probabilities, the product rule, the sum rule, and more."
      },
      {
        "heading": "Notation and diagrams",
        "level": 2,
        "content": "Geneticists use diagrams and symbols to describe inheritance. A gene is represented by one or a few letters. Often a \"+\" symbol is used to mark the usual, non-mutant allele for a gene.\nIn fertilization and breeding experiments (and especially when discussing Mendel's laws) the parents are referred to as the \"P\" generation and the offspring as the \"F1\" (first filial) generation. When the F1 offspring mate with each other, the offspring are called the \"F2\" (second filial) generation. One of the common diagrams used to predict the result of cross-breeding is the Punnett square.\nWhen studying human genetic diseases, geneticists often use pedigree charts to represent the inheritance of traits. These charts map the inheritance of a trait in a family tree."
      },
      {
        "heading": "Multiple gene interactions",
        "level": 2,
        "content": "Organisms have thousands of genes, and in sexually reproducing organisms these genes generally assort independently of each other. This means that the inheritance of an allele for yellow or green pea color is unrelated to the inheritance of alleles for white or purple flowers. This phenomenon, known as \"Mendel's second law\" or the \"law of independent assortment,\" means that the alleles of different genes get shuffled between parents to form offspring with many different combinations. Different genes often interact to influence the same trait. In the Blue-eyed Mary (Omphalodes verna), for example, there exists a gene with alleles that determine the color of flowers: blue or magenta. Another gene, however, controls whether the flowers have color at all or are white. When a plant has two copies of this white allele, its flowers are white—regardless of whether the first gene has blue or magenta alleles. This interaction between genes is called epistasis, with the second gene epistatic to the first.\nMany traits are not discrete features (e.g. purple or white flowers) but are instead continuous features (e.g. human height and skin color). These complex traits are products of many genes. The influence of these genes is mediated, to varying degrees, by the environment an organism has experienced. The degree to which an organism's genes contribute to a complex trait is called heritability. Measurement of the heritability of a trait is relative—in a more variable environment, the environment has a bigger influence on the total variation of the trait. For example, human height is a trait with complex causes. It has a heritability of 89% in the United States. In Nigeria, however, where people experience a more variable access to good nutrition and health care, height has a heritability of only 62%."
      },
      {
        "heading": "Molecular basis for inheritance",
        "level": 1,
        "content": ""
      },
      {
        "heading": "DNA and chromosomes",
        "level": 2,
        "content": "The molecular basis for genes is deoxyribonucleic acid (DNA). DNA is composed of deoxyribose (sugar molecule), a phosphate group, and a base (amine group). There are four types of bases: adenine (A), cytosine (C), guanine (G), and thymine (T). The phosphates make phosphodiester bonds with the sugars to make long phosphate-sugar backbones. Bases specifically pair together (T&A, C&G) between two backbones and make like rungs on a ladder. The bases, phosphates, and sugars together make a nucleotide that connects to make long chains of DNA. Genetic information exists in the sequence of these nucleotides, and genes exist as stretches of sequence along the DNA chain. These chains coil into a double a-helix structure and wrap around proteins called Histones which provide the structural support. DNA wrapped around these histones are called chromosomes. Viruses sometimes use the similar molecule RNA instead of DNA as their genetic material.\nDNA normally exists as a double-stranded molecule, coiled into the shape of a double helix. Each nucleotide in DNA preferentially pairs with its partner nucleotide on the opposite strand: A pairs with T, and C pairs with G. Thus, in its two-stranded form, each strand effectively contains all necessary information, redundant with its partner strand. This structure of DNA is the physical basis for inheritance: DNA replication duplicates the genetic information by splitting the strands and using each strand as a template for synthesis of a new partner strand.\n\nGenes are arranged linearly along long chains of DNA base-pair sequences. In bacteria, each cell usually contains a single circular genophore, while eukaryotic organisms (such as plants and animals) have their DNA arranged in multiple linear chromosomes. These DNA strands are often extremely long; the largest human chromosome, for example, is about 247 million base pairs in length. The DNA of a chromosome is associated with structural proteins that organize, compact, and control access to the DNA, forming a material called chromatin; in eukaryotes, chromatin is usually composed of nucleosomes, segments of DNA wound around cores of histone proteins. The full set of hereditary material in an organism (usually the combined DNA sequences of all chromosomes) is called the genome.\nDNA is most often found in the nucleus of cells, but Ruth Sager helped in the discovery of nonchromosomal genes found outside of the nucleus. In plants, these are often found in the chloroplasts and in other organisms, in the mitochondria. These nonchromosomal genes can still be passed on by either partner in sexual reproduction and they control a variety of hereditary characteristics that replicate and remain active throughout generations.\nWhile haploid organisms have only one copy of each chromosome, most animals and many plants are diploid, containing two of each chromosome and thus two copies of every gene. The two alleles for a gene are located on identical loci of the two homologous chromosomes, each allele inherited from a different parent.\nMany species have so-called sex chromosomes that determine the sex of each organism. In humans and many other animals, the Y chromosome contains the gene that triggers the development of the specifically male characteristics. In evolution, this chromosome has lost most of its content and also most of its genes, while the X chromosome is similar to the other chromosomes and contains many genes. This being said, Mary Frances Lyon discovered that there is X-chromosome inactivation during reproduction to avoid passing on twice as many genes to the offspring. Lyon's discovery led to the discovery of X-linked diseases."
      },
      {
        "heading": "Reproduction",
        "level": 2,
        "content": "When cells divide, their full genome is copied and each daughter cell inherits one copy. This process, called mitosis, is the simplest form of reproduction and is the basis for asexual reproduction. Asexual reproduction can also occur in multicellular organisms, producing offspring that inherit their genome from a single parent. Offspring that are genetically identical to their parents are called clones.\nEukaryotic organisms often use sexual reproduction to generate offspring that contain a mixture of genetic material inherited from two different parents. The process of sexual reproduction alternates between forms that contain single copies of the genome (haploid) and double copies (diploid). Haploid cells fuse and combine genetic material to create a diploid cell with paired chromosomes. Diploid organisms form haploids by dividing, without replicating their DNA, to create daughter cells that randomly inherit one of each pair of chromosomes. Most animals and many plants are diploid for most of their lifespan, with the haploid form reduced to single cell gametes such as sperm or eggs.\nAlthough they do not use the haploid/diploid method of sexual reproduction, bacteria have many methods of acquiring new genetic information. Some bacteria can undergo conjugation, transferring a small circular piece of DNA to another bacterium. Bacteria can also take up raw DNA fragments found in the environment and integrate them into their genomes, a phenomenon known as transformation. These processes result in horizontal gene transfer, transmitting fragments of genetic information between organisms that would be otherwise unrelated. Natural bacterial transformation occurs in many bacterial species, and can be regarded as a sexual process for transferring DNA from one cell to another cell (usually of the same species). Transformation requires the action of numerous bacterial gene products, and its primary adaptive function appears to be repair of DNA damages in the recipient cell."
      },
      {
        "heading": "Recombination and genetic linkage",
        "level": 2,
        "content": "The diploid nature of chromosomes allows for genes on different chromosomes to assort independently or be separated from their homologous pair during sexual reproduction wherein haploid gametes are formed. In this way new combinations of genes can occur in the offspring of a mating pair. Genes on the same chromosome would theoretically never recombine. However, they do, via the cellular process of chromosomal crossover. During crossover, chromosomes exchange stretches of DNA, effectively shuffling the gene alleles between the chromosomes. This process of chromosomal crossover generally occurs during meiosis, a series of cell divisions that creates haploid cells. Meiotic recombination, particularly in microbial eukaryotes, appears to serve the adaptive function of repair of DNA damages.\nThe first cytological demonstration of crossing over was performed by Harriet Creighton and Barbara McClintock in 1931. Their research and experiments on corn provided cytological evidence for the genetic theory that linked genes on paired chromosomes do in fact exchange places from one homolog to the other.\nThe probability of chromosomal crossover occurring between two given points on the chromosome is related to the distance between the points. For an arbitrarily long distance, the probability of crossover is high enough that the inheritance of the genes is effectively uncorrelated. For genes that are closer together, however, the lower probability of crossover means that the genes demonstrate genetic linkage; alleles for the two genes tend to be inherited together. The amounts of linkage between a series of genes can be combined to form a linear linkage map that roughly describes the arrangement of the genes along the chromosome."
      },
      {
        "heading": "Gene expression",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Genetic code",
        "level": 2,
        "content": "Genes express their functional effect through the production of proteins, which are molecules responsible for most functions in the cell. Proteins are made up of one or more polypeptide chains, each composed of a sequence of amino acids. The DNA sequence of a gene is used to produce a specific amino acid sequence. This process begins with the production of an RNA molecule with a sequence matching the gene's DNA sequence, a process called transcription.\nThis messenger RNA molecule then serves to produce a corresponding amino acid sequence through a process called translation. Each group of three nucleotides in the sequence, called a codon, corresponds either to one of the twenty possible amino acids in a protein or an instruction to end the amino acid sequence; this correspondence is called the genetic code. The flow of information is unidirectional: information is transferred from nucleotide sequences into the amino acid sequence of proteins, but it never transfers from protein back into the sequence of DNA—a phenomenon Francis Crick called the central dogma of molecular biology.\nThe specific sequence of amino acids results in a unique three-dimensional structure for that protein, and the three-dimensional structures of proteins are related to their functions. Some are simple structural molecules, like the fibers formed by the protein collagen. Proteins can bind to other proteins and simple molecules, sometimes acting as enzymes by facilitating chemical reactions within the bound molecules (without changing the structure of the protein itself). Protein structure is dynamic; the protein hemoglobin bends into slightly different forms as it facilitates the capture, transport, and release of oxygen molecules within mammalian blood.\nA single nucleotide difference within DNA can cause a change in the amino acid sequence of a protein. Because protein structures are the result of their amino acid sequences, some changes can dramatically change the properties of a protein by destabilizing the structure or changing the surface of the protein in a way that changes its interaction with other proteins and molecules. For example, sickle-cell anemia is a human genetic disease that results from a single base difference within the coding region for the β-globin section of hemoglobin, causing a single amino acid change that changes hemoglobin's physical properties.\nSickle-cell versions of hemoglobin stick to themselves, stacking to form fibers that distort the shape of red blood cells carrying the protein. These sickle-shaped cells no longer flow smoothly through blood vessels, having a tendency to clog or degrade, causing the medical problems associated with this disease.\nSome DNA sequences are transcribed into RNA but are not translated into protein products—such RNA molecules are called non-coding RNA. In some cases, these products fold into structures which are involved in critical cell functions (e.g. ribosomal RNA and transfer RNA). RNA can also have regulatory effects through hybridization interactions with other RNA molecules (such as microRNA)."
      },
      {
        "heading": "Nature and nurture",
        "level": 2,
        "content": "Although genes contain all the information an organism uses to function, the environment plays an important role in determining the ultimate phenotypes an organism displays. The phrase \"nature and nurture\" refers to this complementary relationship. The phenotype of an organism depends on the interaction of genes and the environment. An interesting example is the coat coloration of the Siamese cat. In this case, the body temperature of the cat plays the role of the environment. The cat's genes code for dark hair, thus the hair-producing cells in the cat make cellular proteins resulting in dark hair. But these dark hair-producing proteins are sensitive to temperature (i.e. have a mutation causing temperature-sensitivity) and denature in higher-temperature environments, failing to produce dark-hair pigment in areas where the cat has a higher body temperature. In a low-temperature environment, however, the protein's structure is stable and produces dark-hair pigment normally. The protein remains functional in areas of skin that are colder—such as its legs, ears, tail, and face—so the cat has dark hair at its extremities.\nEnvironment plays a major role in effects of the human genetic disease phenylketonuria. The mutation that causes phenylketonuria disrupts the ability of the body to break down the amino acid phenylalanine, causing a toxic build-up of an intermediate molecule that, in turn, causes severe symptoms of progressive intellectual disability and seizures. However, if someone with the phenylketonuria mutation follows a strict diet that avoids this amino acid, they remain normal and healthy.\nA common method for determining how genes and environment (\"nature and nurture\") contribute to a phenotype involves studying identical and fraternal twins, or other siblings of multiple births. Identical siblings are genetically the same since they come from the same zygote. Meanwhile, fraternal twins are as genetically different from one another as normal siblings. By comparing how often a certain disorder occurs in a pair of identical twins to how often it occurs in a pair of fraternal twins, scientists can determine whether that disorder is caused by genetic or postnatal environmental factors. One famous example involved the study of the Genain quadruplets, who were identical quadruplets all diagnosed with schizophrenia."
      },
      {
        "heading": "Gene regulation",
        "level": 2,
        "content": "The genome of a given organism contains thousands of genes, but not all these genes need to be active at any given moment. A gene is expressed when it is being transcribed into mRNA and there exist many cellular methods of controlling the expression of genes such that proteins are produced only when needed by the cell. Transcription factors are regulatory proteins that bind to DNA, either promoting or inhibiting the transcription of a gene. Within the genome of Escherichia coli bacteria, for example, there exists a series of genes necessary for the synthesis of the amino acid tryptophan. However, when tryptophan is already available to the cell, these genes for tryptophan synthesis are no longer needed. The presence of tryptophan directly affects the activity of the genes—tryptophan molecules bind to the tryptophan repressor (a transcription factor), changing the repressor's structure such that the repressor binds to the genes. The tryptophan repressor blocks the transcription and expression of the genes, thereby creating negative feedback regulation of the tryptophan synthesis process.\n\nDifferences in gene expression are especially clear within multicellular organisms, where cells all contain the same genome but have very different structures and behaviors due to the expression of different sets of genes. All the cells in a multicellular organism derive from a single cell, differentiating into variant cell types in response to external and intercellular signals and gradually establishing different patterns of gene expression to create different behaviors. As no single gene is responsible for the development of structures within multicellular organisms, these patterns arise from the complex interactions between many cells.\nWithin eukaryotes, there exist structural features of chromatin that influence the transcription of genes, often in the form of modifications to DNA and chromatin that are stably inherited by daughter cells. These features are called \"epigenetic\" because they exist \"on top\" of the DNA sequence and retain inheritance from one cell generation to the next. Because of epigenetic features, different cell types grown within the same medium can retain very different properties. Although epigenetic features are generally dynamic over the course of development, some, like the phenomenon of paramutation, have multigenerational inheritance and exist as rare exceptions to the general rule of DNA as the basis for inheritance."
      },
      {
        "heading": "Genetic change",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Mutations",
        "level": 2,
        "content": "During the process of DNA replication, errors occasionally occur in the polymerization of the second strand. These errors, called mutations, can affect the phenotype of an organism, especially if they occur within the protein coding sequence of a gene. Error rates are usually very low—1 error in every 10–100 million bases—due to the \"proofreading\" ability of DNA polymerases. Processes that increase the rate of changes in DNA are called mutagenic: mutagenic chemicals promote errors in DNA replication, often by interfering with the structure of base-pairing, while UV radiation induces mutations by causing damage to the DNA structure. Chemical damage to DNA occurs naturally as well and cells use DNA repair mechanisms to repair mismatches and breaks. The repair does not, however, always restore the original sequence. A particularly important source of DNA damages appears to be reactive oxygen species produced by cellular aerobic respiration, and these can lead to mutations.\n\nIn organisms that use chromosomal crossover to exchange DNA and recombine genes, errors in alignment during meiosis can also cause mutations. Errors in crossover are especially likely when similar sequences cause partner chromosomes to adopt a mistaken alignment; this makes some regions in genomes more prone to mutating in this way. These errors create large structural changes in DNA sequence—duplications, inversions, deletions of entire regions—or the accidental exchange of whole parts of sequences between different chromosomes, chromosomal translocation."
      },
      {
        "heading": "Natural selection and evolution",
        "level": 2,
        "content": "Mutations alter an organism's genotype and occasionally this causes different phenotypes to appear. Most mutations have little effect on an organism's phenotype, health, or reproductive fitness. Mutations that do have an effect are usually detrimental, but occasionally some can be beneficial. Studies in the fly Drosophila melanogaster suggest that if a mutation changes a protein produced by a gene, about 70 percent of these mutations are harmful with the remainder being either neutral or weakly beneficial.\n\nPopulation genetics studies the distribution of genetic differences within populations and how these distributions change over time. Changes in the frequency of an allele in a population are mainly influenced by natural selection, where a given allele provides a selective or reproductive advantage to the organism, as well as other factors such as mutation, genetic drift, genetic hitchhiking, artificial selection and migration.\nOver many generations, the genomes of organisms can change significantly, resulting in evolution. In the process called adaptation, selection for beneficial mutations can cause a species to evolve into forms better able to survive in their environment. New species are formed through the process of speciation, often caused by geographical separations that prevent populations from exchanging genes with each other.\nBy comparing the homology between different species' genomes, it is possible to calculate the evolutionary distance between them and when they may have diverged. Genetic comparisons are generally considered a more accurate method of characterizing the relatedness between species than the comparison of phenotypic characteristics. The evolutionary distances between species can be used to form evolutionary trees; these trees represent the common descent and divergence of species over time, although they do not show the transfer of genetic material between unrelated species (known as horizontal gene transfer and most common in bacteria)."
      },
      {
        "heading": "Research and technology",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Model organisms",
        "level": 2,
        "content": "Although geneticists originally studied inheritance in a wide variety of organisms, the range of species studied has narrowed. One reason is that when significant research already exists for a given organism, new researchers are more likely to choose it for further study, and so eventually a few model organisms became the basis for most genetics research. Common research topics in model organism genetics include the study of gene regulation and the involvement of genes in development and cancer. Organisms were chosen, in part, for convenience—short generation times and easy genetic manipulation made some organisms popular genetics research tools. Widely used model organisms include the gut bacterium Escherichia coli, the plant Arabidopsis thaliana, baker's yeast (Saccharomyces cerevisiae), the nematode Caenorhabditis elegans, the common fruit fly (Drosophila melanogaster), the zebrafish (Danio rerio), and the common house mouse (Mus musculus)."
      },
      {
        "heading": "Medicine",
        "level": 2,
        "content": "Medical genetics seeks to understand how genetic variation relates to human health and disease. When searching for an unknown gene that may be involved in a disease, researchers commonly use genetic linkage and genetic pedigree charts to find the location on the genome associated with the disease. At the population level, researchers take advantage of Mendelian randomization to look for locations in the genome that are associated with diseases, a method especially useful for multigenic traits not clearly defined by a single gene. Once a candidate gene is found, further research is often done on the corresponding (or homologous) genes of model organisms. In addition to studying genetic diseases, the increased availability of genotyping methods has led to the field of pharmacogenetics: the study of how genotype can affect drug responses.\nIndividuals differ in their inherited tendency to develop cancer, and cancer is a genetic disease. The process of cancer development in the body is a combination of events. Mutations occasionally occur within cells in the body as they divide. Although these mutations will not be inherited by any offspring, they can affect the behavior of cells, sometimes causing them to grow and divide more frequently. There are biological mechanisms that attempt to stop this process; signals are given to inappropriately dividing cells that should trigger cell death, but sometimes additional mutations occur that cause cells to ignore these messages. An internal process of natural selection occurs within the body and eventually mutations accumulate within cells to promote their own growth, creating a cancerous tumor that grows and invades various tissues of the body. Normally, a cell divides only in response to signals called growth factors and stops growing once in contact with surrounding cells and in response to growth-inhibitory signals. It usually then divides a limited number of times and dies, staying within the epithelium where it is unable to migrate to other organs. To become a cancer cell, a cell has to accumulate mutations in a number of genes (three to seven). A cancer cell can divide without growth factor and ignores inhibitory signals. Also, it is immortal and can grow indefinitely, even after it makes contact with neighboring cells. It may escape from the epithelium and ultimately from the primary tumor. Then, the escaped cell can cross the endothelium of a blood vessel and get transported by the bloodstream to colonize a new organ, forming deadly metastasis. Although there are some genetic predispositions in a small fraction of cancers, the major fraction is due to a set of new genetic mutations that originally appear and accumulate in one or a small number of cells that will divide to form the tumor and are not transmitted to the progeny (somatic mutations). The most frequent mutations are a loss of function of p53 protein, a tumor suppressor, or in the p53 pathway, and gain of function mutations in the Ras proteins, or in other oncogenes."
      },
      {
        "heading": "Research methods",
        "level": 2,
        "content": "DNA can be manipulated in the laboratory. Restriction enzymes are commonly used enzymes that cut DNA at specific sequences, producing predictable fragments of DNA. DNA fragments can be visualized through use of gel electrophoresis, which separates fragments according to their length.\nThe use of ligation enzymes allows DNA fragments to be connected. By binding (\"ligating\") fragments of DNA together from different sources, researchers can create recombinant DNA, the DNA often associated with genetically modified organisms. Recombinant DNA is commonly used in the context of plasmids: short circular DNA molecules with a few genes on them. In the process known as molecular cloning, researchers can amplify the DNA fragments by inserting plasmids into bacteria and then culturing them on plates of agar (to isolate clones of bacteria cells). \"Cloning\" can also refer to the various means of creating cloned (\"clonal\") organisms.\nDNA can also be amplified using a procedure called the polymerase chain reaction (PCR). By using specific short sequences of DNA, PCR can isolate and exponentially amplify a targeted region of DNA. Because it can amplify from extremely small amounts of DNA, PCR is also often used to detect the presence of specific DNA sequences."
      },
      {
        "heading": "DNA sequencing and genomics",
        "level": 2,
        "content": "DNA sequencing, one of the most fundamental technologies developed to study genetics, allows researchers to determine the sequence of nucleotides in DNA fragments. The technique of chain-termination sequencing, developed in 1977 by a team led by Frederick Sanger, is still routinely used to sequence DNA fragments. Using this technology, researchers have been able to study the molecular sequences associated with many human diseases.\nAs sequencing has become less expensive, researchers have sequenced the genomes of many organisms using a process called genome assembly, which uses computational tools to stitch together sequences from many different fragments. These technologies were used to sequence the human genome in the Human Genome Project completed in 2003. New high-throughput sequencing technologies are dramatically lowering the cost of DNA sequencing, with many researchers hoping to bring the cost of resequencing a human genome down to a thousand dollars.\nNext-generation sequencing (or high-throughput sequencing) came about due to the ever-increasing demand for low-cost sequencing. These sequencing technologies allow the production of potentially millions of sequences concurrently. The large amount of sequence data available has created the subfield of genomics, research that uses computational tools to search for and analyze patterns in the full genomes of organisms. Genomics can also be considered a subfield of bioinformatics, which uses computational approaches to analyze large sets of biological data. A common problem to these fields of research is how to manage and share data that deals with human subject and personally identifiable information."
      },
      {
        "heading": "Society and culture",
        "level": 1,
        "content": "On 19 March 2015, a group of leading biologists urged a worldwide ban on clinical use of methods, particularly the use of CRISPR and zinc finger, to edit the human genome in a way that can be inherited. In April 2015, Chinese researchers reported results of basic research to edit the DNA of non-viable human embryos using CRISPR."
      },
      {
        "heading": "See also",
        "level": 1,
        "content": ""
      },
      {
        "heading": "References",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Further reading",
        "level": 1,
        "content": ""
      },
      {
        "heading": "External links",
        "level": 1,
        "content": "\n Quotations related to Genetics at Wikiquote\n Genetics at Wikibooks\nLibrary resources in your library and in other libraries about Genetics\nGenetics on In Our Time at the BBC"
      }
    ],
    "summary": "Genetics is the study of genes, genetic variation, and heredity in organisms. It is an important branch in biology because heredity is vital to organisms' evolution. Gregor Mendel, a Moravian Augustinian friar working in the 19th century in Brno, was the first to study genetics scientifically. Mendel studied \"trait inheritance\", patterns in the way traits are handed down from parents to offspring over time. He observed that organisms (pea plants) inherit traits by way of discrete \"units of inheritance\". This term, still used today, is a somewhat ambiguous definition of what is referred to as a gene.\nTrait inheritance and molecular inheritance mechanisms of genes are still primary principles of genetics in the 21st century, but modern genetics has expanded to study the function and behavior of genes. Gene structure and function, variation, and distribution are studied within the context of the cell, the organism (e.g. dominance), and within the context of a population. Genetics has given rise to a number of subfields, including molecular genetics, epigenetics, and population genetics. Organisms studied within the broad field span the domains of life (archaea, bacteria, and eukarya).\nGenetic processes work in combination with an organism's environment and experiences to influence development and behavior, often referred to as nature versus nurture. The intracellular or extracellular environment of a living cell or organism may increase or decrease gene transcription. A classic example is two seeds of genetically identical corn, one placed in a temperate climate and one in an arid climate (lacking sufficient waterfall or rain). While the average height the two corn stalks could grow to is genetically determined, the one in the arid climate only grows to half the height of the one in the temperate climate due to lack of water and nutrients in its environment."
  },
  {
    "title": "Genética",
    "source": "https://es.wikipedia.org/wiki/Gen%C3%A9tica",
    "language": "es",
    "chunks": [
      {
        "heading": "Primeros estudios genéticos",
        "level": 1,
        "content": "Gregor Johann Mendel (20 de julio de 1822[4]​-6 de enero de 1884) fue un monje agustino católico y naturalista nacido en Heinzendorf, Austria (actual Hynčice, distrito Nový Jičín, República Checa) que descubrió, por medio de la experimentación de mezclas de diferentes variedades de guisantes, chícharos o arvejas (Pisum sativum), las llamadas Leyes de Mendel que dieron origen a la herencia genética. \nEn 1941 Edward Lawrie Tatum y George Wells Beadle demostraron que los genes ARN mensajero codifican proteínas; luego en 1953 James D. Watson y Francis Crick determinaron que la estructura del ADN es una doble hélice en direcciones antiparalelas, polimerizadas en dirección 5' a 3', para el año 1977 Frederick Sanger, Walter Gilbert, y Allan Maxam secuencian ADN completo del genoma del bacteriófago y en 1990 se funda el Proyecto Genoma Humano."
      },
      {
        "heading": "La ciencia de la genética",
        "level": 1,
        "content": "Aunque la genética juega un papel muy significativo en la apariencia y el comportamiento de los organismos, es la combinación de la genética, replicación, transcripción y procesamiento (maduración del ARN) con las experiencias del organismo la cual determina el resultado final.\nLos genes corresponden a regiones del ADN o ARN, dos moléculas compuestas de una cadena de cuatro tipos diferentes de bases nitrogenadas (adenina, timina, citosina y guanina en ADN), en las cuales tras la transcripción (síntesis de ARN) se cambia la timina por uracilo —la secuencia de estos nucleótidos es la información genética que heredan los organismos. El ADN existe naturalmente en forma bicatenaria, es decir, en dos cadenas en que los nucleótidos de una cadena complementan los de la otra.\nLa secuencia de nucleótidos de un gen es traducida por las células para producir una cadena de aminoácidos, creando proteínas —el orden de los aminoácidos en una proteína corresponde con el orden de los nucleótidos del gen. Esto recibe el nombre de código genético. Los aminoácidos de una proteína determinan cómo se pliega en una forma tridimensional y responsable del funcionamiento de la proteína. Las proteínas ejecutan casi todas las funciones que las células necesitan para vivir.\nEl genoma es la totalidad de la información genética que posee un organismo en particular. Por lo general, al hablar de genoma en los seres eucarióticos se refiere solo al ADN contenido en el núcleo, organizado en cromosomas, pero también la mitocondria contiene genes y es llamada genoma mitocondrial."
      },
      {
        "heading": "Subdivisiones de la genética",
        "level": 2,
        "content": "La genética se subdivide en varias ramas, como:\n\nCitogenética: El eje central de esta disciplina es el estudio del cromosoma y su dinámica, así como el estudio del ciclo celular y su repercusión en la herencia. Está muy vinculada a la biología de la reproducción y a la biología celular.\nClásica o Mendeliana: Se basa en las leyes de Mendel para predecir la herencia de ciertos caracteres o enfermedades. La genética clásica también analiza como el fenómeno de la recombinación o el ligamiento alteran los resultados esperados según las leyes de Mendel.\nCuantitativa: Analiza el impacto de múltiples genes sobre el fenotipo, muy especialmente cuando estos tienen efectos de pequeña escala.\nFilogenia: Es la genética que estudia el parentesco entre los distintos taxones de seres vivos.\nGenética clínica: Aplica la genética para diagnosticar patologías de origen genético.\nGenética preventiva: Hace uso de la genética para mostrar las distintas predisposiciones que se pueden tener a diversos factores.\nGenética de poblaciones: Se preocupa del comportamiento de los genes en una población y de cómo esto determina la evolución de los organismos.\nGenética del desarrollo: Estudia cómo los genes son regulados para formar un organismo completo a partir de una célula inicial.\nGenética molecular: Estudia el ADN, su composición y la manera en que se duplica. Así mismo, estudia la función de los genes desde el punto de vista molecular: Como transmiten su información hasta llegar a sintetizar proteínas.\nMutagénesis: Estudia el origen y las repercusiones de las mutaciones en los diferentes niveles del material genético."
      },
      {
        "heading": "Ingeniería genética",
        "level": 2,
        "content": "La ingeniería genética es la especialidad que utiliza tecnología de la manipulación y transferencia del ADN de unos organismos a otros, permitiendo controlar algunas de sus propiedades genéticas. Mediante la ingeniería genética se pueden potenciar y eliminar cualidades de organismos en el laboratorio (véase Organismo genéticamente modificado). Por ejemplo, se pueden corregir defectos genéticos (terapia génica), fabricar antibióticos en las glándulas mamarias de vacas de granja o clonar animales como la oveja Dolly.\nAlgunas de las formas de controlar esto es mediante transfección (lisar células y usar material genético libre), conjugación (plásmidos) y transducción (uso de fagos o virus), entre otras formas. Además se puede ver la manera de regular esta expresión genética en los organismos.\nRespecto a la terapia génica, antes mencionada, hay que decir que todavía no se ha conseguido llevar a cabo un tratamiento, con éxito, en humanos para curar alguna enfermedad. Todas las investigaciones se encuentran en la fase experimental. Debido a que aún no se ha descubierto la forma de que la terapia funcione (tal vez, aplicando distintos métodos para introducir el ADN), cada vez son menos los fondos dedicados a este tipo de investigaciones. Por otro lado, aunque este es un campo que puede generar muchos beneficios económicos, este tipo de terapias son muy costosas, por lo que, en cuanto se consiga mejorar la técnica y  disminuir su coste, es de suponer que las inversiones subirán."
      },
      {
        "heading": "Genética muscular",
        "level": 1,
        "content": "Investigaciones actuales afirman que los marcadores metabólicos entre los distintos tipos de genética muscular pueden diferenciarse en un 7-18%. La diferencia principal se encuentra en la reacción del cuerpo ante la ingesta de carbohidratos y los niveles de las hormonas sexuales como la testosterona. \nLa genética muscular es un área de la ciencia con potenciales herramientas para mejorar los resultados en el deporte. Determinar la predisposición genética de un individuo: ectomorfo, mesomorfo o endomorfo, es una estrategia utilizada por los profesionales del deporte para incrementar el rendimiento. Se han diferencias en la concentración de creatina en los distintos tipos somatotipos corporales así como diferencias en las concentraciones de distintos marcadores metabólicos.[5]​[6]​"
      },
      {
        "heading": "Cronología de descubrimientos genéticos notables",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Adaptaciones genéticas",
        "level": 1,
        "content": "Los cambios genéticos pueden dotar a las especies de rasgos complejos que les permita expandirse y ocupar nuevos nichos. Estos cambios son claves para la especiación y diversificación. Por ejemplo, para adaptarse a la vida en las alturas de los árboles, diferentes especies de ranas han adquirido evolutivamente rasgos complejos para escalar y planear.[9]​"
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bibliografía",
        "level": 1,
        "content": "GRIFFITHS, A.J.F., S. R. WESSLER, R.C. LEWONTIN & S. B. CARROLL (2008). Genética. MGraw-Hill Interamericana. Novena edición.\nKLUG, W.S. & CUMMINGS, M.R. (1998). Conceptos de Genética. 5.ª Edición. Prentice Hall. España.\nBENITO-JIMÉNEZ, C. (1997). 360 Problemas de Genética. Resueltos paso a paso. 1.ª Edición. Editorial Síntesis. España.\nMENSUA, J.L. (2002). Genética: Problemas y ejercicios resueltos. Prentice."
      },
      {
        "heading": "Bibliografía adicional",
        "level": 1,
        "content": "Alberts, Bruce; Bray, Dennis; Hopkin, Karen; Johnson, Alexander; Lewis, Julian; Raff, Martin; Roberts, Keith; Walter, Peter (2013). Essential Cell Biology, 4th Edition (en inglés). Garland Science. ISBN 978-1-317-80627-1. \nGriffiths, Anthony J.F.; Miller, Jeffrey H.; Suzuki, David T.; Lewontin, Richard C.; Gelbart, eds. (2000). An Introduction to Genetic Analysis (en inglés) (7th edición). New York: W. H. Freeman. ISBN 978-0-7167-3520-5. \nHartl D, Jones E (2005). Genetics: Analysis of Genes and Genomes (en inglés) (6th edición). Jones & Bartlett. ISBN 978-0-7637-1511-3. \nKing, Robert C; Mulligan, Pamela K; Stansfield, William D (2013). A Dictionary of Genetics (en inglés) (8th edición). New York: Oxford University Press. ISBN 978-0-19-976644-4. \nLodish H, Berk A, Zipursky LS, Matsudaira P, Baltimore D, Darnell J (2000). Molecular Cell Biology (en inglés) (4th edición). New York: Scientific American Books. ISBN 978-0-7167-3136-8."
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": " Wikcionario  tiene definiciones y otra información sobre genética.\n Wikiversidad alberga proyectos de aprendizaje sobre Genética.\n Wikinoticias tiene noticias relacionadas con Genética.\n Wikimedia Commons alberga una categoría multimedia sobre Genética.\nSociedad española de genética.\nSociedad de Genética de Chile.\nCentro de Genética Humana, Facultad de Medicina CAS-UDD.\nAsociación Española de Genética Humana.\nInstituto de Genética Humana.\nGrado de genética de la Universidad Autónoma de Barcelona.\nLa genética al alcance de todos.\nCitología y Genética. Revista científica.\nGenética de poblaciones y gráficos de distancias genéticas.\nCentro de Regulación Genómica.\nLeyendo el libro de la vida: Museo Virtual Interactivo sobre la Genética y el ADN.\nCurso genética de la UAB. Plataforma Web 2.0 para la docencia universitaria\nGenética Médica News.\nNoticias de actualidad sobre genética"
      }
    ],
    "summary": "El término genética (del griego antiguo: γενετικός, guennetikós, ‘genetivo’, y este de γένεσις, génesis, ‘origen’;[1]​[2]​[3]​ acuñado en 1905 por William Bateson) alude al área de estudio de la biología que busca comprender y explicar cómo se transmite la herencia biológica de generación en generación mediante el ADN. Se trata de una de las funciones fundamentales de la biología moderna, y abarca en su interior un gran número de disciplinas propias e interdisciplinarias que se relacionan directamente con la bioquímica, la medicina y la biología celular.\nEl principal objeto de estudio de la genética son los genes, formados por segmentos de ADN y ARN, tras la transcripción de ARN mensajero, ARN ribosómico y ARN de transferencia, los cuales se sintetizan a partir de ADN. El ADN controla la estructura y el funcionamiento de cada célula, tiene la capacidad de crear copias exactas de sí mismo tras un proceso llamado replicación."
  },
  {
    "title": "Evolution",
    "source": "https://en.wikipedia.org/wiki/Evolution",
    "language": "en",
    "chunks": [
      {
        "heading": "Heredity",
        "level": 1,
        "content": "Evolution in organisms occurs through changes in heritable characteristics—the inherited characteristics of an organism. In humans, for example, eye colour is an inherited characteristic and an individual might inherit the \"brown-eye trait\" from one of their parents. Inherited traits are controlled by genes and the complete set of genes within an organism's genome (genetic material) is called its genotype.\nThe complete set of observable traits that make up the structure and behaviour of an organism is called its phenotype. Some of these traits come from the interaction of its genotype with the environment while others are neutral. Some observable characteristics are not inherited. For example, suntanned skin comes from the interaction between a person's genotype and sunlight; thus, suntans are not passed on to people's children. The phenotype is the ability of the skin to tan when exposed to sunlight. However, some people tan more easily than others, due to differences in genotypic variation; a striking example are people with the inherited trait of albinism, who do not tan at all and are very sensitive to sunburn.\nHeritable characteristics are passed from one generation to the next via DNA, a molecule that encodes genetic information. DNA is a long biopolymer composed of four types of bases. The sequence of bases along a particular DNA molecule specifies the genetic information, in a manner similar to a sequence of letters spelling out a sentence. Before a cell divides, the DNA is copied, so that each of the resulting two cells will inherit the DNA sequence. Portions of a DNA molecule that specify a single functional unit are called genes; different genes have different sequences of bases. Within cells, each long strand of DNA is called a chromosome. The specific location of a DNA sequence within a chromosome is known as a locus. If the DNA sequence at a locus varies between individuals, the different forms of this sequence are called alleles. DNA sequences can change through mutations, producing new alleles. If a mutation occurs within a gene, the new allele may affect the trait that the gene controls, altering the phenotype of the organism. However, while this simple correspondence between an allele and a trait works in some cases, most traits are influenced by multiple genes in a quantitative or epistatic manner."
      },
      {
        "heading": "Sources of variation",
        "level": 1,
        "content": "Evolution can occur if there is genetic variation within a population. Variation comes from mutations in the genome, reshuffling of genes through sexual reproduction and migration between populations (gene flow). Despite the constant introduction of new variation through mutation and gene flow, most of the genome of a species is very similar among all individuals of that species. However, discoveries in the field of evolutionary developmental biology have demonstrated that even relatively small differences in genotype can lead to dramatic differences in phenotype both within and between species.\nAn individual organism's phenotype results from both its genotype and the influence of the environment it has lived in. The modern evolutionary synthesis defines evolution as the change over time in this genetic variation. The frequency of one particular allele will become more or less prevalent relative to other forms of that gene. Variation disappears when a new allele reaches the point of fixation—when it either disappears from the population or replaces the ancestral allele entirely."
      },
      {
        "heading": "Mutation",
        "level": 2,
        "content": "Mutations are changes in the DNA sequence of a cell's genome and are the ultimate source of genetic variation in all organisms. When mutations occur, they may alter the product of a gene, or prevent the gene from functioning, or have no effect.\nAbout half of the mutations in the coding regions of protein-coding genes are deleterious — the other half are neutral. A small percentage of the total mutations in this region confer a fitness benefit. Some of the mutations in other parts of the genome are deleterious but the vast majority are neutral. A few are beneficial.\nMutations can involve large sections of a chromosome becoming duplicated (usually by genetic recombination), which can introduce extra copies of a gene into a genome. Extra copies of genes are a major source of the raw material needed for new genes to evolve. This is important because most new genes evolve within gene families from pre-existing genes that share common ancestors. For example, the human eye uses four genes to make structures that sense light: three for colour vision and one for night vision; all four are descended from a single ancestral gene.\nNew genes can be generated from an ancestral gene when a duplicate copy mutates and acquires a new function. This process is easier once a gene has been duplicated because it increases the redundancy of the system; one gene in the pair can acquire a new function while the other copy continues to perform its original function. Other types of mutations can even generate entirely new genes from previously noncoding DNA, a phenomenon termed de novo gene birth.\nThe generation of new genes can also involve small parts of several genes being duplicated, with these fragments then recombining to form new combinations with new functions (exon shuffling). When new genes are assembled from shuffling pre-existing parts, domains act as modules with simple independent functions, which can be mixed together to produce new combinations with new and complex functions. For example, polyketide synthases are large enzymes that make antibiotics; they contain up to 100 independent domains that each catalyse one step in the overall process, like a step in an assembly line.\nOne example of mutation is wild boar piglets. They are camouflage coloured and show a characteristic pattern of dark and light longitudinal stripes. However, mutations in the melanocortin 1 receptor (MC1R) disrupt the pattern. The majority of pig breeds carry MC1R mutations disrupting wild-type colour and different mutations causing dominant black colouring."
      },
      {
        "heading": "Sex and recombination",
        "level": 2,
        "content": "In asexual organisms, genes are inherited together, or linked, as they cannot mix with genes of other organisms during reproduction. In contrast, the offspring of sexual organisms contain random mixtures of their parents' chromosomes that are produced through independent assortment. In a related process called homologous recombination, sexual organisms exchange DNA between two matching chromosomes. Recombination and reassortment do not alter allele frequencies, but instead change which alleles are associated with each other, producing offspring with new combinations of alleles. Sex usually increases genetic variation and may increase the rate of evolution.\n\nThe two-fold cost of sex was first described by John Maynard Smith. The first cost is that in sexually dimorphic species only one of the two sexes can bear young. This cost does not apply to hermaphroditic species, like most plants and many invertebrates. The second cost is that any individual who reproduces sexually can only pass on 50% of its genes to any individual offspring, with even less passed on as each new generation passes. Yet sexual reproduction is the more common means of reproduction among eukaryotes and multicellular organisms. The Red Queen hypothesis has been used to explain the significance of sexual reproduction as a means to enable continual evolution and adaptation in response to coevolution with other species in an ever-changing environment. Another hypothesis is that sexual reproduction is primarily an adaptation for promoting accurate recombinational repair of damage in germline DNA, and that increased diversity is a byproduct of this process that may sometimes be adaptively beneficial."
      },
      {
        "heading": "Gene flow",
        "level": 2,
        "content": "Gene flow is the exchange of genes between populations and between species. It can therefore be a source of variation that is new to a population or to a species. Gene flow can be caused by the movement of individuals between separate populations of organisms, as might be caused by the movement of mice between inland and coastal populations, or the movement of pollen between heavy-metal-tolerant and heavy-metal-sensitive populations of grasses.\nGene transfer between species includes the formation of hybrid organisms and horizontal gene transfer. Horizontal gene transfer is the transfer of genetic material from one organism to another organism that is not its offspring; this is most common among bacteria. In medicine, this contributes to the spread of antibiotic resistance, as when one bacteria acquires resistance genes it can rapidly transfer them to other species. Horizontal transfer of genes from bacteria to eukaryotes such as the yeast Saccharomyces cerevisiae and the adzuki bean weevil Callosobruchus chinensis has occurred. An example of larger-scale transfers are the eukaryotic bdelloid rotifers, which have received a range of genes from bacteria, fungi and plants. Viruses can also carry DNA between organisms, allowing transfer of genes even across biological domains.\nLarge-scale gene transfer has also occurred between the ancestors of eukaryotic cells and bacteria, during the acquisition of chloroplasts and mitochondria. It is possible that eukaryotes themselves originated from horizontal gene transfers between bacteria and archaea."
      },
      {
        "heading": "Epigenetics",
        "level": 2,
        "content": "Some heritable changes cannot be explained by changes to the sequence of nucleotides in the DNA. These phenomena are classed as epigenetic inheritance systems. DNA methylation marking chromatin, self-sustaining metabolic loops, gene silencing by RNA interference and the three-dimensional conformation of proteins (such as prions) are areas where epigenetic inheritance systems have been discovered at the organismic level. Developmental biologists suggest that complex interactions in genetic networks and communication among cells can lead to heritable variations that may underlay some of the mechanics in developmental plasticity and canalisation. Heritability may also occur at even larger scales. For example, ecological inheritance through the process of niche construction is defined by the regular and repeated activities of organisms in their environment. This generates a legacy of effects that modify and feed back into the selection regime of subsequent generations. Other examples of heritability in evolution that are not under the direct control of genes include the inheritance of cultural traits and symbiogenesis."
      },
      {
        "heading": "Evolutionary forces",
        "level": 1,
        "content": "From a neo-Darwinian perspective, evolution occurs when there are changes in the frequencies of alleles within a population of interbreeding organisms, for example, the allele for black colour in a population of moths becoming more common. Mechanisms that can lead to changes in allele frequencies include natural selection, genetic drift, and mutation bias."
      },
      {
        "heading": "Natural selection",
        "level": 2,
        "content": "Evolution by natural selection is the process by which traits that enhance survival and reproduction become more common in successive generations of a population. It embodies three principles:\n\nVariation exists within populations of organisms with respect to morphology, physiology and behaviour (phenotypic variation).\nDifferent traits confer different rates of survival and reproduction (differential fitness).\nThese traits can be passed from generation to generation (heritability of fitness).\nMore offspring are produced than can possibly survive, and these conditions produce competition between organisms for survival and reproduction. Consequently, organisms with traits that give them an advantage over their competitors are more likely to pass on their traits to the next generation than those with traits that do not confer an advantage. This teleonomy is the quality whereby the process of natural selection creates and preserves traits that are seemingly fitted for the functional roles they perform. Consequences of selection include nonrandom mating and genetic hitchhiking.\nThe central concept of natural selection is the evolutionary fitness of an organism. Fitness is measured by an organism's ability to survive and reproduce, which determines the size of its genetic contribution to the next generation. However, fitness is not the same as the total number of offspring: instead fitness is indicated by the proportion of subsequent generations that carry an organism's genes. For example, if an organism could survive well and reproduce rapidly, but its offspring were all too small and weak to survive, this organism would make little genetic contribution to future generations and would thus have low fitness.\nIf an allele increases fitness more than the other alleles of that gene, then with each generation this allele has a higher probability of becoming common within the population. These traits are said to be selected for. Examples of traits that can increase fitness are enhanced survival and increased fecundity. Conversely, the lower fitness caused by having a less beneficial or deleterious allele results in this allele likely becoming rarer—they are selected against.\nImportantly, the fitness of an allele is not a fixed characteristic; if the environment changes, previously neutral or harmful traits may become beneficial and previously beneficial traits become harmful. However, even if the direction of selection does reverse in this way, traits that were lost in the past may not re-evolve in an identical form. However, a re-activation of dormant genes, as long as they have not been eliminated from the genome and were only suppressed perhaps for hundreds of generations, can lead to the re-occurrence of traits thought to be lost like hindlegs in dolphins, teeth in chickens, wings in wingless stick insects, tails and additional nipples in humans etc. \"Throwbacks\" such as these are known as atavisms.\n\nNatural selection within a population for a trait that can vary across a range of values, such as height, can be categorised into three different types. The first is directional selection, which is a shift in the average value of a trait over time—for example, organisms slowly getting taller. Secondly, disruptive selection is selection for extreme trait values and often results in two different values becoming most common, with selection against the average value. This would be when either short or tall organisms had an advantage, but not those of medium height. Finally, in stabilising selection there is selection against extreme trait values on both ends, which causes a decrease in variance around the average value and less diversity. This would, for example, cause organisms to eventually have a similar height.\nNatural selection most generally makes nature the measure against which individuals and individual traits, are more or less likely to survive. \"Nature\" in this sense refers to an ecosystem, that is, a system in which organisms interact with every other element, physical as well as biological, in their local environment. Eugene Odum, a founder of ecology, defined an ecosystem as: \"Any unit that includes all of the organisms...in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e., exchange of materials between living and nonliving parts) within the system....\" Each population within an ecosystem occupies a distinct niche, or position, with distinct relationships to other parts of the system. These relationships involve the life history of the organism, its position in the food chain and its geographic range. This broad understanding of nature enables scientists to delineate specific forces which, together, comprise natural selection.\nNatural selection can act at different levels of organisation, such as genes, cells, individual organisms, groups of organisms and species. Selection can act at multiple levels simultaneously. An example of selection occurring below the level of the individual organism are genes called transposons, which can replicate and spread throughout a genome. Selection at a level above the individual, such as group selection, may allow the evolution of cooperation."
      },
      {
        "heading": "Genetic drift",
        "level": 2,
        "content": "Genetic drift is the random fluctuation of allele frequencies within a population from one generation to the next. When selective forces are absent or relatively weak, allele frequencies are equally likely to drift upward or downward in each successive generation because the alleles are subject to sampling error. This drift halts when an allele eventually becomes fixed, either by disappearing from the population or by replacing the other alleles entirely. Genetic drift may therefore eliminate some alleles from a population due to chance alone. Even in the absence of selective forces, genetic drift can cause two separate populations that begin with the same genetic structure to drift apart into two divergent populations with different sets of alleles.\nAccording to the neutral theory of molecular evolution most evolutionary changes are the result of the fixation of neutral mutations by genetic drift. In this model, most genetic changes in a population are thus the result of constant mutation pressure and genetic drift. This form of the neutral theory has been debated since it does not seem to fit some genetic variation seen in nature. A better-supported version of this model is the nearly neutral theory, according to which a mutation that would be effectively neutral in a small population is not necessarily neutral in a large population. Other theories propose that genetic drift is dwarfed by other stochastic forces in evolution, such as genetic hitchhiking, also known as genetic draft. Another concept is constructive neutral evolution (CNE), which explains that complex systems can emerge and spread into a population through neutral transitions due to the principles of excess capacity, presuppression, and ratcheting, and it has been applied in areas ranging from the origins of the spliceosome to the complex interdependence of microbial communities.\nThe time it takes a neutral allele to become fixed by genetic drift depends on population size; fixation is more rapid in smaller populations. The number of individuals in a population is not critical, but instead a measure known as the effective population size. The effective population is usually smaller than the total population since it takes into account factors such as the level of inbreeding and the stage of the lifecycle in which the population is the smallest. The effective population size may not be the same for every gene in the same population.\nIt is usually difficult to measure the relative importance of selection and neutral processes, including drift. The comparative importance of adaptive and non-adaptive forces in driving evolutionary change is an area of current research."
      },
      {
        "heading": "Mutation bias",
        "level": 2,
        "content": "Mutation bias is usually conceived as a difference in expected rates for two different kinds of mutation, e.g., transition-transversion bias, GC-AT bias, deletion-insertion bias. This is related to the idea of developmental bias. Haldane and Fisher argued that, because mutation is a weak pressure easily overcome by selection, tendencies of mutation would be ineffectual except under conditions of neutral evolution or extraordinarily high mutation rates. This opposing-pressures argument was long used to dismiss the possibility of internal tendencies in evolution, until the molecular era prompted renewed interest in neutral evolution.\nNoboru Sueoka and Ernst Freese proposed that systematic biases in mutation might be responsible for systematic differences in genomic GC composition between species. The identification of a GC-biased E. coli mutator strain in 1967, along with the proposal of the neutral theory, established the plausibility of mutational explanations for molecular patterns, which are now common in the molecular evolution literature.\nFor instance, mutation biases are frequently invoked in models of codon usage. Such models also include effects of selection, following the mutation-selection-drift model, which allows both for mutation biases and differential selection based on effects on translation. Hypotheses of mutation bias have played an important role in the development of thinking about the evolution of genome composition, including isochores. Different insertion vs. deletion biases in different taxa can lead to the evolution of different genome sizes. The hypothesis of Lynch regarding genome size relies on mutational biases toward increase or decrease in genome size.\nHowever, mutational hypotheses for the evolution of composition suffered a reduction in scope when it was discovered that (1) GC-biased gene conversion makes an important contribution to composition in diploid organisms such as mammals and (2) bacterial genomes frequently have AT-biased mutation.\nContemporary thinking about the role of mutation biases reflects a different theory from that of Haldane and Fisher. More recent work showed that the original \"pressures\" theory assumes that evolution is based on standing variation: when evolution depends on events of mutation that introduce new alleles, mutational and developmental biases in the introduction of variation (arrival biases) can impose biases on evolution without requiring neutral evolution or high mutation rates.\nSeveral studies report that the mutations implicated in adaptation reflect common mutation biases though others dispute this interpretation."
      },
      {
        "heading": "Genetic hitchhiking",
        "level": 3,
        "content": "Recombination allows alleles on the same strand of DNA to become separated. However, the rate of recombination is low (approximately two events per chromosome per generation). As a result, genes close together on a chromosome may not always be shuffled away from each other and genes that are close together tend to be inherited together, a phenomenon known as linkage. This tendency is measured by finding how often two alleles occur together on a single chromosome compared to expectations, which is called their linkage disequilibrium. A set of alleles that is usually inherited in a group is called a haplotype. This can be important when one allele in a particular haplotype is strongly beneficial: natural selection can drive a selective sweep that will also cause the other alleles in the haplotype to become more common in the population; this effect is called genetic hitchhiking or genetic draft. Genetic draft caused by the fact that some neutral genes are genetically linked to others that are under selection can be partially captured by an appropriate effective population size."
      },
      {
        "heading": "Sexual selection",
        "level": 3,
        "content": "A special case of natural selection is sexual selection, which is selection for any trait that increases mating success by increasing the attractiveness of an organism to potential mates. Traits that evolved through sexual selection are particularly prominent among males of several animal species. Although sexually favoured, traits such as cumbersome antlers, mating calls, large body size and bright colours often attract predation, which compromises the survival of individual males. This survival disadvantage is balanced by higher reproductive success in males that show these hard-to-fake, sexually selected traits."
      },
      {
        "heading": "Natural outcomes",
        "level": 1,
        "content": "Evolution influences every aspect of the form and behaviour of organisms. Most prominent are the specific behavioural and physical adaptations that are the outcome of natural selection. These adaptations increase fitness by aiding activities such as finding food, avoiding predators or attracting mates. Organisms can also respond to selection by cooperating with each other, usually by aiding their relatives or engaging in mutually beneficial symbiosis. In the longer term, evolution produces new species through splitting ancestral populations of organisms into new groups that cannot or will not interbreed. These outcomes of evolution are distinguished based on time scale as macroevolution versus microevolution. Macroevolution refers to evolution that occurs at or above the level of species, in particular speciation and extinction, whereas microevolution refers to smaller evolutionary changes within a species or population, in particular shifts in allele frequency and adaptation. Macroevolution is the outcome of long periods of microevolution. Thus, the distinction between micro- and macroevolution is not a fundamental one—the difference is simply the time involved. However, in macroevolution, the traits of the entire species may be important. For instance, a large amount of variation among individuals allows a species to rapidly adapt to new habitats, lessening the chance of it going extinct, while a wide geographic range increases the chance of speciation, by making it more likely that part of the population will become isolated. In this sense, microevolution and macroevolution might involve selection at different levels—with microevolution acting on genes and organisms, versus macroevolutionary processes such as species selection acting on entire species and affecting their rates of speciation and extinction.\nA common misconception is that evolution has goals, long-term plans, or an innate tendency for \"progress\", as expressed in beliefs such as orthogenesis and evolutionism; realistically, however, evolution has no long-term goal and does not necessarily produce greater complexity. Although complex species have evolved, they occur as a side effect of the overall number of organisms increasing, and simple forms of life still remain more common in the biosphere. For example, the overwhelming majority of species are microscopic prokaryotes, which form about half the world's biomass despite their small size and constitute the vast majority of Earth's biodiversity. Simple organisms have therefore been the dominant form of life on Earth throughout its history and continue to be the main form of life up to the present day, with complex life only appearing more diverse because it is more noticeable. Indeed, the evolution of microorganisms is particularly important to evolutionary research since their rapid reproduction allows the study of experimental evolution and the observation of evolution and adaptation in real time."
      },
      {
        "heading": "Adaptation",
        "level": 2,
        "content": "Adaptation is the process that makes organisms better suited to their habitat. Also, the term adaptation may refer to a trait that is important for an organism's survival. For example, the adaptation of horses' teeth to the grinding of grass. By using the term adaptation for the evolutionary process and adaptive trait for the product (the bodily part or function), the two senses of the word may be distinguished. Adaptations are produced by natural selection. The following definitions are due to Theodosius Dobzhansky:\n\nAdaptation is the evolutionary process whereby an organism becomes better able to live in its habitat or habitats.\nAdaptedness is the state of being adapted: the degree to which an organism is able to live and reproduce in a given set of habitats.\nAn adaptive trait is an aspect of the developmental pattern of the organism which enables or enhances the probability of that organism surviving and reproducing.\nAdaptation may cause either the gain of a new feature, or the loss of an ancestral feature. An example that shows both types of change is bacterial adaptation to antibiotic selection, with genetic changes causing antibiotic resistance by both modifying the target of the drug, or increasing the activity of transporters that pump the drug out of the cell. Other striking examples are the bacteria Escherichia coli evolving the ability to use citric acid as a nutrient in a long-term laboratory experiment, Flavobacterium evolving a novel enzyme that allows these bacteria to grow on the by-products of nylon manufacturing, and the soil bacterium Sphingobium evolving an entirely new metabolic pathway that degrades the synthetic pesticide pentachlorophenol. An interesting but still controversial idea is that some adaptations might increase the ability of organisms to generate genetic diversity and adapt by natural selection (increasing organisms' evolvability).\n\nAdaptation occurs through the gradual modification of existing structures. Consequently, structures with similar internal organisation may have different functions in related organisms. This is the result of a single ancestral structure being adapted to function in different ways. The bones within bat wings, for example, are very similar to those in mice feet and primate hands, due to the descent of all these structures from a common mammalian ancestor. However, since all living organisms are related to some extent, even organs that appear to have little or no structural similarity, such as arthropod, squid and vertebrate eyes, or the limbs and wings of arthropods and vertebrates, can depend on a common set of homologous genes that control their assembly and function; this is called deep homology.\nDuring evolution, some structures may lose their original function and become vestigial structures. Such structures may have little or no function in a current species, yet have a clear function in ancestral species, or other closely related species. Examples include pseudogenes, the non-functional remains of eyes in blind cave-dwelling fish, wings in flightless birds, the presence of hip bones in whales and snakes, and sexual traits in organisms that reproduce via asexual reproduction. Examples of vestigial structures in humans include wisdom teeth, the coccyx, the vermiform appendix, and other behavioural vestiges such as goose bumps and primitive reflexes.\nHowever, many traits that appear to be simple adaptations are in fact exaptations: structures originally adapted for one function, but which coincidentally became somewhat useful for some other function in the process. One example is the African lizard Holaspis guentheri, which developed an extremely flat head for hiding in crevices, as can be seen by looking at its near relatives. However, in this species, the head has become so flattened that it assists in gliding from tree to tree—an exaptation. Within cells, molecular machines such as the bacterial flagella and protein sorting machinery evolved by the recruitment of several pre-existing proteins that previously had different functions. Another example is the recruitment of enzymes from glycolysis and xenobiotic metabolism to serve as structural proteins called crystallins within the lenses of organisms' eyes.\nAn area of current investigation in evolutionary developmental biology is the developmental basis of adaptations and exaptations. This research addresses the origin and evolution of embryonic development and how modifications of development and developmental processes produce novel features. These studies have shown that evolution can alter development to produce new structures, such as embryonic bone structures that develop into the jaw in other animals instead forming part of the middle ear in mammals. It is also possible for structures that have been lost in evolution to reappear due to changes in developmental genes, such as a mutation in chickens causing embryos to grow teeth similar to those of crocodiles. It is now becoming clear that most alterations in the form of organisms are due to changes in a small set of conserved genes."
      },
      {
        "heading": "Coevolution",
        "level": 2,
        "content": "Interactions between organisms can produce both conflict and cooperation. When the interaction is between pairs of species, such as a pathogen and a host, or a predator and its prey, these species can develop matched sets of adaptations. Here, the evolution of one species causes adaptations in a second species. These changes in the second species then, in turn, cause new adaptations in the first species. This cycle of selection and response is called coevolution. An example is the production of tetrodotoxin in the rough-skinned newt and the evolution of tetrodotoxin resistance in its predator, the common garter snake. In this predator-prey pair, an evolutionary arms race has produced high levels of toxin in the newt and correspondingly high levels of toxin resistance in the snake."
      },
      {
        "heading": "Cooperation",
        "level": 2,
        "content": "Not all co-evolved interactions between species involve conflict. Many cases of mutually beneficial interactions have evolved. For instance, an extreme cooperation exists between plants and the mycorrhizal fungi that grow on their roots and aid the plant in absorbing nutrients from the soil. This is a reciprocal relationship as the plants provide the fungi with sugars from photosynthesis. Here, the fungi actually grow inside plant cells, allowing them to exchange nutrients with their hosts, while sending signals that suppress the plant immune system.\nCoalitions between organisms of the same species have also evolved. An extreme case is the eusociality found in social insects, such as bees, termites and ants, where sterile insects feed and guard the small number of organisms in a colony that are able to reproduce. On an even smaller scale, the somatic cells that make up the body of an animal limit their reproduction so they can maintain a stable organism, which then supports a small number of the animal's germ cells to produce offspring. Here, somatic cells respond to specific signals that instruct them whether to grow, remain as they are, or die. If cells ignore these signals and multiply inappropriately, their uncontrolled growth causes cancer.\nSuch cooperation within species may have evolved through the process of kin selection, which is where one organism acts to help raise a relative's offspring. This activity is selected for because if the helping individual contains alleles which promote the helping activity, it is likely that its kin will also contain these alleles and thus those alleles will be passed on. Other processes that may promote cooperation include group selection, where cooperation provides benefits to a group of organisms."
      },
      {
        "heading": "Speciation",
        "level": 2,
        "content": "Speciation is the process where a species diverges into two or more descendant species.\nThere are multiple ways to define the concept of \"species\". The choice of definition is dependent on the particularities of the species concerned. For example, some species concepts apply more readily toward sexually reproducing organisms while others lend themselves better toward asexual organisms. Despite the diversity of various species concepts, these various concepts can be placed into one of three broad philosophical approaches: interbreeding, ecological and phylogenetic. The Biological Species Concept (BSC) is a classic example of the interbreeding approach. Defined by evolutionary biologist Ernst Mayr in 1942, the BSC states that \"species are groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups.\" Despite its wide and long-term use, the BSC like other species concepts is not without controversy, for example, because genetic recombination among prokaryotes is not an intrinsic aspect of reproduction; this is called the species problem. Some researchers have attempted a unifying monistic definition of species, while others adopt a pluralistic approach and suggest that there may be different ways to logically interpret the definition of a species.\nBarriers to reproduction between two diverging sexual populations are required for the populations to become new species. Gene flow may slow this process by spreading the new genetic variants also to the other populations. Depending on how far two species have diverged since their most recent common ancestor, it may still be possible for them to produce offspring, as with horses and donkeys mating to produce mules. Such hybrids are generally infertile. In this case, closely related species may regularly interbreed, but hybrids will be selected against and the species will remain distinct. However, viable hybrids are occasionally formed and these new species can either have properties intermediate between their parent species, or possess a totally new phenotype. The importance of hybridisation in producing new species of animals is unclear, although cases have been seen in many types of animals, with the grey tree frog being a particularly well-studied example.\nSpeciation has been observed multiple times under both controlled laboratory conditions and in nature. In sexually reproducing organisms, speciation results from reproductive isolation followed by genealogical divergence. There are four primary geographic modes of speciation. The most common in animals is allopatric speciation, which occurs in populations initially isolated geographically, such as by habitat fragmentation or migration. Selection under these conditions can produce very rapid changes in the appearance and behaviour of organisms. As selection and drift act independently on populations isolated from the rest of their species, separation may eventually produce organisms that cannot interbreed.\nThe second mode of speciation is peripatric speciation, which occurs when small populations of organisms become isolated in a new environment. This differs from allopatric speciation in that the isolated populations are numerically much smaller than the parental population. Here, the founder effect causes rapid speciation after an increase in inbreeding increases selection on homozygotes, leading to rapid genetic change.\nThe third mode is parapatric speciation. This is similar to peripatric speciation in that a small population enters a new habitat, but differs in that there is no physical separation between these two populations. Instead, speciation results from the evolution of mechanisms that reduce gene flow between the two populations. Generally this occurs when there has been a drastic change in the environment within the parental species' habitat. One example is the grass Anthoxanthum odoratum, which can undergo parapatric speciation in response to localised metal pollution from mines. Here, plants evolve that have resistance to high levels of metals in the soil. Selection against interbreeding with the metal-sensitive parental population produced a gradual change in the flowering time of the metal-resistant plants, which eventually produced complete reproductive isolation. Selection against hybrids between the two populations may cause reinforcement, which is the evolution of traits that promote mating within a species, as well as character displacement, which is when two species become more distinct in appearance.\n\nFinally, in sympatric speciation species diverge without geographic isolation or changes in habitat. This form is rare since even a small amount of gene flow may remove genetic differences between parts of a population. Generally, sympatric speciation in animals requires the evolution of both genetic differences and nonrandom mating, to allow reproductive isolation to evolve.\nOne type of sympatric speciation involves crossbreeding of two related species to produce a new hybrid species. This is not common in animals as animal hybrids are usually sterile. This is because during meiosis the homologous chromosomes from each parent are from different species and cannot successfully pair. However, it is more common in plants because plants often double their number of chromosomes, to form polyploids. This allows the chromosomes from each parental species to form matching pairs during meiosis, since each parent's chromosomes are represented by a pair already. An example of such a speciation event is when the plant species Arabidopsis thaliana and Arabidopsis arenosa crossbred to give the new species Arabidopsis suecica. This happened about 20,000 years ago, and the speciation process has been repeated in the laboratory, which allows the study of the genetic mechanisms involved in this process. Indeed, chromosome doubling within a species may be a common cause of reproductive isolation, as half the doubled chromosomes will be unmatched when breeding with undoubled organisms.\nSpeciation events are important in the theory of punctuated equilibrium, which accounts for the pattern in the fossil record of short \"bursts\" of evolution interspersed with relatively long periods of stasis, where species remain relatively unchanged. In this theory, speciation and rapid evolution are linked, with natural selection and genetic drift acting most strongly on organisms undergoing speciation in novel habitats or small populations. As a result, the periods of stasis in the fossil record correspond to the parental population and the organisms undergoing speciation and rapid evolution are found in small populations or geographically restricted habitats and therefore rarely being preserved as fossils."
      },
      {
        "heading": "Extinction",
        "level": 2,
        "content": "Extinction is the disappearance of an entire species. Extinction is not an unusual event, as species regularly appear through speciation and disappear through extinction. Nearly all animal and plant species that have lived on Earth are now extinct, and extinction appears to be the ultimate fate of all species. These extinctions have happened continuously throughout the history of life, although the rate of extinction spikes in occasional mass extinction events. The Cretaceous–Paleogene extinction event, during which the non-avian dinosaurs became extinct, is the most well-known, but the earlier Permian–Triassic extinction event was even more severe, with approximately 96% of all marine species driven to extinction. The Holocene extinction event is an ongoing mass extinction associated with humanity's expansion across the globe over the past few thousand years. Present-day extinction rates are 100–1000 times greater than the background rate and up to 30% of current species may be extinct by the mid 21st century. Human activities are now the primary cause of the ongoing extinction event; global warming may further accelerate it in the future. Despite the estimated extinction of more than 99% of all species that ever lived on Earth, about 1 trillion species are estimated to be on Earth currently with only one-thousandth of 1% described.\nThe role of extinction in evolution is not very well understood and may depend on which type of extinction is considered. The causes of the continuous \"low-level\" extinction events, which form the majority of extinctions, may be the result of competition between species for limited resources (the competitive exclusion principle). If one species can out-compete another, this could produce species selection, with the fitter species surviving and the other species being driven to extinction. The intermittent mass extinctions are also important, but instead of acting as a selective force, they drastically reduce diversity in a nonspecific manner and promote bursts of rapid evolution and speciation in survivors."
      },
      {
        "heading": "Applications",
        "level": 1,
        "content": "Concepts and models used in evolutionary biology, such as natural selection, have many applications.\nArtificial selection is the intentional selection of traits in a population of organisms. This has been used for thousands of years in the domestication of plants and animals. More recently, such selection has become a vital part of genetic engineering, with selectable markers such as antibiotic resistance genes being used to manipulate DNA. Proteins with valuable properties have evolved by repeated rounds of mutation and selection (for example modified enzymes and new antibodies) in a process called directed evolution.\nUnderstanding the changes that have occurred during an organism's evolution can reveal the genes needed to construct parts of the body, genes which may be involved in human genetic disorders. For example, the Mexican tetra is an albino cavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves. This helped identify genes required for vision and pigmentation.\nEvolutionary theory has many applications in medicine. Many human diseases are not static phenomena, but capable of evolution. Viruses, bacteria, fungi and cancers evolve to be resistant to host immune defences, as well as to pharmaceutical drugs. These same problems occur in agriculture with pesticide and herbicide resistance. It is possible that we are facing the end of the effective life of most of available antibiotics and predicting the evolution and evolvability of our pathogens and devising strategies to slow or circumvent it is requiring deeper knowledge of the complex forces driving evolution at the molecular level.\nIn computer science, simulations of evolution using evolutionary algorithms and artificial life started in the 1960s and were extended with simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s. He used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Henry Holland. Practical applications also include automatic evolution of computer programmes. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers and also to optimise the design of systems."
      },
      {
        "heading": "Evolutionary history of life",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Origin of life",
        "level": 2,
        "content": "The Earth is about 4.54 billion years old. The earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago, during the Eoarchean Era after a geological crust started to solidify following the earlier molten Hadean Eon. Microbial mat fossils have been found in 3.48 billion-year-old sandstone in Western Australia. Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old metasedimentary rocks discovered in Western Greenland as well as \"remains of biotic life\" found in 4.1 billion-year-old rocks in Western Australia. Commenting on the Australian findings, Stephen Blair Hedges wrote: \"If life arose relatively quickly on Earth, then it could be common in the universe.\"  In July 2016, scientists reported identifying a set of 355 genes from the last universal common ancestor (LUCA) of all organisms living on Earth.\nMore than 99% of all species, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.9 million are estimated to have been named and 1.6 million documented in a central database to date, leaving at least 80% not yet described.\nHighly energetic chemistry is thought to have produced a self-replicating molecule around 4 billion years ago, and half a billion years later the last common ancestor of all life existed. The current scientific consensus is that the complex biochemistry that makes up life came from simpler chemical reactions. The beginning of life may have included self-replicating molecules such as RNA and the assembly of simple cells."
      },
      {
        "heading": "Common descent",
        "level": 2,
        "content": "All organisms on Earth are descended from a common ancestor or ancestral gene pool. Current species are a stage in the process of evolution, with their diversity the product of a long series of speciation and extinction events. The common descent of organisms was first deduced from four simple facts about organisms: First, they have geographic distributions that cannot be explained by local adaptation. Second, the diversity of life is not a set of completely unique organisms, but organisms that share morphological similarities. Third, vestigial traits with no clear purpose resemble functional ancestral traits. Fourth, organisms can be classified using these similarities into a hierarchy of nested groups, similar to a family tree.\n\nDue to horizontal gene transfer, this \"tree of life\" may be more complicated than a simple branching tree, since some genes have spread independently between distantly related species. To solve this problem and others, some authors prefer to use the \"Coral of life\" as a metaphor or a mathematical model to illustrate the evolution of life. This view dates back to an idea briefly mentioned by Darwin but later abandoned.\nPast species have also left records of their evolutionary history. Fossils, along with the comparative anatomy of present-day organisms, constitute the morphological, or anatomical, record. By comparing the anatomies of both modern and extinct species, palaeontologists can infer the lineages of those species. However, this approach is most successful for organisms that had hard body parts, such as shells, bones or teeth. Further, as prokaryotes such as bacteria and archaea share a limited set of common morphologies, their fossils do not provide information on their ancestry.\nMore recently, evidence for common descent has come from the study of biochemical similarities between organisms. For example, all living cells use the same basic set of nucleotides and amino acids. The development of molecular genetics has revealed the record of evolution left in organisms' genomes: dating when species diverged through the molecular clock produced by mutations. For example, these DNA sequence comparisons have revealed that humans and chimpanzees share 98% of their genomes and analysing the few areas where they differ helps shed light on when the common ancestor of these species existed."
      },
      {
        "heading": "Evolution of life",
        "level": 2,
        "content": "Prokaryotes inhabited the Earth from approximately 3–4 billion years ago. No obvious changes in morphology or cellular organisation occurred in these organisms over the next few billion years. The eukaryotic cells emerged between 1.6 and 2.7 billion years ago. The next major change in cell structure came when bacteria were engulfed by eukaryotic cells, in a cooperative association called endosymbiosis. The engulfed bacteria and the host cell then underwent coevolution, with the bacteria evolving into either mitochondria or hydrogenosomes. Another engulfment of cyanobacterial-like organisms led to the formation of chloroplasts in algae and plants.\nThe history of life was that of the unicellular eukaryotes, prokaryotes and archaea until around 1.7 billion years ago, when multicellular organisms began to appear, with differentiated cells performing specialised functions. The evolution of multicellularity occurred in multiple independent events, in organisms as diverse as sponges, brown algae, cyanobacteria, slime moulds and myxobacteria. In January 2016, scientists reported that, about 800 million years ago, a minor genetic change in a single molecule called GK-PID may have allowed organisms to go from a single cell organism to one of many cells.\nApproximately 538.8 million years ago, a remarkable amount of biological diversity appeared over a span of around 10 million years in what is called the Cambrian explosion. Here, the majority of types of modern animals appeared in the fossil record, as well as unique lineages that subsequently became extinct. Various triggers for the Cambrian explosion have been proposed, including the accumulation of oxygen in the atmosphere from photosynthesis.\nAbout 500 million years ago, plants and fungi colonised the land and were soon followed by arthropods and other animals. Insects were particularly successful and even today make up the majority of animal species. Amphibians first appeared around 364 million years ago, followed by early amniotes and birds around 155 million years ago (both from \"reptile\"-like lineages), mammals around 129 million years ago, Homininae around 10 million years ago and modern humans around 250,000 years ago. However, despite the evolution of these large animals, smaller organisms similar to the types that evolved early in this process continue to be highly successful and dominate the Earth, with the majority of both biomass and species being prokaryotes."
      },
      {
        "heading": "History of evolutionary thought",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Classical antiquity",
        "level": 2,
        "content": "The proposal that one type of organism could descend from another type goes back to some of the first pre-Socratic Greek philosophers, such as Anaximander and Empedocles. Such proposals survived into Roman times. The poet and philosopher Lucretius followed Empedocles in his masterwork De rerum natura (lit. 'On the Nature of Things')."
      },
      {
        "heading": "Middle Ages",
        "level": 2,
        "content": "In contrast to these materialistic views, Aristotelianism had considered all natural things as actualisations of fixed natural possibilities, known as forms. This became part of a medieval teleological understanding of nature in which all things have an intended role to play in a divine cosmic order. Variations of this idea became the standard understanding of the Middle Ages and were integrated into Christian learning, but Aristotle did not demand that real types of organisms always correspond one-for-one with exact metaphysical forms and specifically gave examples of how new types of living things could come to be.\nA number of Arab Muslim scholars wrote about evolution, most notably Ibn Khaldun, who wrote the book Muqaddimah in 1377, in which he asserted that humans developed from \"the world of the monkeys\", in a process by which \"species become more numerous\"."
      },
      {
        "heading": "Pre-Darwinian",
        "level": 2,
        "content": "The \"New Science\" of the 17th century rejected the Aristotelian approach. It sought to explain natural phenomena in terms of physical laws that were the same for all visible things and that did not require the existence of any fixed natural categories or divine cosmic order. However, this new approach was slow to take root in the biological sciences: the last bastion of the concept of fixed natural types. John Ray applied one of the previously more general terms for fixed natural types, \"species\", to plant and animal types, but he strictly identified each type of living thing as a species and proposed that each species could be defined by the features that perpetuated themselves generation after generation. The biological classification introduced by Carl Linnaeus in 1735 explicitly recognised the hierarchical nature of species relationships, but still viewed species as fixed according to a divine plan.\nOther naturalists of this time speculated on the evolutionary change of species over time according to natural laws. In 1751, Pierre Louis Maupertuis wrote of natural modifications occurring during reproduction and accumulating over many generations to produce new species. Georges-Louis Leclerc, Comte de Buffon, suggested that species could degenerate into different organisms, and Erasmus Darwin proposed that all warm-blooded animals could have descended from a single microorganism (or \"filament\"). The first full-fledged evolutionary scheme was Jean-Baptiste Lamarck's \"transmutation\" theory of 1809, which envisaged spontaneous generation continually producing simple forms of life that developed greater complexity in parallel lineages with an inherent progressive tendency, and postulated that on a local level, these lineages adapted to the environment by inheriting changes caused by their use or disuse in parents. (The latter process was later called Lamarckism.) These ideas were condemned by established naturalists as speculation lacking empirical support. In particular, Georges Cuvier insisted that species were unrelated and fixed, their similarities reflecting divine design for functional needs. In the meantime, Ray's ideas of benevolent design had been developed by William Paley into the Natural Theology or Evidences of the Existence and Attributes of the Deity (1802), which proposed complex adaptations as evidence of divine design and which was admired by Charles Darwin."
      },
      {
        "heading": "Darwinian revolution",
        "level": 2,
        "content": "The crucial break from the concept of constant typological classes or types in biology came with the theory of evolution through natural selection, which was formulated by Charles Darwin and Alfred Wallace in terms of variable populations. Darwin used the expression descent with modification rather than evolution. Partly influenced by An Essay on the Principle of Population (1798) by Thomas Robert Malthus, Darwin noted that population growth would lead to a \"struggle for existence\" in which favourable variations prevailed as others perished. In each generation, many offspring fail to survive to an age of reproduction because of limited resources. This could explain the diversity of plants and animals from a common ancestry through the working of natural laws in the same way for all types of organism. Darwin developed his theory of \"natural selection\" from 1838 onwards and was writing up his \"big book\" on the subject when Alfred Russel Wallace sent him a version of virtually the same theory in 1858. Their separate papers were presented together at an 1858 meeting of the Linnean Society of London. At the end of 1859, Darwin's publication of his \"abstract\" as On the Origin of Species explained natural selection in detail and in a way that led to an increasingly wide acceptance of Darwin's concepts of evolution at the expense of alternative theories. Thomas Henry Huxley applied Darwin's ideas to humans, using palaeontology and comparative anatomy to provide strong evidence that humans and apes shared a common ancestry. Some were disturbed by this since it implied that humans did not have a special place in the universe.\nOthniel C. Marsh, America's first palaeontologist, was the first to provide solid fossil evidence to support Darwin's theory of evolution by unearthing the ancestors of the modern horse.  In 1877, Marsh delivered a very influential speech before the annual meeting of the American Association for the Advancement of Science, providing a demonstrative argument for evolution.  For the first time, Marsh traced the evolution of vertebrates from fish all the way through humans.  Sparing no detail, he listed a wealth of fossil examples of past life forms.  The significance of this speech was immediately recognised by the scientific community, and it was printed in its entirety in several scientific journals.\n\nIn 1880, Marsh caught the attention of the scientific world with the publication of Odontornithes: a Monograph on Extinct Birds of North America, which included his discoveries of birds with teeth.  These skeletons helped bridge the gap between dinosaurs and birds, and provided invaluable support for Darwin's theory of evolution.  Darwin wrote to Marsh saying, \"Your work on these old birds & on the many fossil animals of N. America has afforded the best support to the theory of evolution, which has appeared within the last 20 years\" (since Darwin's publication of Origin of Species)."
      },
      {
        "heading": "Pangenesis and heredity",
        "level": 2,
        "content": "The mechanisms of reproductive heritability and the origin of new traits remained a mystery. Towards this end, Darwin developed his provisional theory of pangenesis. In 1865, Gregor Mendel reported that traits were inherited in a predictable manner through the independent assortment and segregation of elements (later known as genes). Mendel's laws of inheritance eventually supplanted most of Darwin's pangenesis theory. August Weismann made the important distinction between germ cells that give rise to gametes (such as sperm and egg cells) and the somatic cells of the body, demonstrating that heredity passes through the germ line only. Hugo de Vries connected Darwin's pangenesis theory to Weismann's germ/soma cell distinction and proposed that Darwin's pangenes were concentrated in the cell nucleus and when expressed they could move into the cytoplasm to change the cell's structure. De Vries was also one of the researchers who made Mendel's work well known, believing that Mendelian traits corresponded to the transfer of heritable variations along the germline. To explain how new variants originate, de Vries developed a mutation theory that led to a temporary rift between those who accepted Darwinian evolution and biometricians who allied with de Vries. In the 1930s, pioneers in the field of population genetics, such as Ronald Fisher, Sewall Wright and J. B. S. Haldane set the foundations of evolution onto a robust statistical philosophy. The false contradiction between Darwin's theory, genetic mutations, and Mendelian inheritance was thus reconciled."
      },
      {
        "heading": "The 'modern synthesis'",
        "level": 2,
        "content": "In the 1920s and 1930s, the modern synthesis connected natural selection and population genetics, based on Mendelian inheritance, into a unified theory that included random genetic drift, mutation, and gene flow. This new version of evolutionary theory focused on changes in allele frequencies in population. It explained patterns observed across species in populations, through fossil transitions in palaeontology."
      },
      {
        "heading": "Further syntheses",
        "level": 2,
        "content": "Since then, further syntheses have extended evolution's explanatory power in the light of numerous discoveries, to cover biological phenomena across the whole of the biological hierarchy from genes to populations.\nThe publication of the structure of DNA by James Watson and Francis Crick with contribution of Rosalind Franklin in 1953 demonstrated a physical mechanism for inheritance. Molecular biology improved understanding of the relationship between genotype and phenotype. Advances were also made in phylogenetic systematics, mapping the transition of traits into a comparative and testable framework through the publication and use of evolutionary trees. In 1973, evolutionary biologist Theodosius Dobzhansky penned that \"nothing in biology makes sense except in the light of evolution\", because it has brought to light the relations of what first seemed disjointed facts in natural history into a coherent explanatory body of knowledge that describes and predicts many observable facts about life on this planet.\nOne extension, known as evolutionary developmental biology and informally called \"evo-devo\", emphasises how changes between generations (evolution) act on patterns of change within individual organisms (development). Since the beginning of the 21st century, some biologists have argued for an extended evolutionary synthesis, which would account for the effects of non-genetic inheritance modes, such as epigenetics, parental effects, ecological inheritance and cultural inheritance, and evolvability."
      },
      {
        "heading": "Social and cultural responses",
        "level": 1,
        "content": "In the 19th century, particularly after the publication of On the Origin of Species in 1859, the idea that life had evolved was an active source of academic debate centred on the philosophical, social and religious implications of evolution. Today, the modern evolutionary synthesis is accepted by a vast majority of scientists. However, evolution remains a contentious concept for some theists.\nWhile various religions and denominations have reconciled their beliefs with evolution through concepts such as theistic evolution, there are creationists who believe that evolution is contradicted by the creation myths found in their religions and who raise various objections to evolution. As had been demonstrated by responses to the publication of Vestiges of the Natural History of Creation in 1844, the most controversial aspect of evolutionary biology is the implication of human evolution that humans share common ancestry with apes and that the mental and moral faculties of humanity have the same types of natural causes as other inherited traits in animals. In some countries, notably the United States, these tensions between science and religion have fuelled the current creation–evolution controversy, a religious conflict focusing on politics and public education. While other scientific fields such as cosmology and Earth science also conflict with literal interpretations of many religious texts, evolutionary biology experiences significantly more opposition from religious literalists.\nThe teaching of evolution in American secondary school biology classes was uncommon in most of the first half of the 20th century. The Scopes Trial decision of 1925 caused the subject to become very rare in American secondary biology textbooks for a generation, but it was gradually re-introduced later and became legally protected with the 1968 Epperson v. Arkansas decision. Since then, the competing religious belief of creationism was legally disallowed in secondary school curricula in various decisions in the 1970s and 1980s, but it returned in pseudoscientific form as intelligent design (ID), to be excluded once again in the 2005 Kitzmiller v. Dover Area School District case. The debate over Darwin's ideas did not generate significant controversy in China."
      },
      {
        "heading": "See also",
        "level": 1,
        "content": "Devolution (biology) – Notion that species can revert to primitive forms\nChronospecies"
      },
      {
        "heading": "References",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bibliography",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Further reading",
        "level": 1,
        "content": ""
      },
      {
        "heading": "External links",
        "level": 1,
        "content": " \n\nGeneral information\n\"Evolution\" on In Our Time at the BBC\n\"Evolution Resources from the National Academies\". Washington, D.C.: National Academy of Sciences. Retrieved 30 May 2011.\n\"Understanding Evolution: your one-stop resource for information on evolution\". Berkeley, California: University of California, Berkeley. Retrieved 30 May 2011.\n\"Evolution of Evolution – 150 Years of Darwin's 'On the Origin of Species'\". Arlington County, Virginia: National Science Foundation. Archived from the original on 19 May 2011. Retrieved 30 May 2011.\n\"Human Evolution Timeline Interactive\". Smithsonian Institution, National Museum of Natural History. 28 January 2010. Retrieved 14 July 2018. Adobe Flash required.\n\"History of Evolution in the United States\". Salon. Retrieved 2021-08-24.\nVideo (1980; Cosmos animation; 8:01): \"Evolution\" – Carl Sagan on YouTube\nExperiments\nLenski, Richard E. \"Experimental Evolution\". East Lansing, Michigan: Michigan State University. Retrieved 31 July 2013.\nChastain, Erick; Livnat, Adi; Papadimitriou, Christos; Vazirani, Umesh (22 July 2014). \"Algorithms, games, and evolution\". PNAS. 111 (29): 10620–10623. Bibcode:2014PNAS..11110620C. doi:10.1073/pnas.1406556111. ISSN 0027-8424. PMC 4115542. PMID 24979793.\nOnline lectures\n\"Evolution Matters Lecture Series\". Harvard Online Learning. Cambridge, Massachusetts: Harvard University. Archived from the original on 18 December 2017. Retrieved 15 July 2018.\nStearns, Stephen C. \"EEB 122: Principles of Evolution, Ecology and Behavior\". Open Yale Courses. New Haven, Connecticut: Yale University. Archived from the original on 1 December 2017. Retrieved 14 July 2018."
      }
    ],
    "summary": "Evolution is the change in the heritable characteristics of biological populations over successive generations. It occurs when evolutionary processes such as natural selection and genetic drift act on genetic variation, resulting in certain characteristics becoming more or less common within a population over successive generations. The process of evolution has given rise to biodiversity at every level of biological organisation.\nThe scientific theory of evolution by natural selection was conceived independently by two British naturalists, Charles Darwin and Alfred Russel Wallace, in the mid-19th century as an explanation for why organisms are adapted to their physical and biological environments. The theory was first set out in detail in Darwin's book On the Origin of Species. Evolution by natural selection is established by observable facts about living organisms: (1) more offspring are often produced than can possibly survive; (2) traits vary among individuals with respect to their morphology, physiology, and behaviour; (3) different traits confer different rates of survival and reproduction (differential fitness); and (4) traits can be passed from generation to generation (heritability of fitness). In successive generations, members of a population are therefore more likely to be replaced by the offspring of parents with favourable characteristics for that environment.\nIn the early 20th century, competing ideas of evolution were refuted and evolution was combined with Mendelian inheritance and population genetics to give rise to modern evolutionary theory. In this synthesis the basis for heredity is in DNA molecules that pass information from generation to generation. The processes that change DNA in a population include natural selection, genetic drift, mutation, and gene flow.\nAll life on Earth—including humanity—shares a last universal common ancestor (LUCA), which lived approximately 3.5–3.8 billion years ago. The fossil record includes a progression from early biogenic graphite to microbial mat fossils to fossilised multicellular organisms. Existing patterns of biodiversity have been shaped by repeated formations of new species (speciation), changes within species (anagenesis), and loss of species (extinction) throughout the evolutionary history of life on Earth. Morphological and biochemical traits tend to be more similar among species that share a more recent common ancestor, which historically was used to reconstruct phylogenetic trees, although direct comparison of genetic sequences is a more common method today.\nEvolutionary biologists have continued to study various aspects of evolution by forming and testing hypotheses as well as constructing theories based on evidence from the field or laboratory and on data generated by the methods of mathematical and theoretical biology. Their discoveries have influenced not just the development of biology but also other fields including agriculture, medicine, and computer science."
  },
  {
    "title": "Evolución biológica",
    "source": "https://es.wikipedia.org/wiki/Evoluci%C3%B3n_biol%C3%B3gica",
    "language": "es",
    "chunks": [
      {
        "heading": "La evolución como un hecho probado",
        "level": 1,
        "content": "La evolución es tanto un «hecho» (una observación, medición u otra forma de evidencia) como una «teoría» (una explicación completa de algún aspecto de la naturaleza que está respaldada por una gran cantidad de evidencia). La National Academies of Sciences, Engineering, and Medicine afirma que la teoría de la evolución es «una explicación científica que ha sido probada y confirmada tantas veces que ya no existe una razón de peso para seguir probándola o buscar ejemplos adicionales».[25]​"
      },
      {
        "heading": "Evidencia del proceso evolutivo",
        "level": 2,
        "content": "La evidencia del proceso evolutivo surge del conjunto de pruebas que los científicos han reunido para demostrar que la evolución es un proceso característico de los seres vivos y que todos los organismos que viven en la Tierra descienden de un último antepasado común universal.[26]​ Las especies actuales son un estado en el proceso evolutivo y su riqueza relativa y niveles de complejidad biológica son el producto de una larga serie de eventos de especiación y de extinción.[27]​\nLa existencia de un ancestro común puede deducirse a partir de unas características simples de los organismos. Primero, existe evidencia proveniente de la biogeografía: tanto Charles Darwin como Alfred Russell Wallace se percataron de que la distribución geográfica de especies diferentes depende de la distancia y el aislamiento de las áreas que ocupan, y no de condiciones ecológicas y climatológicas similares, como sería de esperar si las especies hubieran aparecido al mismo tiempo ya adaptadas a su medio ambiente. Posteriormente, el descubrimiento de la tectónica de placas fue muy importante para la teoría de la evolución, al proporcionar una explicación para las similitudes entre muchos grupos de especies en continentes que se encontraban unidos en el pasado.[28]​ Segundo, la diversidad de la vida sobre la Tierra no se resuelve en un conjunto de organismos completamente únicos, sino que los mismos comparten una gran cantidad de similitudes morfológicas. Así, cuando se comparan los órganos de los distintos seres vivos, se encuentran semejanzas en su constitución que señalan el parentesco que existe entre especies diferentes. Estas semejanzas y su origen permiten clasificar a los órganos en homólogos, si tienen un mismo origen embrionario y evolutivo, y análogos, si tienen diferente origen embrionario y evolutivo, pero la misma función. Los estudios anatómicos han encontrado homología en muchas estructuras superficialmente tan diferentes como las espinas de los cactos y las trampas de varias plantas insectívoras que indican que son simplemente hojas que han experimentado modificaciones adaptativas.[29]​ Los procesos evolutivos explican asimismo la presencia de órganos vestigiales, que están reducidos y no tienen función aparente, pero que muestran claramente que derivan de órganos funcionales presentes en otras especies, tales como los huesos rudimentarios de las patas posteriores presentes en algunas serpientes.[3]​\nLa embriología, a través de los estudios comparativos de las etapas embrionarias de distintas clases de animales, ofrece otro conjunto de indicios del proceso evolutivo. Se ha encontrado que en estas primeras etapas del desarrollo, muchos organismos muestran características comunes que sugieren la existencia de un patrón de desarrollo compartido entre ellas, lo que, a su vez, sugiere la existencia de un antepasado común. El hecho de que los embriones tempranos de vertebrados como los mamíferos y aves posean hendiduras branquiales, que luego desaparecen conforme avanza el desarrollo, puede explicarse si se hallan emparentados con los peces.[30]​\nOtro grupo de pistas proviene del campo de la sistemática. Los organismos pueden ser clasificados usando las similitudes mencionadas en grupos anidados jerárquicamente, muy similares a un árbol genealógico.[31]​[17]​ Si bien las investigaciones modernas sugieren que, debido a la transferencia horizontal de genes, este árbol de la vida puede ser más complicado que lo que se pensaba, ya que muchos genes se han distribuido independientemente entre especies distantemente relacionadas.[32]​[33]​\nLas especies que han vivido en épocas remotas han dejado registros de su historia evolutiva. Los fósiles, conjuntamente con la anatomía comparada de los organismos actuales, constituyen la evidencia paleontológica del proceso evolutivo. Mediante la comparación de las anatomías de las especies modernas con las ya extintas, los paleontólogos pueden inferir los linajes a los que unas y otras pertenecen. Sin embargo, la investigación paleontológica para buscar conexiones evolutivas tiene ciertas limitaciones. De hecho, es útil solo en aquellos organismos que presentan partes del cuerpo duras, tales como caparazones, dientes o huesos. Más aún, ciertos otros organismos, como los procariotas ―las bacterias y arqueas― presentan una cantidad limitada de características comunes, por lo que sus fósiles no proveen información sobre sus ancestros.[34]​\nUn método más reciente para probar el proceso evolutivo es el estudio de las similitudes bioquímicas entre los organismos. Por ejemplo, todas las células utilizan el mismo conjunto básico de nucleótidos y aminoácidos.[35]​ El desarrollo de la genética molecular ha revelado que el registro evolutivo reside en el genoma de cada organismo y que es posible datar el momento de la divergencia de las especies a través del reloj molecular basado en las mutaciones acumuladas en el proceso de evolución molecular.[36]​ Por ejemplo, la comparación entre las secuencias del ADN del humano y del chimpancé ha confirmado la estrecha similitud entre las dos especies y ha ayudado a elucidar cuándo existió el ancestro común de ambas.[37]​"
      },
      {
        "heading": "El origen de la vida",
        "level": 2,
        "content": "El origen de la vida, aunque atañe al estudio de los seres vivos, es un tema que no es abordado por la teoría de la evolución; pues esta última solo se ocupa del cambio en los seres vivos, y no del origen, cambios e interacciones de las moléculas orgánicas de las que estos proceden.[38]​\nNo se sabe mucho sobre las etapas más tempranas y previas al desarrollo de la vida, y los intentos realizados para tratar de desvelar la historia más temprana del origen de la vida generalmente se enfocan en el comportamiento de las macromoléculas, debido a que el consenso científico actual es que la compleja bioquímica que constituye la vida provino de reacciones químicas simples, si bien persisten las controversias acerca de cómo ocurrieron las mismas.[39]​ Sin embargo, los científicos están de acuerdo en que todos los organismos existentes comparten ciertas características ―incluyendo la presencia de estructura celular y de código genético― que estarían relacionadas con el origen de la vida.[40]​\nTampoco está claro cuáles fueron los primeros desarrollos de la vida (protobiontes), la estructura de los primeros seres vivos o la identidad y la naturaleza del último antepasado común universal.[41]​[42]​ Las bacterias y arqueas, los primeros organismos que dejaron una huella en el registro fósil, son demasiado complejas para haber surgido directamente de los materiales no vivos.[43]​ La falta de indicios geoquímicos o fósiles de organismos anteriores ha dejado un amplio campo libre para las hipótesis. Aunque no hay consenso científico sobre cómo comenzó la vida, se acepta la existencia del último antepasado común universal porque sería prácticamente imposible que dos o más linajes separados pudieran haber desarrollado de manera independiente los muchos complejos mecanismos bioquímicos comunes a todos los organismos vivos.[35]​[44]​\nSe ha propuesto que el inicio de la vida pueden haber sido moléculas autorreplicantes como el ARN,[45]​ o ensamblajes de células simples denominadas nanocélulas.[46]​ Los científicos han sugerido que la vida surgió en respiraderos hidrotermales en las profundidades del mar, géiseres o fumarolas durante el Hádico. Una hipótesis alternativa es la del comienzo de la vida en otras partes del Universo, desde donde habría llegado a la Tierra en cometas o meteoritos, en el proceso denominado panspermia.[47]​"
      },
      {
        "heading": "La evolución de la vida en la Tierra",
        "level": 2,
        "content": "Detallados estudios químicos basados en isótopos de carbono de rocas del eón Arcaico sugieren que las primeras formas de vida emergieron en la Tierra probablemente hace más de 4200 millones de años, al final del eón Hádico, y hay claros indicios geoquímicos ―tales como la presencia en rocas antiguas de isótopos de azufre producidos por la reducción microbiana de sulfatos― que indican su presencia en la era Paleoarcaica, hace tres mil cuatrocientos setenta millones de años.[48]​ Los estromatolitos ―capas de roca producidas por comunidades de microorganismos― más antiguos se reconocen en estratos de 3700 millones de años,[49]​ mientras que los microfósiles filiformes más antiguos se encuentran en rocas sedimentarias de respiraderos hidrotermales de hace 3770 millones de años hallados en Canadá.[50]​\nAsimismo, los fósiles moleculares derivados de los lípidos de la membrana plasmática y del resto de la célula ―denominados «biomarcadores»― confirman que ciertos organismos similares a cianobacterias habitaron los océanos arcaicos hace más de 2700 millones de años. Estos microbios fotoautótrofos liberaban oxígeno, que comenzó a acumularse en la atmósfera hace aproximadamente 2200 millones de años y transformó definitivamente la composición de esta.[51]​[52]​ La aparición de una atmósfera rica en oxígeno tras el surgimiento de organismos fotosintéticos puede también rastrearse por los depósitos laminares de hierro y las bandas rojas de los óxidos de hierro posteriores. La abundancia de oxígeno posibilitó el desarrollo de la respiración celular aeróbica, que emergió hace aproximadamente 2000 millones de años.[53]​\nDesde la formación de estas primeras formas de vida compleja, los procariotas, hace 4200 millones de años,[54]​[55]​ pasaron miles de millones de años sin ningún cambio significativo en la morfología u organización celular en estos organismos,[56]​ hasta que surgieron los eucariotas a partir de la integración de una arquea Asgard (Promethearchaeota) y una alfaproteobacteria formando una asociación cooperativa denominada endosimbiosis.[57]​[58]​ Los eucariotas cladisticamente se consideran un clado más dentro de las arqueas.[59]​ Las bacterias incorporadas a las células hospedantes arqueanas, iniciaron un proceso de coevolución, por el cual las bacterias originaron las mitocondrias o hidrogenosomas en los eucariotas.[60]​ También se postula que un virus gigante de ADN similar a los poxvirus originó el núcleo de las células eucariotas al haberse incorporado el virus dentro de la célula donde en lugar de replicarse y destruir la célula huésped, permanecería dentro de la célula originando posteriormente el núcleo y dando lugar a otras innovaciones genómicas. Esta teoría es conocida como la \"eucariogénesis viral\".[61]​[62]​ La evidencia tanto molecular como paleontológica indica que las primeras células eucarióticas surgieron hace unos 2500 millones.[63]​\nUn segundo evento independiente de endosimbiosis se dio hace 2100-1900 millones de años[63]​ que llevó a la formación de los cloroplastos a partir de una cianobacteria y un protozoo los cuales darían origen a las algas rojas y algas verdes, posteriormente de algas verdes que lograron salir del medio acuático evolucionaron las plantas durante el Cámbrico. Por otra parte, las otras algas como las diatomeas o las algas pardas obtuvieron sus cloroplastos por endosimbiosis secundarias entre protozoos con algas rojas o verdes, pero evolutivamente no están emparentadas con las algas rojas o verdes.[64]​[65]​\nLa vida pluricelular surgió de la unión colonial de microorganismos al lograr formar órganos, tejidos y cuerpo fructíferos. De acuerdo con análisis moleculares y estructurales, los animales se originaron de una unión colonial de protozoos similares a los coanoflegelados formando coronas de microvellosidades y con una tendencia a la especialización celular, los coanoflagelados son protozoos similares a los espermatozoides animales y a las células de tipo coanocito que presentan algunos animales que genéticamente son los protistas más cercanos a los animales. Los hongos evolucionaron de protozoos parasitarios ameboides que por causa de su parasitismo perdieron la fagocitosis, reemplazandola por la osmosis y que tuvieron la tendencia a formar colonias filamentosas. Los hongos y los animales son los reinos de la naturaleza genéticamente más cercanos entre sí y se los agrupa en el clado Opisthokonta junto con los protozoos más cercanos a ellos. Además sus células son uniflageladas y opistocontas similar a los espermatozoides. Aunque los hongos más evolucionados como las setas o los mohos carecen de flagelos en sus células, este se conserva en los hongos más primitivos como los quitridios o los microsporidios, por lo que los hongos evolucionaron de ancestros similares a los de los animales.[66]​[67]​[68]​ Los mohos mucilaginosos podrían ser un mejor modelo de como se puede transitar de la vida unicelular a la pluricelular, ya que ciertas amebas plasmodiales pueden agruparse entre sí en colonias y forman un cuerpo fructífero capaz de deslizarse por el suelo. Por otro lado, las mixobacterias cuando se agrupan entre sí en colonias pueden formar pequeños cuerpos fructíferos como las amebas, por tanto esto demuestra que la pluricelularidad no solo se puede ganar en los eucariotas, sino también en los procariotas.\nLos fósiles más antiguos que se considerarían eucariotas correspoden a la biota francevillense de 2100 millones de años que probablemente fueron mohos mucilaginosos que representarían los primeros indicios de vida pluricelular. Los organismos midieron alrededor de 12 cm y consistían en discos planos con una morfología característica e incluía individuos circulares y alargados. En ciertos aspectos son parecidos a algunos organismos de la Biota Ediacara. Además según los estudios científicos podían tener un estado de vida pluricelular y uno unicelular, ya que se desarrollarían de agregados celulares capaces de formar cuerpos fructíferos plasmodiales.[69]​[70]​[52]​[71]​[72]​ \nLa historia de la vida sobre la Tierra fue la de los eucariotas unicelulares, bacterias y arqueas hasta hace aproximadamente 580 millones de años, momento en el que los primeros organismos multicelulares aparecieron en los océanos en el período denominado Ediacárico. Estos organismos son conocidos como la Biota del periodo Ediacárico.[54]​[73]​[74]​\nEs posible que algunos organismos ediacáricos estuvieran estrechamente relacionados con grupos que predominaron más adelante, como los poríferos o los cnidarios.[75]​ No obstante, debido a la dificultad a la hora de deducir las relaciones evolutivas en estos organismos, algunos paleontólogos han sugerido que la biota de Ediacara representa una rama completamente extinta, un «experimento fallido» de la vida multicelular, y que la vida multicelular posterior volvió a evolucionar más tarde a partir de organismos unicelulares no relacionados.[74]​ En cualquier caso, la evolución de los organismos pluricelulares ocurrió en múltiples eventos independientes, en organismos tan diversos como las esponjas, algas pardas, cianobacterias, hongos mucosos y mixobacterias.[76]​\nPoco después de la aparición de los primeros organismos multicelulares, una gran diversidad de formas de vida apareció en un período de diez millones de años, en un evento denominado explosión cámbrica, un lapso breve en términos geológicos, pero que implicó una diversificación animal sin paralelo documentada en los fósiles encontrados en los sedimentos de Burgess Shale, Canadá. Durante este período, la mayoría de los filos animales actuales aparecieron en los registros fósiles, como así también una gran cantidad de linajes únicos que ulteriormente se extinguieron. La mayoría de los planes corporales de los animales modernos se originaron durante este período.[77]​\nEntre los posibles desencadenantes de la explosión cámbrica se incluye la acumulación de oxígeno en la atmósfera debido a la fotosíntesis.[78]​[79]​\nAproximadamente hace 500 millones de años, las plantas y los hongos colonizaron la tierra y les siguieron rápidamente los artrópodos y otros animales.[80]​\nLos anfibios aparecieron en la historia de la Tierra hace alrededor de 300 millones de años, seguidos por los primeros amniotas, y luego por los mamíferos, hace unos 200 millones de años, y las aves, hace 150 millones de años. Sin embargo, los organismos microscópicos, similares a aquellos que evolucionaron tempranamente, continúan siendo la forma de vida predominante en la Tierra, ya que la mayor parte de las especies y la biomasa terrestre está constituida por procariotas.[81]​"
      },
      {
        "heading": "Historia del pensamiento evolucionista",
        "level": 1,
        "content": "Varios filósofos griegos de la antigüedad contemplaron la posibilidad de cambios en los organismos vivos a través del tiempo. Anaximandro (ca. 610-546 a. C.) sugirió que los primeros animales vivían en el agua y que dieron origen a los animales terrestres.[83]​ Empédocles (ca. 490-430 a. C.) escribió que los primeros seres vivos provenían de la tierra y las especies surgieron mediante procesos naturales sin un organizador o una causa final.[84]​ Tales propuestas sobrevivieron hasta la época romana. El poeta y filósofo Lucrecio, siguió a Empédocles en su obra maestra De rerum natura (Sobre la naturaleza de las cosas) donde el universo funciona a través de mecanismos naturalistas, sin ninguna intervención sobrenatural.[85]​[86]​[87]​[88]​ Si la visión mecanicista se encuentra en estos filósofos, la teleológica ocurre en Heráclito, quien concibe el proceso como un desarrollo racional, de acuerdo con el Logos. El desarrollo, así como el proceso de convertirse, en general, fue negado por los filósofos eleáticos.[89]​\nLas obras de Aristóteles (384-322 a. C.), el primer naturalista cuyo trabajo se ha conservado con detalle, contienen observaciones e interpretaciones muy sagaces, si bien mezcladas con mitos y errores diversos que reflejan el estado del conocimiento en su época;[90]​ es notable su esfuerzo en exponer las relaciones existentes entre los seres vivos como una scala naturae ―tal como se describe en Historia animalium― en la que los organismos se clasifican de acuerdo con una estructura jerárquica, «escalera de la vida» o «cadena del Ser», ordenados según la complejidad de sus estructuras y funciones, con los organismos que muestran una mayor vitalidad y capacidad de movimiento descritos como «organismos superiores».[91]​[92]​ En contraste con estos puntos de vista materialistas, el aristotelismo consideraba todas las cosas naturales como actualizaciones de posibilidades naturales fijas, conocidas como formas.[93]​ Esto era parte de una comprensión teleológica de la naturaleza en la que todas las cosas tienen un papel destinado a jugar en un orden cósmico divino. Toda la transición de potencialidad a actualidad (dedynamis a entelecheia) no es más que una transición de lo inferior a lo superior, a lo perfecto, a lo Divino. Aristóteles criticó la teoría evolutiva materialista de Empédocles, en la que accidentes azarosos pudieran conducir a resultados ordenados,[94]​ sin embargo, no argumenta que las especies no puedan cambiar o extinguirse[95]​ y aceptó que nuevos tipos de animales pueden ocurrir en casos muy raros.[96]​[97]​\nLos estoicos siguieron a Heráclito y Aristóteles en las líneas principales de su física. Con ellos, todo el proceso se lleva a cabo de acuerdo a los fines de la Divinidad.[89]​ Las variaciones de esta idea se convirtieron en la comprensión estándar de la Edad Media y se integraron al cristianismo.[98]​ San Agustín toma una visión evolutiva como base para su filosofía de la historia.[99]​ Erigena y algunos de sus seguidores parecen enseñar una especie de evolución.[89]​ Tomás de Aquino no detectó ningún conflicto en un universo divinamente creado y desarrollado con el tiempo a través de mecanismos naturales, argumentando que la autonomía de la naturaleza era signo de Dios (Quinta vía).[100]​ \nAlgunos antiguos pensadores chinos expresaron igualmente la idea de que las especies biológicas cambian. Zhuangzi, un filósofo taoísta que vivió alrededor del siglo IV a. C., mencionó que las formas de vida tienen una habilidad innata o el poder (hua 化) para transformarse y adaptarse a su entorno.[101]​\nSegún Joseph Needham, el taoísmo niega explícitamente la inmutabilidad de las especies biológicas y los filósofos taoístas especularon que las mismas desarrollaron diferentes atributos en respuesta a distintos entornos. De hecho, el taoísmo se refiere a los seres humanos, la naturaleza y el cielo como existentes en un estado de «constante transformación», en contraste con la visión más estática de la naturaleza típica del pensamiento occidental.[102]​\n\nSi bien la idea de la evolución biológica ha existido desde épocas remotas y en diferentes culturas —por ejemplo, en la sociedad musulmana la esbozaron en el siglo IX Al-Jahiz y en el siglo XIII Nasir al-Din al-Tusi respectivamente—,[103]​ la teoría moderna no se estableció hasta llegados los siglos XVIII y XIX, con la contribución de científicos como Christian Pander, Jean-Baptiste Lamarck y Charles Darwin.[104]​\nEn el siglo XVII, el nuevo método de la ciencia moderna rechazó el enfoque aristotélico, la idea de las causas finales. Buscó explicaciones de los fenómenos naturales en términos de leyes físicas que eran las mismas para todas las cosas visibles y que no requerían la existencia de ninguna categoría natural fija u orden divino cósmico. En biología, sin embargo, persistió durante más tiempo la teleología, es decir, la visión según la cual existen fines en la naturaleza. Este nuevo enfoque tardó en arraigarse en las ciencias biológicas, el último bastión del concepto de tipos naturales fijos.[105]​ John Ray aplicó uno de los términos anteriormente más generales para los tipos naturales fijos, \"especies\", a los tipos de plantas y animales, pero identificó estrictamente cada tipo de ser vivo como especie y propuso que cada especie pudiera definirse por las características que perpetuaban ellos mismos generación tras generación.[106]​ La clasificación biológica introducida por Carlos Linneo en 1735 reconoció explícitamente la naturaleza jerárquica de las relaciones entre especies, pero aún consideraba a las especies como fijas según un plan divino.[107]​\nEn el siglo XVIII, la oposición entre fijismo y transformismo fue ambigua. Algunos autores, por ejemplo, admitieron la transformación de las especies a nivel de géneros, pero negaban la posibilidad de que cambiaran de un género a otro. Otros naturalistas hablaban de «progresión» en la naturaleza orgánica, pero es muy difícil determinar si con ello hacían referencia a una transformación real de las especies o se trataba, simplemente, de una modulación de la clásica idea de la scala naturae.[108]​ Entre los filósofos alemanes, Herder estableció la doctrina de un desarrollo continuo en la unidad de la naturaleza, de lo inorgánico a lo orgánico, de la piedra a la planta, de la planta al animal y del animal al hombre. Kant también se menciona a menudo como uno de los primeros maestros de la teoría moderna de la descendencia.[89]​\nGeorges-Louis Leclerc de Buffon (1707-1788 ) sugirió que las especies podrían degenerar en diferentes organismos, y Erasmus Darwin (1731-1802) propuso que todos los animales de sangre caliente podrían haber descendido de un solo microorganismo (o \"filamento\").[109]​\nJean-Baptiste Lamarck (1744-1829) formuló la primera teoría de la evolución o \"transmutación\"[110]​ y propuso que los organismos, es toda su variedad, habían evolucionado desde formas simples creadas por Dios[105]​ y postuló que los responsables de esa evolución habían sido los propios organismos por su capacidad de adaptarse al ambiente: los cambios en ese ambiente generaban nuevas necesidades en los organismos y esas nuevas necesidades conllevarían una modificación de los mismos que sería heredable. Se apoyó para la formulación de su teoría en la existencia de restos de formas intermedias extintas.[13]​ Con esta teoría Lamarck se enfrentó a la creencia general por la que todas las especies habían sido creadas y permanecían inmutables desde su creación y también se opuso al influyente Georges Cuvier (1769-1832) que justificaba la desaparición de las especies no porque fueran formas intermedias entre las primigenias y las actuales, sino porque se trataba de formas de vida diferentes, extinguidas en los diferentes cataclismos geológicos sufridos por la Tierra.[111]​[112]​ Mientras tanto, las ideas de John Ray y de diseño benevolente habían sido desarrolladas por William Paley en la Teología Natural (1802), que propuso adaptaciones complejas como evidencia del diseño divino y quien fue admirado por Charles Darwin.[113]​[114]​\n\nNo fue sino hasta la publicación de El origen de las especies de Charles Darwin cuando el hecho de la evolución comenzó a ser ampliamente aceptado. Una carta de Alfred Russel Wallace, en la cual revelaba su propio descubrimiento de la selección natural, impulsó a Darwin a publicar su trabajo en evolución. Por lo tanto, a veces se les concede a ambos el crédito por la teoría de la evolución, llamándola también teoría de Darwin-Wallace.[92]​\nUn debate particularmente interesante en el campo evolutivo fue el que sostuvieron los naturalistas franceses Georges Cuvier y Étienne Geoffroy Saint-Hilaire en el año 1830. Ambos discrepaban en los criterios fundamentales para describir las relaciones entre los seres vivos; mientras Cuvier se basaba en características anatómicas funcionales, Geoffroy daba más importancia a la morfología. La distinción entre función y forma trajo consigo el desarrollo de dos campos de investigación, conocidos respectivamente como anatomía funcional y anatomía trascendental. Gracias al trabajo del anatomista británico Richard Owen, los dos puntos de vista empezaron a reconciliarse, proceso completado en la teoría de la evolución de Darwin.[115]​[116]​\nA pesar de que la teoría de Darwin sacudió profundamente la opinión científica con respecto al desarrollo de la vida, llegando incluso tener influencias sociales, no pudo explicar la fuente de variación existente entre las especies, y además la propuesta de Darwin de la existencia de un mecanismo hereditario (pangénesis) no satisfizo a la mayoría de los biólogos. No fue recién hasta fines del siglo XIX y comienzos del XX, que estos mecanismos pudieron establecerse.[117]​\nCuando alrededor del 1900 se «redescubrió» el trabajo que Gregor Mendel llevó a cabo a fines del siglo XIX sobre la naturaleza de la herencia, se estableció una discusión entre los mendelianos (Charles Benedict Davenport) y los biométricos (Walter Frank Raphael Weldon y Karl Pearson), quienes insistían en que la mayoría de los caminos importantes para la evolución debían mostrar una variación continua que no era explicable a través del análisis mendeliano. Finalmente, los dos modelos fueron conciliados y fusionados, principalmente a través del trabajo del biólogo y estadístico Ronald Fisher.[111]​\nEste enfoque combinado, que aplica un modelo estadístico riguroso a las teorías de Mendel de la herencia vía genes, se dio a conocer en los años 1930 y 1940 y se conoce como la teoría sintética de la evolución.[118]​\nEn los años de la década de 1940, siguiendo el experimento de Griffith, Avery, MacLeod y McCarty lograron identificar de forma definitiva al ácido desoxirribonucleico (ADN) como el «principio transformador» responsable de la transmisión de la información genética.[119]​ En 1953, Francis Crick y James Watson publicaron su famoso trabajo sobre la estructura del ADN, basado en la investigación de Rosalind Franklin y Maurice Wilkins. Estos avances iniciaron la era de la biología molecular y condujeron a la interpretación de la evolución como un proceso molecular.[cita requerida]\nA mediados de la década de 1970, Motoo Kimura formuló la teoría neutralista de la evolución molecular, estableciendo de manera firme la importancia de la deriva génica como el principal mecanismo de la evolución. Hasta la fecha continúan los debates en esta área de investigación. Uno de los más importantes es acerca de la teoría del equilibrio puntuado, una teoría propuesta por Niles Eldredge y Stephen Jay Gould para explicar la escasez de formas transicionales entre especies.[120]​"
      },
      {
        "heading": "Darwinismo",
        "level": 2,
        "content": "Esta etapa del pensamiento evolutivo se inicia con la publicación en agosto de 1858 de un trabajo conjunto de Darwin y Wallace,[15]​ al que siguió en 1859 el libro de Darwin El origen de las especies, en el que designa el principio de la selección natural como el principal motor del proceso evolutivo y acepta la tesis lamarckiana de la herencia de los caracteres adquiridos como una fuente de variabilidad biológica; por este motivo, aunque Wallace rechazaba el lamarckismo, se acepta la denominación de «Lamarck-Darwin-Wallace» para referirse a este estadio.[3]​ \nDarwin usó la expresión \"descendencia con modificación\" en lugar de \"evolución\". En parte influenciado por Ensayo sobre el principio de la población (1798) de Thomas Malthus, Darwin señaló que el crecimiento de la población conduciría a una \"lucha por la existencia\" en la que prevalecían variaciones favorables mientras otros perecían.[121]​ En cada generación, muchos descendientes no logran sobrevivir a una edad de reproducción debido a los recursos limitados. Esto podría explicar la diversidad de plantas y animales de un ancestro común a través del funcionamiento de las leyes naturales de la misma manera para todos los tipos de organismos.[122]​\n\nEl origen de las especies contenía «una muy ingeniosa teoría para explicar la aparición y perpetuación de las variedades y de las formas específicas en nuestro planeta» según palabras del prólogo escrito por Charles Lyell (1797-1895) y William Jackson Hooker (1785-1865). De hecho, este trabajo presentó por primera vez la hipótesis de la selección natural. Esta hipótesis contenía cinco afirmaciones fundamentales: \n\ntodos los organismos producen más descendencia de la que el ambiente puede sostener;\nexiste una abundante variabilidad intraespecífica para la mayoría de los caracteres;\nla competencia por los recursos limitados lleva a la lucha «por la vida» (según Darwin) o «por la existencia» (según Wallace);\nse produce descendencia con modificaciones heredables\ny como resultado, se originan nuevas especies.[123]​\nDarwin desarrolló su teoría de la \"selección natural\" a partir de 1838 y estaba escribiendo su \"gran libro\" sobre el tema cuando Alfred Russel Wallace le envió una versión de prácticamente la misma teoría en 1858. Sus documentos separados se presentaron juntos en una reunión de 1858 del Sociedad Linneana de Londres.[124]​ Lyell y Hooker reconocieron a Darwin como el primero en formular las ideas presentadas en el trabajo conjunto, adjuntando como prueba un ensayo de Darwin de 1844 y una carta que envió a Asa Gray en 1857, ambos publicados junto con un artículo de Wallace. Un análisis comparativo detallado de las publicaciones de Darwin y Wallace revela que las contribuciones de este último fueron más importantes de lo que usualmente se suele reconocer,[125]​[126]​ Thomas Henry Huxley aplicó las ideas de Darwin a los humanos, utilizando la paleontología y la anatomía comparada para proporcionar pruebas sólidas de que los humanos y los simios compartían un ancestro común.[127]​\nTreinta años más tarde, el codescubridor de la selección natural publicó una serie de conferencias bajo el título de «Darwinism» que tratan los mismos temas que ya había tratado Darwin, pero a la luz de los hechos y de los datos que eran desconocidos en tiempos de Darwin, quien falleció en 1882.[128]​ Sin embargo, en su Origen de las especies', Darwin fue el primero en resumir un conjunto coherente de observaciones que solidificó el concepto de la evolución de la vida en una verdadera teoría científica ―es decir, en un sistema de hipótesis―.[129]​\nLa lista de las propuestas de Darwin presentadas en esta obra se expone a continuación:[3]​\n\n \n\nEl gran logro de Darwin fue demostrar que es posible explicar teleología aparente en términos no-teleológicos o términos causales corrientes. La vida no es direccional, no está encaminada de antemano.[105]​"
      },
      {
        "heading": "Neodarwinismo",
        "level": 2,
        "content": "Neodarwinismo es un término acuñado en 1895 por el naturalista y psicólogo inglés George John Romanes (1848-1894) en su obra Darwin and after Darwin.[130]​ El término describe un estado en el desarrollo de la teoría evolutiva que se remonta al citólogo y zoólogo germano August Weismann (1834-1914), quien en 1892 aportó evidencia experimental en contra de la herencia lamarckiana y postuló que el desarrollo del organismo no influye en el material hereditario y que la reproducción sexual en cada generación introduce nuevas variaciones en la población de individuos. La selección natural, entonces, puede actuar sobre la variabilidad de la población y determina el curso del cambio evolutivo.[131]​ El neodarwinismo enriqueció el concepto original de Darwin, al destacar el origen de las variaciones entre individuos y excluir la herencia lamarckiana como una explicación viable del mecanismo de herencia. Wallace, quien popularizó el término «darwinismo» en 1889,[128]​ incorporó plenamente las nuevas conclusiones de Weismann y fue, por consiguiente, uno de los primeros proponentes del neodarwinismo.[3]​"
      },
      {
        "heading": "Síntesis evolutiva moderna",
        "level": 2,
        "content": "Este sistema de hipótesis del proceso evolutivo se originó entre 1937 y 1950.[133]​\n\nEn contraste con el neodarwinismo de Weismann y Wallace, que daba primacía a la selección natural y postulaba la genética mendeliana como el mecanismo de transmisión de caracteres entre generaciones, la teoría sintética incorporó datos de campos diversos de la biología, como la genética molecular, la sistemática y la paleontología e introdujo nuevos mecanismos para la evolución. Por estas razones, se trata de diferentes teorías aunque a veces se usen los términos indistintamente.[134]​[135]​[136]​De acuerdo a la gran mayoría de los historiadores de la Biología, los conceptos básicos de la teoría sintética están basados esencialmente en el contenido de seis libros, cuyos autores fueron: el naturalista y genetista ruso americano Theodosius Dobzhansky (1900-1975); el naturalista y taxónomo alemán americano Ernst Mayr (1904-2005); el zoólogo británico Julian Huxley (1887-1975); el paleontólogo americano George G. Simpson (1902-1984); el zoólogo germano Bernhard Rensch (1900-1990) y el botánico estadounidense George Ledyard Stebbins (1906-2000).[135]​\nLos términos «síntesis evolutiva» y «teoría sintética» fueron acuñados por Julian Huxley en su libro Evolución: la síntesis moderna (1942), en el que también introdujo el término Biología evolutiva en vez de la frase «estudio de la evolución».[137]​[138]​ De hecho Huxley fue el primero en señalar que la evolución «debía ser considerada el problema más central y el más importante de la biología y cuya explicación debía ser abordada mediante hechos y métodos de cada rama de la ciencia, desde la ecología, la genética, la paleontología, la embriología, la sistemática hasta la anatomía comparada y la distribución geográfica, sin olvidar los de otras disciplinas como la geología, la geografía y las matemáticas».[139]​\nLa llamada «síntesis evolutiva moderna» es una robusta teoría que actualmente proporciona explicaciones y modelos matemáticos de los mecanismos generales de la evolución o los fenómenos evolutivos, como la adaptación o la especiación. Como cualquier teoría científica, sus hipótesis están sujetas a constante crítica y comprobación experimental.[cita requerida]\n\nLos entes dónde actúa la evolución son las poblaciones de organismos y no los individuos. Theodosius Dobzhansky, uno de los fundadores de la síntesis moderna, lo expresó la evolución del siguiente modo: «La evolución es un cambio en la composición genética de las poblaciones. El estudio de los mecanismos evolutivos corresponde a la genética poblacional».[140]​ Esta idea llevó al «concepto biológico de especie» desarrollado por Mayr en 1942: una comunidad de poblaciones que se entrecruzan y que está reproductivamente aislada de otras comunidades.[141]​[142]​\nLa variabilidad fenotípica y genética en las poblaciones de plantas y de animales se produce por la recombinación genética ―reorganización de segmentos de cromosomas durante la reproducción sexual— y por las mutaciones aleatorias. La cantidad de variación genética que una población de organismos con reproducción sexual puede producir es enorme. Considérese la posibilidad de un solo individuo con un número «N» de genes, cada uno con solo dos alelos. Este individuo puede producir 2N espermatozoides u óvulos genéticamente diferentes. Debido a que la reproducción sexual implica dos progenitores, cada descendiente puede, por tanto, poseer una de las 4N combinaciones diferentes de genotipos. Así, si cada progenitor tiene 150 genes con dos alelos cada uno ―una subestimación del genoma humano―, cada uno de los padres puede dar lugar a más de 1045 gametos genéticamente diferentes y más de 1090 descendientes genéticamente diferentes.[cita requerida]\nLa selección natural es la fuerza más importante que modela el curso de la evolución fenotípica. En ambientes cambiantes, la selección direccional es de especial importancia, porque produce un cambio en la media de la población hacia un fenotipo nuevo que se adapta mejor a las condiciones ambientales alteradas. Además, en las poblaciones pequeñas, la deriva génica aleatoria ―la pérdida de genes del acervo genético― puede ser significativa.[cita requerida]\nLa especiación puede definirse como «un paso en el proceso evolutivo (en el que) las formas... se vuelven incapaces de hibridarse».[143]​ Se han descubierto y estudiado en profundidad diversos mecanismos de aislamiento reproductivo. Se cree que el aislamiento geográfico de la población fundadora es responsable del origen de las nuevas especies en las islas y otros hábitats aislados y es probable que la especiación alopátrica ―evolución divergente de poblaciones que están geográficamente aisladas unas de otras― sea el mecanismo de especiación predominante en el origen de muchas especies de animales.[144]​ Sin embargo, la especiación simpátrica ―la aparición de nuevos especies sin aislamiento geográfico― también está documentada en muchos taxones, sobre todo en las plantas vasculares, los insectos, los peces y las aves.[145]​\nLas transiciones evolutivas en estas poblaciones suelen ser graduales, es decir, las nuevas especies evolucionan a partir de las variedades preexistentes por medio de procesos lentos y en cada etapa se mantiene su adaptación específica.[cita requerida]\nLa macroevolución ―la evolución filogenética por encima del nivel de especie o la aparición de taxones superiores― es un proceso gradual, paso a paso, que no es más que la extrapolación de la microevolución ―el origen de las razas y variedades, y de las especies―.[cita requerida]"
      },
      {
        "heading": "Equilibrio puntuado",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Teoría neutralista de la evolución molecular",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Biología evolutiva del desarrollo",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Síntesis evolutiva moderna",
        "level": 1,
        "content": "En la época de Darwin los científicos no conocían cómo se heredaban las características. Posteriormente se descubrió la relación de la mayoría de las características hereditarias con entidades persistentes llamadas genes, fragmentos de las moléculas lineales de ácido desoxirribonucleico (ADN) del núcleo de las células. El ADN varía entre los miembros de una misma especie y también sufre cambios, mutaciones, o reorganizaciones por recombinación genética."
      },
      {
        "heading": "Variabilidad",
        "level": 2,
        "content": "El fenotipo de un organismo individual es el resultado de su genotipo y la influencia del ambiente en el que vive y ha vivido. Una parte sustancial de la variación entre fenotipos dentro de una población está causada por las diferencias entre sus genotipos.[160]​ La síntesis evolutiva moderna define la evolución como el cambio de esa variación genética a través del tiempo. La frecuencia de cada alelo fluctúa, siendo más o menos prevalente en relación con otras formas alternativas del mismo gen. Las fuerzas evolutivas actúan mediante la dirección de esos cambios en las frecuencias alélicas en uno u otro sentido. La variación de una población para un gen dado desaparece cuando se produce la fijación de un alelo que ha reemplazado enteramente a todas las otras formas alternativas de ese mismo gen.[161]​\nLa variabilidad surge en las poblaciones naturales por mutaciones en el material genético, migraciones entre poblaciones (flujo genético) y por la reorganización de los genes a través de la reproducción sexual. La variabilidad también puede provenir del intercambio de genes entre diferentes especies, por ejemplo a través de la transferencia horizontal de genes en las bacterias o la hibridación interespecífica en las plantas.[162]​ A pesar de la constante introducción de variantes nuevas a través de estos procesos, la mayor parte del genoma de una especie es idéntica en todos los individuos que pertenecen a la misma.[163]​ Sin embargo, aun pequeños cambios en el genotipo pueden llevar a modificaciones sustanciales del fenotipo. Así, los chimpancés y los seres humanos, por ejemplo, solo difieren en aproximadamente el 5 % de sus genomas.[164]​"
      },
      {
        "heading": "Mutación",
        "level": 3,
        "content": "Darwin no conocía la fuente de las variaciones en los organismos individuales, pero observó que parecían ocurrir aleatoriamente. En trabajos posteriores se atribuyó la mayor parte de estas variaciones a las mutaciones. La mutación es un cambio permanente y transmisible en el material genético ―usualmente el ADN o el ARN― de una célula, producido por «errores de copia» en el material genético durante la división celular o por la exposición a radiación, sustancias químicas o la acción de virus. Las mutaciones aleatorias ocurren constantemente en el genoma de todos los organismos, creando nueva variabilidad genética.[165]​[166]​[167]​ Las mutaciones pueden no tener efecto alguno sobre el fenotipo del organismo, o ser perjudiciales o beneficiosas. A modo de ejemplo, los estudios realizados sobre la mosca de la fruta (Drosophila melanogaster), sugieren que, si una mutación determina un cambio en la proteína producida por un gen, ese cambio será perjudicial en el 70 % de los casos y neutro o levemente beneficioso en los restantes.[168]​\nLa frecuencia de nuevas mutaciones en un gen o secuencia de ADN en cada generación se denomina tasa de mutación. En escenarios de rápido cambio ambiental, una tasa de mutación alta aumenta la probabilidad de que algunos individuos tengan una variante genética adecuada para adaptarse y sobrevivir; por otro lado, también aumenta el número de mutaciones perjudiciales o deletéreas que disminuyen la adaptación de los individuos y eleva la probabilidad de extinción de la especie.[169]​ Debido a los efectos contrapuestos que las mutaciones pueden tener sobre los organismos,[165]​ la tasa de mutación óptima para una población es una compensación entre costos y beneficios,[170]​ que depende de la especie y refleja la historia evolutiva como respuesta a los retos impuestos por el ambiente.[169]​ Los virus, por ejemplo, presentan una alta tasa de mutación,[171]​ lo que supone una ventaja adaptativa ya que deben evolucionar rápida y constantemente para sortear a los sistemas inmunes de los organismos que afectan.[172]​\nLa duplicación génica introduce en el genoma copias extras de un gen y, de ese modo, proporciona el material de base para que las nuevas copias inicien su propio camino evolutivo.[173]​[174]​[175]​ Si el gen inicial sigue funcionando normalmente, sus copias pueden adquirir nuevas mutaciones sin perjuicio para el organismo que los alberga y llegar con el tiempo a adoptar nuevas funciones.[176]​[177]​ Por ejemplo, en los seres humanos son necesarios cuatro genes para construir las estructuras necesarias para detectar la luz: tres para la visión de los colores y uno para la visión nocturna. Los cuatro genes han evolucionado a partir de un solo gen ancestral por duplicación y posterior divergencia.[178]​ Otros tipos de mutación pueden ocasionalmente crear nuevos genes a partir del denominado ADN no codificante.[179]​[180]​ También pueden surgir nuevos genes con diferentes funciones a partir de fragmentos de genes duplicados que se recombinan para formar nuevas secuencias de ADN.[181]​[182]​[183]​\nLas mutaciones cromosómicas ―también denominadas, aberraciones cromosómicas― son una fuente adicional de variabilidad hereditaria. Así, las translocaciones, inversiones, deleciones, translocaciones robertsonianas y duplicaciones, usualmente ocasionan variantes fenotípicas que se transmiten a la descendencia. Por ejemplo, en el género Homo tuvo lugar una fusión cromosómica que dio lugar al cromosoma 2 de los seres humanos, mientras que otros simios conservan 24 pares de cromosomas.[184]​ No obstante las consecuencias fenotípicas que pueden tener tales mutaciones cromosómicas, su mayor importancia evolutiva reside en acelerar la divergencia de las poblaciones que presentan diferentes configuraciones cromosómica: el flujo genético entre ellas se reduce severamente debido a la esterilidad o semiesterilidad de los individuos heterocigóticos. De este modo, las mutaciones cromosómicas actúan como mecanismos de aislamiento reproductivo que conducen a que las diferentes poblaciones mantengan su identidad como especies a través del tiempo.[185]​\nLos fragmentos de ADN que pueden cambiar de posición en los cromosomas, tales como los transposones, constituyen una importante fracción del material genético de plantas y animales y pueden haber desempeñado un papel destacado en su evolución.[186]​ Al insertarse en o escindirse de otras partes del genoma estas secuencias pueden activar, inhibir, eliminar o mutar otros genes y, por ende, crear nueva variabilidad genética.[166]​ Asimismo, ciertas de estas secuencias se repiten miles o millones de veces en el genoma y muchas de ellas han adoptado funciones importantes, como por ejemplo, la regulación de la expresión genética.[187]​"
      },
      {
        "heading": "Recombinación genética",
        "level": 3,
        "content": "La recombinación genética es el proceso mediante el cual la información genética se redistribuye por transposición de fragmentos de ADN entre dos cromosomas durante la meiosis ―y más raramente en la mitosis―. Los efectos son similares a los de las mutaciones, es decir, si los cambios no son deletéreos se transmiten a la descendencia y contribuyen a incrementar la diversidad dentro de cada especie.[cita requerida]\nEn los organismos asexuales, los genes se heredan en conjunto, o ligados, ya que no se mezclan con los de otros organismos durante los ciclos de recombinación que usualmente se producen durante la reproducción sexual. En contraste, los descendientes de los organismos que se reproducen sexualmente contienen una mezcla aleatoria de los cromosomas de sus progenitores, la cual se produce durante la recombinación meiótica y la posterior fecundación.[188]​ La recombinación no altera las frecuencias alélicas sino que modifica la asociación existente entre alelos pertenecientes a genes diferentes, produciendo descendientes con combinaciones únicas de genes.[189]​ La recombinación generalmente incrementa la variabilidad genética y puede aumentar también las tasas de evolución.[190]​[191]​[192]​ No obstante, la existencia de la reproducción asexual, tal como ocurre en las plantas apomícticas o en los animales partenogenéticos, indica que este modo de reproducción puede también ser ventajoso en ciertos ambientes.[193]​ Jens Christian Clausen fue uno de los primeros en reconocer formalmente que la apomixis, particularmente la apomixis facultativa, no necesariamente conduce a una pérdida de variabilidad genética y de potencial evolutivo. Utilizando una analogía entre el proceso adaptativo y la producción a gran escala de automóviles, Clausen arguyó que la combinación de sexualidad (que permite la producción de nuevos genotipos) y de apomixis (que permite la producción ilimitada de los genotipos más adaptados) potencia la capacidad de una especie para el cambio adaptativo.[194]​\nAunque el proceso de recombinación posibilita que los genes agrupados en un cromosoma puedan heredarse independientemente, la tasa de recombinación es baja ―aproximadamente dos eventos por cromosoma y por generación―. Como resultado, los genes adyacentes tienden a heredarse conjuntamente, en un fenómeno que se denomina ligamiento.[195]​ Un grupo de alelos que usualmente se heredan conjuntamente por hallarse ligados se denomina haplotipo. Cuando uno de los alelos en haplotipo es altamente beneficioso, la selección natural puede conducir a un barrido selectivo que aumenta la proporción dentro de la población del resto de los alelos en el haplotipo; este efecto se denomina arrastre por ligamiento o «efecto autostop» (en inglés, genetic hitchhiking).[196]​\nCuando los alelos no se recombinan, como es el caso en el cromosoma Y de los mamíferos o en los organismos asexuales, los genes con mutaciones deletéreas se acumulan, lo que se denomina trinquete de Muller (Muller ratchet en inglés).[197]​[198]​ De este modo, al romper los conjuntos de genes ligados, la reproducción sexual facilita la eliminación de las mutaciones perjudiciales y la retención de las beneficiosas,[199]​ además de la aparición de individuos con combinaciones genéticas nuevas y favorables. Estos beneficios deben contrarrestar otros efectos perjudiciales de la reproducción sexual, como la menor tasa reproductiva de las poblaciones de organismos sexuales y la separación de combinaciones favorables de genes. En todas las especies sexuales, y con la excepción de los organismos hermafroditas, cada población está constituida por individuos de dos sexos, de los cuales solo uno es capaz de engendrar la prole. En una especie asexual, en cambio, todos los miembros de la población tienen esa capacidad, lo que implica un crecimiento más rápido de la población asexual en cada generación. Otro costo del sexo es que los machos y las hembras deben buscarse entre ellos para aparearse y la selección sexual suele favorecer caracteres que reducen la aptitud de los individuos. Este costo del sexo fue expresado por primera vez en términos matemáticos por John Maynard Smith.[200]​[199]​ Las razones de la evolución de la reproducción sexual son todavía poco claras y es un interrogante que constituye un área activa de investigación en Biología evolutiva,[201]​[202]​ que ha inspirado ideas tales como la hipótesis de la Reina Roja.[203]​ El escritor científico Matt Ridley, que popularizó el término en su libro The Red Queen: Sex and the Evolution of Human Nature, sostiene que existe una carrera armamentista cíclica entre los organismos y sus parásitos y especula que el sexo sirve para preservar los genes circunstancialmente desfavorables, pero potencialmente beneficiosos ante futuros cambios en las poblaciones de parásitos.[cita requerida]"
      },
      {
        "heading": "Genética de poblaciones",
        "level": 3,
        "content": "Como se ha descrito previamente, desde un punto de vista genético la evolución es un cambio intergeneracional en la frecuencia de los alelos dentro de una población que comparte un mismo patrimonio genético.[204]​ Una población es un grupo de individuos de la misma especie que comparten un ámbito geográfico. Por ejemplo, todas las polillas de una misma especie que viven en un bosque aislado forman una población. Un gen determinado dentro de la población puede presentar diversas formas alternativas, que son las responsables de la variación entre los diferentes fenotipos de los organismos. Un ejemplo puede ser un gen de la coloración en las polillas que tenga dos alelos: uno para color blanco y otro para color negro. El patrimonio o acervo genético es el conjunto completo de los alelos de una población, de forma que cada alelo aparece un número determinado de veces en un acervo génico. La fracción de genes del patrimonio genético que están representadas por un alelo determinado recibe el nombre de frecuencia alélica, por ejemplo, la fracción de polillas en la población que presentan el alelo para color negro. La evolución tiene lugar cuando hay cambios en la frecuencia alélica en una población de organismos que se reproducen entre ellos, por ejemplo, si el alelo para color negro se hace más común en una población de polillas.[205]​\nPara comprender los mecanismos que hacen que evolucione una población, es útil conocer las condiciones necesarias para que la población no evolucione. El principio de Hardy-Weinberg determina que la frecuencia de los alelos de una población suficientemente grande permanecerá constante solo si la única fuerza que actúa es la recombinación aleatoria de alelos durante la formación de los gametos y la posterior combinación de los mismos durante la fertilización.[206]​ En ese caso, la población se encuentra en equilibrio de Hardy-Weinberg y, por lo tanto, no evoluciona.[205]​"
      },
      {
        "heading": "Flujo genético",
        "level": 3,
        "content": "El flujo genético es el intercambio de genes entre poblaciones, usualmente de la misma especie. Como ejemplos de flujo génico se pueden mencionar el cruzamiento de individuos tras la inmigración de una población en el territorio de otra, o, en el caso de las plantas, el intercambio de polen entre poblaciones diferentes. La transferencia de genes entre especies conlleva la formación de híbridos o la transferencia horizontal de genes.[208]​\nLa inmigración y la emigración de individuos en las poblaciones naturales pueden causar cambios en las frecuencias alélicas, como así también la introducción ―o desaparición― de variantes alélicas dentro de un acervo genético ya establecido. Las separaciones físicas en el tiempo, espacio o nichos ecológicos específicos que puede existir entre las poblaciones naturales restringen o imposibilitan el flujo génico. Además de estas restricciones al intercambio de genes entre poblaciones existen otros mecanismos de aislamiento reproductivo conformados por características, comportamientos y procesos fisiológicos que impiden que los miembros de dos especies diferentes puedan cruzarse o aparearse entre sí, producir descendencia o que esta sea viable o fértil. Estas barreras constituyen una fase indispensable en la formación de nuevas especies ya que mantienen las características propias de las mismas a través del tiempo al restringir o eliminar el flujo genético entre los individuos de diferentes poblaciones.[209]​[210]​[211]​[212]​\nLas especies distintas pueden ser interfértiles, dependiendo de cuánto han divergido desde su ancestro común; por ejemplo, la yegua y el asno pueden aparearse y producir la mula.[213]​ Tales híbridos son generalmente estériles debido a las diferencias cromosómicas entre las especies parentales, que impiden el emparejamiento correcto de los cromosomas durante la meiosis. En este caso, las especies estrechamente relacionadas pueden cruzarse con regularidad, pero la selección natural actúa contra los híbridos. Sin embargo, de vez en cuando se forman híbridos viables y fértiles que pueden presentar propiedades intermedias entre sus especies paternales o poseer un fenotipo totalmente nuevo.[214]​\nLa importancia de la hibridación en la creación de nuevas especies de animales no está clara, aunque existen ejemplos bien documentados como el de la rana Hyla versicolor.[215]​[216]​ La hibridación es, sin embargo, un mecanismo importante de formación de nuevas especies en las plantas, ya que estas toleran la poliploidía ―la duplicación de todos los cromosomas de un organismo― más fácilmente que los animales;[217]​[218]​ la poliploidía restaura la fertilidad en los híbridos interespecíficos debido a que cada cromosoma es capaz de aparearse con un compañero idéntico durante la meiosis.[219]​[220]​\nHay dos mecanismos básicos de cambio evolutivo: la selección natural y la deriva genética. La selección natural favorece a los genes que mejoran la capacidad de supervivencia y reproducción del organismo. La deriva genética es el cambio en la frecuencia de los alelos, provocado por transmisión aleatoria de los genes de una generación a la siguiente. La importancia relativa de la selección natural y de la deriva genética en una población varía dependiendo de la fuerza de la selección y del tamaño poblacional efectivo, que es el número de ejemplares de esa población capaces de reproducirse.[221]​ La selección natural suele predominar en las poblaciones grandes, mientras que la deriva genética predomina en las pequeñas. El predominio de la deriva genética en poblaciones pequeñas puede llevar incluso a la fijación de mutaciones ligeramente deletéreas.[222]​ Como resultado de ello, los cambios en el tamaño de una población pueden influir significativamente en el curso de la evolución. Los denominados «cuellos de botella», o descensos drásticos temporarios del tamaño efectivo de la población, suponen una pérdida o erosión de la variabilidad genética y conllevan la formación de poblaciones genéticamente más uniformes. Los cuellos de botella pueden ser el resultado de catástrofes, variaciones en el medio ambiente o alteraciones en el flujo genético causadas por una migración reducida, la expansión a nuevos hábitats, o una subdivisión de la población.[221]​"
      },
      {
        "heading": "Selección natural",
        "level": 3,
        "content": "La selección natural es el proceso por el cual las mutaciones genéticas que mejoran la capacidad reproductiva se vuelven, y permanecen, cada vez más frecuentes en las sucesivas generaciones de una población. Se la califica a menudo de «mecanismo autoevidente», pues es la consecuencia necesaria de tres hechos simples: (a) dentro de las poblaciones de organismos hay variación heredable (b) los organismos producen más descendientes de los que pueden sobrevivir, y (c) tales descendientes tienen diferentes capacidades para sobrevivir y reproducirse.[223]​\nEl concepto central de la selección natural es la aptitud biológica de un organismo.[224]​ La aptitud, ajuste o adecuación influye en la medida de la contribución genética de un organismo a la generación siguiente.[224]​ Sin embargo, la aptitud no es simplemente igual al número total de descendientes de un determinado organismo, ya que también cuantifica la proporción de generaciones posteriores que llevan los genes de ese organismo.[225]​ Por ejemplo, si un organismo puede sobrevivir y reproducirse, pero sus descendientes son demasiado pequeños o enfermizos como para llegar a la edad reproductiva, la contribución genética de ese organismo a las futuras generaciones será muy baja y, por ende, su aptitud también lo es.[224]​\nPor consiguiente, si un alelo aumenta la aptitud más que otros, con cada generación el alelo será más común dentro de la población. Se dice que tales rasgos son «seleccionados favorablemente». Una mejora de la supervivencia o una mayor fecundidad son ejemplos de rasgos que pueden aumentar la aptitud. En cambio, la menor aptitud causada por un alelo menos beneficioso o deletéreo hace que este sea cada vez más raro en la población y sufra una «selección negativa».[226]​ Hay que subrayar que la aptitud de un alelo no es una característica fija: si el ambiente cambia, los rasgos que antes eran neutros o nocivos pueden ser beneficiosos y viceversa.[227]​ Por ejemplo, la polilla Biston betularia presenta dos colores, uno claro denominado forma typica y otro oscuro llamado forma carbonaria. La forma typica, como su nombre indica, es la más frecuente en esta especie. No obstante, durante la revolución industrial en el Reino Unido los troncos de muchos árboles sobre los que se posaban las polillas se ennegrecieron por el hollín, lo que les daba las polillas de color oscuro una mayor oportunidad de sobrevivir y producir más descendientes al pasar más fácilmente desapercibidas para los depredadores. Solo cincuenta años después de que se descubriera la primera polilla melánica, casi la totalidad de las polillas del área industrial de Mánchester eran oscuras. Este proceso se revirtió a causa de la «Ley del aire limpio» (Clean Air Act) de 1956, que redujo la polución industrial. Al aclararse el color de los troncos, las polillas oscuras volvieron a ser más fácilmente visibles por los depredadores y su número disminuyó.[228]​ Sin embargo, aunque la dirección de la selección cambie, los rasgos que se hubiesen perdido en el pasado no pueden volver a obtenerse de forma idéntica ―situación que describe la Ley de Dollo o «Ley de la irreversibilidad evolutiva»―.[229]​ De acuerdo con esta hipótesis, una estructura u órgano que se ha perdido o descartado durante el transcurso de la evolución no volverá a aparecer en ese mismo linaje de organismos.[230]​[231]​\nSegún Richard Dawkins, esta hipótesis es «una declaración sobre la improbabilidad estadística de seguir exactamente la misma trayectoria evolutiva dos veces o, de hecho, una misma trayectoria particular en ambas direcciones».[232]​\nDentro de una población, la selección natural para un determinado rasgo que varía en forma continua, como la altura, se puede categorizar en tres tipos diferentes. El primero es la «selección direccional», que es un cambio en el valor medio de un rasgo a lo largo del tiempo; por ejemplo, cuando los organismos cada vez son más altos.[233]​ En segundo lugar se halla la «selección disruptiva» que es la selección de los valores extremos de un determinado rasgo, lo que a menudo determina que los valores extremos sean más comunes y que la selección actúe en contra del valor medio; esto implica, en el ejemplo anterior, que los organismos bajos y altos tienen una ventaja, pero los de altura media no. Finalmente, en la «selección estabilizadora», la selección actúa en contra de los valores extremos, lo que determina una disminución de la varianza alrededor del promedio y una menor variabilidad de la población para ese carácter en particular;[223]​[234]​ si se diera este tipo de selección, todos los organismos de una población adquirirían paulatinamente una altura similar.[cita requerida]\nUn tipo especial de selección natural es la selección sexual, que actúa a favor de cualquier rasgo que aumente el éxito reproductivo por aumentar el atractivo de un organismo para sus parejas potenciales.[235]​ Ciertos rasgos adquiridos por los machos por selección sexual ―tales como los cuernos voluminosos, cantos de apareamiento o colores brillantes― pueden reducir las posibilidades de supervivencia, por ejemplo, por atraer a los depredadores.[236]​ No obstante, esta desventaja reproductiva se compensa por un mayor éxito reproductivo de los machos que presentan estos rasgos.[237]​\nUn área de estudio activo es la denominada «unidad de selección»; se ha dicho que la selección natural actúa a nivel de genes, células, organismos individuales, grupos de organismos e incluso especies.[238]​[239]​ Ninguno de estos modelos es mutuamente exclusivo, y la selección puede actuar en múltiples niveles a la vez.[240]​ Por ejemplo, debajo del nivel del individuo, hay genes denominados transposones que intentan replicarse en todo el genoma.[241]​ La selección por encima del nivel del individuo, como la selección de grupo, puede permitir la evolución de la cooperación.[242]​"
      },
      {
        "heading": "Deriva genética",
        "level": 3,
        "content": "La deriva genética es el cambio en la frecuencia de los alelos entre una generación y la siguiente, y tiene lugar porque los alelos de la descendencia son una muestra aleatoria de los padres, y por el papel que juega el azar en la hora de determinar si un ejemplar determinado sobrevivirá y se reproducirá.[161]​ En términos matemáticos, los alelos están sujetos a errores de muestreo. Como resultado de ello, cuando las fuerzas selectivas están ausentes o son relativamente débiles, la frecuencia de los alelos tiende a «derivar» hacia arriba o hacia abajo aleatoriamente (en un paseo aleatorio). Esta deriva se detiene cuando un alelo se convierte finalmente fijado, es decir, o bien desaparece de la población, o bien sustituye totalmente el resto de genes. Así pues, la deriva genética puede eliminar algunos alelos de una población simplemente debido al azar. Incluso en la ausencia de fuerzas selectivas, la deriva genética puede hacer que dos poblaciones separadas que empiezan con la misma estructura genética se separen en dos poblaciones divergentes con un conjunto de alelos diferentes.[243]​\nEl tiempo necesario para que un alelo quede fijado por la deriva genética depende del tamaño de la población; la fijación tiene lugar más rápido en poblaciones más pequeñas.[244]​ La medida precisa de las poblaciones que es importante en este caso recibe el nombre de tamaño poblacional efectivo, que fue definida por Sewall Wright como el número teórico de ejemplares reproductivos que presenten el mismo grado observado de consanguinidad.\nAunque la selección natural es responsable de la adaptación, la importancia relativa de las dos fuerzas, selección natural y deriva genética, como impulsoras del cambio evolutivo en general es actualmente un campo de investigación en la biología evolutiva.[245]​ Estas investigaciones fueron inspiradas por la teoría neutralista de la evolución molecular, que postula que la mayoría de cambios evolutivos son el resultado de la fijación de mutaciones neutras, que no tienen ningún efecto inmediato sobre la aptitud de un organismo.[246]​ Por tanto, en este modelo, la mayoría de los cambios genéticos en una población son el resultado de una presión de mutación constante y de deriva genética.[247]​"
      },
      {
        "heading": "Las consecuencias de la evolución",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Adaptación",
        "level": 3,
        "content": "La adaptación es el proceso mediante el cual una población se adecua mejor a su hábitat y también el cambio en la estructura o en el funcionamiento de un organismo que lo hace más adecuado a su entorno.[248]​[249]​ Este proceso se produce por selección natural[250]​ durante muchas generaciones[251]​ y es uno de los fenómenos básicos de la biología.[252]​ \n\n \n\nLas especies tienden a adaptarse a diferentes nichos ecológicos para reducir al mínimo la competencia entre ellas.[254]​ Esto se conoce como principio de exclusión competitiva en ecología: dos especies no pueden ocupar el mismo nicho en el mismo ambiente por un largo tiempo.[255]​ \nLa adaptación no implica necesariamente cambios importantes en parte física de un cuerpo. Como ejemplo puede mencionarse a los trematodos ―parásitos internos con estructuras corporales muy simples, pero con un ciclo de vida muy complejo― en los que sus adaptaciones a un medio ambiente tan inusual no son el producto de caracteres observables a simple vista sino en aspectos críticos de su ciclo vital.[256]​ En general, el concepto de adaptación incluye, además del proceso adaptativo mismo, todos los aspectos de los organismos, de las poblaciones o de las especies que son su resultado. Mediante la utilización del término «adaptación» para el proceso evolutivo y «rasgo o carácter adaptativo» para el producto del mismo, los dos sentidos del concepto se distinguen perfectamente. Según Theodosius Dobzhansky la «adaptación» es el proceso evolutivo por el cual un organismo se vuelve más capaz de vivir en su hábitat o hábitats,[257]​ mientras que la «adaptabilidad» es el estado de estar adaptado, o sea, el grado en que un organismo es capaz de vivir y reproducirse en un determinado conjunto de hábitats.[258]​ Finalmente, un «carácter adaptativo» es uno de los aspectos del desarrollo de un organismo que aumenta su probabilidad de sobrevivir y reproducirse.[259]​\nLa adaptación a veces implica la ganancia de una nueva característica; ejemplos notables son la evolución en laboratorio de las bacterias Escherichia coli para que puedan ser capaces de utilizar el ácido cítrico como un nutriente, cuando las bacterias de tipo silvestre no lo pueden hacer;[260]​ la evolución de una nueva enzima en Flavobacterium que permite que estas bacterias puedan crecer en los subproductos de la fabricación del nailon;[261]​[262]​ y la evolución de una vía metabólica completamente nueva en la bacteria del suelo Sphingobium que le permite degradar el pesticida sintético pentaclorofenol.[263]​[264]​ En ocasiones, también puede darse la pérdida de una función ancestral. Un ejemplo que muestra los dos tipos de cambio es la adaptación de la bacteria Pseudomonas aeruginosa a la fluoroquinolona con cambios genéticos que modifican la molécula sobre la que actúa y por el aumento de la actividad de los transportadores que bombean el antibiótico fuera de la célula.[265]​ Una idea todavía controvertida, es que algunas adaptaciones pueden aumentar la capacidad de los organismos para generar diversidad genética y para adaptarse por selección natural ―o sea, aumentarían la capacidad de evolución―.[266]​[267]​\n\nUna consecuencia de la adaptación es la existencia de estructuras con organización interna similar y diferentes funciones en organismos relacionados. Este es el resultado de la modificación de una estructura ancestral para adaptarla a diferentes ambientes y nichos ecológicos. Los huesos de las alas de los murciélagos, por ejemplo, son muy similares a los de los pies del ratón y los de las manos de los primates, debido a que todas estas estructuras estaban presentes en un ancestro común de los mamíferos.[269]​ Dado que todos los organismos vivos están relacionados en cierta medida, incluso las estructuras que presentan diferencias profundas, como los ojos de los artrópodos, del calamar y de los vertebrados, o las extremidades y las alas de artrópodos y vertebrados, dependen de un conjunto común de genes homólogos que controlan su desarrollo y funcionamiento, lo que se denomina homología profunda.[270]​[271]​\nDurante la adaptación pueden aparecer estructuras vestigiales,[272]​ carentes de funcionalidad en una especie; sin embargo, la misma estructura es funcional en la especie ancestral o en otras especies relacionadas. Los ejemplos incluyen los pseudogenes,[273]​ los ojos de los peces cavernícolas ciegos,[274]​ las alas en las especies de aves que no vuelan[275]​ y los huesos de la cadera presentes en las ballenas y en las serpientes.[268]​ En los seres humanos también existen ejemplos de estructuras vestigiales, como las muelas de juicio,[276]​ el coxis,[272]​ el apéndice vermiforme,[272]​ y reacciones involuntarias como la piel de gallina[277]​ y otros reflejos primitivos.[278]​[279]​[280]​[281]​\nAlgunos rasgos que parecen ser simples adaptaciones son, de hecho, exaptaciones: estructuras originalmente adaptadas para una función, pero que coincidentalmente se hicieron útiles para otro propósito.[282]​ Un ejemplo es el lagarto africano Holaspis guentheri que desarrolló una cabeza y tronco muy aplastados para esconderse en las grietas, como puede observarse en otros lagartos del mismo género. Sin embargo, en esta especie, el cuerpo achatado le permite planear de árbol en árbol.[282]​ Los pulmones de los peces pulmonados ancestrales son una exaptación de las vejigas natatorias de los peces teleósteos empleadas como regulador de la flotación.[283]​\nUna rama de la biología evolutiva estudia el desarrollo de las adaptaciones y de las exaptaciones.[284]​ Esta área de investigación aborda el origen y la evolución del desarrollo embrionario y cómo surgen nuevas características a partir de modificaciones del desarrollo.[285]​ Estos estudios han demostrado, por ejemplo, que las mismas estructuras óseas de los embriones que forman parte de la mandíbula en algunos animales, en cambio forman parte del oído medio en los mamíferos.[286]​ Cambios en los genes que controlan el desarrollo también pueden causar que reaparezcan estructuras perdidas durante la evolución, como por ejemplo, los dientes de embriones de pollos mutantes, similares a los de los cocodrilos.[287]​ De hecho, cada vez es más claro que la mayoría de las alteraciones en la forma de los organismos se deben a cambios en un pequeño conjunto de genes conservados.[288]​"
      },
      {
        "heading": "Coevolución",
        "level": 3,
        "content": "La interacción entre organismos puede producir conflicto o cooperación. Cuando interactúan dos especies diferentes, como un patógeno y un hospedador, o un depredador y su presa, la evolución de una de ellas causa adaptaciones en la otra; estos cambios en la segunda especie causan, a su vez, nuevas adaptaciones en la primera. Este ciclo de selección y respuesta recibe el nombre de coevolución.[289]​ Un ejemplo es la producción de tetradotoxina por parte del tritón de Oregón y la evolución de la resistencia a esta toxina en su predador, la serpiente de jarretera. En esta pareja predador-presa, la carrera armamentista evolutiva (Hipótesis de la Reina Roja) ha tenido como resultado una elevada producción de toxina en el tritón, y un aumento correspondiente de resistencia a ella en la serpiente.[290]​ Un ejemplo de coevolución que no involucra competencia (Hipótesis del Rey Rojo) son las interacciones simbióticas y mutualistas entre las micorrizas y las raíces de las plantas o las abejas y las plantas que polinizan. Otro ejemplo de entidad que coevoluciona con su huésped son los virus con las células. Las células suelen desarrollar defensas en su sistema inmune para evitar infecciones virales, sin embargo los virus deben mutar rápidamente para poder evitar dichas defensas celulares o el sistema inmune del huésped. Por tanto, los virus son las únicas entidades que evolucionan más rápidamente que cualquier otra para su existencia. Los virus son entidades que solamente pueden evolucionar y sobrevivir en las células. Según estudios recientes han venido coevolucionando con las células desde sus orígenes (protobionte) y por ello su origen es anterior al del último antepasado común universal. Los virus pudieron en surgir en esos protobiontes o antes, para luego servir como un medio de transferencia horizontal de genes (elemento genético móvil) y regular la población de determinados organismos celulares. De hecho nuevos tipos de virus también pudieron ir surgiendo durante diferentes etapas de la evolución, mediante diversos mecanismos moleculares como recombinaciones entre elementos genéticos móviles con otros virus. Incluso pueden saltar entre diversos organismos celulares y adquirir nuevos huéspedes.[291]​[292]​[293]​"
      },
      {
        "heading": "Especiación",
        "level": 3,
        "content": "La especiación (o cladogénesis) es el proceso por el cual una especie diverge en dos o más especies descendientes.[294]​ Los biólogos evolutivos ven las especies como fenómenos estadísticos y no como categorías o tipos. Este planteamiento es contrario al aún muy arraigado concepto clásico de especie como una clase de organismos ejemplificados por un «espécimen tipo» que posee todas las características comunes a dicha especie. En su lugar, una especie se concibe ahora como un linaje que comparte un único acervo genético y evoluciona independientemente. Según esta descripción, los límites entre especies son difusos, a pesar de que se utilizan propiedades tanto genéticas como morfológicas para ayudar a diferenciar entre linajes estrechamente relacionados.[295]​ De hecho, la definición exacta del término «especie» está todavía en discusión, particularmente para organismos basados en células procariotas;[296]​ es lo que se denomina «problema de las especies».[297]​ Diversos autores han propuesto una serie de definiciones basadas en criterios diferentes, pero la aplicación de una u otra es finalmente una cuestión práctica, dependiendo en cada caso concreto de las particularidades del grupo de organismos en estudio.[297]​ Actualmente, la unidad de análisis principal en biología es la población, un conjunto observable de individuos que interactúan, en lugar de la especie, un conjunto observable de individuos que se parecen entre sí.[cita requerida]\n\nLa especiación ha sido observada en múltiples ocasiones tanto en condiciones de laboratorio controladas como en la naturaleza.[298]​ En los organismos que se reproducen sexualmente, la especiación es el resultado de un aislamiento reproductivo seguido de una divergencia genealógica. Hay cuatro modalidades de especiación. La más habitual en los animales es la especiación alopátrica, que tiene lugar en poblaciones que quedan geográficamente aisladas, como en el caso de la fragmentación de hábitat o las migraciones. En estas condiciones, la selección puede causar cambios muy rápidos en la apariencia y el comportamiento de los organismos.[299]​[300]​ Debido a los procesos de selección y deriva genética, la separación de poblaciones puede tener como resultado la aparición de descendientes que no se pueden reproducir entre ellos.[301]​\nLa segunda modalidad de especiación es la especiación peripátrica, que tiene lugar cuando poblaciones pequeñas quedan aisladas en un nuevo medio. Se diferencia de la especiación alopátrica en que las poblaciones aisladas son numéricamente mucho menores que la población madre. Esto causa una especiación rápida por medio de la aceleración de la deriva genética y la selección en un acervo génico pequeño, proceso conocido como el efecto fundador.[302]​\nLa tercera modalidad de especiación es la especiación parapátrica. Se parece a la especiación peripátrica en que una pequeña población coloniza un nuevo hábitat, pero se diferencia en que no hay ninguna separación física entre las dos poblaciones. En cambio, la especiación es el resultado de la evolución de mecanismos que reducen el flujo génico entre ambas poblaciones.[294]​ Generalmente, esto ocurre cuando ha habido un cambio drástico en el medio dentro del hábitat de la especie madre. Un ejemplo es la hierba Anthoxanthum odoratum, que puede sufrir una especiación parapátrica en respuesta a contaminación metálica localizada proveniente de minas.[303]​ En este caso, evolucionan plantas con una resistencia a niveles altos de metales en el suelo; el aislamiento reproductivo ocurre porque la selección favorece una época de floración distinta a la de las especie madre para las nuevas plantas, que no pueden perder entonces por hibridación los genes que les otorgan la resistencia. La selección en contra de los híbridos puede reforzarse por la diferenciación de los rasgos que promueven la reproducción entre miembros de la misma especie, así como por el aumento de las diferencias de apariencia en el área geográfica en la que la se solapan.[304]​\nFinalmente, en la especiación simpátrica, las especies divergen sin que haya aislamiento geográfico o cambios en el hábitat. Esta modalidad es rara, pues incluso una pequeña cantidad de flujo génico puede eliminar las diferencias genéticas entre partes de una población.[305]​ En general, en los animales, la especiación simpátrica requiere la evolución de diferencias genéticas y un apareamiento no aleatorio, para que se pueda desarrollar un aislamiento reproductivo.[306]​\nUn tipo de especiación simpátrica es el cruce de dos especies relacionadas para producir una nueva especie híbrida. Esto no es habitual en los animales, porque los cromosomas homólogos de progenitores de especies diferentes no pueden aparearse con éxito durante la meiosis. Es más habitual en las plantas, que duplican a menudo su número de cromosomas para formar poliploides.[307]​ Esto permite a los cromosomas de cada especie parental formar una pareja complementaria durante la meiosis, ya que los cromosomas de cada padre ya son representados por una pareja.[308]​ Un ejemplo de este tipo de especiación tuvo lugar cuando las especies vegetales Arabidopsis thaliana y Arabidopsis arenosa se cruzaron para producir la nueva especie Arabidopsis suecica.[309]​ Esto tuvo lugar hace aproximadamente 20 000 años,[310]​ y el proceso de especiación ha sido reproducido en el laboratorio, lo que ha permitido estudiar los mecanismos genéticos implicados en este proceso.[311]​ De hecho, la duplicación de cromosomas dentro de una especie puede ser una causa habitual de aislamiento reproductivo, pues la mitad de los cromosomas duplicados quedarán sin pareja cuando se aparean con los de organismos no duplicados.[297]​\nLos episodios de especiación son importantes en la teoría del equilibrio puntuado, que explica la presencia en el registro fósil de rápidos momentos de especiación intercalados con periodos relativamente largos de estasis, durante los que las especies permanecen prácticamente sin modificar.[7]​ En esta la teoría, la especiación está relacionada con la evolución rápida, y la selección natural y la deriva genética actúan de forma particularmente intensa sobre los organismos que sufren una especiación en hábitats nuevos o pequeñas poblaciones. Como resultado de ello, los períodos de estasis del registro fósil corresponden a la población madre, y los organismos que sufren especiación y evolución rápida se encuentran en poblaciones pequeñas o hábitats geográficamente restringidos, por lo que raramente quedan preservados en forma de fósiles.[312]​"
      },
      {
        "heading": "Extinción",
        "level": 3,
        "content": "La extinción es la desaparición de una especie entera. No es un acontecimiento inusual,[313]​ y, de hecho, la práctica totalidad de especies animales y vegetales que han vivido en la Tierra están actualmente extinguidas,[314]​ por lo que parece que la extinción es el destino final de todas las especies.[315]​ Las extinciones tienen lugar continuamente durante la historia de la vida, aunque el ritmo de extinción aumenta drásticamente en los ocasionales eventos de extinción.[316]​ La extinción del Cretácico-Terciario, durante la cual se extinguieron los dinosaurios, es la más conocida, pero la anterior extinción Permo-Triásica fue aún más severa, causando la extinción de casi el 96 % de las especies.[316]​ La extinción del Holoceno todavía dura y está asociada con la expansión de la humanidad por el globo terrestre en los últimos milenios. El ritmo de extinción actual es de 100 a 1000 veces mayor que el ritmo medio, y hasta un 30 % de las especies pueden desaparecer a mediados del siglo XXI.[317]​ Las actividades humanas son actualmente la causa principal de la extinción;[318]​ es posible que el calentamiento global la acelere aún más en el futuro.[319]​\nLas causas de la extinción determinan su impacto en la evolución.[316]​ Las mayoría de las extinciones, que tienen lugar continuamente, podrían ser el resultado de la competencia entre especies por recursos limitados (exclusión competitiva).[3]​ Si la competencia entre especies altera la probabilidad de extinción, se podría considerar la selección de especies como un nivel de la selección natural.[238]​ Las extinciones masivas intermitentes también son importantes, pero en lugar de actuar como fuerza selectiva, reducen drásticamente la diversidad de manera indiscriminada y promueven explosiones de rápida evolución y especiación en los supervivientes.[320]​"
      },
      {
        "heading": "Microevolución y macroevolución",
        "level": 2,
        "content": "Microevolución es un término usado para referirse a cambios en pequeña escala de las frecuencias génicas en una población durante el transcurso de varias generaciones. La adaptación de los insectos al uso de plaguicidas o la variación del color de piel de los humanos son ejemplos de microevolución.[321]​[322]​ Estos cambios pueden deberse a varios procesos: mutación, flujo génico, deriva génica o selección natural.[321]​ La genética de poblaciones es una rama de la biología evolutiva aplica métodos bioestadísticos al estudio de los procesos de la microevolución, como el color de la piel en la población mundial.[cita requerida]\nLos cambios a mayor escala, desde la especiación hasta las grandes transformaciones evolutivas ocurridas en largos períodos, son comúnmente denominados macroevolución. La evolución de los anfibios a partir de un grupo de peces óseos es un ejemplo de macroevolución. Los biólogos no acostumbran hacer una separación absoluta entre macroevolución y microevolución, pues consideran que macroevolución es simplemente microevolución acumulada durante escalas de tiempo grandes.[323]​ Una minoría de teóricos, sin embargo, considera que los mecanismos de la teoría sintética para la microevolución no bastan para hacer esa extrapolación y que se necesitan otros mecanismos. La teoría de los equilibrios puntuados, propuesta por Gould y Eldredge, intenta explicar ciertas tendencias macroevolutivas que se observan en el registro fósil.[324]​"
      },
      {
        "heading": "Filogenia",
        "level": 2,
        "content": "La filogenia es la relación de parentesco entre especies o taxones en general.[325]​ Aunque el término también aparece en lingüística histórica para referirse a la clasificación de las lenguas humanas según su origen común, el término se utiliza principalmente en su sentido biológico. La filogenética es una disciplina de la biología evolutiva que se ocupa de comprender las relaciones históricas entre diferentes grupos de organismos a partir de la distribución en un árbol o cladograma dicotómico de los caracteres derivados (sinapomorfias) de un antecesor común a dos o más taxones que contiene aquellos caracteres plesiomórficos en común.[cita requerida]"
      },
      {
        "heading": "Monofilia, parafilia y polifilia",
        "level": 3,
        "content": ""
      },
      {
        "heading": "Ampliación de la síntesis moderna",
        "level": 1,
        "content": "En las últimas décadas se ha hecho evidente que los patrones y los mecanismos evolutivos son mucho más variados que los que fueran postulados por los pioneros de la Biología evolutiva (Darwin, Wallace o Weismann) y los arquitectos de la teoría sintética (Dobzhansky, Mayr y Huxley, entre otros). Los nuevos conceptos e información en la biología molecular del desarrollo, la sistemática, la geología y el registro fósil de todos los grupos de organismos necesitan ser integrados en lo que se ha denominado «síntesis evolutiva ampliada». Los campos de estudio mencionados muestran que los fenómenos evolutivos no pueden ser comprendidos solamente a través de la extrapolación de los procesos observados a nivel de las poblaciones y especies modernas.[326]​[327]​[328]​[329]​[111]​ En las próximas secciones se presentan los aspectos considerados como la ampliación de la síntesis moderna."
      },
      {
        "heading": "Paleobiología y tasas de evolución",
        "level": 2,
        "content": "En el momento en que Darwin propuso su teoría de evolución, caracterizada por modificaciones pequeñas y sucesivas, el registro fósil disponible era todavía muy fragmentario y no se habían hallado fósiles previos al período Cámbrico. El dilema de Darwin, o sea, la inexistencia aparente de registros fósiles del Precámbrico, fue utilizado como el principal argumento en contra de su propuesta de que todos los organismos de la Tierra provienen de un antepasado común.[3]​\nAdemás de la ausencia de fósiles antiguos, a Darwin también le preocupaba la carencia de formas intermedias o enlaces conectores en el registro fósil, lo cual desafiaba su visión gradualística de la especiación y de la evolución. De hecho en tiempos de Darwin, con la excepción de Archaeopteryx, que muestra una mezcla de características de ave y de reptil, virtualmente no se conocían otros ejemplos de formas intermedias o eslabones perdidos, como se los denominó coloquialmente.[3]​\nIncluso en 1944, cuando se publicó el libro de Simpson Tempo and mode in evolution, no se conocían todavía fósiles del Precámbrico y solo se disponía de unos pocos ejemplos de formas intermedias en el registro fósil que enlazaran las formas antiguas con las modernas. Desde entonces los científicos han explorado el período Precámbrico con detalle y se sabe que la vida es mucho más antigua de lo que se creía en los tiempos de Darwin. También se sabe que esas antiguas formas de vida fueron los ancestros de todos los organismos aparecidos posteriormente en el planeta. Asimismo desde finales del siglo XX se han descubierto, descrito y analizado una gran cantidad de ejemplos representativos de formas fósiles intermedias que enlazan a los principales grupos de vertebrados e, incluso, fósiles de las primeras plantas con flor.[52]​ Como resultado de estos y otros avances científicos, ha surgido una nueva disciplina de la Paleontología, denominada Paleobiología.[326]​[333]​[334]​[335]​[336]​[337]​\nUn ejemplo de forma transicional entre los peces y los anfibios es el género extinto Panderichthys, que habitó la tierra hace unos 370 millones de años y es el enlace intermedio entre Eustenopteron (género de peces de 380 millones de años) y Acanthostega (anfibios de hace 363 millones de años).[338]​ Entre los animales terrestres surgió el género Pederpes hace 350 millones de años, que conecta a los principales anfibios acuáticos del Devónico superior con los tetrápodos tempranos.[339]​ Asimismo, la historia evolutiva de varios grupos de organismos extintos, tales como los dinosaurios, ha sido reconstruida con notable detalle.[340]​ Un ejemplo de eslabón entre los sinápsidos no mamíferos y los mamíferos es el Thrinaxodon, un sinápsido con características de mamífero que habitó el planeta hace 230 millones de años.[341]​ El Microraptor, un dromeosáurido con cuatro alas que podía planear y que vivió hace 126 millones de años, representa un estado intermedio entre los terópodos y las primitivas aves voladoras como Archaeopteryx.[342]​ Una forma transicional entre los mamíferos terrestres y la vaca marina es Pezosiren, un sirénido cuadrúpedo primitivo con adaptaciones terrestres y acuáticas que vivió hace 50 millones de años.[343]​ Los mamíferos terrestres con pezuñas y las ballenas se hallan conectados a través de los géneros extintos Ambulocetus y Rodhocetus que habitaron el planeta hace 48 a 47 millones de años.[344]​ Para finalizar esta enumeración de ejemplos de formas transicionales, el ancestro común de los chimpancés y de los seres humanos es el género Sahelanthropus, un homínido con aspecto de mono que exhibía un mosaico de caracteres de chimpancé y de hominino y que habitó África hace 7 a 5 millones de años.[345]​\nEn su libro Variation and evolution in plants (1950), Stebbins también se lamentaba por la ausencia de un registro fósil que permitiera comprender el origen de las primeras plantas con flores, las angiospermas. De hecho, el propio Darwin caracterizó al origen de las angiospermas como un «abominable misterio». No obstante, este vacío de conocimiento está siendo rápidamente completado con los descubrimientos realizados desde fines del siglo XX y hasta la actualidad. En 1998 se descubrió en China, en los estratos provenientes del Jurásico Superior (de más de 125 millones de años de antigüedad), un fósil de un eje con frutos, que se ha denominado Archaefructus[346]​ Este descubrimiento, que permite establecer la edad de las angiospermas más antiguas, hizo mundialmente famosa a la Formación Yixian, donde fue hallado este fósil. En la misma formación se encontraron el fósil de otra angiosperma, Sinocarpus,[347]​ y, en 2007, una flor que presenta la organización típica de las angiospermas, con la presencia de tépalos, estambres y gineceo. Esta especie ha sido bautizada como Euanthus (del griego, «flor verdadera») por sus descubridores, y demuestra que en el Cretácico inferior ya existían flores como las de las angiospermas actuales.[348]​"
      },
      {
        "heading": "Causas ambientales de las extinciones masivas",
        "level": 2,
        "content": "Darwin no solo consideró el origen, sino también la disminución y la desaparición de las especies; propuso que la competencia interespecífica por recursos limitados era una causa importante de la extinción de poblaciones y especies: durante el tiempo evolutivo, las especies superiores surgirían para reemplazar a especies menos adaptadas. Esta perspectiva ha cambiado en los últimos años, con una mejor comprensión de las causas de las extinciones masivas, episodios de la historia de la Tierra, donde parecen no cumplirse las «reglas» de la selección natural.[316]​[3]​\nMayr planteó la nueva perspectiva en su libro Animal species and evolution, en el que señaló que la extinción debe considerarse como uno de los fenómenos evolutivos más conspicuos. Mayr discutió las causas de los eventos de extinción y especuló que la aparición de nuevas enfermedades, la invasión de un ecosistema por otras especies o los cambios en el ambiente biótico pueden ser los responsables: \n\n \n\nEsta hipótesis, no sustentada por hechos probados cuando fue propuesta, ha adquirido desde entonces un considerable apoyo. El término «extinción masiva», mencionado por Mayr[350]​ sin una definición asociada, se utiliza cuando una gran cantidad de especies se extinguen en un plazo geológicamente breve; los eventos pueden estar relacionados con una causa única o con una combinación de causas, y las especies extintas son plantas y animales de todo tamaño, tanto marinos como terrestres.[334]​ Al menos han ocurrido cinco extinciones masivas: la extinción masiva del Cámbrico-Ordovícico, las extinciones masivas del Ordovícico-Silúrico, la extinción masiva del Devónico, la extinción masiva del Pérmico-Triásico y la extinción masiva del Cretácico-Terciario.[351]​\nLa extinción biológica que se produjo en el Pérmico-Triásico hace unos 250 millones de años representa el más grave evento de extinción en los últimos 550 millones de años. Se estima que en este evento se extinguieron alrededor del 70 % de las familias de vertebrados terrestres, muchas gimnospermas leñosas y más del 90 % de las especies oceánicas. Se han propuesto varias causas para explicar este evento, como el vulcanismo, el impacto de un asteroide o un cometa, la anoxia oceánica y el cambio ambiental. No obstante, es aparente en la actualidad que las gigantescas erupciones volcánicas, que tuvieron lugar durante un intervalo de tiempo de solo unos pocos cientos de miles de años, fueron la causa principal de la catástrofe de la biosfera durante el Pérmico tardío.[352]​ El límite Cretácico-Terciario registra el segundo mayor evento de extinción masivo. Esta catástrofe mundial acabó con el 70 % de todas las especies, entre las cuales los dinosaurios son el ejemplo más popularmente conocido. Los pequeños mamíferos sobrevivieron para heredar los nichos ecológicos vacantes, lo que permitió el ascenso y la radiación adaptativa de los linajes que en última instancia se convertirían en Homo sapiens[335]​ y otras especies actuales. Los paleontólogos han propuesto numerosas hipótesis para explicar este evento; las más aceptadas en la actualidad son las del impacto de un asteroide[353]​ y la de fenómenos de vulcanismo.[335]​\nEn resumen, la hipótesis de los trastornos ambientales como causas de las extinciones masivas ha sido confirmada, lo cual indica que si bien gran parte de historia de la evolución puede ser gradual, de vez en cuando ciertos acontecimientos catastróficos han marcado su ritmo de fondo. Es evidente que los pocos «afortunados sobrevivientes» determinan los subsecuentes patrones de evolución en la historia de la vida.[3]​[316]​"
      },
      {
        "heading": "Selección sexual y altruismo",
        "level": 2,
        "content": "Determinadas características en una especie son sexualmente atractivas aunque carezcan de otro significado adaptativo. Por ejemplo, la atracción de las hembras de algunas especies de aves por los machos más capaces de hinchar los cuellos, ha traído como resultado —en el transcurso de las generaciones― la selección de machos que pueden hinchar los cuellos hasta un nivel extraordinario. Darwin concluyó que si bien la selección natural guía el curso de la evolución, la selección sexual influye su curso aunque no parezca existir ninguna razón evidente para ello. Los argumentos de Darwin a favor de la selección sexual aparecen en el capítulo cuarto de El origen de las especies y, muy especialmente, en The Descent of Man, and Selection in Relation to Sex de 1871. En ambos casos, se esgrime la analogía con el mundo artificial:\n\n \n\nEn su libro The Descent of Man describió numerosos ejemplos, tales como la cola del pavo real y de la melena del león. Darwin argumentó que la competencia entre los machos es el resultado de la selección de los rasgos que incrementan el éxito del apareamiento de los machos competidores, rasgos que podrían, sin embargo, disminuir las posibilidades de supervivencia del individuo. De hecho, los colores brillantes hacen a los animales más visibles a los depredadores, el plumaje largo de los machos de pavo real y de las aves del paraíso, o la enorme cornamenta de los ciervos son cargas incómodas en el mejor de casos. Darwin sabía que no era esperable que la selección natural favoreciera la evolución de tales rasgos claramente desventajosos, y propuso que los mismos surgieron por selección sexual, \n\n \n\nPara Darwin, la selección sexual incluía fundamentalmente dos fenómenos: la preferencia de las hembras por ciertos machos ―selección intersexual, femenina, o epigámica― y, en las especies polígamas, las batallas de los machos por el harén más grande ―selección intrasexual―. En este último caso, el tamaño corporal grande y la musculatura proporcionan ventajas en el combate, mientras que en el primero, son otros rasgos masculinos, como el plumaje colorido y el complejo comportamiento de cortejo los que se seleccionan a favor para aumentar la atención de las hembras. Las ideas de Darwin en este sentido no fueron ampliamente aceptadas y los defensores de la teoría sintética (Dobzhansky, Mayr y Huxley) en gran medida ignoraron el concepto de selección sexual.[cita requerida]\nEl estudio de la selección sexual solo cobró impulso en la era postsíntesis.[354]​ Se ha argumentado que, según propuso Wallace, los machos con plumaje brillante demuestran de ese modo su buena salud y su alta calidad como parejas sexuales. De acuerdo con esta hipótesis de la «selección sexual de los buenos genes», la elección de pareja masculina por parte de las hembras ofrece una ventaja evolutiva.[125]​ Esta perspectiva ha recibido apoyo empírico en las últimas décadas. Por ejemplo, se ha hallado una asociación, aunque pequeña, entre la supervivencia de la descendencia y los caracteres sexuales secundarios masculinos en un gran número de taxones, tales como aves, anfibios, peces e insectos).[355]​ Además, las investigaciones con mirlos han proporcionado la primera evidencia empírica de que existe una correlación entre un carácter sexual secundario y un rasgo que incrementa la supervivencia ya que los machos con los más brillantes colores presentan un sistema inmune más fuerte.[356]​ Así, la selección femenina podría promover la salud general de las poblaciones en esta especie. Estos y otros datos son coherentes con el concepto de que la elección de la hembra influye en los rasgos de los machos e, incluso, que puede ser beneficiosa para la especie en formas que no tienen ninguna relación directa con el éxito del apareamiento.[354]​\nUna de las primeras referencias a la cooperación animal proviene de Charles Darwin, quien la señaló como un problema potencial para su teoría de la selección natural.[357]​ Desde la publicación del Origen de las especies, otros autores han argumentado que el comportamiento altruista —los actos abnegados realizados en beneficio de los demás— es incompatible con el principio de la selección natural, a pesar de que los ejemplos de comportamiento altruista, como el cuidado de las crías por los padres y el mutualismo, se dan en todo el reino animal, desde los invertebrados hasta los mamíferos.[358]​[359]​[360]​[361]​ \n\nDurante la mayor parte del siglo XIX, intelectuales como Thomas Henry Huxley y Peter Kropotkin debatieron fervientemente sobre si los animales cooperan y muestran comportamientos altruistas.[362]​ En 1902, Kropotkin ofreció una visión alternativa de la supervivencia humana y animal en su libro El apoyo mutuo, donde declaró que la asociación ofrece las mejores perspectivas de desarrollo para las especies.[363]​[364]​ Kropotkin acuñó el término evolución progresiva para describir cómo la ayuda mutua se convirtió en la condición sine qua non de toda la vida social, animal y humana.[365]​\nUna de las formas más notorias de altruismo se da en ciertos insectos eusociales, como las hormigas, abejas y avispas, que tienen una clase de trabajadoras estériles. La respuesta a la cuestión general de la evolución del altruismo, de la sociabilidad de ciertos insectos o de la existencia de abejas u hormigas obreras que no dejan descendientes proviene de la teoría de la aptitud inclusiva, también llamada teoría de selección familiar.[366]​ De acuerdo con el principio de Darwin/Wallace la selección natural actúa sobre las diferencias en el éxito reproductivo (ER) de cada individuo, donde ER es el número de descendientes vivos producidos por ese individuo durante toda la vida. Hamilton (1972) amplió esta idea e incluyó los efectos de ER de los familiares del individuo: la aptitud inclusiva es el ER de cada individuo, más el ER de sus familiares, cada uno devaluado por el correspondiente grado de parentesco. Numerosos estudios en una gran variedad de especies animales han demostrado que el altruismo no está en conflicto con la teoría evolutiva. Por esta razón, es necesario realizar una modificación y ampliación de la visión tradicional de que la selección opera sobre un solo organismo aislado en una población: el individuo aislado ya no parece tener una importancia central desde el punto de vista evolutivo, sino como parte de una compleja red familiar.[367]​[368]​[369]​"
      },
      {
        "heading": "Macroevolución, monstruos prometedores y equilibrio puntuado",
        "level": 2,
        "content": "Cuando se define macroevolución como el proceso responsable del surgimiento de los taxones de rango superior, se está utilizando un lenguaje metafórico. En sentido estricto, solo «surgen» nuevas especies, ya que la especie es el único taxón que posee estatus ontológico.[370]​ La macroevolución da cuenta de la emergencia de discontinuidades morfológicas importantes entre grupos de especies, razón por la cual se las clasifica como grupos marcadamente diferenciados, es decir, pertenecientes a unidades taxonómicas distintas y de alto rango. Es en los mecanismos que explican el surgimiento de estas discontinuidades que las diferentes concepciones y aproximaciones disciplinarias se contraponen.[cita requerida]\n\nEl gradualismo es el modelo macroevolucionista ortodoxo. Explica la macroevolución como el producto de un cambio lento, de la acumulación de muchos pequeños cambios en el transcurso del tiempo. Este cambio gradual debería reflejarse en el registro fósil con la aparición de numerosas formas de transición entre los grupos de organismos. Sin embargo, el registro no es abundante en formas intermedias. Los gradualistas atribuyen esta discrepancia entre su modelo y las pruebas halladas a la imperfección del propio registro geológico —según Darwin, el registro geológico es una narración de la que se han perdido algunos volúmenes y muchas páginas—. El modelo del equilibrio puntuado, propuesto en 1972 por N. Eldredge y S. J. Gould, sostiene en cambio que el registro fósil es un fiel reflejo de lo que en realidad ocurrió. Las especies aparecen repentinamente en los estratos geológicos, se las encuentra en ellos por 5 a 10 millones de años sin grandes cambios morfológicos y luego desaparecen abruptamente del registro, sustituidas por otra especie emparentada, pero distinta. Eldredge y Gould utilizan los términos estasis e interrupción, respectivamente, para designar estos períodos. Según su modelo, las interrupciones abruptas en el registro fósil de una especie reflejarían el momento en que esta fue reemplazada por una pequeña población periférica ―en la cual el ritmo de evolución habría sido más rápido― que compitió con la especie originaria y terminó por sustituirla. De acuerdo con este patrón, la selección natural no solo opera dentro de la población, sino también entre especies, y los cambios cualitativamente importantes en los organismos ocurrirían en períodos geológicos relativamente breves separados por largos períodos de equilibrio.[cita requerida]\nEn biología evolutiva, un monstruo prometedor es un organismo con un fenotipo profundamente mutante que tiene el potencial para establecer un nuevo linaje evolutivo. El término se utiliza para describir un evento de especiación saltacional que puede ser el origen de nuevos grupos de organismos. La frase fue acuñada por el genetista alemán Richard Goldschmidt, quien pensaba que los cambios pequeños y graduales que dan lugar a la microevolución no pueden explicar la macroevolución. La relevancia evolutiva de los monstruos prometedores ha sido rechazada o puesta en duda por muchos científicos que defienden la Teoría sintética de evolución biológica.[371]​\nEn su obra The material basis of evolution (La base material de la evolución), Goldschmidt escribió que «el cambio desde una especie a otra no es un cambio que no implica más y más cambios atomísticos, sino una modificación completa del patrón principal o del sistema de reacción principal en uno nuevo, el que, más tarde puede nuevamente producir variación intraespecífica por medio de micromutaciones».[372]​\nLa tesis de Goldschmidt fue universalmente rechazada y ampliamente ridiculizada dentro la comunidad científica, que favorecía las explicaciones neodarwinianas de R. A. Fisher, J. B. S. Haldane y Sewall Wright.[373]​\nNo obstante, varias líneas de evidencia sugieren que los monstruos prometedores juegan un papel significativo durante el origen de innovaciones clave y de planes corporales noveles por evolución saltacional, más que por evolución gradual.[371]​ Stephen Jay Gould expuso en 1977 que los genes o secuencias reguladoras ofrecían cierto apoyo a los postulados de Goldschmidt. De hecho, arguyó que los ejemplos de evolución rápida no minan el darwinismo ―como Goldschmidt suponía―, pero tampoco merecen un descrédito inmediato, como muchos neodarwinistas pensaban.[374]​ Gould insistió en que la creencia de Charles Darwin en el gradualismo no fue jamás un componente esencial de su teoría de evolución por selección natural. Thomas Henry Huxley también advirtió a Darwin que había sobrecargado su trabajo con una innecesaria dificultad al adoptar sin reservas el principio Natura non facit saltum.[375]​ Huxley temía que ese supuesto podría desalentar a aquellos naturalistas que creían que los cataclismos y los grandes saltos evolutivos jugaban un papel significativo en la historia de la vida. En este sentido, Gould escribió:"
      },
      {
        "heading": "La síntesis de la biología del desarrollo y la teoría de la evolución",
        "level": 2,
        "content": "Aunque Darwin discutió en detalle la biología del desarrollo —antes llamada embriología—, esta rama de la biología no contribuyó a la síntesis evolutiva. Ernst Mayr, en su ensayo What was the evolutionary synthesis? («Qué fue la síntesis evolutiva?»),[20]​ explicó que varios de los embriólogos del período en que surgió la síntesis moderna tenían una postura contraria a la teoría evolutiva: \n\n \n En las dos últimas décadas, sin embargo, la biología del desarrollo y la biología evolutiva se han unido para formar una nueva rama de la investigación biológica llamada Biología evolutiva del desarrollo o, coloquialmente, «Evo-devo», que explora cómo han evolucionado los procesos del desarrollo y cómo ha surgido la organización de las diversas partes del cuerpo de los organismos antiguos y actuales.[376]​[377]​ \nEl principal descubrimiento responsable de la integración de la biología del desarrollo con la teoría de la evolución fue el de un grupo de genes reguladores, la familia de genes homeóticos (genes HOX). Estos genes codifican proteínas de unión al ADN (factores de transcripción) que influyen profundamente en el desarrollo embrionario.[378]​ Por ejemplo, la supresión de las extremidades abdominales de los insectos está determinada por los cambios funcionales en una proteína llamada Ultrabithorax, que es codificada por un gen Hox.[379]​ La familia de genes Hox existe en los artrópodos (insectos, crustáceos, quelicerados, miriápodos), cordados (peces, anfibios, reptiles, aves, mamíferos), y hay análogos entre las especies de plantas y hongos.[378]​\nLos genes HOX influyen en la morfogénesis de los embriones de los vertebrados al expresarse en diferentes regiones a lo largo del eje anteroposterior del cuerpo. Esta familia de genes es homóloga tanto funcional como estructuralmente al complejo homeótico (HOM-C) de Drosophila melanogaster. Sobre la base de la comparación de genes de varios taxones, se ha logrado reconstruir la evolución de los grupos de genes HOX en vertebrados. Los 39 genes que comprenden la familia de genes HOX en humanos y ratones, por ejemplo, están organizados en cuatro complejos genómicos localizados en diferentes cromosomas, HOXA en el brazo corto del cromosoma 7, HOXB en el 17, HOXC en el 12 y HOXD en el 2, y cada uno de ellos comprende de 9 a 11 genes acomodados en una secuencia homóloga a la del genoma de D. melanogaster.[380]​\nAunque el ancestro común del ratón y del humano vivió hace alrededor de 75 millones de años, la distribución y arquitectura de sus genes HOX son idénticas.[381]​ Por lo tanto, la familia de genes HOX es muy antigua y aparentemente muy conservada, lo que tiene profundas implicaciones para la evolución de los patrones y procesos de desarrollo.[381]​[376]​[378]​"
      },
      {
        "heading": "Microbiología y transferencia horizontal de genes",
        "level": 2,
        "content": "Las primeras teorías evolucionistas prácticamente ignoraron la microbiología, debido a la escasez de rasgos morfológicos y la falta de un concepto de especie particularmente entre los procariotas.[382]​ Los recientes avances en el estudio de la genómica microbiana han contribuido a comprender mejor la fisiología y ecología de estos organismos y a facilitar la investigación de su la taxonomía y evolución.[383]​\nEstos estudios han revelado niveles de diversidad totalmente inesperados entre los microbios.[384]​[81]​\nParticularmente importante fue el descubrimiento en 1959 en Japón de la transferencia horizontal de genes.[385]​ El intercambio de material genético entre diferentes especies de bacterias ha desempeñado un papel señalado en la propagación de la resistencia a los antibióticos,[386]​ y, al albor de un mejor conocimiento de los genomas, ha surgido la teoría de que la importancia de la transferencia horizontal de material genético en la evolución no se limita a los microorganismos, sino que alcanza a todos los seres vivos.[387]​ La transferencia horizontal de genes en los seres vivos se da principalmente a través de unos vehículos denominados elementos genéticos móviles o elementos genéticos egoístas como los plásmidos, los transposones y los virus. Los virus pueden incorporar material genético de un huésped y llevarlo a otro. Por otra parte, los plásmidos moléculas de ADN circular que portan genes, se pueden intercambiar entre bacterias, arqueas y levaduras que estén cerca, los transposones secuencias de ADN con unos pocos genes, se pueden trasferir horizontalmente entre organismos que vivan en simbiosis. Principalmente se transfieren los genes operativos (los implicados en la limpieza), mientras que los genes informativos (los implicados en la transcripción, traducción y procesos relacionados) rara vez se transfieren horizontalmente.[388]​[389]​[390]​ Según algunas estimaciones alrededor de 145 genes del genoma humano se adquirieron por transferencia horizontal de otros organismos.[391]​ Los altos niveles de transferencia horizontal de genes han llevado a cuestionar el árbol genealógico de los organismos:[32]​[33]​ en efecto, como parte de la teoría endosimbiótica del origen de los orgánulos, la transferencia horizontal de genes fue un paso crítico en la evolución de eucariotas como los hongos, las plantas y los animales.[392]​[393]​"
      },
      {
        "heading": "Endosimbiosis y el origen de las células eucariotas",
        "level": 2,
        "content": "La evolución de los primeros eucariontes desde los procariotas ha recibido una considerable atención por parte de los científicos.[394]​ Este acontecimiento clave en la historia de la vida se produjo entre hace 2500 a comienzos del Paleoproterozoico.[63]​ Existen dos hipótesis no excluyentes mutuamente para explicar el origen de los eucariotas: la endosimbiosis y la autogénesis. La hipótesis o teoría de la endosimbiosis postula que la evolución de las primeras células eucariotas es el resultado de la incorporación permanente en una célula hospedadora arqueana de lo que fueron células procariotas fisiológicamente diferentes y autónomas. De acuerdo con este concepto, las mitocondrias han evolucionado a partir de una antigua alfaproteobacteria aerobia, mientras que los cloroplastos provienen de células procariotas del tipo de las cianobacterias. También es probable que un virus gigante similar a los poxvirus originara el núcleo. Estos procesos simbiogenéticos se habrían iniciado con el contacto de la célula huésped con una bacteria, en una relación que podría ser al principio parasitaria para devenir mutualista con el paso del tiempo al obtener el hospedador ventajas provenientes de características y especialidades del hospedado. De no llegar a este punto, la selección natural penalizaría esta relación, y el número de estos individuos disminuirían en el conjunto de la población; por el contrario, una relación fructífera se vería favorecida por la selección natural y los individuos simbióticos proliferarían, portando parte o el conjunto de los dos genomas originales.[395]​[396]​ Esta hipótesis es la que ha contado con mayor aceptación y ha sido respaldada por los análisis de filogenética molecular, en el que se sugiere que los eucariotas forman un subgrupo dentro de las arqueas.[59]​\nEn contraste, la hipótesis autogénica sostiene que las mitocondrias y los cloroplastos ―así como otros orgánulos eucariotas tales como el retículo endoplasmático― se desarrollaron como consecuencia de las presiones de selección para la especialización fisiológica dentro de una antigua célula procariota. Según esta hipótesis, la membrana de la célula hospedadora se habría invaginado para encapsular porciones fisiológicamente diferentes de la célula ancestral. Con el transcurso del tiempo, estas regiones unidas a la membrana se convirtieron en estructuras cada vez más especializadas hasta conformar los diferentes orgánulos que actualmente definen la célula eucariota.[397]​ No obstante, varias observaciones sobre la estructura de la membrana, el tipo de reproducción, la secuencia de ADN y la susceptibilidad a los antibióticos de los cloroplastos y de las mitocondrias tienden a sustentar la hipótesis simbiogenética.[398]​"
      },
      {
        "heading": "Origen de la reproducción sexual",
        "level": 2,
        "content": "Según la hipótesis fagotrófica, la pérdida de la pared celular que permitió la fagocitosis también podría haber permitido la reproducción sexual muy tempranamente en la historia de la evolución biológica, ya que no se tienen registros de que haya existido eucariontes primitivos asexuales. El citoesqueleto, los motores moleculares y el sistema de endomembranas también facilitan la reproducción sexual. Es así como la reproducción sexual evolucionó hasta el proceso que es hoy, esto es, meiosis seguida de fertilización; y se producen gametos (el producto de la meiosis) en las eucariontes más primitivos que existen en la actualidad, los protistas.\nEntre los protistas, la fusión de células esta relativamente extendida; y la mayoría tienen también fusión nuclear y meiosis, mientras que otros, denominados agámicos, no las tienen, por ejemplo, el alga cercozoo Chlorarachnion y la haptofita Reticulosphaera. A partir de ello se ha postulado que la fusión de células podría haberse desarrollado sin que el objetivo original fuese la reproducción sexual. Por ejemplo, los plasmodios, grandes células multinucledas que se producen entre los mohos mucilaginosos, tienen como objetivo la búsqueda del alimento. Una fusión nuclear accidental o un fallo en la mitosis harían un núcleo poliploide; por ello se cree que la meiosis podría haberse desarrollado como un mecanismo para reparar estos errores; ya que los organismos necesitan replicar su material genético de manera eficiente y fiable. De esta forma, podría haber habido originalmente una fase intermedia con fusión de células y meiosis pero sin fusión nuclear. Es así, como la necesidad de reparar los daños genéticos es una de las teorías principales que explican el origen de la reproducción sexual. Igualmente los individuos diploides pueden reparar una sección mutada de su ADN mediante la recombinación genética, ya que hay dos copias del gen en la célula, y se supone que una de ellas permanece sin daños. Por otro lado, una mutación en un individuo haploide tiene más probabilidad de persistir, ya que la maquinaria de reparación del ADN no tiene manera de saber cuál era la secuencia original sin daños.[399]​ La forma más primitiva de sexo podría haber sido un organismo con ADN dañado replicando una hebra sin dañar de un organismo similar para repararse a sí mismo.[400]​\nOtra teoría es que la reproducción sexual se originó a partir de elementos genéticos móviles parasitarios que intercambian material genético (esto es: copias de su propio genoma) para transmitirse y propagarse, como transposones, plásmidos o virus. En algunos organismos, se ha demostrado que la reproducción sexual mejora la difusión de los elementos genéticos parasitarios (por ejemplo en las levaduras o los hongos filamentosos).[401]​ La conjugación bacteriana, una forma de intercambio genético que algunas fuentes describen como sexo, no es una forma de reproducción. Sin embargo, respalda la teoría del elemento genético móvil, ya que se propaga mediante uno de esos \"genes móviles\", el plásmido F.[400]​ Por otra parte se ha planteado que el ciclo celular moderno, por el cual la mitosis, la meiosis y el sexo ocurren en todos los eucariotas, evolucionó debido al equilibrio alcanzado por los virus, que característicamente siguen un patrón de compensación entre infectar a tantos huéspedes como sea posible y matar a un huésped individual a través de la proliferación viral. Hipotéticamente, los ciclos de replicación viral se pueden reflejar con los de los plásmidos o los transposones.[402]​\nUna tercera teoría menos aceptada, dice que el sexo podría haber evolucionado como una forma de canibalismo. Un organismo primitivo se comió a otro pero, en lugar de digerirlo completamente, parte del ADN del organismo 'comido' se habría incorporado al organismo 'comedor'.[400]​"
      },
      {
        "heading": "Variaciones en la expresión de los genes involucrados en la herencia",
        "level": 2,
        "content": "Hallazgos recientes han confirmado importantes ejemplos de cambios hereditarios que no pueden explicarse por cambios en la secuencia de nucleótidos en el ADN. Estos fenómenos se clasifican como sistemas de herencia epigenética.[403]​ La metilación del ADN que marca la cromatina, los bucles metabólicos autosostenidos, el silenciamiento génico por ARN de interferencia y la conformación tridimensional de proteínas (como los priones) son áreas en las que se han descubierto sistemas de herencia epigenética a nivel del organismo.[404]​ Los biólogos del desarrollo sugieren que las interacciones complejas en las redes genéticas y la comunicación entre las células pueden conducir a variaciones hereditarias que pueden ser la base de algunos de los mecanismos de la plasticidad fenotípica y la canalización del desarrollo.[405]​ La heredabilidad también puede ocurrir a escalas aún mayores. Por ejemplo, la herencia ecológica o extragenética a través del proceso de construcción de nichos se define por las actividades regulares y repetidas de los organismos en su entorno. Esto genera un legado de efectos que modifican y retroalimentan el régimen de selección de generaciones posteriores. Los descendientes heredan genes más las características ambientales generadas por las acciones ecológicas de los antepasados.[406]​ Otros ejemplos de heredabilidad en la evolución que no están bajo el control directo de los genes incluyen la herencia de rasgos culturales y simbiogénesis.[407]​[408]​"
      },
      {
        "heading": "Otras teorías de la evolución y críticas científicas de la teoría sintética",
        "level": 1,
        "content": "Richard Dawkins, en su obra El gen egoísta de 1976, hizo la siguiente afirmación:\n\n \n\nAunque la evolución biológica es aceptada como un hecho desde el siglo XVIII, su explicación científica ha suscitado muchos debates.[410]​ La teoría denominada síntesis evolutiva moderna (o simplemente teoría sintética), es el modelo actualmente aceptado por la comunidad científica para describir los fenómenos evolutivos; y aunque no existe hoy una sólida teoría alternativa desarrollada, científicos como Motō Kimura o Niles Eldredge y Stephen Jay Gould han reclamado la necesidad de realizar una reforma, ampliación o sustitución de la teoría sintética, con nuevos modelos capaces de integrar por ejemplo la biología del desarrollo o incorporar dentro de la teoría actual una serie de descubrimientos biológicos cuyo papel evolutivo se está debatiendo; tales como ciertos mecanismos hereditarios epigenéticos, la transferencia horizontal de genes, o propuestas como la existencia de múltiples niveles jerárquicos de selección o la plausibilidad de los fenómenos de asimilación genómica para explicar procesos macroevolutivos.[411]​\nLos aspectos más criticados y debatidos de la teoría de la síntesis evolutiva moderna son:\nel gradualismo, que ha obtenido como respuesta el modelo del equilibrio puntuado de Eldredge y Gould;[412]​\nla preponderancia de la selección natural frente a los procesos puramente estocásticos;\nla falta de una explicación satisfactoria al comportamiento altruista y\nel reduccionismo geneticista que contradiría las características holísticas y las propiedades emergentes inherentes a cualquier sistema biológico complejo.[413]​\nA pesar de lo indicado, sin embargo, hay que considerar que el actual consenso científico es que la teoría misma (en sus fundamentos) no ha sido rebatida en el campo de la biología evolutiva, siendo solo perfeccionada; y por ello se la sigue considerando como la «piedra angular de la biología moderna».[414]​[415]​"
      },
      {
        "heading": "Otras hipótesis minoritarias",
        "level": 2,
        "content": "Entre otras hipótesis minoritarias, destaca la de la bióloga estadounidense Lynn Margulis, quien consideró que, del mismo modo que las células eucariotas surgieron a través de la interacción simbiogenética de varias células procariotas, muchas otras características de los organismos y el fenómeno de especiación eran la consecuencia de interacciones simbiogenéticas similares. En su obra Captando Genomas. Una teoría sobre el origen de las especies Margulis argumentó que la simbiogénesis es la fuerza principal en la evolución. Según su teoría, la adquisición y acumulación de mutaciones al azar no son suficientes para explicar cómo se producen variaciones hereditarias, y postuló que las organelas, los organismos y las especies surgieron como el resultado de la simbiogénesis.[396]​ Mientras que la síntesis evolutiva moderna hace hincapié en la competencia como la principal fuerza detrás de la evolución, Margulis propuso que la cooperación es motor del cambio evolutivo.[416]​ Argumenta que las bacterias, junto con otros microorganismos, ayudaron a crear las condiciones que se requieren para la vida en la Tierra, tales como una concentración alta de oxígeno. Margulis sostuvo que estos microorganismos son la principal razón por la que las condiciones actuales se mantienen. Afirmó, asimismo, que las bacterias son capaces de intercambiar genes con mayor rapidez y facilidad que los eucariotas, y debido a esto son más versátiles y las artífices de la complejidad de los seres vivos.[396]​\nIgualmente, Máximo Sandín rechazó vehementemente cualquiera de las versiones del darwinismo presentes en la actual teoría y propuso una hipótesis alternativa para explicar la evolución. En primer lugar, justiprecia la obra de Lamarck, y sugiere que las hipótesis o predicciones, conocidas como Lamarckismo, realizadas por este biólogo se ven corroboradas por los hechos.[417]​[418]​ Por ejemplo, Sandín formuló su hipótesis a partir de la observación de que las circunstancias ambientales pueden condicionar, no solo la expresión de la información genética (fenómenos epigenéticos, control del splicing alternativo, estrés genómico…), sino la dinámica del proceso de desarrollo embrionario,[419]​[420]​[421]​[422]​ y postuló que el cimiento fundamental de los ecosistemas es el equilibrio y no la competencia.[423]​[424]​\nConforme a sus ideas, se puede apreciar la tendencia de las formas orgánicas a una mayor complejidad,[425]​[426]​ como consecuencia de unas leyes que gobiernan la variabilidad de los organismos,[427]​[428]​ y que están, de alguna manera, inscritas en los organismos.[429]​[430]​ Habida cuenta de que el 98,5 % del genoma humano, por ejemplo, está compuesto por secuencias repetidas con función reguladora así como una notable cantidad de virus endógenos, Sandín concluye que esa conformación del genoma no puede ser el resultado del azar y de la selección naturaly que se produce en cambio por la presión del medio ambiente, lo que provoca que ciertos virus se inserten en el genoma o determinadas secuencias génicas se modifiquen y, como consecuencia, se generen organismos completamente nuevos, con sustanciales diferencias non respecto a sus predecesores. Según esta teoría, que rechaza la tesis del «ADN egoísta» de Dawkins, el mecanismo fundamental del cambio evolutivo es solo la capacidad de integración de los virus en genomas ya existentes mediante la transferencia horizontal de sus genes. Además, Sandín opina que el medio ambiente, y no las mutaciones aleatorias, es la causa de que determinados grupos de seres vivos asuman muevas características, no de forma gradual, sino en episodios específicos y sin fases intermedias.[431]​[410]​ Según el filósofo Maurício Abdalla, la hipótesis sostenida por Sandín está fundamentada por una gran cantidad de datos científicos y abre una nueva área de investigación en el campo de la biología.[410]​"
      },
      {
        "heading": "Experimentos y estudios sobre el proceso evolutivo",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Observación directa del proceso evolutivo en bacterias",
        "level": 2,
        "content": "Richard Lenski, profesor de la Universidad Estatal de Míchigan, inició en 1989 un experimento para estudiar la evolución de las bacterias, propiciado por la rápida reproducción de estos microorganismos. Lenski estableció varios subcultivos a partir de una cepa de Escherichia coli, con el objetivo de observar si se producía alguna diferencia entre las bacterias originales y sus descendientes. Los diferentes cultivos se mantenían en condiciones estables y cada setenta y cinco días —aproximadamente, unas quinientas generaciones— los investigadores extraían una muestra de cada uno de ellos la congelaban, procediendo del mismo modo para los subcultivos de estos subcultivos. Hacia 2010, el experimento abarcaba unas cincuenta mil generaciones de bacterias.[cita requerida]\nTras diez mil generaciones, las bacterias ya mostraban bastantes diferencias con la cepa ancestral. Las nuevas bacterias eran más grandes y se dividían mucho más rápidamente en el medio de cultivo DM, utilizado para el experimento. El cambio más llamativo consistió en que en uno de los subcultivos en la generación 31 500, las bacterias comenzaron a consumir el citrato presente en el medio DM y que E. coli no es normalmente capaz de metabolizar. Por lo tanto, las bacterias en ese subcultivo evolucionaron para adaptarse y crecer mejor en las condiciones de su ambiente.Otro cambio evolutivo importante se produjo pasadas veinte mil generaciones: las bacterias de un segundo subcultivo experimentaron un cambio en su tasa de mutación, provocando una acumulación de mutaciones en su genoma (fenotipo hipermutable). Al tratarse de un cultivo en medio ambiente constante, la mayor parte de las nuevas mutaciones fueron neutrales, pero también se observó un incremento de las mutaciones beneficiosas en los descendientes de este subcultivo.[432]​\nLos resultados de Lenski propiciaron que se establecieran otros experimentos parecidos, pero con diferentes condiciones de temperatura y fuentes de alimentación, con presencia de antibióticos. También se investigaron diferentes microorganismos: Pseudomonas fluorescens, Myxococcus xanthus, e incluso levaduras. En todos ellos se encontraron resultados parecidos: los microorganismos cambiaban, evolucionaban y se adaptaban a las condiciones del cultivo.[cita requerida]"
      },
      {
        "heading": "Simulación computarizada del proceso de evolución biológica",
        "level": 2,
        "content": "Salvo excepciones, la evolución biológica es un proceso demasiado lento para ser observado directamente. Por ello se recurre a disciplinas como la Paleontología, Biología evolutiva o Filogenia, entre otras áreas, para lo observación y el estudio indirecto de la evolución. Desde la última década del siglo XX, el desarrollo de la bioinformática ha propiciado el uso de herramientas informáticas para el estudio de diversos aspectos del proceso evolutivo.[433]​\nUna de las aplicaciones de las herramientas informáticas al estudio de la evolución, consiste en la simulación in silico del proceso evolutivo, usando organismos digitales, una serie de programas que utilizan los recursos disponibles en el procesador para sobrevivir y reproducirse. Un ejemplo de esta aplicación es el programa Tierra, desarrollado en 1990 por el ecólogo Thomas Ray para el estudio de la evolución y la ecología.[434]​[435]​ Herramientas semejantes se utilizan para investigar la base evolutiva de conductas como el altruismo, la tasa de mutaciones, y la genética de poblaciones.[cita requerida]"
      },
      {
        "heading": "Reacciones sociales y culturales",
        "level": 1,
        "content": "En el siglo XIX, especialmente tras la publicación de El origen de las especies, la idea de que la vida había evolucionado fue un tema de intenso debate académico centrado en las implicaciones filosóficas, sociales y religiosas de la evolución. Hoy en día, el hecho de que los organismos evolucionan es indiscutible en la literatura científica, y la síntesis evolutiva moderna tiene una amplia aceptación entre los científicos.[3]​ Sin embargo, la evolución sigue siendo un concepto controvertido entre los grupos religiosos.[437]​ Los biólogos evolutivos han continuado estudiando varios aspectos de la evolución mediante la formulación de hipótesis, así como la construcción de teorías basadas en evidencia de campo o laboratorio y en datos generados por los métodos de la biología matemática y teórica. Sus descubrimientos han influido no solo en el desarrollo de la biología, sino en muchos otros campos científicos e industriales, incluidos la agricultura, la medicina y la informática.[23]​\nEl progresivo aumento en el conocimiento de los fenómenos evolutivos ha tenido como resultado la revisión, rechazo o, por lo menos, el cuestionamiento de las explicaciones tradicionales creacionistas y fijistas de algunas posturas religiosas y místicas y, de hecho, algunos conceptos, como el de la descendencia de un ancestro común, aún suscitan rechazo en algunas personas,[90]​ que creen que la evolución contradice con el mito de creación en su religión.[438]​[439]​[440]​ Como fuera reconocido por el propio Darwin, las repercusiones más controvertidas de la biología evolutiva conciernen los orígenes del hombre.[129]​ En algunos países ―notoriamente en los Estados Unidos― esta tensión entre la ciencia y la religión ha alimentado la controversia creación–evolución, un conflicto religioso que aún dura, centrado en la política y la educación pública.[441]​ Mientras que otros campos de la ciencia, como la cosmología,[442]​ y las ciencias de la Tierra[443]​ también se contradicen con interpretaciones literales de muchos textos religiosos, la biología evolutiva encuentra una oposición significativamente mayor por parte de muchos creyentes religiosos.[cita requerida]\nEl impacto más importante de la teoría evolutiva se da a nivel de la historia del pensamiento moderno y la relación de este con la sociedad, debido a la naturaleza no teleológica de los mecanismos evolutivos: la evolución no sigue un fin u objetivo. Las estructuras y especies no «aparecen» por necesidad ni por designio divino sino que a partir de la variedad de formas existentes solo las más adaptadas se conservan en el tiempo.[444]​ Este mecanismo «ciego», independiente de un plan, de una voluntad divina o de una fuerza sobrenatural ha sido en consecuencia explorado en otras ramas del saber.[cita requerida]"
      },
      {
        "heading": "Evolución y política",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Eugenesia y fascismo",
        "level": 3,
        "content": "La adopción de la perspectiva evolutiva para abordar problemas en otros campos se ha mostrado enriquecedora y muy vigente; sin embargo en el proceso también se han dado abusos ―atribuir un valor biológico a las diferencias culturales y cognitivas― o deformaciones de la misma ―como justificativo de posturas eugenéticas― las cuales han sido usadas como «Argumentum ad consequentiam» a través de la historia de las objeciones a la teoría de la evolución.[445]​\nPor ejemplo, Francis Galton utilizó argumentos eugenésicos para promover políticas de mejoramiento del patrimonio génico humano, como incentivos para la reproducción de aquellos con «buenos» genes, y la esterilización forzosa, pruebas prenatales, contracepción e, incluso, la eliminación de las personas con «malos» genes.[446]​ Otro ejemplo de este uso de la teoría de la evolución es el darwinismo social, concebida por Herbert Spencer, que popularizó el término de «supervivencia del más apto» y que se utilizó para justificar el sexismo, el imperialismo, la desigualdad social, la superioridad racial (que ayudaron a inspirar la ideología nazi)[447]​[448]​[449]​ como consecuencias inevitables de las leyes naturales.[450]​ En Estados Unidos se usó el darwinismo social para argumentar a favor de restringir la inmigración o para promulgar leyes estatales que requieran esterilización de \"deficientes mentales\".[448]​ Sin embargo, el mismo Darwin rechazó varias de estas ideas,[451]​ y los científicos y filósofos contemporáneos consideran que estas ideas no se hallan implícitas en la teoría evolutiva ni están respaldadas por la información disponible.[452]​[453]​[454]​"
      },
      {
        "heading": "Socialismo y comunismo",
        "level": 3,
        "content": "Por otro lado, los filósofos comunistas Karl Marx y Friedrich Engels vieron en la nueva comprensión biológica de la evolución por selección natural de Darwin como esencial para la nueva interpretación del socialismo científico,[455]​[456]​[457]​ ya que, según Marx, sirve una \"base en las ciencias naturales para la lucha de clases en la historia\".[458]​ El mismo Marx se consideraba como un admirador de Darwin y lo citó varias veces a en sus obras. En El capital, el cual le regaló una copia,[459]​ concluyó que “se debe escribir una historia de la tecnología como la que Darwin ha escrito en el mundo natural sobre la formación de los órganos animales y vegetales”.[456]​ Marx también \"llegó a ver el darwinismo como una ideología burguesa que reflejaba la lucha competitiva burguesa en la sociedad capitalista\".[460]​\nFriedrich Engels consideró \"absolutamente espléndido\" a Darwin.[461]​ Aceptó la teoría de la evolución darwiniana (selección natural), defendiéndola de críticos como Eugen Dühring,[462]​ pero rechazó su método como incompleto. En contra del darwinismo social Engels subraya la armonía dialéctica. Así \"el reino vegetal suministra oxígeno y alimentos al reino animal mientras el reino animal, a su vez, suministra ácido carbónico y estiércol al reino vegetal\".[463]​ Sostuvo en El papel del trabajo en la transformación del mono en hombre que la aplicación de los implementos de trabajo desempeñó un papel decisivo en la diferenciación del hombre respecto del mundo animal.[464]​ Engels también recurrió a la teoría de la evolución social de Lewis H. Morgan en su obra El origen de la familia, la propiedad privada y el estado. Según Alexander Vucinich, \"Engels dio crédito a Marx por extender la teoría de Darwin al estudio de la dinámica interna y el cambio en la sociedad humana\".[465]​ En la década de 1890, el anarquista Piotr Kropotkin publicó El apoyo mutuo como respuesta crítica anarcocomunista al darwinismo social vigente, y en particular al ensayo La lucha por la existencia de Thomas H. Huxley.\nEl conflicto entre compatibilizar la teoría marxista y el darwinismo siguió durante siglo XX. Ludwig Woltmann y August Bebel argumentaron que el darwinismo y el socialismo \"no son mutuamente antagónicos, y que la teoría darwiniana de la supervivencia del más fuerte en la lucha por la existencia encontrará su expresión en el Estado socialista\".[466]​ En Materialismo y empiriocriticismo, Lenin sostuvo la convergencia de las leyes de la evolución de la política económica (Marx), de la biología (Darwin) y de la física (Hermann von Helmholtz). El biólogo evolutivo J. B. S. Haldane, uno de los principales contribuyentes del darwinismo moderno, fue un marxista convencido[467]​ y valoró los aportes de Engels en la biología en Dialética de la naturaleza.[468]​[469]​ Por otro lado, el marxista ortodoxo Karl Kautsky promovió la imagen futura del «hombre nuevo» mediante instrumentos sociales y eugenésicos.[470]​ El lamarckismo se mantuvo en la Unión Soviética etiquetado como \"darwinismo soviético creativo\" hasta 1965.[471]​[472]​ Mientras tanto, por otro lado, el lysenkoismo era oficialmente impulsado como doctrina supuestamente científica (en consonancia con el hecho que 1948, cinco años antes de la muerte de Iósif Stalin, la genética fue oficialmente declarada como una “pseudociencia burguesa”). No obstante, la concepción marxista aplicada a las ciencias naturales y a la evolución fue defendida por el ecólogo Richard Levins y el biólogo Richard Lewontin en el libro El biólogo dialéctico.[473]​\nEn A Darwinian Left, el filósofo Peter Singer sostiene que la visión de la naturaleza humana proporcionada por la ciencia evolutiva, particularmente por la psicología evolutiva, es compatible con el marco ideológico de la izquierda y debe incorporarse a él.[474]​"
      },
      {
        "heading": "Evolución y religión",
        "level": 2,
        "content": "Antes de que la geología se convirtiera en una ciencia, a principios del siglo XIX, tanto las religiones occidentales como los científicos descontaban o condenaban de manera dogmática y casi unánime cualquier propuesta que implicara que la vida fuera el resultado de un proceso evolutivo. Sin embargo, cuando la evidencia geológica empezó a acumularse en todo el mundo, un grupo de científicos comenzó a cuestionar si estos nuevos descubrimientos podían reconciliarse con una interpretación literal de la creación relatada en la Biblia judeocristiana. Algunos geólogos religiosos, como Dean William Auckland en Inglaterra, Edward Hitchcock en Estados Unidos y Hugh Miller en Escocia siguieron explicando la evidencia geológica y fósil solo en términos de un Diluvio universal; pero una vez que Charles Darwin publicara su Origen de las especies en 1859, la opinión científica comenzó a alejarse rápidamente del literalismo bíblico. Este debate temprano acerca de la validez literal de la Biblia se llevó a cabo públicamente, y desestabilizó la opinión educativa en Europa y América, hasta instigar una «contrarreforma» en la forma de un renacimiento religioso en ambos continentes entre 1857 y 1860.[476]​\nEn los países o regiones en las que la mayoría de la población mantiene fuertes creencias religiosas, el creacionismo posee un atractivo mucho mayor que en los países donde la mayoría de la gente posee creencias seculares. Desde los años 1920 hasta el presente en los Estados Unidos, han ocurrido varios ataques religiosos a la enseñanza de la teoría evolutiva, particularmente por parte de cristianos fundamentalistas evangélicos o pentecostales. A pesar de la abrumadora evidencia de la teoría de la evolución,[477]​ algunos grupos consideran verdadera la descripción bíblica de la creación de los seres humanos y de cada una de las otras especies, como especies separadas y acabadas, por un ser divino. Este punto de vista es comúnmente llamado creacionismo, y sigue siendo defendido por algunos grupos integristas religiosos, particularmente los protestantes estadounidenses; principalmente a través de una forma de creacionismo llamada «diseño inteligente».[cita requerida]\nLos oponentes de la evolución obtuvieron una victoria en el \"Scopes Monkey Trial (Juicio del mono Scopes)\" de 1925 cuando la Legislatura de Tennessee aprobó una ley que convierte en delito enseñar \"cualquier teoría que niegue la historia de la Creación Divina del hombre como se enseña en la Biblia\". Mientras tanto, a finales de la década de 1960, la Corte Suprema de Estados Unidos emitió severas restricciones a los gobiernos estatales que se oponían a la enseñanza de la evolución. En 1968, en el juicio Epperson v. Arkansas, el tribunal superior anuló una ley de Arkansas que prohibía la enseñanza de la evolución en las escuelas públicas.[448]​\nLos «lobbies» religiosos-creacionistas desean excluir la enseñanza de la evolución biológica de la educación pública de ese país; aunque actualmente es un fenómeno más bien local, ya que la enseñanza de base en ciencias es obligatoria dentro de los currículos, las encuestas revelan una gran sensibilidad del público estadounidense a este mensaje, lo que no tiene equivalente en ninguna otra parte del mundo. Otro de los episodios más conocidos de este enfrentamiento se produjo en 2005 en el juicio que se realizó en una corte federal de Estados Unidos contra un distrito escolar de Dover, Pensilvania, que exigía la explicación del diseño inteligente como una alternativa a la evolución. En ese año el Consejo de Educación del Estado de Kansas (en inglés: Kansas State Board of Education) decidió permitir que se enseñaran las doctrinas creacionistas como una alternativa de la teoría científica de la evolución.[478]​ Tras esta decisión se produjo una fuerte respuesta ciudadana, que tuvo una de sus consecuencias más conocidas en la creación de una parodia de religión, el pastafarismo, una invención de Bobby Henderson, licenciado en Física de la Universidad Estatal de Oregón, para demostrar irónicamente que no corresponde y es equivocado enseñar el diseño inteligente como teoría científica. En el juicio Kitzmiller contra Dover, el juez John E. Jones III sentenció \"el DI (diseño inteligente) es nada menos que la progenie del creacionismo [...] una visión religiosa, un mero re-etiquetado del creacionismo y no una teoría científica\" y concluyó declarando como \"inconstitucional enseñar DI como una alternativa a la evolución en un aula de ciencias de una escuela pública.\"[479]​ Posteriormente, el Consejo de Educación del Estado de Kansas revocó su decisión en agosto de 2006.[480]​ Este conflicto educativo también ha afectado a otros países; por ejemplo, en el año 2005 en Italia hubo un intento de suspensión de la enseñanza de la teoría de la evolución.[481]​[482]​\nEn respuesta a las pruebas científicas de la teoría de la evolución, muchos religiosos y filósofos han tratado de unificar los puntos de vista científico y religioso, ya sea de manera formal o informal; a través de un «creacionismo proevolución». Así por ejemplo algunos religiosos han adoptado un enfoque creacionista desde la evolución teísta o el creacionismo evolutivo, y defienden que Dios provee una chispa divina que inicia el proceso de la evolución, y (o) donde Dios creó el curso de la evolución.[cita requerida] Entre los científicos cristianos contemporáneos que aceptan la evolución se encuentran el biólogo Kenneth Miller y el genetista Francis Collins.[25]​ El propio Darwin usó de ejemplo al botánico Asa Gray y afirmó:\n\n \n\nA partir de 1950 la Iglesia católica tomó una posición neutral con respecto a la evolución con la encíclica Humani generis del papa Pío XII.[484]​ En ella se distingue entre el alma, tal como fue creada por Dios, y el cuerpo físico, cuyo desarrollo puede ser objeto de un estudio empírico:\n\n \n\nPor otro lado, la encíclica no respalda ni rechaza la creencia general en la evolución debido a que se consideró que la evidencia en aquel momento no era convincente. Permite, sin embargo, la posibilidad de aceptarla en el futuro:\n\n \n\nEn 1996, Juan Pablo II afirmó que «la teoría de la evolución es más que una hipótesis» y recordó que «el Magisterio de la Iglesia está interesado directamente en la cuestión de la evolución, porque influye en la concepción del hombre».[487]​ El papa Benedicto XVI ha afirmado que «existen muchas pruebas científicas en favor de la evolución, que se presenta como una realidad que debemos ver y que enriquece nuestro conocimiento de la vida y del ser como tal. Pero la doctrina de la evolución no responde a todos los interrogantes y sobre todo no responde al gran interrogante filosófico: ¿de dónde viene todo esto y cómo todo toma un camino que desemboca finalmente en el hombre?».[488]​ La otras denominaciones como la Asamblea General de la Iglesia Presbiteriana declararon que «no hay contradicción entre una teoría evolutiva de los orígenes humanos y la doctrina de Dios como Creador». En cuanto en el judaísmo, la Conferencia Central de Rabinos Americanos declaró que «la ignorancia de los estudiantes sobre la evolución socavará seriamente su comprensión del mundo y las leyes naturales que lo gobiernan, y su introducción a otras explicaciones descritas como 'científicas' les dará ideas falsas sobre métodos y criterios científicos».[25]​\nLa reacción musulmana a la teoría de la evolución fue sumamente variada, desde aquellos que creían en una interpretación literal de la creación según el Corán, hasta la de muchos musulmanes educados que suscribieron una versión de evolución teísta o guiada, en la que el Corán reforzaba más que contradecía a la ciencia. Esta última reacción se vio favorecida debido a que Al-Jahiz, un erudito musulmán del siglo IX, había propuesto un concepto similar al de la selección natural.[489]​ Sin embargo, la aceptación de la evolución sigue siendo baja en el mundo musulmán ya que figuras prominentes rechazan la teoría evolutiva como una negación de Dios y como poco fiable para explicar el origen de los seres humanos.[489]​ Otras objeciones de los eruditos y escritores musulmanes reflejan en gran medida las presentadas en el mundo occidental.[490]​ Por otro lado, el debate sobre las ideas de Darwin no generó una controversia significativa en países como China.[491]​\nIndependientemente de su aceptación por las principales jerarquías religiosas, las mismas objeciones iniciales a la teoría de Darwin siguen siendo utilizadas en contra de la teoría evolutiva actual. Las ideas de que las especies cambian con el tiempo a través de procesos naturales y que distintas especies comparten sus ancestros parece contradecir el relato del Génesis de la Creación. Los creyentes en la infalibilidad bíblica atacaron al darwinismo como una herejía. La teología natural del siglo XIX se caracterizó por la analogía del relojero de William Paley, un argumento de diseño todavía utilizado por el movimiento creacionista. Cuando la teoría de Darwin se publicó, las ideas de la evolución teísta se presentaron de modo de indicar que la evolución es una causa secundaria abierta a la investigación científica, al tiempo que mantenían la creencia en Dios como causa primera, con un rol no especificado en la orientación de la evolución y en la creación de los seres humanos.[492]​"
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": "Evolución experimental\nEvolución humana\nEvolución humana reciente\nEvolución convergente\nEvidencia de antepasado común\nSelección natural\nDinámica de sistemas\nTeoría de sistemas\nGran Historia\nEspecie en anillo"
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bibliografía",
        "level": 1,
        "content": "Avise, J. C. (1994): Molecular markers, natural history and evolution. New York, Chapman and Hall. 511 págs. ISBN 0-412-03771-8 [Relato de los descubrimientos evolutivos realizados gracias a los estudios moleculares, con especial atención a los aspectos conservacionistas].\nAyala, F.J. (1994): La teoría de la evolución. De Darwin a los últimos avances de la genética. Ediciones Temas de Hoy, S.A. Colección Fin de Siglo / Serie Mayor, 60. 237 págs. Madrid ISBN 84-7880-457-9\nAyala, F. J. y Valentine, J. W. (1983 [1979]): La evolución en acción. Teoría y procesos de la evolución orgánica. Editorial Alhambra, S.A. Alhambra Universidad. 412 págs. Madrid ISBN 84-205-0981-7\nBuskes, Chris (2008 [2009]): La herencia de Darwin. La evolución en nuestra visión del mundo. Herder Editorial. 541 págs. Barcelona ISBN 978-84-254-2621-6\nCabello, M. y Lope, S. (1987): Evolución. Editorial Alhambra, S.A. Biblioteca de Recursos Didácticos Alhambra, 12. 114 págs. Madrid ISBN 84-205-1535-3\nCarter, G. S. (1958 [1959]): Cien años de evolución. Editorial Taurus, S.A. Ser y Tiempo, 5. 223 págs. Madrid\nCastrodeza, C. (1988): Teoría Histórica de la Selección Natural. Editorial Alhambra, S.A. Exedra, 160. 284 págs. Madrid ISBN 84-205-1740-2\nCrusafont, M., Meléndez, B. y Aguirre, E. (Eds.) (1966): La evolución. La Editorial Católica, S.A. Biblioteca de Autores Cristianos [B.A.C.]. Sección VI (Filosofía), 258. 1014 págs. Madrid ISBN 978-84-220-0676-3 (de la 4.ª ed., 1986).\nRichard Dawkins. (2000): El gen egoísta. Barcelona: Salvat Editores, S.A., 2.ª edición, 407 páginas, ISBN 84-345-0178-3\nDevillers, C. y Chaline, J. (1989 [1993]): La teoría de la evolución. Estado de la cuestión a la luz de los conocimientos científicos actuales. Ediciones Akal, S.A. Ciencia Hoy, 5. 383 págs. Madrid ISBN 84-7600-989-5\nDobzhansky, Th., Ayala, F. J., Stebbins, G. L. y Valentine, J. W. (1977 [1979]): Evolución. Barcelona: Omega. 558 págs. ISBN 84-282-0568-X\nGould, S. J. (2002 [2004]): La estructura de la teoría de la evolución. Barcelona: Tusquets (Metatemas), 82. 1426 págs. ISBN 84-8310-950-6\nLamarck, Jean-Baptiste (1809). Philosophie Zoologique. Paris: Dentu et L'Auteur. OCLC 2210044.  Evolución biológica en Internet Archive\nLimoges, C. (1970 [1976]):La selección natural. Ensayo sobre la primera constitución de un concepto (1837-1859). Siglo Veintiuno Editores, S.A. Ciencia y Técnica. 183 págs. México D.F.\nMaynard Smith, J. (1972 [1979]): Acerca de la Evolución. H. Blume Ediciones. 136 págs. Madrid ISBN 84-7214-182-9\nMilner, R. (1990 [1995]): Diccionario de la evolución. La humanidad a la búsqueda de sus orígenes. Barcelona: Biblograf (Vox). 684 págs. ISBN 84-7153-871-7\nMoya, A. (1989): Sobre la estructura de la teoría de la evolución. Editorial Anthropos, S.A. - Servicio Editorial de la Univ. del País Vasco. Nueva ciencia, 5. 174 págs. ISBN 84-7658-154-8\nTemplado, J. (1974): Historia de las teorías evolucionistas. Editorial Alhambra, S.A. Exedra, 100. 170 págs. Madrid ISBN 84-205-0900-0\nVV. AA. (1978 [1979]): Evolución. Barcelona: Labor (Libros de Investigación y Ciencia). 173 págs. ISBN 84-335-5002-0\nVV. AA. (1982): Charles R. Darwin: La evolución y el origen del hombre. Revista de Occidente, Extraordinario IV, 18-19: 1-235 ISSN 0034-8635"
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": "Wikimedia Commons alberga una categoría multimedia sobre Evolución biológica.\n Wikiquote alberga frases célebres de o sobre Evolución biológica.\n Wikiversidad alberga proyectos de aprendizaje sobre Evolución biológica."
      },
      {
        "heading": "En español",
        "level": 2,
        "content": "«La evolución es un hecho y una teoría», artículo en el sitio web Chile Skeptic.\n«La evolución biológica como un hecho y como una teoría», artículo de Stephen Jay Gould en el sitio web Sin Dioses.\n«La evolución biológica» Archivado el 18 de abril de 2009 en Wayback Machine., artículo de Antonio Barbadilla donde expone los conceptos de evolución y selección natural.\nEvolución y ambiente.\nCollado González, Santiago: «Teoría de la evolución», artículo en el sitio web Philosophica: Enciclopedia filosófica en línea\n«Comprendiendo la evolución»."
      },
      {
        "heading": "En inglés",
        "level": 2,
        "content": "Un estudio para saber de dónde venimos y adónde vamos\n«Evolution». New Scientist. ISSN 0262-4079. Consultado el 30 de mayo de 2011. \n«Evolution Resources from the National Academies». Washington, DC: National Academy of Sciences. Consultado el 30 de mayo de 2011. \n«Understanding Evolution: your one-stop resource for information on evolution». Berkeley, California: University of California, Berkeley. Consultado el 30 de mayo de 2011. \n«Evolution of Evolution – 150 Years of Darwin's 'On the Origin of Species'». Arlington County, Virginia: National Science Foundation. Archivado desde el original el 19 de mayo de 2011. Consultado el 30 de mayo de 2011. \n«Human Evolution Timeline Interactive». Smithsonian Institution, National Museum of Natural History. 28 de enero de 2010. Consultado el 14 de julio de 2018.  Usar Adobe Flash (en inglés)\nExperimentos sobre procesos evolutivos\nLenski, Richard E. «Experimental Evolution». East Lansing, Michigan: Michigan State University. Consultado el 31 de julio de 2013. \nChastain, Erick; Livnat, Adi; Papadimitriou, Christos; Vazirani, Umesh (22 de julio de 2014). «Algorithms, games, and evolution». Proc. Natl. Acad. Sci. U.S.A. 111 (29): 10620-10623. Bibcode:2014PNAS..11110620C. ISSN 0027-8424. PMC 4115542. PMID 24979793. doi:10.1073/pnas.1406556111.  (en inglés)"
      }
    ],
    "summary": "La evolución biológica es el conjunto de cambios en caracteres fenotípicos y genéticos de poblaciones biológicas a través de generaciones.[1]​ Dicho proceso ha originado la diversidad de formas de vida que existen sobre la Tierra a partir de un antepasado común.[2]​[3]​ Los procesos evolutivos han producido la biodiversidad en cada nivel de la organización biológica, incluyendo los de especie, población, organismos individuales y molecular (evolución molecular),[4]​ moldeados por formaciones repetidas de nuevas especies (especiación), cambios dentro de las especies (anagenesis) y desaparición de especies (extinción).[5]​ Los rasgos morfológicos y bioquímicos son más similares entre las especies que comparten un ancestro común más reciente y pueden usarse para reconstruir árboles filogenéticos.[6]​ El registro fósil muestra rápidos momentos de especiación intercalados con periodos relativamente largos de estasis mostrando pocos cambios evolutivos durante la mayor parte de su historia geológica (equilibrio puntuado).[7]​ Toda la vida en la Tierra procede de un último antepasado común universal que existió hace aproximadamente 4200 millones de años.[8]​  \n[9]​[10]​ \nLa palabra «evolución» se utiliza para describir los cambios y fue aplicada por primera vez en el siglo XVIII por un biólogo suizo, Charles Bonnet, en su obra Consideration sur les corps organisés.[11]​[12]​ No obstante, el concepto de que la vida en la Tierra evolucionó a partir de un ancestro común ya había sido formulado por varios filósofos griegos,[13]​ y la hipótesis de que las especies se transforman continuamente fue postulada por numerosos científicos de los siglos XVIII y XIX, a los cuales Charles Darwin citó en el primer capítulo de su libro El origen de las especies.[14]​ Algunos filósofos griegos antiguos contemplaron la posibilidad de cambios en los organismos a través del tiempo. \nLos naturalistas Charles Darwin y Alfred Russel Wallace propusieron de forma independiente en 1858 que la selección natural era el mecanismo básico responsable del origen de nuevas variantes genotípicas y en última instancia, de nuevas especies.[15]​[16]​ Sin embargo, fue el propio Darwin, en El origen de las especies,[17]​ quien sintetizó un cuerpo coherente de observaciones y profundizó el mecanismo de cambio llamado selección natural, lo que consolidó el concepto de la evolución biológica hasta convertirlo en una verdadera teoría científica.[3]​ Anteriormente, el concepto de selección natural ya había sido aportado en el siglo IX por Al-Jahiz (776-868), en su Libro de los animales, con postulados claves sobre la lucha por la supervivencia de las especies, y la herencia de características exitosas mediante reproducción.[18]​[19]​\nDesde la década de 1940 la teoría de la evolución combina las propuestas de Darwin y Wallace con las leyes de Mendel y otros avances posteriores en la genética; por eso se la denomina síntesis moderna o «teoría sintética».[3]​ Según esta teoría, la evolución se define como un cambio en la frecuencia de los alelos de una población a lo largo de las generaciones. Este cambio puede ser causado por diferentes mecanismos, tales como la selección natural, la deriva genética, la mutación y la migración o flujo genético. La teoría sintética recibe en la actualidad una aceptación general de la comunidad científica, aunque también algunas críticas como el hecho de no incorporar el papel que tiene la construcción de nicho y la herencia extragenética. Los avances de otras disciplinas relacionadas, como la biología molecular, la genética del desarrollo o la paleontología han enriquecido la teoría sintética desde su formulación, en torno a 1940.[20]​ \nEn el siglo XIX, la idea de que la vida había evolucionado fue un tema de intenso debate académico centrado en las implicaciones filosóficas, sociales y religiosas de la evolución. La evolución como propiedad inherente a los seres vivos no es materia de debate en la comunidad científica dedicada a su estudio;[3]​ sin embargo, los mecanismos que explican la transformación y diversificación de las especies se hallan bajo intensa y continua investigación científica, surgiendo nuevas hipótesis sobre los mecanismos del cambio evolutivo basadas en datos empíricos tomados de organismos vivos.[21]​[22]​ \nLos biólogos evolutivos han continuado estudiando varios aspectos de la evolución mediante la formulación de hipótesis, así como la construcción de teorías basadas en evidencia de campo o laboratorio y en datos generados por los métodos de la biología matemática y teórica. Sus descubrimientos han influido no solo en el desarrollo de la biología, sino en muchos otros campos científicos e industriales, incluidos la agricultura, la medicina y las ciencias de la computación.[23]​[24]​"
  },
  {
    "title": "Ecology",
    "source": "https://en.wikipedia.org/wiki/Ecology",
    "language": "en",
    "chunks": [
      {
        "heading": "Levels, scope, and scale of organization",
        "level": 1,
        "content": "The scope of ecology contains a wide array of interacting levels of organization spanning micro-level (e.g., cells) to a planetary scale (e.g., biosphere) phenomena. Ecosystems, for example, contain abiotic resources and interacting life forms (i.e., individual organisms that aggregate into populations which aggregate into distinct ecological communities). Because ecosystems are dynamic and do not necessarily follow a linear successional route, changes might occur quickly or slowly over thousands of years before specific forest successional stages are brought about by biological processes.\nAn ecosystem's area can vary greatly, from tiny to vast. A single tree is of little consequence to the classification of a forest ecosystem, but is critically relevant to organisms living in and on it. Several generations of an aphid population can exist over the lifespan of a single leaf. Each of those aphids, in turn, supports diverse bacterial communities. The nature of connections in ecological communities cannot be explained by knowing the details of each species in isolation, because the emergent pattern is neither revealed nor predicted until the ecosystem is studied as an integrated whole. Some ecological principles, however, do exhibit collective properties where the sum of the components explain the properties of the whole, such as birth rates of a population being equal to the sum of individual births over a designated time frame.\nThe main subdisciplines of ecology, population (or community) ecology and ecosystem ecology, exhibit a difference not only in scale but also in two contrasting paradigms in the field. The former focuses on organisms' distribution and abundance, while the latter focuses on materials and energy fluxes."
      },
      {
        "heading": "Hierarchy",
        "level": 2,
        "content": "The scale of ecological dynamics can operate like a closed system, such as aphids migrating on a single tree, while at the same time remaining open about broader scale influences, such as atmosphere or climate. Hence, ecologists classify ecosystems hierarchically by analyzing data collected from finer scale units, such as vegetation associations, climate, and soil types, and integrate this information to identify emergent patterns of uniform organization and processes that operate on local to regional, landscape, and chronological scales.\nTo structure the study of ecology into a conceptually manageable framework, the biological world is organized into a hierarchy, ranging in scale from (as far as ecology is concerned) organisms, to populations, to guilds, to communities, to ecosystems, to biomes, and up to the level of the biosphere. This framework forms a panarchy and exhibits non-linear behaviors; this means that \"effect and cause are disproportionate, so that small changes to critical variables, such as the number of nitrogen fixers, can lead to disproportionate, perhaps irreversible, changes in the system properties.\": 14"
      },
      {
        "heading": "Biodiversity",
        "level": 2,
        "content": "Biodiversity (an abbreviation of \"biological diversity\") describes the diversity of life from genes to ecosystems and spans every level of biological organization. The term has several interpretations, and there are many ways to index, measure, characterize, and represent its complex organization. Biodiversity includes species diversity, ecosystem diversity, and genetic diversity and scientists are interested in the way that this diversity affects the complex ecological processes operating at and among these respective levels. \nBiodiversity plays an important role in ecosystem services which by definition maintain and improve human quality of life. Conservation priorities and management techniques require different approaches and considerations to address the full ecological scope of biodiversity. Natural capital that supports populations is critical for maintaining ecosystem services and species migration (e.g., riverine fish runs and avian insect control) has been implicated as one mechanism by which those service losses are experienced. An understanding of biodiversity has practical applications for species and ecosystem-level conservation planners as they make management recommendations to consulting firms, governments, and industry."
      },
      {
        "heading": "Habitat",
        "level": 2,
        "content": "The habitat of a species describes the environment over which a species is known to occur and the type of community that is formed as a result. More specifically, \"habitats can be defined as regions in environmental space that are composed of multiple dimensions, each representing a biotic or abiotic environmental variable; that is, any component or characteristic of the environment related directly (e.g. forage biomass and quality) or indirectly (e.g. elevation) to the use of a location by the animal.\": 745  For example, a habitat might be an aquatic or terrestrial environment that can be further categorized as a montane or alpine ecosystem. \nHabitat shifts provide important evidence of competition in nature where one population changes relative to the habitats that most other individuals of the species occupy. For example, one population of a species of tropical lizard (Tropidurus hispidus) has a flattened body relative to the main populations that live in open savanna. The population that lives in an isolated rock outcrop hides in crevasses where its flattened body offers a selective advantage. Habitat shifts also occur in the developmental life history of amphibians, and in insects that transition from aquatic to terrestrial habitats. Biotope and habitat are sometimes used interchangeably, but the former applies to a community's environment, whereas the latter applies to a species' environment."
      },
      {
        "heading": "Niche",
        "level": 2,
        "content": "Definitions of the niche date back to 1917, but G. Evelyn Hutchinson made conceptual advances in 1957 by introducing a widely adopted definition: \"the set of biotic and abiotic conditions in which a species is able to persist and maintain stable population sizes.\": 519  The ecological niche is a central concept in the ecology of organisms and is sub-divided into the fundamental and the realized niche. The fundamental niche is the set of environmental conditions under which a species is able to persist. The realized niche is the set of environmental plus ecological conditions under which a species persists. The Hutchinsonian niche is defined more technically as a \"Euclidean hyperspace whose dimensions are defined as environmental variables and whose size is a function of the number of values that the environmental values may assume for which an organism has positive fitness.\": 71 \nBiogeographical patterns and range distributions are explained or predicted through knowledge of a species' traits and niche requirements. Species have functional traits that are uniquely adapted to the ecological niche. A trait is a measurable property, phenotype, or characteristic of an organism that may influence its survival. Genes play an important role in the interplay of development and environmental expression of traits. Resident species evolve traits that are fitted to the selection pressures of their local environment. This tends to afford them a competitive advantage and discourages similarly adapted species from having an overlapping geographic range. The competitive exclusion principle states that two species cannot coexist indefinitely by living off the same limiting resource; one will always out-compete the other. When similarly adapted species overlap geographically, closer inspection reveals subtle ecological differences in their habitat or dietary requirements. Some models and empirical studies, however, suggest that disturbances can stabilize the co-evolution and shared niche occupancy of similar species inhabiting species-rich communities. The habitat plus the niche is called the ecotope, which is defined as the full range of environmental and biological variables affecting an entire species."
      },
      {
        "heading": "Niche construction",
        "level": 2,
        "content": "Organisms are subject to environmental pressures, but they also modify their habitats. The regulatory feedback between organisms and their environment can affect conditions from local (e.g., a beaver pond) to global scales, over time and even after death, such as decaying logs or silica skeleton deposits from marine organisms. The process and concept of ecosystem engineering are related to niche construction, but the former relates only to the physical modifications of the habitat whereas the latter also considers the evolutionary implications of physical changes to the environment and the feedback this causes on the process of natural selection. Ecosystem engineers are defined as: \"organisms that directly or indirectly modulate the availability of resources to other species, by causing physical state changes in biotic or abiotic materials. In so doing they modify, maintain and create habitats.\": 373 \nThe ecosystem engineering concept has stimulated a new appreciation for the influence that organisms have on the ecosystem and evolutionary process. The term \"niche construction\" is more often used in reference to the under-appreciated feedback mechanisms of natural selection imparting forces on the abiotic niche. An example of natural selection through ecosystem engineering occurs in the nests of social insects, including ants, bees, wasps, and termites. There is an emergent homeostasis or homeorhesis in the structure of the nest that regulates, maintains and defends the physiology of the entire colony. Termite mounds, for example, maintain a constant internal temperature through the design of air-conditioning chimneys. The structure of the nests themselves is subject to the forces of natural selection. Moreover, a nest can survive over successive generations, so that progeny inherit both genetic material and a legacy niche that was constructed before their time."
      },
      {
        "heading": "Biome",
        "level": 2,
        "content": "Biomes are larger units of organization that categorize regions of the Earth's ecosystems, mainly according to the structure and composition of vegetation. There are different methods to define the continental boundaries of biomes dominated by different functional types of vegetative communities that are limited in distribution by climate, precipitation, weather, and other environmental variables. Biomes include tropical rainforest, temperate broadleaf and mixed forest, temperate deciduous forest, taiga, tundra, hot desert, and polar desert. Other researchers have recently categorized other biomes, such as the human and oceanic microbiomes. To a microbe, the human body is a habitat and a landscape. Microbiomes were discovered largely through advances in molecular genetics, which have revealed a hidden richness of microbial diversity on the planet. The oceanic microbiome plays a significant role in the ecological biogeochemistry of the planet's oceans."
      },
      {
        "heading": "Biosphere",
        "level": 2,
        "content": "The largest scale of ecological organization is the biosphere: the total sum of ecosystems on the planet. Ecological relationships regulate the flux of energy, nutrients, and climate all the way up to the planetary scale. For example, the dynamic history of the planetary atmosphere's CO2 and O2 composition has been affected by the biogenic flux of gases coming from respiration and photosynthesis, with levels fluctuating over time in relation to the ecology and evolution of plants and animals. Ecological theory has also been used to explain self-emergent regulatory phenomena at the planetary scale: for example, the Gaia hypothesis is an example of holism applied in ecological theory. The Gaia hypothesis states that there is an emergent feedback loop generated by the metabolism of living organisms that maintains the core temperature of the Earth and atmospheric conditions within a narrow self-regulating range of tolerance."
      },
      {
        "heading": "Population ecology",
        "level": 2,
        "content": "Population ecology studies the dynamics of species populations and how these populations interact with the wider environment. A population consists of individuals of the same species that live, interact, and migrate through the same niche and habitat.\nA primary law of population ecology is the Malthusian growth model which states, \"a population will grow (or decline) exponentially as long as the environment experienced by all individuals in the population remains constant.\": 18  Simplified population models usually starts with four variables: death, birth, immigration, and emigration.\nAn example of an introductory population model describes a closed population, such as on an island, where immigration and emigration does not take place. Hypotheses are evaluated with reference to a null hypothesis which states that random processes create the observed data. In these island models, the rate of population change is described by:\n\n  \n    \n      \n        \n          \n            \n              d\n              ⁡\n              N\n              (\n              t\n              )\n            \n            \n              d\n              ⁡\n              t\n            \n          \n        \n        =\n        b\n        N\n        (\n        t\n        )\n        −\n        d\n        N\n        (\n        t\n        )\n        =\n        (\n        b\n        −\n        d\n        )\n        N\n        (\n        t\n        )\n        =\n        r\n        N\n        (\n        t\n        )\n        ,\n      \n    \n    {\\displaystyle {\\frac {\\operatorname {d} N(t)}{\\operatorname {d} t}}=bN(t)-dN(t)=(b-d)N(t)=rN(t),}\n  \n\nwhere N is the total number of individuals in the population, b and d are the per capita rates of birth and death respectively, and r is the per capita rate of population change.\nUsing these modeling techniques, Malthus' population principle of growth was later transformed into a model known as the logistic equation by Pierre Verhulst:\n\n  \n    \n      \n        \n          \n            \n              d\n              ⁡\n              N\n              (\n              t\n              )\n            \n            \n              d\n              ⁡\n              t\n            \n          \n        \n        =\n        r\n        N\n        (\n        t\n        )\n        −\n        α\n        N\n        (\n        t\n        \n          )\n          \n            2\n          \n        \n        =\n        r\n        N\n        (\n        t\n        )\n        \n          (\n          \n            \n              \n                K\n                −\n                N\n                (\n                t\n                )\n              \n              K\n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {\\operatorname {d} N(t)}{\\operatorname {d} t}}=rN(t)-\\alpha N(t)^{2}=rN(t)\\left({\\frac {K-N(t)}{K}}\\right),}\n  \n\nwhere N(t) is the number of individuals measured as biomass density as a function of time, t, r is the maximum per-capita rate of change commonly known as the intrinsic rate of growth, and \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n is the crowding coefficient, which represents the reduction in population growth rate per individual added. The formula states that the rate of change in population size (\n  \n    \n      \n        \n          d\n        \n        N\n        (\n        t\n        )\n        \n          /\n        \n        \n          d\n        \n        t\n      \n    \n    {\\displaystyle \\mathrm {d} N(t)/\\mathrm {d} t}\n  \n) will grow to approach equilibrium, where (\n  \n    \n      \n        \n          d\n        \n        N\n        (\n        t\n        )\n        \n          /\n        \n        \n          d\n        \n        t\n        =\n        0\n      \n    \n    {\\displaystyle \\mathrm {d} N(t)/\\mathrm {d} t=0}\n  \n), when the rates of increase and crowding are balanced, \n  \n    \n      \n        r\n        \n          /\n        \n        α\n      \n    \n    {\\displaystyle r/\\alpha }\n  \n. A common, analogous model fixes the equilibrium, \n  \n    \n      \n        r\n        \n          /\n        \n        α\n      \n    \n    {\\displaystyle r/\\alpha }\n  \n as K, which is known as the \"carrying capacity.\"\nPopulation ecology builds upon these introductory models to further understand demographic processes in real study populations. Commonly used types of data include life history, fecundity, and survivorship, and these are analyzed using mathematical techniques such as matrix algebra. The information is used for managing wildlife stocks and setting harvest quotas. In cases where basic models are insufficient, ecologists may adopt different kinds of statistical methods, such as the Akaike information criterion, or use models that can become mathematically complex as \"several competing hypotheses are simultaneously confronted with the data.\""
      },
      {
        "heading": "Metapopulations and migration",
        "level": 2,
        "content": "The concept of metapopulations was defined in 1969 as \"a population of populations which go extinct locally and recolonize\".: 105  Metapopulation ecology is another statistical approach that is often used in conservation research. Metapopulation models simplify the landscape into patches of varying levels of quality, and metapopulations are linked by the migratory behaviours of organisms. Animal migration is set apart from other kinds of movement because it involves the seasonal departure and return of individuals from a habitat. Migration is also a population-level phenomenon, as with the migration routes followed by plants as they occupied northern post-glacial environments. Plant ecologists use pollen records that accumulate and stratify in wetlands to reconstruct the timing of plant migration and dispersal relative to historic and contemporary climates. These migration routes involved an expansion of the range as plant populations expanded from one area to another. There is a larger taxonomy of movement, such as commuting, foraging, territorial behavior, stasis, and ranging. Dispersal is usually distinguished from migration because it involves the one-way permanent movement of individuals from their birth population into another population.\nIn metapopulation terminology, migrating individuals are classed as emigrants (when they leave a region) or immigrants (when they enter a region), and sites are classed either as sources or sinks. A site is a generic term that refers to places where ecologists sample populations, such as ponds or defined sampling areas in a forest. Source patches are productive sites that generate a seasonal supply of juveniles that migrate to other patch locations. Sink patches are unproductive sites that only receive migrants; the population at the site will disappear unless rescued by an adjacent source patch or environmental conditions become more favorable. Metapopulation models examine patch dynamics over time to answer potential questions about spatial and demographic ecology. The ecology of metapopulations is a dynamic process of extinction and colonization. Small patches of lower quality (i.e., sinks) are maintained or rescued by a seasonal influx of new immigrants. A dynamic metapopulation structure evolves from year to year, where some patches are sinks in dry years and are sources when conditions are more favorable. Ecologists use a mixture of computer models and field studies to explain metapopulation structure."
      },
      {
        "heading": "Community ecology",
        "level": 2,
        "content": "Community ecology is the study of the interactions among a collection of species that inhabit the same geographic area. Community ecologists study the determinants of patterns and processes for two or more interacting species. Research in community ecology might measure species diversity in grasslands in relation to soil fertility. It might also include the analysis of predator-prey dynamics, competition among similar plant species, or mutualistic interactions between crabs and corals."
      },
      {
        "heading": "Ecosystem ecology",
        "level": 2,
        "content": "Ecosystems may be habitats within biomes that form an integrated whole and a dynamically responsive system having both physical and biological complexes. Ecosystem ecology is the science of determining the fluxes of materials (e.g. carbon, phosphorus) between different pools (e.g., tree biomass, soil organic material). Ecosystem ecologists attempt to determine the underlying causes of these fluxes. Research in ecosystem ecology might measure primary production (g C/m^2) in a wetland in relation to decomposition and consumption rates (g C/m^2/y). This requires an understanding of the community connections between plants (i.e., primary producers) and the decomposers (e.g., fungi and bacteria).\nThe underlying concept of an ecosystem can be traced back to 1864 in the published work of George Perkins Marsh (\"Man and Nature\"). Within an ecosystem, organisms are linked to the physical and biological components of their environment to which they are adapted. Ecosystems are complex adaptive systems where the interaction of life processes form self-organizing patterns across different scales of time and space. Ecosystems are broadly categorized as terrestrial, freshwater, atmospheric, or marine. Differences stem from the nature of the unique physical environments that shapes the biodiversity within each. A more recent addition to ecosystem ecology are technoecosystems, which are affected by or primarily the result of human activity."
      },
      {
        "heading": "Food webs",
        "level": 2,
        "content": "A food web is the archetypal ecological network. Plants capture solar energy and use it to synthesize simple sugars during photosynthesis. As plants grow, they accumulate nutrients and are eaten by grazing herbivores, and the energy is transferred through a chain of organisms by consumption. The simplified linear feeding pathways that move from a basal trophic species to a top consumer is called the food chain. Food chains in an ecological community create a complex food web. Food webs are a type of concept map that is used to illustrate and study pathways of energy and material flows.\nEmpirical measurements are generally restricted to a specific habitat, such as a cave or a pond, and principles gleaned from small-scale studies are extrapolated to larger systems. Feeding relations require extensive investigations, e.g. into the gut contents of organisms, which can be difficult to decipher, or stable isotopes can be used to trace the flow of nutrient diets and energy through a food web. Despite these limitations, food webs remain a valuable tool in understanding community ecosystems.\nFood webs illustrate important principles of ecology: some species have many weak feeding links (e.g., omnivores) while some are more specialized with fewer stronger feeding links (e.g., primary predators). Such linkages explain how ecological communities remain stable over time and eventually can illustrate a \"complete\" web of life.\nThe disruption of food webs may have a dramatic impact on the ecology of individual species or whole ecosystems. For instance, the replacement of an ant species by another (invasive) ant species has been shown to affect how elephants reduce tree cover and thus the predation of lions on zebras."
      },
      {
        "heading": "Trophic levels",
        "level": 2,
        "content": "A trophic level (from Greek troph, τροφή, trophē, meaning \"food\" or \"feeding\") is \"a group of organisms acquiring a considerable majority of its energy from the lower adjacent level (according to ecological pyramids) nearer the abiotic source.\": 383  Links in food webs primarily connect feeding relations or trophism among species. Biodiversity within ecosystems can be organized into trophic pyramids, in which the vertical dimension represents feeding relations that become further removed from the base of the food chain up toward top predators, and the horizontal dimension represents the abundance or biomass at each level. When the relative abundance or biomass of each species is sorted into its respective trophic level, they naturally sort into a 'pyramid of numbers'.\nSpecies are broadly categorized as autotrophs (or primary producers), heterotrophs (or consumers), and Detritivores (or decomposers). Autotrophs are organisms that produce their own food (production is greater than respiration) by photosynthesis or chemosynthesis. Heterotrophs are organisms that must feed on others for nourishment and energy (respiration exceeds production). Heterotrophs can be further sub-divided into different functional groups, including primary consumers (strict herbivores), secondary consumers (carnivorous predators that feed exclusively on herbivores), and tertiary consumers (predators that feed on a mix of herbivores and predators). Omnivores do not fit neatly into a functional category because they eat both plant and animal tissues. It has been suggested that omnivores have a greater functional influence as predators because compared to herbivores, they are relatively inefficient at grazing.\nTrophic levels are part of the holistic or complex systems view of ecosystems. Each trophic level contains unrelated species that are grouped together because they share common ecological functions, giving a macroscopic view of the system. While the notion of trophic levels provides insight into energy flow and top-down control within food webs, it is troubled by the prevalence of omnivory in real ecosystems. This has led some ecologists to \"reiterate that the notion that species clearly aggregate into discrete, homogeneous trophic levels is fiction.\": 815  Nonetheless, recent studies have shown that real trophic levels do exist, but \"above the herbivore trophic level, food webs are better characterized as a tangled web of omnivores.\": 612"
      },
      {
        "heading": "Keystone species",
        "level": 2,
        "content": "A keystone species is a species that is connected to a disproportionately large number of other species in the food-web. Keystone species have lower levels of biomass in the trophic pyramid relative to the importance of their role. The many connections that a keystone species holds means that it maintains the organization and structure of entire communities. The loss of a keystone species results in a range of dramatic cascading effects (termed trophic cascades) that alters trophic dynamics, other food web connections, and can cause the extinction of other species. The term keystone species was coined by Robert Paine in 1969 and is a reference to the keystone architectural feature as the removal of a keystone species can result in a community collapse just as the removal of the keystone in an arch can result in the arch's loss of stability.\nSea otters (Enhydra lutris) are commonly cited as an example of a keystone species because they limit the density of sea urchins that feed on kelp. If sea otters are removed from the system, the urchins graze until the kelp beds disappear, and this has a dramatic effect on community structure. Hunting of sea otters, for example, is thought to have led indirectly to the extinction of the Steller's sea cow (Hydrodamalis gigas). While the keystone species concept has been used extensively as a conservation tool, it has been criticized for being poorly defined from an operational stance. It is difficult to experimentally determine what species may hold a keystone role in each ecosystem. Furthermore, food web theory suggests that keystone species may not be common, so it is unclear how generally the keystone species model can be applied."
      },
      {
        "heading": "Complexity",
        "level": 1,
        "content": "Complexity is understood as a large computational effort needed to piece together numerous interacting parts exceeding the iterative memory capacity of the human mind. Global patterns of biological diversity are complex. This biocomplexity stems from the interplay among ecological processes that operate and influence patterns at different scales that grade into each other, such as transitional areas or ecotones spanning landscapes. Complexity stems from the interplay among levels of biological organization as energy, and matter is integrated into larger units that superimpose onto the smaller parts. \"What were wholes on one level become parts on a higher one.\": 209  Small scale patterns do not necessarily explain large scale phenomena, otherwise captured in the expression (coined by Aristotle) 'the sum is greater than the parts'.\n\"Complexity in ecology is of at least six distinct types: spatial, temporal, structural, process, behavioral, and geometric.\": 3  From these principles, ecologists have identified emergent and self-organizing phenomena that operate at different environmental scales of influence, ranging from molecular to planetary, and these require different explanations at each integrative level. Ecological complexity relates to the dynamic resilience of ecosystems that transition to multiple shifting steady-states directed by random fluctuations of history. Long-term ecological studies provide important track records to better understand the complexity and resilience of ecosystems over longer temporal and broader spatial scales. These studies are managed by the International Long Term Ecological Network (LTER). The longest experiment in existence is the Park Grass Experiment, which was initiated in 1856. Another example is the Hubbard Brook study, which has been in operation since 1960."
      },
      {
        "heading": "Holism",
        "level": 2,
        "content": "Holism remains a critical part of the theoretical foundation in contemporary ecological studies. Holism addresses the biological organization of life that self-organizes into layers of emergent whole systems that function according to non-reducible properties. This means that higher-order patterns of a whole functional system, such as an ecosystem, cannot be predicted or understood by a simple summation of the parts. \"New properties emerge because the components interact, not because the basic nature of the components is changed.\": 8 \nEcological studies are necessarily holistic as opposed to reductionistic. Holism has three scientific meanings or uses that identify with ecology: 1) the mechanistic complexity of ecosystems, 2) the practical description of patterns in quantitative reductionist terms where correlations may be identified but nothing is understood about the causal relations without reference to the whole system, which leads to 3) a metaphysical hierarchy whereby the causal relations of larger systems are understood without reference to the smaller parts. Scientific holism differs from mysticism that has appropriated the same term. An example of metaphysical holism is identified in the trend of increased exterior thickness in shells of different species. The reason for a thickness increase can be understood through reference to principles of natural selection via predation without the need to reference or understand the biomolecular properties of the exterior shells."
      },
      {
        "heading": "Relation to evolution",
        "level": 1,
        "content": "Ecology and evolutionary biology are considered sister disciplines of the life sciences. Natural selection, life history, development, adaptation, populations, and inheritance are examples of concepts that thread equally into ecological and evolutionary theory. Morphological, behavioural, and genetic traits, for example, can be mapped onto evolutionary trees to study the historical development of a species in relation to their functions and roles in different ecological circumstances. In this framework, the analytical tools of ecologists and evolutionists overlap as they organize, classify, and investigate life through common systematic principles, such as phylogenetics or the Linnaean system of taxonomy. The two disciplines often appear together, such as in the title of the journal Trends in Ecology and Evolution. There is no sharp boundary separating ecology from evolution, and they differ more in their areas of applied focus. Both disciplines discover and explain emergent and unique properties and processes operating across different spatial or temporal scales of organization. While the boundary between ecology and evolution is not always clear, ecologists study the abiotic and biotic factors that influence evolutionary processes, and evolution can be rapid, occurring on ecological timescales as short as one generation."
      },
      {
        "heading": "Behavioural ecology",
        "level": 2,
        "content": "All organisms can exhibit behaviours. Even plants express complex behaviour, including memory and communication. Behavioural ecology is the study of an organism's behaviour in its environment and its ecological and evolutionary implications. Ethology is the study of observable movement or behaviour in animals. This could include investigations of motile sperm of plants, mobile phytoplankton, zooplankton swimming toward the female egg, the cultivation of fungi by weevils, the mating dance of a salamander, or social gatherings of amoeba.\nAdaptation is the central unifying concept in behavioural ecology. Behaviours can be recorded as traits and inherited in much the same way that eye and hair colour can. Behaviours can evolve by means of natural selection as adaptive traits conferring functional utilities that increases reproductive fitness.\n\nPredator-prey interactions are an introductory concept into food-web studies as well as behavioural ecology. Prey species can exhibit different kinds of behavioural adaptations to predators, such as avoid, flee, or defend. Many prey species are faced with multiple predators that differ in the degree of danger posed. To be adapted to their environment and face predatory threats, organisms must balance their energy budgets as they invest in different aspects of their life history, such as growth, feeding, mating, socializing, or modifying their habitat. \nHypotheses posited in behavioural ecology are generally based on adaptive principles of conservation, optimization, or efficiency. For example, \"[t]he threat-sensitive predator avoidance hypothesis predicts that prey should assess the degree of threat posed by different predators and match their behaviour according to current levels of risk\" or \"[t]he optimal flight initiation distance occurs where expected postencounter fitness is maximized, which depends on the prey's initial fitness, benefits obtainable by not fleeing, energetic escape costs, and expected fitness loss due to predation risk.\"\nElaborate sexual displays and posturing are encountered in the behavioural ecology of animals. The birds-of-paradise, for example, sing and display elaborate ornaments during courtship. These displays serve a dual purpose of signalling healthy or well-adapted individuals and desirable genes. The displays are driven by sexual selection as an advertisement of quality of traits among suitors."
      },
      {
        "heading": "Cognitive ecology",
        "level": 2,
        "content": "Cognitive ecology integrates theory and observations from evolutionary ecology and neurobiology, primarily cognitive science, in order to understand the effect that animal interaction with their habitat has on their cognitive systems and how those systems restrict behavior within an ecological and evolutionary framework. \"Until recently, however, cognitive scientists have not paid sufficient attention to the fundamental fact that cognitive traits evolved under particular natural settings. With consideration of the selection pressure on cognition, cognitive ecology can contribute intellectual coherence to the multidisciplinary study of cognition.\" As a study involving the 'coupling' or interactions between organism and environment, cognitive ecology is closely related to enactivism, a field based upon the view that \"...we must see the organism and environment as bound together in reciprocal specification and selection...\"."
      },
      {
        "heading": "Social ecology",
        "level": 2,
        "content": "Social-ecological behaviours are notable in the social insects, slime moulds, social spiders, human society, and naked mole-rats where eusocialism has evolved. Social behaviours include reciprocally beneficial behaviours among kin and nest mates and evolve from kin and group selection. Kin selection explains altruism through genetic relationships, whereby an altruistic behaviour leading to death is rewarded by the survival of genetic copies distributed among surviving relatives. The social insects, including ants, bees, and wasps are most famously studied for this type of relationship because the male drones are clones that share the same genetic make-up as every other male in the colony. In contrast, group selectionists find examples of altruism among non-genetic relatives and explain this through selection acting on the group; whereby, it becomes selectively advantageous for groups if their members express altruistic behaviours to one another. Groups with predominantly altruistic members survive better than groups with predominantly selfish members."
      },
      {
        "heading": "Coevolution",
        "level": 2,
        "content": "Ecological interactions can be classified broadly into a host and an associate relationship. A host is any entity that harbours another that is called the associate. Relationships between species that are mutually or reciprocally beneficial are called mutualisms. Examples of mutualism include fungus-growing ants employing agricultural symbiosis, bacteria living in the guts of insects and other organisms, the fig wasp and yucca moth pollination complex, lichens with fungi and photosynthetic algae, and corals with photosynthetic algae. If there is a physical connection between host and associate, the relationship is called symbiosis. Approximately 60% of all plants, for example, have a symbiotic relationship with arbuscular mycorrhizal fungi living in their roots forming an exchange network of carbohydrates for mineral nutrients.\nIndirect mutualisms occur where the organisms live apart. For example, trees living in the equatorial regions of the planet supply oxygen into the atmosphere that sustains species living in distant polar regions of the planet. This relationship is called commensalism because many others receive the benefits of clean air at no cost or harm to trees supplying the oxygen. If the associate benefits while the host suffers, the relationship is called parasitism. Although parasites impose a cost to their host (e.g., via damage to their reproductive organs or propagules, denying the services of a beneficial partner), their net effect on host fitness is not necessarily negative and, thus, becomes difficult to forecast. Co-evolution is also driven by competition among species or among members of the same species under the banner of reciprocal antagonism, such as grasses competing for growth space. The Red Queen Hypothesis, for example, posits that parasites track down and specialize on the locally common genetic defense systems of its host that drives the evolution of sexual reproduction to diversify the genetic constituency of populations responding to the antagonistic pressure."
      },
      {
        "heading": "Biogeography",
        "level": 2,
        "content": "Biogeography (an amalgamation of biology and geography) is the comparative study of the geographic distribution of organisms and the corresponding evolution of their traits in space and time. The Journal of Biogeography was established in 1974. Biogeography and ecology share many of their disciplinary roots. For example, the theory of island biogeography, published by the Robert MacArthur and Edward O. Wilson in 1967 is considered one of the fundamentals of ecological theory.\nBiogeography has a long history in the natural sciences concerning the spatial distribution of plants and animals. Ecology and evolution provide the explanatory context for biogeographical studies. Biogeographical patterns result from ecological processes that influence range distributions, such as migration and dispersal. and from historical processes that split populations or species into different areas. The biogeographic processes that result in the natural splitting of species explain much of the modern distribution of the Earth's biota. The splitting of lineages in a species is called vicariance biogeography and it is a sub-discipline of biogeography. There are also practical applications in the field of biogeography concerning ecological systems and processes. For example, the range and distribution of biodiversity and invasive species responding to climate change is a serious concern and active area of research in the context of global warming."
      },
      {
        "heading": "r/K selection theory",
        "level": 3,
        "content": "A population ecology concept is r/K selection theory, one of the first predictive models in ecology used to explain life-history evolution. The premise behind the r/K selection model is that natural selection pressures change according to population density. For example, when an island is first colonized, density of individuals is low. The initial increase in population size is not limited by competition, leaving an abundance of available resources for rapid population growth. These early phases of population growth experience density-independent forces of natural selection, which is called r-selection. As the population becomes more crowded, it approaches the island's carrying capacity, thus forcing individuals to compete more heavily for fewer available resources. Under crowded conditions, the population experiences density-dependent forces of natural selection, called K-selection.\nIn the r/K-selection model, the first variable r is the intrinsic rate of natural increase in population size and the second variable K is the carrying capacity of a population. Different species evolve different life-history strategies spanning a continuum between these two selective forces. An r-selected species is one that has high birth rates, low levels of parental investment, and high rates of mortality before individuals reach maturity. Evolution favours high rates of fecundity in r-selected species. Many kinds of insects and invasive species exhibit r-selected characteristics. In contrast, a K-selected species has low rates of fecundity, high levels of parental investment in the young, and low rates of mortality as individuals mature. Humans and elephants are examples of species exhibiting K-selected characteristics, including longevity and efficiency in the conversion of more resources into fewer offspring."
      },
      {
        "heading": "Molecular ecology",
        "level": 2,
        "content": "The important relationship between ecology and genetic inheritance predates modern techniques for molecular analysis. Molecular ecological research became more feasible with the development of rapid and accessible genetic technologies, such as the polymerase chain reaction (PCR). The rise of molecular technologies and the influx of research questions into this new ecological field resulted in the publication Molecular Ecology in 1992. Molecular ecology uses various analytical techniques to study genes in an evolutionary and ecological context. In 1994, John Avise also played a leading role in this area of science with the publication of his book, Molecular Markers, Natural History and Evolution. \nNewer technologies opened a wave of genetic analysis into organisms once difficult to study from an ecological or evolutionary standpoint, such as bacteria, fungi, and nematodes. Molecular ecology engendered a new research paradigm for investigating ecological questions considered otherwise intractable. Molecular investigations revealed previously obscured details in the tiny intricacies of nature and improved resolution into probing questions about behavioural and biogeographical ecology. For example, molecular ecology revealed promiscuous sexual behaviour and multiple male partners in tree swallows previously thought to be socially monogamous. In a biogeographical context, the marriage between genetics, ecology, and evolution resulted in a new sub-discipline called phylogeography."
      },
      {
        "heading": "Human ecology",
        "level": 1,
        "content": "Ecology is as much a biological science as it is a human science. Human ecology is an interdisciplinary investigation into the ecology of our species. \"Human ecology may be defined: (1) from a bioecological standpoint as the study of man as the ecological dominant in plant and animal communities and systems; (2) from a bioecological standpoint as simply another animal affecting and being affected by his physical environment; and (3) as a human being, somehow different from animal life in general, interacting with physical and modified environments in a distinctive and creative way. A truly interdisciplinary human ecology will most likely address itself to all three.\": 3  The term was formally introduced in 1921, but many sociologists, geographers, psychologists, and other disciplines were interested in human relations to natural systems centuries prior, especially in the late 19th century.\nThe ecological complexities human beings are facing through the technological transformation of the planetary biome has brought on the Anthropocene. The unique set of circumstances has generated the need for a new unifying science called coupled human and natural systems that builds upon, but moves beyond the field of human ecology. Ecosystems tie into human societies through the critical and all-encompassing life-supporting functions they sustain. In recognition of these functions and the incapability of traditional economic valuation methods to see the value in ecosystems, there has been a surge of interest in social-natural capital, which provides the means to put a value on the stock and use of information and materials stemming from ecosystem goods and services. Ecosystems produce, regulate, maintain, and supply services of critical necessity and beneficial to human health (cognitive and physiological), economies, and they even provide an information or reference function as a living library giving opportunities for science and cognitive development in children engaged in the complexity of the natural world. Ecosystems relate importantly to human ecology as they are the ultimate base foundation of global economics as every commodity, and the capacity for exchange ultimately stems from the ecosystems on Earth."
      },
      {
        "heading": "Restoration Ecology",
        "level": 2,
        "content": "Ecology is an employed science of restoration, repairing disturbed sites through human intervention, in natural resource management, and in environmental impact assessments. Edward O. Wilson predicted in 1992 that the 21st century \"will be the era of restoration in ecology\". Ecological science has boomed in the industrial investment of restoring ecosystems and their processes in abandoned sites after disturbance. Natural resource managers, in forestry, for example, employ ecologists to develop, adapt, and implement ecosystem based methods into the planning, operation, and restoration phases of land-use. \nAnother example of conservation is seen on the east coast of the United States in Boston, MA. The city of Boston implemented the Wetland Ordinance, improving the stability of their wetland environments by implementing soil amendments that will improve groundwater storage and flow, and trimming or removal of vegetation that could cause harm to water quality. Ecological science is used in the methods of sustainable harvesting, disease, and fire outbreak management, in fisheries stock management, for integrating land-use with protected areas and communities, and conservation in complex geo-political landscapes."
      },
      {
        "heading": "Relation to the environment",
        "level": 1,
        "content": "The environment of ecosystems includes both physical parameters and biotic attributes. It is dynamically interlinked and contains resources for organisms at any time throughout their life cycle. Like ecology, the term environment has different conceptual meanings and overlaps with the concept of nature. Environment \"includes the physical world, the social world of human relations and the built world of human creation.\": 62  The physical environment is external to the level of biological organization under investigation, including abiotic factors such as temperature, radiation, light, chemistry, climate and geology. The biotic environment includes genes, cells, organisms, members of the same species (conspecifics) and other species that share a habitat.\nThe distinction between external and internal environments, however, is an abstraction parsing life and environment into units or facts that are inseparable in reality. There is an interpenetration of cause and effect between the environment and life. The laws of thermodynamics, for example, apply to ecology by means of its physical state. With an understanding of metabolic and thermodynamic principles, a complete accounting of energy and material flow can be traced through an ecosystem. In this way, the environmental and ecological relations are studied through reference to conceptually manageable and isolated material parts. After the effective environmental components are understood through reference to their causes; however, they conceptually link back together as an integrated whole, or holocoenotic system as it was once called. This is known as the dialectical approach to ecology. The dialectical approach examines the parts but integrates the organism and the environment into a dynamic whole (or umwelt). Change in one ecological or environmental factor can concurrently affect the dynamic state of an entire ecosystem."
      },
      {
        "heading": "Disturbance and resilience",
        "level": 2,
        "content": "A disturbance is any process that changes or removes biomass from a community, such as a fire, flood, drought, or predation. Disturbances are both the cause and product of natural fluctuations within an ecological community. Biodiversity can protect ecosystems from disturbances.\nThe effect of a disturbance is often hard to predict, but there are numerous examples in which a single species can massively disturb an ecosystem. For example, a single-celled protozoan has been able to kill up to 100% of sea urchins in some coral reefs in the Red Sea and Western Indian Ocean. Sea urchins enable complex reef ecosystems to thrive by eating algae that would otherwise inhibit coral growth. Similarly, invasive species can wreak havoc on ecosystems. For instance, invasive Burmese pythons have caused a 98% decline of small mammals in the Everglades."
      },
      {
        "heading": "Metabolism and the early atmosphere",
        "level": 2,
        "content": "The Earth was formed approximately 4.5 billion years ago. As it cooled and a crust and oceans formed, its atmosphere transformed from being dominated by hydrogen to one composed mostly of methane and ammonia. Over the next billion years, the metabolic activity of life transformed the atmosphere into a mixture of carbon dioxide, nitrogen, and water vapor. These gases changed the way that light from the sun hit the Earth's surface and greenhouse effects trapped heat. There were untapped sources of free energy within the mixture of reducing and oxidizing gasses that set the stage for primitive ecosystems to evolve and, in turn, the atmosphere also evolved.\n\nThroughout history, the Earth's atmosphere and biogeochemical cycles have been in a dynamic equilibrium with planetary ecosystems. The history is characterized by periods of significant transformation followed by millions of years of stability. The evolution of the earliest organisms, likely anaerobic methanogen microbes, started the process by converting atmospheric hydrogen into methane (4H2 + CO2 → CH4 + 2H2O). Anoxygenic photosynthesis reduced hydrogen concentrations and increased atmospheric methane, by converting hydrogen sulfide into water or other sulfur compounds (for example, 2H2S + CO2 + hv → CH2O + H2O + 2S). Early forms of fermentation also increased levels of atmospheric methane. The transition to an oxygen-dominant atmosphere (the Great Oxidation) did not begin until approximately 2.4–2.3 billion years ago, but photosynthetic processes started 0.3–1 billion years prior."
      },
      {
        "heading": "Radiation: heat, temperature and light",
        "level": 2,
        "content": "The biology of life operates within a certain range of temperatures. Heat is a form of energy that regulates temperature. Heat affects growth rates, activity, behaviour, and primary production. Temperature is largely dependent on the incidence of solar radiation. The latitudinal and longitudinal spatial variation of temperature greatly affects climates and consequently the distribution of biodiversity and levels of primary production in different ecosystems or biomes across the planet. Heat and temperature relate importantly to metabolic activity. Poikilotherms, for example, have a body temperature that is largely regulated and dependent on the temperature of the external environment. In contrast, homeotherms regulate their internal body temperature by expending metabolic energy.\nThere is a relationship between light, primary production, and ecological energy budgets. Sunlight is the primary input of energy into the planet's ecosystems. Light is composed of electromagnetic energy of different wavelengths. Radiant energy from the sun generates heat, provides photons of light measured as active energy in the chemical reactions of life, and also acts as a catalyst for genetic mutation. Plants, algae, and some bacteria absorb light and assimilate the energy through photosynthesis. Organisms capable of assimilating energy by photosynthesis or through inorganic fixation of H2S are autotrophs. Autotrophs—responsible for primary production—assimilate light energy which becomes metabolically stored as potential energy in the form of biochemical enthalpic bonds."
      },
      {
        "heading": "Physical environments",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Water",
        "level": 3,
        "content": "Diffusion of carbon dioxide and oxygen is approximately 10,000 times slower in water than in air. When soils are flooded, they quickly lose oxygen, becoming hypoxic (an environment with O2 concentration below 2 mg/liter) and eventually completely anoxic where anaerobic bacteria thrive among the roots. Water also influences the intensity and spectral composition of light as it reflects off the water surface and submerged particles. Aquatic plants exhibit a wide variety of morphological and physiological adaptations that allow them to survive, compete, and diversify in these environments. For example, their roots and stems contain large air spaces (aerenchyma) that regulate the efficient transportation of gases (for example, CO2 and O2) used in respiration and photosynthesis. \nSalt water plants (halophytes) have additional specialized adaptations, such as the development of special organs for shedding salt and osmoregulating their internal salt (NaCl) concentrations, to live in estuarine, brackish, or oceanic environments. Anaerobic soil microorganisms in aquatic environments use nitrate, manganese ions, ferric ions, sulfate, carbon dioxide, and some organic compounds; other microorganisms are facultative anaerobes and use oxygen during respiration when the soil becomes drier. The activity of soil microorganisms and the chemistry of the water reduces the oxidation-reduction potentials of the water. Carbon dioxide, for example, is reduced to methane (CH4) by methanogenic bacteria. The physiology of fish is also specially adapted to compensate for environmental salt levels through osmoregulation. Their gills form electrochemical gradients that mediate salt excretion in salt water and uptake in fresh water."
      },
      {
        "heading": "Gravity",
        "level": 3,
        "content": "The shape and energy of the land are significantly affected by gravitational forces. On a large scale, the distribution of gravitational forces on the earth is uneven and influences the shape and movement of tectonic plates as well as influencing geomorphic processes such as orogeny and erosion. These forces govern many of the geophysical properties and distributions of ecological biomes across the Earth. \nOn the organismal scale, gravitational forces provide directional cues for plant and fungal growth (gravitropism), orientation cues for animal migrations, and influence the biomechanics and size of animals. Ecological traits, such as allocation of biomass in trees during growth are subject to mechanical failure as gravitational forces influence the position and structure of branches and leaves. The cardiovascular systems of animals are functionally adapted to overcome the pressure and gravitational forces that change according to the features of organisms (e.g., height, size, shape), their behaviour (e.g., diving, running, flying), and the habitat occupied (e.g., water, hot deserts, cold tundra)."
      },
      {
        "heading": "Pressure",
        "level": 3,
        "content": "Climatic and osmotic pressure places physiological constraints on organisms, especially those that fly and respire at high altitudes, or dive to deep ocean depths. These constraints influence vertical limits of ecosystems in the biosphere, as organisms are physiologically sensitive and adapted to atmospheric and osmotic water pressure differences. For example, oxygen levels decrease with decreasing pressure and are a limiting factor for life at higher altitudes. \nWater transportation by plants is another important ecophysiological process affected by osmotic pressure gradients. Water pressure in the depths of oceans requires that organisms adapt to these conditions. For example, diving animals such as whales, dolphins, and seals are specially adapted to deal with changes in sound due to water pressure differences. Differences between hagfish species provide another example of adaptation to deep-sea pressure through specialized protein adaptations."
      },
      {
        "heading": "Wind and turbulence",
        "level": 3,
        "content": "Turbulent forces in air and water affect the environment and ecosystem distribution, form, and dynamics. On a planetary scale, ecosystems are affected by circulation patterns in the global trade winds. Wind power and the turbulent forces it creates can influence heat, nutrient, and biochemical profiles of ecosystems. For example, wind running over the surface of a lake creates turbulence, mixing the water column and influencing the environmental profile to create thermally layered zones, affecting how fish, algae, and other parts of the aquatic ecosystem are structured. \nWind speed and turbulence also influence evapotranspiration rates and energy budgets in plants and animals. Wind speed, temperature and moisture content can vary as winds travel across different land features and elevations. For example, the westerlies come into contact with the coastal and interior mountains of western North America to produce a rain shadow on the leeward side of the mountain. The air expands and moisture condenses as the winds increase in elevation; this is called orographic lift and can cause precipitation. This environmental process produces spatial divisions in biodiversity, as species adapted to wetter conditions are range-restricted to the coastal mountain valleys and unable to migrate across the xeric ecosystems (e.g., of the Columbia Basin in western North America) to intermix with sister lineages that are segregated to the interior mountain systems."
      },
      {
        "heading": "Fire",
        "level": 3,
        "content": "Plants convert carbon dioxide into biomass and emit oxygen into the atmosphere. By approximately 350 million years ago (the end of the Devonian period), photosynthesis had brought the concentration of atmospheric oxygen above 17%, which allowed combustion to occur. Fire releases CO2 and converts fuel into ash and tar. Fire is a significant ecological parameter that raises many issues pertaining to its control and suppression. While the issue of fire in relation to ecology and plants has been recognized for a long time, Charles Cooper brought attention to the issue of forest fires in relation to the ecology of forest fire suppression and management in the 1960s.\nNative North Americans were among the first to influence fire regimes by controlling their spread near their homes or by lighting fires to stimulate the production of herbaceous foods and basketry materials. Fire creates a heterogeneous ecosystem age and canopy structure, and the altered soil nutrient supply and cleared canopy structure opens new ecological niches for seedling establishment. Most ecosystems are adapted to natural fire cycles. Plants, for example, are equipped with a variety of adaptations to deal with forest fires. Some species (e.g., Pinus halepensis) cannot germinate until after their seeds have lived through a fire or been exposed to certain compounds from smoke. Environmentally triggered germination of seeds is called serotiny. Fire plays a major role in the persistence and resilience of ecosystems."
      },
      {
        "heading": "Soils",
        "level": 3,
        "content": "Soil is the living top layer of mineral and organic dirt that covers the surface of the planet. It is the chief organizing centre of most ecosystem functions, and it is of critical importance in agricultural science and ecology. The decomposition of dead organic matter (for example, leaves on the forest floor), results in soils containing minerals and nutrients that feed into plant production. The whole of the planet's soil ecosystems is called the pedosphere where a large biomass of the Earth's biodiversity organizes into trophic levels. Invertebrates that feed and shred larger leaves, for example, create smaller bits for smaller organisms in the feeding chain. Collectively, these organisms are the detritivores that regulate soil formation. Tree roots, fungi, bacteria, worms, ants, beetles, centipedes, spiders, mammals, birds, reptiles, amphibians, and other less familiar creatures all work to create the trophic web of life in soil ecosystems. \nSoils form composite phenotypes where inorganic matter is enveloped into the physiology of a whole community. As organisms feed and migrate through soils they physically displace materials, an ecological process called bioturbation. This aerates soils and stimulates heterotrophic growth and production. Soil microorganisms are influenced by and are fed back into the trophic dynamics of the ecosystem. No single axis of causality can be discerned to segregate the biological from geomorphological systems in soils. Paleoecological studies of soils places the origin for bioturbation to a time before the Cambrian period. Other events, such as the evolution of trees and the colonization of land in the Devonian period played a significant role in the early development of ecological trophism in soils."
      },
      {
        "heading": "Biogeochemistry and climate",
        "level": 3,
        "content": "Ecologists study and measure nutrient budgets to understand how these materials are regulated, flow, and recycled through the environment. This research has led to an understanding that there is global feedback between ecosystems and the physical parameters of this planet, including minerals, soil, pH, ions, water, and atmospheric gases. Six major elements (hydrogen, carbon, nitrogen, oxygen, sulfur, and phosphorus; H, C, N, O, S, and P) form the constitution of all biological macromolecules and feed into the Earth's geochemical processes. From the smallest scale of biology, the combined effect of billions upon billions of ecological processes amplify and ultimately regulate the biogeochemical cycles of the Earth. Understanding the relations and cycles mediated between these elements and their ecological pathways has significant bearing toward understanding global biogeochemistry.\nThe ecology of global carbon budgets gives one example of the linkage between biodiversity and biogeochemistry. It is estimated that the Earth's oceans hold 40,000 gigatonnes (Gt) of carbon, that vegetation and soil hold 2,070 Gt, and that fossil fuel emissions are 6.3 Gt carbon per year. There have been major restructurings in these global carbon budgets during the Earth's history, regulated to a large extent by the ecology of the land. For example, through the early-mid Eocene volcanic outgassing, the oxidation of methane stored in wetlands, and seafloor gases increased atmospheric CO2 (carbon dioxide) concentrations to levels as high as 3500 ppm.\nIn the Oligocene, from twenty-five to thirty-two million years ago, there was another significant restructuring of the global carbon cycle as grasses evolved a new mechanism of photosynthesis, C4 photosynthesis, and expanded their ranges. This new pathway evolved in response to the drop in atmospheric CO2 concentrations below 550 ppm. The relative abundance and distribution of biodiversity alters the dynamics between organisms and their environment such that ecosystems can be both cause and effect in relation to climate change. \nHuman-driven modifications to the planet's ecosystems (e.g., disturbance, biodiversity loss, agriculture) contributes to rising atmospheric greenhouse gas levels. Transformation of the global carbon cycle in the next century is projected to raise planetary temperatures, lead to more extreme fluctuations in weather, alter species distributions, and increase extinction rates. The effect of global warming is already being registered in melting glaciers, melting mountain ice caps, and rising sea levels. Consequently, species distributions are changing along waterfronts and in continental areas where migration patterns and breeding grounds are tracking the prevailing shifts in climate. \nLarge sections of permafrost are also melting to create a new mosaic of flooded areas having increased rates of soil decomposition activity that raises methane (CH4) emissions. There is concern over increases in atmospheric methane in the context of the global carbon cycle, because methane is a greenhouse gas that is 23 times more effective at absorbing long-wave radiation than CO2 on a 100-year time scale. Hence, there is a relationship between global warming, decomposition and respiration in soils and wetlands producing significant climate feedbacks and globally altered biogeochemical cycles."
      },
      {
        "heading": "History",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Early beginnings",
        "level": 2,
        "content": "Ecology has a complex origin, due in large part to its interdisciplinary nature. Ancient Greek philosophers such as Hippocrates and Aristotle were among the first to record observations on natural history. However, they viewed life in terms of essentialism, where species were conceptualized as static unchanging things while varieties were seen as aberrations of an idealized type. This contrasts against the modern understanding of ecological theory where varieties are viewed as the real phenomena of interest and having a role in the origins of adaptations by means of natural selection. Early conceptions of ecology, such as a balance and regulation in nature can be traced to Herodotus (died c. 425 BC), who described one of the earliest accounts of mutualism in his observation of \"natural dentistry\". Basking Nile crocodiles, he noted, would open their mouths to give sandpipers safe access to pluck leeches out, giving nutrition to the sandpiper and oral hygiene for the crocodile. Aristotle was an early influence on the philosophical development of ecology. He and his student Theophrastus made extensive observations on plant and animal migrations, biogeography, physiology, and their behavior, giving an early analogue to the modern concept of an ecological niche.\n\nEcological concepts such as food chains, population regulation, and productivity were first developed in the 1700s, through the published works of microscopist Antonie van Leeuwenhoek (1632–1723) and botanist Richard Bradley (1688?–1732). Biogeographer Alexander von Humboldt (1769–1859) was an early pioneer in ecological thinking and was among the first to recognize ecological gradients, where species are replaced or altered in form along environmental gradients, such as a cline forming along a rise in elevation. Humboldt drew inspiration from Isaac Newton, as he developed a form of \"terrestrial physics\". In Newtonian fashion, he brought a scientific exactitude for measurement into natural history and even alluded to concepts that are the foundation of a modern ecological law on species-to-area relationships. Natural historians, such as Humboldt, James Hutton, and Jean-Baptiste Lamarck (among others) laid the foundations of the modern ecological sciences. The term \"ecology\" (German: Oekologie, Ökologie) was coined by Ernst Haeckel in his book Generelle Morphologie der Organismen (1866). Haeckel was a zoologist, artist, writer, and later in life a professor of comparative anatomy.\nOpinions differ on who was the founder of modern ecological theory. Some mark Haeckel's definition as the beginning; others say it was Eugenius Warming with the writing of Oecology of Plants: An Introduction to the Study of Plant Communities (1895), or Carl Linnaeus' principles on the economy of nature that matured in the early 18th century. Linnaeus founded an early branch of ecology that he called the economy of nature. His works influenced Charles Darwin, who adopted Linnaeus' phrase on the economy or polity of nature in The Origin of Species. Linnaeus was the first to frame the balance of nature as a testable hypothesis. Haeckel, who admired Darwin's work, defined ecology in reference to the economy of nature, which has led some to question whether ecology and the economy of nature are synonymous.\n\nFrom Aristotle until Darwin, the natural world was predominantly considered static and unchanging. Prior to The Origin of Species, there was little appreciation or understanding of the dynamic and reciprocal relations between organisms, their adaptations, and the environment. An exception is the 1789 publication Natural History of Selborne by Gilbert White (1720–1793), considered by some to be one of the earliest texts on ecology. While Charles Darwin is mainly noted for his treatise on evolution, he was one of the founders of soil ecology, and he made note of the first ecological experiment in The Origin of Species. Evolutionary theory changed the way that researchers approached the ecological sciences."
      },
      {
        "heading": "Since 1900",
        "level": 2,
        "content": "Modern ecology is a young science that first attracted substantial scientific attention toward the end of the 19th century (around the same time that evolutionary studies were gaining scientific interest). The scientist Ellen Swallow Richards adopted the term \"oekology\" (which eventually morphed into home economics) in the U.S. as early as 1892.\nIn the early 20th century, ecology transitioned from a more descriptive form of natural history to a more analytical form of scientific natural history. Frederic Clements published the first American ecology book, Research Methods in Ecology in 1905, presenting the idea of plant communities as a superorganism. This publication launched a debate between ecological holism and individualism that lasted until the 1970s. Clements' superorganism concept proposed that ecosystems progress through regular and determined stages of seral development that are analogous to the developmental stages of an organism. The Clementsian paradigm was challenged by Henry Gleason, who stated that ecological communities develop from the unique and coincidental association of individual organisms. This perceptual shift placed the focus back onto the life histories of individual organisms and how this relates to the development of community associations.\nThe Clementsian superorganism theory was an overextended application of an idealistic form of holism. The term \"holism\" was coined in 1926 by Jan Christiaan Smuts, a South African general and polarizing historical figure who was inspired by Clements' superorganism concept. Around the same time, Charles Elton pioneered the concept of food chains in his classical book Animal Ecology. Elton defined ecological relations using concepts of food chains, food cycles, and food size, and described numerical relations among different functional groups and their relative abundance. Elton's 'food cycle' was replaced by 'food web' in a subsequent ecological text. Alfred J. Lotka brought in many theoretical concepts applying thermodynamic principles to ecology.\nIn 1942, Raymond Lindeman wrote a landmark paper on the trophic dynamics of ecology, which was published posthumously after initially being rejected for its theoretical emphasis. Trophic dynamics became the foundation for much of the work to follow on energy and material flow through ecosystems. Robert MacArthur advanced mathematical theory, predictions, and tests in ecology in the 1950s, which inspired a resurgent school of theoretical mathematical ecologists. Ecology has also developed through contributions from other nations, including Russia's Vladimir Vernadsky and his founding of the biosphere concept in the 1920s and Japan's Kinji Imanishi and his concepts of harmony in nature and habitat segregation in the 1950s. Scientific recognition of contributions to ecology from non-English-speaking cultures is hampered by language and translation barriers.\n\nEcology surged in popular and scientific interest during the 1960–1970s environmental movement. There are strong historical and scientific ties between ecology, environmental management, and protection. The historical emphasis and poetic naturalistic writings advocating the protection of wild places by notable ecologists in the history of conservation biology, such as Aldo Leopold and Arthur Tansley, have been seen as far removed from urban centres where, it is claimed, the concentration of pollution and environmental degradation is located. Palamar (2008) notes an overshadowing by mainstream environmentalism of pioneering women in the early 1900s who fought for urban health ecology (then called euthenics) and brought about changes in environmental legislation. Women such as Ellen Swallow Richards and Julia Lathrop, among others, were precursors to the more popularized environmental movements after the 1950s.\nIn 1962, marine biologist and ecologist Rachel Carson's book Silent Spring helped to mobilize the environmental movement by alerting the public to toxic pesticides, such as DDT (C14H9Cl5), bioaccumulating in the environment. Carson used ecological science to link the release of environmental toxins to human and ecosystem health. Since then, ecologists have worked to bridge their understanding of the degradation of the planet's ecosystems with environmental politics, law, restoration, and natural resources management."
      },
      {
        "heading": "See also",
        "level": 1,
        "content": "Lists"
      },
      {
        "heading": "Notes",
        "level": 1,
        "content": ""
      },
      {
        "heading": "References",
        "level": 1,
        "content": ""
      },
      {
        "heading": "External links",
        "level": 1,
        "content": "\n\"Ecology\" entry  by Alkistis Elliott-Graves in the Stanford Encyclopedia of Philosophy\nThe Nature Education Knowledge Project: Ecology"
      }
    ],
    "summary": "Ecology (from Ancient Greek  οἶκος (oîkos) 'house' and  -λογία (-logía) 'study of') is the natural science of the relationships among living organisms and their environment. Ecology considers organisms at the individual, population, community, ecosystem, and biosphere levels. Ecology overlaps with the closely related sciences of biogeography, evolutionary biology, genetics, ethology, and natural history.\nEcology is a branch of biology, and is the study of abundance, biomass, and distribution of organisms in the context of the environment. It encompasses life processes, interactions, and adaptations; movement of materials and energy through living communities; successional development of ecosystems; cooperation, competition, and predation within and between species; and patterns of biodiversity and its effect on ecosystem processes.\nEcology has practical applications in fields such as conservation biology, wetland management, natural resource management, and human ecology.\nThe word ecology (German: Ökologie) was coined in 1866 by the German scientist Ernst Haeckel. The science of ecology as we know it today began with a group of American botanists in the 1890s. Evolutionary concepts relating to adaptation and natural selection are cornerstones of modern ecological theory.\nEcosystems are dynamically interacting systems of organisms, the communities they make up, and the non-living (abiotic) components of their environment. Ecosystem processes, such as primary production, nutrient cycling, and niche construction, regulate the flux of energy and matter through an environment. Ecosystems have biophysical feedback mechanisms that moderate processes acting on living (biotic) and abiotic components of the planet. Ecosystems sustain life-supporting functions and provide ecosystem services like biomass production (food, fuel, fiber, and medicine), the regulation of climate, global biogeochemical cycles, water filtration, soil formation, erosion control, flood protection, and many other natural features of scientific, historical, economic, or intrinsic value."
  },
  {
    "title": "Ecología",
    "source": "https://es.wikipedia.org/wiki/Ecolog%C3%ADa",
    "language": "es",
    "chunks": [
      {
        "heading": "Historia",
        "level": 1,
        "content": "El término ökologie fue acuñado en 1869[2]​ por el naturalista y filósofo alemán Ernst Haeckel a partir de las palabras griegas oikos (casa, vivienda, hogar) y logos (estudio o tratado); por ello ecología significa «el estudio del hogar».[3]​\nEn un principio, Haeckel entendía por ecología la ciencia que estudia las relaciones de los seres vivos con su ambiente, pero más tarde amplió esta definición al estudio de las características del medio, que también incluye el transporte de materia y energía, y su transformación por las comunidades biológicas."
      },
      {
        "heading": "Precursores",
        "level": 2,
        "content": "Hay que reconocer a los biólogos y geógrafos el papel fundamental en los inicios de la ecología. Es importante recordar el aporte considerable de los griegos clásicos. Por ejemplo, Aristóteles, además de filósofo, fue un biólogo y naturalista de gran talla. Baste citar sus libros sobre la vida y costumbres de los peces, fruto de sus diálogos con pescadores, y sus largas horas de observación personal. Su discípulo Teofrasto describió por primera vez las interrelaciones entre organismos y su entorno.[4]​ Las primeras concepciones de la ecología, como el equilibrio y la regulación en la naturaleza, se remontan a Heródoto, quien describió uno de los primeros relatos del mutualismo en su observación de la \"ontología natural\".[5]​ \nSi nos trasladamos al siglo XVIII, cuando la biología y la geografía recién se estaban transformando en las ciencias modernas que hoy conocemos, es imprescindible reconocer el carácter absolutamente ecológico del trabajo de los fisiologistas en su progresivo descubrimiento de las relaciones entre la vida vegetal y animal con los factores abióticos tales como la luz, el agua o el carbono. Entre los diferentes ejemplos posibles, es suficiente recordar las investigaciones de René Antoine Ferchault de Réaumur en el campo de la temperatura, así como las de Anton van Leeuwenhoek acerca de la formación del almidón en las plantas verdes. Destacan también en esta época, los trabajos de Louis Receveur, botánico , geólogo , químico , meteorólogo, astrónomo y sacerdote francés.\nTambién se realizaron durante el siglo algunos de los grandes viajes científicos que permitieron un conocimiento más metodológico de los paisajes geográficos de los diversos continentes, ejemplo entre otros de Georges-Louis Leclerc de Buffon, autor de los primeros tratados de biología y geología no basados en la Biblia; o  de Alexander von Humboldt, que exploró y estudió durante cinco años las tierras de América Latina.\nEl papel de los precursores del evolucionismo es asimismo fundamental, porque intuían que no había ningún tipo de predeterminismo en la gran variedad de especies vivientes existentes, sino progresivas adaptaciones ambientales.\nErasmus Darwin, abuelo del universalmente famoso Charles Darwin, predijo algunas de las grandes tesis evolucionistas que desarrolló años más tarde su nieto y que influyeron de modo decisivo en las corrientes de pensamiento del siglo XIX.\nSin duda alguna, la polémica entre deterministas y evolucionistas fue uno de los principales debates científicos del siglo XIX, enfrentando a hombres de la categoría de Cuvier, Owen, Agassiz y Kölliker, contra los nuevos «transformistas» como Lamarck, Darwin, Spencer, Müller, Haeckel, etc.\nEl calor de la polémica fue muy fecundo, porque exigió de los transformistas que multiplicaran sus observaciones para justificar las nuevas teorías del evolucionismo.\nEn alguno de ellos se manifestó una conversión forzada por las evidencias; por ejemplo en el científico galés Richard Owen, que aun siendo vivamente adversario de la nueva teoría evolucionista, realizó descubrimientos que él mismo no podía justificar si no era recurriendo a la teoría de Darwin."
      },
      {
        "heading": "Objeto de estudio",
        "level": 1,
        "content": "La ecología es la rama de la biología que estudia las interacciones de los seres vivos con su hábitat. Esto incluye factores abióticos, esto es, condiciones ambientales tales como: climatológicas, edáficas, etc.; pero también incluye factores bióticos, esto es, condiciones derivadas de las relaciones que se establecen con otros seres vivos. Mientras que otras ramas se ocupan de niveles de organización inferiores (desde la bioquímica y la biología molecular pasando por la biología celular, la histología y la fisiología hasta la sistemática), la ecología se ocupa del nivel superior a estas, ocupándose de las poblaciones, las comunidades, los ecosistemas y la biosfera. Por esta razón, y por ocuparse de las interacciones entre los individuos y su ambiente, la ecología es una ciencia multidisciplinaria que utiliza herramientas de otras ramas de la ciencia, especialmente geología, meteorología, geografía, sociología, física, química y matemáticas.\nLos ecólogos tratan de explicar:\n\nLos procesos de la vida, interacciones y adaptaciones\nEl movimiento de materiales y energía a través de las comunidades vivas\nLa sucesión ecológica de los ecosistemas\nLa abundancia y la distribución de los organismos y de la biodiversidad en el contexto del medio ambiente.\nHay muchas aplicaciones prácticas de la ecología en biología de la conservación, manejo de los humedales, manejo de recursos naturales (la agroecología, la agricultura, la silvicultura, la agroforestería, la pesca), la planificación de la ciudad (ecología urbana), la salud comunitaria, la economía, la ciencia básica aplicada, y la interacción social humana (ecología humana). Los organismos (incluidos los seres humanos) y los recursos componen los ecosistemas que, a su vez, mantienen los mecanismos de retroalimentación biofísicos son componentes del planeta que moderan los procesos que actúan sobre la vida (bióticos) y no vivos (abióticos). Los ecosistemas sostienen funciones que sustentan la vida y producen el capital natural como la producción de biomasa (alimentos, combustibles, fibras y medicamentos), los ciclos biogeoquímicos globales, filtración de agua, la formación del suelo, control de la erosión, la protección contra inundaciones y muchos otros elementos naturales de interés científico, histórico o económico.\nLos trabajos de investigación en esta disciplina se diferencian con respecto de la mayoría de los trabajos en las demás ramas de la Biología por su mayor uso de herramientas matemáticas, como la estadística y los modelos matemáticos. Además, la comprensión de los procesos ecológicos se basa fuertemente en los postulados evolutivos (Dobzhansky, 1973)."
      },
      {
        "heading": "Principios y conceptos",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Teoría de sistemas",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Ciclo biogeoquímico",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Niveles de organización",
        "level": 2,
        "content": "Para los ecólogos modernos (Begon, Harper y Townsend, 1999)(Molles, 2006), la ecología puede ser estudiada a varios niveles o escalas:\n\nOrganismo (las interacciones de un ser vivo dado con las condiciones abióticas directas que lo rodean)\nPoblación (las interacciones de un ser vivo dado con los seres de su misma especie)\nComunidad (las interacciones de una población dada con las poblaciones de especies que la rodean)\nEcosistema (las interacciones propias de la biocenosis sumadas a todos los flujos de materia y energía que tienen lugar en ella)\nBiosfera (el conjunto de todos los seres vivos conocidos)"
      },
      {
        "heading": "Cadena trófica",
        "level": 2,
        "content": "La cadena alimenticia o cadena trófica señala las relaciones alimenticias entre productores,consumidores y descomponedores. En otras palabras, la cadena refleja quién se come a quién.\nLas cadenas tróficas, son una serie de cadenas alimentarias íntimamente relacionadas por las que circulan energía y materiales en un ecosistema. Se entiende por cadena alimentaria cada una de las relaciones alimenticias que se establecen de forma lineal entre organismos que pertenecen a distintos niveles tróficos. La cadena trófica está dividida en dos grandes categorías: la cadena o red de pastoreo, que se inicia con las plantas verdes, algas o plancton que realiza la fotosíntesis, y la cadena o red de detritos que comienza con los detritos orgánicos. Estas redes están formadas por cadenas alimentarias independientes. En la red de pastoreo, los materiales pasan desde las plantas a los consumidores de plantas (herbívoros) y de estos a los consumidores de carne (carnívoros). En la red de detritos, los materiales pasan desde las plantas y sustancias animales a las bacterias y a los hongos (descomponedores), y de estos a los que se alimentan de detritos (detritívoros) y de ellos a sus depredadores (carnívoros).\nPor lo general, entre las cadenas tróficas existen muchas interconexiones; por ejemplo, los hongos que descomponen la materia en una red de detritos pueden dar origen a setas que son consumidas por ardillas, ratones y ciervos en una red de pastoreo. Los petirrojos son omnívoros, es decir, consumen plantas y animales, y por esta razón están presentes en las redes de pastoreo y de detritos. Los petirrojos se suelen alimentar de lombrices de tierra que son detritívoras y se alimentan de hojas en estado de putrefacción."
      },
      {
        "heading": "Producción y productividad",
        "level": 2,
        "content": "En un ecosistema, las conexiones entre las especies se relacionan generalmente con su papel en la cadena alimentaria. Hay tres categorías de organismos:\n\nProductores o autótrofos —Generalmente las plantas o las cianobacterias que son capaces de fotosintetizar pero podrían ser otros organismos tales como las bacterias cerca de los respiraderos del océano que son capaces de quimiosintetizar.\nConsumidores o heterótrofos —Animales, que pueden ser consumidores primarios (herbívoros), o consumidores secundarios o terciarios (carnívoros y omnívoros).\nDescomponedores o detritívoros —Bacterias, hongos, e insectos que degradan la materia orgánica de todos los tipos y restauran los alimentos al ambiente. Entonces los productores consumirán los alimentos, terminando el ciclo.\nEstas relaciones forman las secuencias, en las cuales cada individuo consume al precedente y es consumido por el siguiente, lo que se llama cadenas alimentarias o las redes del alimento. En una red de alimento habrá pocos organismos en cada nivel como uno sigue los acoplamientos de la red encima de la cadena, formando una pirámide.\nEstos conceptos llevan a la idea de biomasa (la materia viva total en un ecosistema), de la productividad primaria (el aumento en compuestos orgánicos), y de la productividad secundaria (la materia viva producida por los consumidores y los descomponedores en un rato dado). Estas dos ideas pasadas son dominantes, puesto que permiten evaluar la capacidad de carga —el número de organismos que se pueden apoyar por un ecosistema dado. En ninguna red del alimento se transfiere totalmente la energía contenida en el nivel de los productores a los consumidores. Se pierden ascendentes cuanto más alta es la cadena, mayor la energía y los recursos. Así, puramente desde el punto de vista del alimento y la energía, es más eficiente que los seres humanos sean consumidores primarios (subsistir de vehículos, de granos, de las legumbres, de la fruta, etc.) que consumidores secundarios (herbívoros consumidores, omnívoros, o sus productos), y aún más que sean consumidores terciarios (carnívoros consumidores, omnívoros, o sus productos). Un ecosistema es inestable cuando sobra la capacidad de carga. La productividad total de los ecosistemas es estimada a veces comparando tres tipos de ecosistemas con base en tierra y el total de ecosistemas acuáticos; se estima que la mitad de la producción primaria puede ocurrir en tierra, y el resto en el océano.\n\nLos bosques (1/3 de la superficie terrestre de la Tierra) contienen biomasas densas y muy productivas.\nSabanas, praderas, y pantanos (1/3 de la superficie terrestre de la Tierra) contienen biomasas menos densas, pero es productiva. Estos ecosistemas representan a las mayores partes de las que dependen el alimento humano.\nEcosistemas extremos en las áreas con climas más extremos —desiertos y semi-desiertos, tundra, prados alpestres, y estepas -- (1/3 de la superficie terrestre de la Tierra). Tienen biomasas muy escasas y baja productividad.\nFinalmente, los ecosistemas del agua marina y dulce (3/4 de la superficie terrestre de la Tierra) contiene biomasas muy escasas (aparte de las zonas costeras).\nLos ecosistemas difieren en su biomasa (carbón de los gramos por metro cuadrado) y la productividad (carbón de los gramos por metro cuadrado por día), y las comparaciones directas de la biomasa y la productividad puede no ser válida. Un ecosistema como este en la taiga puede ser alto en biomasa, pero de crecimiento lento y así bajo en productividad. Los ecosistemas se comparan a menudo en base de su volumen de ventas (cociente de la producción) o del tiempo del volumen de ventas que sean los recíprocos del volumen de ventas. Las acciones humanas durante los últimos siglos han reducido seriamente la cantidad de la tierra cubierta por los bosques (tala de árboles), y han aumentado agroecosistemas. En últimas décadas ha ocurrido un aumento en las áreas ocupadas por ecosistemas extremos, como en el caso de la desertificación."
      },
      {
        "heading": "Tasa de renovación",
        "level": 2,
        "content": "Es la relación que existe entre la producción y la biomasa. Sirve para indicar la riqueza de un ecosistema o nivel trófico, ya que representa la velocidad con que se renueva la biomasa, por lo que también recibe el nombre de tasa de renovación.\nSu valor es el cociente Pn/B. (producción neta entre biomasa)"
      },
      {
        "heading": "Biodiversidad",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Biosfera",
        "level": 2,
        "content": "La capa exterior del planeta Tierra puede ser dividida en varios compartimentos: la hidrosfera (o esfera de agua), la litosfera (o ámbito de los suelos y rocas), y la atmósfera (o la esfera de aire). La biosfera (o la esfera de la vida), a veces descrita como \"el cuarto sobre\" es la materia viva del planeta, o la parte del planeta ocupada por la vida. Alcanza así en los otros tres ámbitos, aunque no hay habitantes permanentes de la atmósfera. En relación con el volumen de la Tierra, la biosfera es solo la capa superficial muy delgada que se extiende 11 000 metros bajo el nivel del mar a 15 000 metros por encima.\nSe piensa que la vida se desarrolló por primera vez en la hidrosfera, a profundidades someras, en la zona fótica. (Sin embargo, recientemente, una teoría de la competencia se ha convertido, de que la vida se originó alrededor de fuentes hidrotermales en la profundidad de océano. Véase el origen de la vida.) Luego aparecieron los organismos multicelulares y colonizaron las zonas bentónicas. Organismos fotosintéticos gradualmente emitieron, mediante reacciones químicas, los gases hasta llegar a las actuales concentraciones, especialmente la abundancia de oxígeno, que caracterizan a nuestro planeta. La vida terrestre se desarrolló más tarde, protegida de los rayos UV por la capa de ozono. La diversificación de las especies terrestres se piensa que fue incrementada por la deriva de los continentes por aparte, o, alternativamente, chocar. La biodiversidad se expresa en el nivel ecológico (ecosistema), nivel de población (diversidad intraespecífica), especies (diversidad específica), y nivel genético.\nLa biosfera contiene grandes cantidades de elementos tales como carbono, hidrógeno, oxígeno y nitrógeno. Otros elementos, tales como el fósforo, calcio y potasio, también son esenciales para la vida, pero están presentes en cantidades más pequeñas. En el ecosistema y los niveles de la biosfera ocurre un continuo reciclaje de todos estos elementos que se alternan entre los estados minerales y orgánicos.\nAunque hay una ligera entrada de la energía geotérmica, la mayor parte del funcionamiento de los ecosistemas se basa en la aporte de la energía solar. Las plantas y los microorganismos fotosintéticos convierten la luz en energía química mediante el proceso de fotosíntesis, lo que crea la glucosa (un azúcar simple) y libera oxígeno libre. La glucosa se convierte así en la segunda fuente de energía que impulsa el ecosistema. Parte de esta glucosa se utiliza directamente por otros organismos para la energía. Otras moléculas de azúcar pueden ser convertidas en otras moléculas como los aminoácidos. Las plantas usan alguna de estos azúcares, concentrado en el néctar, para atraer a los polinizadores para la ayuda en la reproducción.\nLa respiración celular es el proceso mediante el cual los organismos (como los mamíferos) rompen de glucosa hacia abajo en sus mandantes, el agua y el dióxido de carbono, por lo tanto, recuperar la energía almacenada originalmente dio el sol a las plantas. La proporción de la actividad fotosintética de las plantas y otros fotosintetizadores a la respiración de otros organismos determina la composición de la atmósfera de la Tierra, en particular su nivel de oxígeno. Las corrientes de aire globales unen la atmósfera manteniendo casi el mismo equilibrio de los elementos en áreas de intensa actividad biológica y las áreas de la actividad biológica ligera.\nEl agua es también intercambiada entre la hidrosfera, la litosfera, la atmósfera, la biosfera y en ciclos regulares. Los océanos son grandes depósitos que almacenan el agua, aseguran la estabilidad térmica y climática, y facilitan el transporte de elementos químicos gracias a las grandes corrientes oceánicas.\nPara una mejor comprensión de cómo funciona la biosfera, y las diversas disfunciones relacionadas con la actividad humana, científicos americanos trataron de simular la biosfera en un modelo en pequeña escala, llamado Biosfera 2."
      },
      {
        "heading": "Ecosistema",
        "level": 2,
        "content": "Un principio central de la ecología es que cada organismo vivo tiene una relación permanente y continua con todos los demás elementos que componen su entorno. La suma total de la interacción de los organismos vivos (la biocenosis) y su medio no viviente (biotopo) en una zona que se denomina un ecosistema. Los estudios de los ecosistemas por lo general se centran en la circulación de la energía y la materia a través del sistema.\nCasi todos los ecosistemas funcionan con energía del sol capturada por los productores primarios a través de la fotosíntesis. Esta energía fluye a través de la cadena alimentaria a los consumidores primarios (herbívoros que comen y digieren las plantas), y los consumidores secundarios y terciarios (ya sea omnívoros o carnívoros). La energía se pierde a los organismos vivos cuando se utiliza por los organismos para hacer el trabajo, o se pierde como calor residual.\nLa materia es incorporada a los organismos vivos por los productores primarios. Las plantas fotosintetizadoras fijan el carbono a partir del dióxido de carbono y del nitrógeno de la atmósfera o nitratos presentes en el suelo para producir aminoácidos. Gran parte de los contenidos de carbono y nitrógeno en los ecosistemas es creado por las instalaciones de ese tipo, y luego es incorporado por los consumidores secundarios y terciarios. Los nutrientes son generalmente devueltos a los ecosistemas a través de la descomposición. Todo el movimiento de los productos químicos en un ecosistema que se denomina un ciclo biogeoquímico, e incluye el ciclo del carbono y del nitrógeno.\nLos ecosistemas de cualquier tamaño se pueden estudiar, por ejemplo, una roca y la vida de las plantas que crecen en ella puede ser considerado un ecosistema. Esta roca puede estar dentro de un llano, con muchas de estas rocas, hierbas pequeñas, y animales que pastorean - también un ecosistema-. Este puede ser simple en la tundra, que también es un ecosistema (aunque una vez que son de este tamaño, por lo general se denomina ecozonas o biomas). De hecho, toda la superficie terrestre de la Tierra, toda la materia que lo compone, el aire que está directamente encima de este, y todos los organismos vivos que viven dentro de ella puede ser considerados como un solo gran ecosistema.\nLos ecosistemas se pueden dividir en los ecosistemas terrestres (incluidos los ecosistemas de bosques, estepas, sabanas, etc), los ecosistemas de agua dulce (lagos, estanques y ríos), y los ecosistemas marinos, en función del biotopo dominante."
      },
      {
        "heading": "Relaciones espaciales y subdivisiones de la Tierra",
        "level": 2,
        "content": "Los ecosistemas no están aislados unos de otros sino interrelacionados; por ejemplo, el agua puede circular entre los ecosistemas por medio de un río o corriente oceánica. El agua en sí, como un medio líquido, incluso define los ecosistemas. Algunas especies, como el salmón o la anguila de agua dulce se mueven entre los sistemas marinos y de agua dulce. Estas relaciones entre los ecosistemas conducen a la idea de \"bioma\". Un bioma es una formación homogénea ecológica que existe en una amplia región, como la tundra y las estepas. La biosfera comprende la totalidad de los biomas de la Tierra -la totalidad de los lugares donde la vida es posible-, desde las montañas más altas a las profundidades oceánicas.\nLos biomas están bastante bien distribuidos a lo largo de las subdivisiones a las latitudes, desde el ecuador hacia los polos, con las diferencias basadas en el entorno físico (por ejemplo, los océanos o cordilleras) y el clima. Su variación está generalmente relacionada con la distribución de las especies de acuerdo a su capacidad para tolerar la temperatura, la sequedad, o ambos. Por ejemplo, se pueden encontrar algas fotosintéticas solo en la parte luminosa de los océanos (donde penetra la luz), mientras que las coníferas se encuentran principalmente en las montañas.\nAunque esta es una simplificación de un sistema más complicado, la latitud y la altitud representan de manera adecuada la distribución de la diversidad biológica dentro de la biosfera. En general, la riqueza de la diversidad biológica (así como de los animales como para las especies de plantas) está disminuyendo más rápidamente cerca del ecuador y más lentamente a medida que nos aproximamos a los polos.\nLa biosfera también puede ser dividida en ecozonas, que están muy bien definidas y sobre todo hoy en día sigue las fronteras continentales. Las zonas ecológicas son divididas en las ecorregiones, aunque no hay acuerdo sobre sus límites."
      },
      {
        "heading": "Disciplinas",
        "level": 1,
        "content": "Como disciplina científica en la que intervienen diferentes caracteres la ecología no puede dictar qué es \"bueno\" o \"malo\". Aun así, se puede considerar que el mantenimiento de la biodiversidad y sus objetivos relacionados han proporcionado la base científica para expresar los objetivos del ecologismo y, asimismo, le ha provisto la metodología y terminología para expresar los problemas ambientales.\nLa economía y la ecología comparten formalismo en muchas de sus áreas; algunas herramientas utilizadas en esta disciplina, como tablas de vida y teoría de juegos, tuvieron su origen en la economía. La disciplina que integra ambas ciencias es la economía ecológica.\n\nLa aerobiología es una ciencia multidisciplinaria en la que se incluyen los procesos ecológicos relacionados con las partículas biológicas transportadas pasivamente a través del aire.\nLa ecología microbiana es la rama de la ecología que estudia a los microorganismos en su ambiente natural, los cuales mantienen una actividad continua imprescindible para la vida en la Tierra. En los últimos años se han logrado numerosos avances en esta disciplina con las técnicas disponibles de biología molecular. Los mecanismos que mantienen la diversidad microbiana de la biosfera son la base de la dinámica de los ecosistemas terrestres, acuáticos y aéreos. Es decir, la base de la existencia de las selvas y de los sistemas agrícolas, entre otros. Por otra parte, la diversidad microbiana del suelo es la causa de la fertilidad del mismo.\nLa biogeografía: es la ciencia que estudia la distribución de los seres vivos sobre la Tierra, así como los procesos que la han originado, que la modifican y que la pueden hacer desaparecer. Es una ciencia interdisciplinaria, de manera que aunque formalmente es una rama de la geografía, recibiendo parte de sus fundamentos de especialidades como la climatología y otras ciencias de la Tierra, es a la vez parte de la biología. La superficie de la Tierra no es uniforme, ni en toda ella existen las mismas características. El espacio isotrópico que utilizan, o suponen, los esquemas teóricos de localización es tan solo una construcción matemática del espacio.\nLa ecología matemática se dedica a la aplicación de los teoremas y métodos matemáticos a los problemas de la relación de los seres vivos con su medio y es, por tanto, una rama de la biología. Esta disciplina provee de la base formal para la enunciación de gran parte de la ecología teórica\nLa ecología urbana es una disciplina cuyo objeto de estudio son las interrelaciones entre los habitantes de una aglomeración urbana y sus múltiples interacciones con el ambiente.\nLa ecología de la recreación es el estudio científico de las relaciones ecológicas entre el ser humano y la naturaleza dentro de un contexto recreativo.\nLa ecología del paisaje es una disciplina a caballo entre la geografía física y la biología. Estudia los paisajes naturales prestando especial atención a los grupos humanos como agentes transformadores de la dinámica físico-ecológica de estos. Ha recibido aportes tanto de la geografía física como de la biología: la geografía aporta las visiones estructurales del paisaje (el estudio de la estructura horizontal o del mosaico de subecosistemas que conforman el paisaje), mientras que la biología aporta la visión funcional del paisaje (las relaciones verticales de materia y energía). Este concepto comienza en 1898, con el geógrafo, padre de la pedología rusa, Vasily Vasilievich Dokuchaev y fue más tarde continuado por el geógrafo alemán Carl Troll. Es una disciplina muy relacionada con otras áreas como la geoquímica, la geobotánica, las ciencias forestales o la pedología.\nLa limnología es la rama de la ecología que se centra en el estudio de los sistemas acuáticos continentales: ríos, lagos, lagunas, etcétera.\nLa dendroecología se centra en el estudio de la ecología de los árboles.\nLa ecología regional es una disciplina que estudia los procesos ecosistémicos como el flujo de energía, el ciclo de la materia o la producción de gases de invernadero a escala de paisaje regional o bioma. Considera que existen grandes regiones que funcionan como un único ecosistema.\nLa agronomía, la pesquería y, en general, toda disciplina que tenga relación con la explotación o conservación de recursos naturales, en especial seres vivos, pueden interpretarse como ecología aplicada. Es decir, tienen la misma relación con la ecología que gran parte de las ingenierías con la matemática, la física o la química."
      },
      {
        "heading": "Otras disciplinas",
        "level": 2,
        "content": "Biología de la conservación\nDerecho ambiental\nEcología de comunidades\nEcología de poblaciones\nEcología evolutiva\nEcología del comportamiento\nEtoecología\nEcología humana\nEcología reproductiva\nEcología social\nEcología cultural"
      },
      {
        "heading": "Ecólogos célebres",
        "level": 1,
        "content": "Ramón Margalef\nFernando González Bernáldez\nEugene P. Odum\nMiguel Ángel de Quevedo\nErnst Haeckel\nEdward Osborne Wilson"
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bibliografía",
        "level": 1,
        "content": "Dobzhansky, Theodosius (1973), «Nothing in Biology Makes Sense Except in the Light of Evolution», The American Biology Teacher (en inglés) 35 (3): 125-129, ISSN 0002-7685, archivado desde el original el 5 de noviembre de 2015, resumen divulgativo .\nMargalef, Ramón (1998). «1». Ecología (9.ª edición). Barcelona: Omega. ISBN 8428204055. \nMolles, Manuel C. Jr. (2006). Ecología: Conceptos y aplicaciones. (3.ª edición). Madrid: McGraw-Hill. ISBN 844814595X. \nMalacalza, Leonardo, ed. (2014). Ecología y ambiente. Asociación de Universidades Grupo Montevideo - Universidad Nacional de La Plata. p. 303. ISSN 2314-1743. Consultado el 11 de agosto de 2014. \nSantana, Adalberto Coord. (2011). Energía, medio ambiente y política en América Latina. México: UNAM. ISBN 978-607-02-2814-8"
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": "\n Wikiversidad alberga proyectos de aprendizaje sobre Ecología.\n\n Wikcionario  tiene definiciones y otra información sobre ecología.\n\nEl Diccionario de la Real Academia Española tiene una definición para ecología.\nTérminos básicos de la ecología.\nRevista digital de ecología.\nRevista digital de ecología y medio ambiente."
      }
    ],
    "summary": "La ecología es la rama de la biología que estudia las relaciones de los diferentes seres vivos entre sí y la relación de estos con el entorno: «la biología de los ecosistemas».[1]​ Estudia cómo estas interacciones entre los organismos y su ambiente afectan a propiedades como la distribución o la abundancia. En el ambiente se incluyen las propiedades físicas y químicas que pueden ser descritas como la suma de factores abióticos locales, como el clima y la geología, y los demás organismos que comparten ese hábitat (factores bióticos). \nLos ecosistemas están compuestos de partes que interactúan dinámicamente entre ellas junto con los organismos, las comunidades que integran, y también los componentes no vivos de su entorno. Los procesos del ecosistema, como la producción primaria, la pedogénesis, el ciclo de nutrientes, y las diversas actividades de construcción del hábitat, regulan el flujo de energía y materia a través de un entorno. Estos procesos se sustentan en los organismos con rasgos específicos históricos de la vida, y la variedad de organismos que se denominan biodiversidad. La visión integradora de la ecología plantea el estudio científico de los procesos que influyen en la distribución y abundancia de los organismos, así como las interacciones entre los organismos y la transformación de los flujos de energía. La ecología es un campo interdisciplinario que incluye a la biología y las ciencias de la Tierra.\nLa ecología evolucionó a partir de la historia natural de los antiguos filósofos griegos, como Hipócrates, Aristóteles y Teofrasto, sentando las bases de la ecología en sus estudios sobre la historia natural. Las bases posteriores para la ecología moderna se establecieron en los primeros trabajos de los fisiólogos de plantas y animales. Los conceptos evolutivos sobre la adaptación y la selección natural se convirtieron en piedras angulares de la teoría ecológica moderna transformándola en una ciencia más rigurosa en el siglo XIX. Está estrechamente relacionada con la biología evolutiva, la genética y la etología. La comprensión de cómo la biodiversidad afecta a la función ecológica es un área importante enfocada en los estudios ecológicos."
  },
  {
    "title": "Classical mechanics",
    "source": "https://en.wikipedia.org/wiki/Classical_mechanics",
    "language": "en",
    "chunks": [
      {
        "heading": "Branches",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Traditional division",
        "level": 2,
        "content": "Classical mechanics was traditionally divided into three main branches. \nStatics is the branch of classical mechanics that is concerned with the analysis of force and torque acting on a physical system that does not experience an acceleration, but rather is in equilibrium with its environment.  Kinematics describes the motion of points, bodies (objects), and systems of bodies (groups of objects) without considering the forces that cause them to move. Kinematics, as a field of study, is often referred to as the \"geometry of motion\" and is occasionally seen as a branch of mathematics. Dynamics goes beyond merely describing objects' behavior and also considers the forces which explain it.\nSome authors (for example, Taylor (2005) and Greenwood (1997)) include special relativity within classical dynamics."
      },
      {
        "heading": "Forces vs. energy",
        "level": 2,
        "content": "Another division is based on the choice of mathematical formalism. Classical mechanics can be mathematically presented in multiple different ways. The physical content of these different formulations is the same, but they provide different insights and facilitate different types of calculations. While the term \"Newtonian mechanics\" is sometimes used as a synonym for non-relativistic classical physics, it can also refer to a particular formalism based on Newton's laws of motion. Newtonian mechanics in this sense emphasizes force as a vector quantity.\nIn contrast, analytical mechanics uses scalar properties of motion representing the system as a whole—usually its kinetic energy and potential energy. The equations of motion are derived from the scalar quantity by some underlying principle about the scalar's variation. Two dominant branches of analytical mechanics are Lagrangian mechanics, which uses generalized coordinates and corresponding generalized velocities in configuration space, and Hamiltonian mechanics, which uses coordinates and corresponding momenta in phase space. Both formulations are equivalent by a Legendre transformation on the generalized coordinates, velocities and momenta; therefore, both contain the same information for describing the dynamics of a system. There are other formulations such as Hamilton–Jacobi theory, Routhian mechanics, and Appell's equation of motion. All equations of motion for particles and fields, in any formalism, can be derived from the widely applicable result called the principle of least action. One result is Noether's theorem, a statement which connects conservation laws to their associated symmetries."
      },
      {
        "heading": "By region of application",
        "level": 2,
        "content": "Alternatively, a division can be made by region of application:\n\nCelestial mechanics, relating to stars, planets and other celestial bodies\nContinuum mechanics, for materials modelled as a continuum, e.g., solids and fluids (i.e., liquids and gases).\nRelativistic mechanics (i.e. including the special and general theories of relativity), for bodies whose speed is close to the speed of light.\nStatistical mechanics, which provides a framework for relating the microscopic properties of individual atoms and molecules to the macroscopic or bulk thermodynamic properties of materials."
      },
      {
        "heading": "Description of objects and their motion",
        "level": 1,
        "content": "For simplicity, classical mechanics often models real-world objects as point particles, that is, objects with negligible size. The motion of a point particle is determined by a small number of parameters: its position, mass, and the forces applied to it. Classical mechanics also describes the more complex motions of extended non-pointlike objects. Euler's laws provide extensions to Newton's laws in this area. The concepts of angular momentum rely on the same calculus used to describe one-dimensional motion. The rocket equation extends the notion of rate of change of an object's momentum to include the effects of an object \"losing mass\". (These generalizations/extensions are derived from Newton's laws, say, by decomposing a solid body into a collection of points.)\nIn reality, the kind of objects that classical mechanics can describe always have a non-zero size. (The behavior of very small particles, such as the electron, is more accurately described by quantum mechanics.) Objects with non-zero size have more complicated behavior than hypothetical point particles, because of the additional degrees of freedom, e.g., a baseball can spin while it is moving. However, the results for point particles can be used to study such objects by treating them as composite objects, made of a large number of collectively acting point particles. The center of mass of a composite object behaves like a point particle.\nClassical mechanics assumes that matter and energy have definite, knowable attributes such as location in space and speed. Non-relativistic mechanics also assumes that forces act instantaneously (see also Action at a distance)."
      },
      {
        "heading": "Kinematics",
        "level": 2,
        "content": "The position of a point particle is defined in relation to a coordinate system centered on an arbitrary fixed reference point in space called the origin O. A simple coordinate system might describe the position of a particle P with a vector notated by an arrow labeled r that points from the origin O to point P. In general, the point particle does not need to be stationary relative to O. In cases where P is moving relative to O, r is defined as a function of t, time. In pre-Einstein relativity (known as Galilean relativity), time is considered an absolute, i.e., the time interval that is observed to elapse between any given pair of events is the same for all observers. In addition to relying on absolute time, classical mechanics assumes Euclidean geometry for the structure of space."
      },
      {
        "heading": "Velocity and speed",
        "level": 3,
        "content": "The velocity, or the rate of change of displacement with time, is defined as the derivative of the position with respect to time:\n\n  \n    \n      \n        \n          v\n        \n        =\n        \n          \n            \n              \n                d\n              \n              \n                r\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n        \n      \n    \n    {\\displaystyle \\mathbf {v} ={\\mathrm {d} \\mathbf {r}  \\over \\mathrm {d} t}\\,\\!}\n  \n.\nIn classical mechanics, velocities are directly additive and subtractive. For example, if one car travels east at 60 km/h and passes another car traveling in the same direction at 50 km/h, the slower car perceives the faster car as traveling east at 60 − 50 = 10 km/h. However, from the perspective of the faster car, the slower car is moving 10 km/h to the west, often denoted as −10 km/h where the sign implies opposite direction. Velocities are directly additive as vector quantities; they must be dealt with using vector analysis.\nMathematically, if the velocity of the first object in the previous discussion is denoted by the vector u = ud and the velocity of the second object by the vector v = ve, where u is the speed of the first object, v is the speed of the second object, and d and e are unit vectors in the directions of motion of each object respectively, then the velocity of the first object as seen by the second object is:\n\n  \n    \n      \n        \n          \n            u\n          \n          ′\n        \n        =\n        \n          u\n        \n        −\n        \n          v\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {u} '=\\mathbf {u} -\\mathbf {v} \\,.}\n  \n\nSimilarly, the first object sees the velocity of the second object as:\n\n  \n    \n      \n        \n          \n            v\n            ′\n          \n        \n        =\n        \n          v\n        \n        −\n        \n          u\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {v'} =\\mathbf {v} -\\mathbf {u} \\,.}\n  \n\nWhen both objects are moving in the same direction, this equation can be simplified to:\n\n  \n    \n      \n        \n          \n            u\n          \n          ′\n        \n        =\n        (\n        u\n        −\n        v\n        )\n        \n          d\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {u} '=(u-v)\\mathbf {d} \\,.}\n  \n\nOr, by ignoring direction, the difference can be given in terms of speed only:\n\n  \n    \n      \n        \n          u\n          ′\n        \n        =\n        u\n        −\n        v\n        \n        .\n      \n    \n    {\\displaystyle u'=u-v\\,.}"
      },
      {
        "heading": "Acceleration",
        "level": 3,
        "content": "The acceleration, or rate of change of velocity, is the derivative of the velocity with respect to time (the second derivative of the position with respect to time):\n\n  \n    \n      \n        \n          a\n        \n        =\n        \n          \n            \n              \n                d\n              \n              \n                v\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  d\n                  \n                    2\n                  \n                \n              \n              \n                r\n              \n            \n            \n              \n                d\n              \n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {a} ={\\mathrm {d} \\mathbf {v}  \\over \\mathrm {d} t}={\\mathrm {d^{2}} \\mathbf {r}  \\over \\mathrm {d} t^{2}}.}\n  \n\nAcceleration represents the velocity's change over time.  Velocity can change in magnitude, direction, or both. Occasionally, a decrease in the magnitude of velocity \"v\" is referred to as deceleration, but generally any change in the velocity over time, including deceleration, is referred to as acceleration."
      },
      {
        "heading": "Frames of reference",
        "level": 3,
        "content": "While the position, velocity and acceleration of a particle can be described with respect to any observer in any state of motion, classical mechanics assumes the existence of a special family of reference frames in which the mechanical laws of nature take a comparatively simple form. These special reference frames are called inertial frames. An inertial frame is an idealized frame of reference within which an object with zero net force acting upon it moves with a constant velocity; that is, it is either at rest or moving uniformly in a straight line. In an inertial frame Newton's law of motion, \n  \n    \n      \n        F\n        =\n        m\n        a\n      \n    \n    {\\displaystyle F=ma}\n  \n, is valid.: 185 \nNon-inertial reference frames accelerate in relation to another inertial frame. A body rotating with respect to an inertial frame is not an inertial frame. When viewed from an inertial frame, particles in the non-inertial frame appear to move in ways not explained by forces from existing fields in the reference frame. Hence, it appears that there are other forces that enter the equations of motion solely as a result of the relative acceleration. These forces are referred to as fictitious forces, inertia forces, or pseudo-forces.\nConsider two reference frames S and S'. For observers in each of the reference frames an event has space-time coordinates of (x,y,z,t) in frame S and (x',y',z',t') in frame S'. Assuming time is measured the same in all reference frames, if we require x = x' when t = 0, then the relation between the space-time coordinates of the same event observed from the reference frames S' and S, which are moving at a relative velocity u in the x direction, is:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  x\n                  ′\n                \n              \n              \n                \n                =\n                x\n                −\n                t\n                u\n                ,\n              \n            \n            \n              \n                \n                  y\n                  ′\n                \n              \n              \n                \n                =\n                y\n                ,\n              \n            \n            \n              \n                \n                  z\n                  ′\n                \n              \n              \n                \n                =\n                z\n                ,\n              \n            \n            \n              \n                \n                  t\n                  ′\n                \n              \n              \n                \n                =\n                t\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}x'&=x-tu,\\\\y'&=y,\\\\z'&=z,\\\\t'&=t.\\end{aligned}}}\n  \n\nThis set of formulas defines a group transformation known as the Galilean transformation (informally, the Galilean transform). This group is a limiting case of the Poincaré group used in special relativity. The limiting case applies when the velocity u is very small compared to c, the speed of light.\nThe transformations have the following consequences:\n\nv′ = v − u (the velocity v′ of a particle from the perspective of S′ is slower by u than its velocity v from the perspective of S)\na′ = a (the acceleration of a particle is the same in any inertial reference frame)\nF′ = F (the force on a particle is the same in any inertial reference frame)\nthe  speed of light is not a constant in classical mechanics, nor does the special position given to the speed of light in relativistic mechanics have a counterpart in classical mechanics.\nFor some problems, it is convenient to use rotating coordinates (reference frames). Thereby one can either keep a mapping to a convenient inertial frame, or introduce additionally a fictitious centrifugal force and Coriolis force."
      },
      {
        "heading": "Newtonian mechanics",
        "level": 1,
        "content": "A force in physics is any action that causes an object's velocity to change; that is, to accelerate.  A force originates from within a field, such as an electro-static field (caused by static electrical charges), electro-magnetic field (caused by moving charges), or gravitational field (caused by mass), among others.\nNewton was the first to mathematically express the relationship between force and momentum. Some physicists interpret Newton's second law of motion as a definition of force and mass, while others consider it a fundamental postulate, a law of nature. Either interpretation has the same mathematical consequences, historically known as \"Newton's Second Law\":\n\n  \n    \n      \n        \n          F\n        \n        =\n        \n          \n            \n              \n                d\n              \n              \n                p\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          \n            \n              \n                d\n              \n              (\n              m\n              \n                v\n              \n              )\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} ={\\mathrm {d} \\mathbf {p}  \\over \\mathrm {d} t}={\\mathrm {d} (m\\mathbf {v} ) \\over \\mathrm {d} t}.}\n  \n\nThe quantity mv is called the (canonical) momentum. The net force on a particle is thus equal to the rate of change of the momentum of the particle with time. Since the definition of acceleration is a = dv/dt, the second law can be written in the simplified and more familiar form:\n\n  \n    \n      \n        \n          F\n        \n        =\n        m\n        \n          a\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =m\\mathbf {a} \\,.}\n  \n\nSo long as the force acting on a particle is known, Newton's second law is sufficient to describe the motion of a particle. Once independent relations for each force acting on a particle are available, they can be substituted into Newton's second law to obtain an ordinary differential equation, which is called the equation of motion.\nAs an example, assume that friction is the only force acting on the particle, and that it may be modeled as a function of the velocity of the particle, for example:\n\n  \n    \n      \n        \n          \n            F\n          \n          \n            \n              R\n            \n          \n        \n        =\n        −\n        λ\n        \n          v\n        \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {F} _{\\rm {R}}=-\\lambda \\mathbf {v} \\,,}\n  \n\nwhere λ is a positive constant, the negative sign states that the force is opposite the sense of the velocity. Then the equation of motion is\n\n  \n    \n      \n        −\n        λ\n        \n          v\n        \n        =\n        m\n        \n          a\n        \n        =\n        m\n        \n          \n            \n              \n                d\n              \n              \n                v\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle -\\lambda \\mathbf {v} =m\\mathbf {a} =m{\\mathrm {d} \\mathbf {v}  \\over \\mathrm {d} t}\\,.}\n  \n\nThis can be integrated to obtain\n\n  \n    \n      \n        \n          v\n        \n        =\n        \n          \n            v\n          \n          \n            0\n          \n        \n        \n          e\n          \n            \n              −\n              λ\n              t\n            \n            \n              /\n            \n            \n              m\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {v} =\\mathbf {v} _{0}e^{{-\\lambda t}/{m}}}\n  \n\nwhere v0 is the initial velocity. This means that the velocity of this particle decays exponentially to zero as time progresses. In this case, an equivalent viewpoint is that the kinetic energy of the particle is absorbed by friction (which converts it to heat energy in accordance with the conservation of energy), and the particle is slowing down. This expression can be further integrated to obtain the position r of the particle as a function of time.\nImportant forces include the gravitational force and the Lorentz force for electromagnetism. In addition, Newton's third law can sometimes be used to deduce the forces acting on a particle: if it is known that particle A exerts a force F on another particle B, it follows that B must exert an equal and opposite reaction force, −F, on A. The strong form of Newton's third law requires that F and −F act along the line connecting A and B, while the weak form does not. Illustrations of the weak form of Newton's third law are often found for magnetic forces."
      },
      {
        "heading": "Work and energy",
        "level": 2,
        "content": "If a constant force F is applied to a particle that makes a displacement Δr, the work done by the force is defined as the scalar product of the force and displacement vectors:\n\n  \n    \n      \n        W\n        =\n        \n          F\n        \n        ⋅\n        Δ\n        \n          r\n        \n        \n        .\n      \n    \n    {\\displaystyle W=\\mathbf {F} \\cdot \\Delta \\mathbf {r} \\,.}\n  \n\nMore generally, if the force varies as a function of position as the particle moves from r1 to r2 along a path C, the work done on the particle is given by the line integral\n\n  \n    \n      \n        W\n        =\n        \n          ∫\n          \n            C\n          \n        \n        \n          F\n        \n        (\n        \n          r\n        \n        )\n        ⋅\n        \n          d\n        \n        \n          r\n        \n        \n        .\n      \n    \n    {\\displaystyle W=\\int _{C}\\mathbf {F} (\\mathbf {r} )\\cdot \\mathrm {d} \\mathbf {r} \\,.}\n  \n\nIf the work done in moving the particle from r1 to r2 is the same no matter what path is taken, the force is said to be conservative. Gravity is a conservative force, as is the force due to an idealized spring, as given by Hooke's law. The force due to friction is non-conservative.\nThe kinetic energy Ek of a particle of mass m travelling at speed v is given by\n\n  \n    \n      \n        \n          E\n          \n            \n              k\n            \n          \n        \n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        m\n        \n          v\n          \n            2\n          \n        \n        \n        .\n      \n    \n    {\\displaystyle E_{\\mathrm {k} }={\\tfrac {1}{2}}mv^{2}\\,.}\n  \n\nFor extended objects composed of many particles, the kinetic energy of the composite body is the sum of the kinetic energies of the particles.\nThe work–energy theorem states that for a particle of constant mass m, the total work W done on the particle as it moves from position r1 to r2 is equal to the change in kinetic energy Ek of the particle:\n\n  \n    \n      \n        W\n        =\n        Δ\n        \n          E\n          \n            \n              k\n            \n          \n        \n        =\n        \n          E\n          \n            \n              \n                k\n                \n                  2\n                \n              \n            \n          \n        \n        −\n        \n          E\n          \n            \n              \n                k\n                \n                  1\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        m\n        \n          (\n          \n            \n              v\n              \n                2\n              \n              \n                \n                2\n              \n            \n            −\n            \n              v\n              \n                1\n              \n              \n                \n                2\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle W=\\Delta E_{\\mathrm {k} }=E_{\\mathrm {k_{2}} }-E_{\\mathrm {k_{1}} }={\\tfrac {1}{2}}m\\left(v_{2}^{\\,2}-v_{1}^{\\,2}\\right).}\n  \n\nConservative forces can be expressed as the gradient of a scalar function, known as the potential energy and denoted Ep:\n\n  \n    \n      \n        \n          F\n        \n        =\n        −\n        \n          ∇\n        \n        \n          E\n          \n            \n              p\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =-\\mathbf {\\nabla } E_{\\mathrm {p} }\\,.}\n  \n\nIf all the forces acting on a particle are conservative, and Ep is the total potential energy (which is defined as a work of involved forces to rearrange mutual positions of bodies), obtained by summing the potential energies corresponding to each force\n\n  \n    \n      \n        \n          F\n        \n        ⋅\n        Δ\n        \n          r\n        \n        =\n        −\n        \n          ∇\n        \n        \n          E\n          \n            \n              p\n            \n          \n        \n        ⋅\n        Δ\n        \n          r\n        \n        =\n        −\n        Δ\n        \n          E\n          \n            \n              p\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} \\cdot \\Delta \\mathbf {r} =-\\mathbf {\\nabla } E_{\\mathrm {p} }\\cdot \\Delta \\mathbf {r} =-\\Delta E_{\\mathrm {p} }\\,.}\n  \n\nThe decrease in the potential energy is equal to the increase in the kinetic energy\n\n  \n    \n      \n        −\n        Δ\n        \n          E\n          \n            \n              p\n            \n          \n        \n        =\n        Δ\n        \n          E\n          \n            \n              k\n            \n          \n        \n        ⇒\n        Δ\n        (\n        \n          E\n          \n            \n              k\n            \n          \n        \n        +\n        \n          E\n          \n            \n              p\n            \n          \n        \n        )\n        =\n        0\n        \n        .\n      \n    \n    {\\displaystyle -\\Delta E_{\\mathrm {p} }=\\Delta E_{\\mathrm {k} }\\Rightarrow \\Delta (E_{\\mathrm {k} }+E_{\\mathrm {p} })=0\\,.}\n  \n\nThis result is known as conservation of energy and states that the total energy,\n\n  \n    \n      \n        ∑\n        E\n        =\n        \n          E\n          \n            \n              k\n            \n          \n        \n        +\n        \n          E\n          \n            \n              p\n            \n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle \\sum E=E_{\\mathrm {k} }+E_{\\mathrm {p} }\\,,}\n  \n\nis constant in time. It is often useful, because many commonly encountered forces are conservative."
      },
      {
        "heading": "Lagrangian mechanics",
        "level": 1,
        "content": "Lagrangian mechanics is a formulation of classical mechanics founded on the stationary-action principle (also known as the principle of least action). It was introduced by the Italian-French mathematician and astronomer Joseph-Louis Lagrange in his presentation to the Turin Academy of Science in 1760 culminating in his 1788 grand opus, Mécanique analytique. Lagrangian mechanics describes a mechanical system as a pair \n  \n    \n      \n        (\n        M\n        ,\n        L\n        )\n      \n    \n    {\\textstyle (M,L)}\n  \n consisting of a configuration space \n  \n    \n      \n        M\n      \n    \n    {\\textstyle M}\n  \n and a smooth function \n  \n    \n      \n        L\n      \n    \n    {\\textstyle L}\n  \n within that space called a Lagrangian. For many systems, \n  \n    \n      \n        L\n        =\n        T\n        −\n        V\n        ,\n      \n    \n    {\\textstyle L=T-V,}\n  \n where \n  \n    \n      \n        T\n      \n    \n    {\\textstyle T}\n  \n and \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n are the kinetic and potential energy of the system, respectively. The stationary action principle requires that the action functional of the system derived from \n  \n    \n      \n        L\n      \n    \n    {\\textstyle L}\n  \n must remain at a stationary point (a maximum, minimum, or saddle) throughout the time evolution of the system. This constraint allows the calculation of the equations of motion of the system using Lagrange's equations."
      },
      {
        "heading": "Hamiltonian mechanics",
        "level": 1,
        "content": "Hamiltonian mechanics emerged in 1833 as a reformulation of Lagrangian mechanics. Introduced by Sir William Rowan Hamilton, Hamiltonian mechanics replaces (generalized) velocities \n  \n    \n      \n        \n          \n            \n              \n                q\n                ˙\n              \n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\dot {q}}^{i}}\n  \n used in Lagrangian mechanics with (generalized) momenta. Both theories provide interpretations of classical mechanics and describe the same physical phenomena. Hamiltonian mechanics has a close relationship with geometry (notably, symplectic geometry and Poisson structures) and serves as a link between classical and quantum mechanics.\nIn this formalism, the dynamics of a system are governed by Hamilton's equations, which express the time derivatives of position and momentum variables in terms of partial derivatives of a function called the Hamiltonian:\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                q\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              \n                \n                  H\n                \n              \n            \n            \n              ∂\n              \n                p\n              \n            \n          \n        \n        ,\n        \n        \n          \n            \n              \n                d\n              \n              \n                p\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        −\n        \n          \n            \n              ∂\n              \n                \n                  H\n                \n              \n            \n            \n              ∂\n              \n                q\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} {\\boldsymbol {q}}}{\\mathrm {d} t}}={\\frac {\\partial {\\mathcal {H}}}{\\partial {\\boldsymbol {p}}}},\\quad {\\frac {\\mathrm {d} {\\boldsymbol {p}}}{\\mathrm {d} t}}=-{\\frac {\\partial {\\mathcal {H}}}{\\partial {\\boldsymbol {q}}}}.}\n  \n\nThe Hamiltonian is the Legendre transform of the Lagrangian, and in many situations of physical interest it is equal to the total energy of the system."
      },
      {
        "heading": "Limits of validity",
        "level": 1,
        "content": "Many branches of classical mechanics are simplifications or approximations of more accurate forms; two of the most accurate being general relativity and relativistic statistical mechanics. Geometric optics is an approximation to the quantum theory of light, and does not have a superior \"classical\" form.\nWhen both quantum mechanics and classical mechanics cannot apply, such as at the quantum level with many degrees of freedom, quantum field theory (QFT) is of use. QFT deals with small distances, and large speeds with many degrees of freedom as well as the possibility of any change in the number of particles throughout the interaction. When treating large degrees of freedom at the macroscopic level, statistical mechanics becomes useful. Statistical mechanics describes the behavior of large (but countable) numbers of particles and their interactions as a whole at the macroscopic level. Statistical mechanics is mainly used in thermodynamics for systems that lie outside the bounds of the assumptions of classical thermodynamics. In the case of high velocity objects approaching the speed of light, classical mechanics is enhanced by special relativity. In case that objects become extremely heavy (i.e., their Schwarzschild radius is not negligibly small for a given application), deviations from Newtonian mechanics become apparent and can be quantified by using the parameterized post-Newtonian formalism. In that case, general relativity (GR) becomes applicable. However, until now there is no theory of quantum gravity unifying GR and QFT in the sense that it could be used when objects become extremely small and heavy.[4][5]"
      },
      {
        "heading": "Newtonian approximation to special relativity",
        "level": 2,
        "content": "In special relativity, the momentum of a particle is given by\n\n  \n    \n      \n        \n          p\n        \n        =\n        \n          \n            \n              m\n              \n                v\n              \n            \n            \n              1\n              −\n              \n                \n                  \n                    v\n                    \n                      2\n                    \n                  \n                  \n                    c\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {p} ={\\frac {m\\mathbf {v} }{\\sqrt {1-{\\frac {v^{2}}{c^{2}}}}}}\\,,}\n  \n\nwhere m is the particle's rest mass, v its velocity, v is the modulus of v, and c is the speed of light.\nIf v is very small compared to c, v2/c2 is approximately zero, and so\n\n  \n    \n      \n        \n          p\n        \n        ≈\n        m\n        \n          v\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {p} \\approx m\\mathbf {v} \\,.}\n  \n\nThus the Newtonian equation p = mv is an approximation of the relativistic equation for bodies moving with low speeds compared to the speed of light.\nFor example, the relativistic cyclotron frequency of a cyclotron, gyrotron, or high voltage magnetron is given by\n\n  \n    \n      \n        f\n        =\n        \n          f\n          \n            \n              c\n            \n          \n        \n        \n          \n            \n              m\n              \n                0\n              \n            \n            \n              \n                m\n                \n                  0\n                \n              \n              +\n              \n                \n                  T\n                  \n                    c\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle f=f_{\\mathrm {c} }{\\frac {m_{0}}{m_{0}+{\\frac {T}{c^{2}}}}}\\,,}\n  \n\nwhere fc is the classical frequency of an electron (or other charged particle) with kinetic energy T and (rest) mass m0 circling in a magnetic field. The (rest) mass of an electron is 511 keV. So the frequency correction is 1% for a magnetic vacuum tube with a 5.11 kV direct current accelerating voltage."
      },
      {
        "heading": "Classical approximation to quantum mechanics",
        "level": 2,
        "content": "The ray approximation of classical mechanics breaks down when the de Broglie wavelength is not much smaller than other dimensions of the system. For non-relativistic particles, this wavelength is\n\n  \n    \n      \n        λ\n        =\n        \n          \n            h\n            p\n          \n        \n      \n    \n    {\\displaystyle \\lambda ={\\frac {h}{p}}}\n  \n\nwhere h is the Planck constant and p is the momentum.\nAgain, this happens with electrons before it happens with heavier particles. For example, the electrons used by Clinton Davisson and Lester Germer in 1927, accelerated by 54 V, had a wavelength of 0.167 nm, which was long enough to exhibit a single diffraction side lobe when reflecting from the face of a nickel crystal with atomic spacing of 0.215 nm. With a larger vacuum chamber, it would seem relatively easy to increase the angular resolution from around a radian to a milliradian and see quantum diffraction from the periodic patterns of integrated circuit computer memory.\nMore practical examples of the failure of classical mechanics on an engineering scale are conduction by quantum tunneling in tunnel diodes and very narrow transistor gates in integrated circuits.\nClassical mechanics is the same extreme high frequency approximation as geometric optics. It is more often accurate because it describes particles and bodies with rest mass. These have more momentum and therefore shorter De Broglie wavelengths than massless particles, such as light, with the same kinetic energies."
      },
      {
        "heading": "History",
        "level": 1,
        "content": "The study of the motion of bodies is an ancient one, making classical mechanics one of the oldest and largest subjects in science, engineering, and technology. The development of classical mechanics lead to the development of many areas of mathematics.: 54  \nSome Greek philosophers of antiquity, among them Aristotle, founder of Aristotelian physics, may have been the first to maintain the idea that \"everything happens for a reason\" and that theoretical principles can assist in the understanding of nature. While to a modern reader, many of these preserved ideas come forth as eminently reasonable, there is a conspicuous lack of both mathematical theory and controlled experiment, as we know it. These later became decisive factors in forming modern science, and their early application came to be known as classical mechanics. In his Elementa super demonstrationem ponderum, medieval mathematician Jordanus de Nemore introduced the concept of \"positional gravity\" and the use of component forces.\n\nThe first published causal explanation of the motions of planets was Johannes Kepler's Astronomia nova, published in 1609. He concluded, based on Tycho Brahe's observations on the orbit of Mars, that the planet's orbits were ellipses. This break with ancient thought was happening around the same time that Galileo was proposing abstract mathematical laws for the motion of objects. He may (or may not) have performed the famous experiment of dropping two cannonballs of different weights from the tower of Pisa, showing that they both hit the ground at the same time. The reality of that particular experiment is disputed, but he did carry out quantitative experiments by rolling balls on an inclined plane. His theory of accelerated motion was derived from the results of such experiments and forms a cornerstone of classical mechanics. In 1673 Christiaan Huygens described in his Horologium Oscillatorium the first two laws of motion. The work is also the first modern treatise in which a physical problem (the accelerated motion of a falling body) is idealized by a set of parameters then analyzed mathematically and constitutes one of the seminal works of applied mathematics. \n\nNewton founded his principles of natural philosophy on three proposed laws of motion: the law of inertia, his second law of acceleration (mentioned above), and the law of action and reaction; and hence laid the foundations for classical mechanics. Both Newton's second and third laws were given the proper scientific and mathematical treatment in Newton's Philosophiæ Naturalis Principia Mathematica. Here they are distinguished from earlier attempts at explaining similar phenomena, which were either incomplete, incorrect, or given little accurate mathematical expression. Newton also enunciated the principles of conservation of momentum and angular momentum. In mechanics, Newton was also the first to provide the first correct scientific and mathematical formulation of gravity in Newton's law of universal gravitation. The combination of Newton's laws of motion and gravitation provides the fullest and most accurate description of classical mechanics. He demonstrated that these laws apply to everyday objects as well as to celestial objects. In particular, he obtained a theoretical explanation of Kepler's laws of motion of the planets.\nNewton had previously invented the calculus; however, the Principia was formulated entirely in terms of long-established geometric methods in emulation of Euclid. Newton, and most of his contemporaries, with the notable exception of Huygens, worked on the assumption that classical mechanics would be able to explain all phenomena, including light, in the form of geometric optics. Even when discovering the so-called Newton's rings (a wave interference phenomenon) he maintained his own corpuscular theory of light.\n\nAfter Newton, classical mechanics became a principal field of study in mathematics as well as physics. Mathematical formulations progressively allowed finding solutions to a far greater number of problems. The first notable mathematical treatment was in 1788 by Joseph Louis Lagrange. Lagrangian mechanics was in turn re-formulated in 1833 by William Rowan Hamilton.\n\nSome difficulties were discovered in the late 19th century that could only be resolved by more modern physics. Some of these difficulties related to compatibility with electromagnetic theory, and the famous Michelson–Morley experiment. The resolution of these problems led to the special theory of relativity, often still considered a part of classical mechanics.\nA second set of difficulties were related to thermodynamics. When combined with thermodynamics, classical mechanics leads to the Gibbs paradox of classical statistical mechanics, in which entropy is not a well-defined quantity. Black-body radiation was not explained without the introduction of quanta. As experiments reached the atomic level, classical mechanics failed to explain, even approximately, such basic things as the energy levels and sizes of atoms and the photo-electric effect. The effort at resolving these problems led to the development of quantum mechanics.\nSince the end of the 20th century, classical mechanics in physics has no longer been an independent theory. Instead, classical mechanics is now considered an approximate theory to the more general quantum mechanics. Emphasis has shifted to understanding the fundamental forces of nature as in the Standard Model and its more modern extensions into a unified theory of everything. Classical mechanics is a theory useful for the study of the motion of non-quantum mechanical, low-energy particles in weak gravitational fields."
      },
      {
        "heading": "See also",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Notes",
        "level": 1,
        "content": ""
      },
      {
        "heading": "References",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Further reading",
        "level": 1,
        "content": "Alonso, M.; Finn, J. (1992). Fundamental University Physics. Addison-Wesley.\nFeynman, Richard (1999). The Feynman Lectures on Physics. Perseus Publishing. ISBN 978-0-7382-0092-7.\nFeynman, Richard; Phillips, Richard (1998). Six Easy Pieces. Perseus Publishing. ISBN 978-0-201-32841-7.\nGoldstein, Herbert; Charles P. Poole; John L. Safko (2002). Classical Mechanics (3rd ed.). Addison Wesley. ISBN 978-0-201-65702-9.\nKibble, Tom W.B.; Berkshire, Frank H. (2004). Classical Mechanics (5th ed.). Imperial College Press. ISBN 978-1-86094-424-6.\nKleppner, D.; Kolenkow, R.J. (1973). An Introduction to Mechanics. McGraw-Hill. ISBN 978-0-07-035048-9.\nLandau, L.D.; Lifshitz, E.M. (1972). Course of Theoretical Physics, Vol. 1 – Mechanics. Franklin Book Company. ISBN 978-0-08-016739-8.\nMorin, David (2008). Introduction to Classical Mechanics: With Problems and Solutions (1st ed.). Cambridge: Cambridge University Press. ISBN 978-0-521-87622-3.\nGerald Jay Sussman; Jack Wisdom (2001). Structure and Interpretation of Classical Mechanics. MIT Press. ISBN 978-0-262-19455-6.\nO'Donnell, Peter J. (2015). Essential Dynamics and Relativity. CRC Press. ISBN 978-1-4665-8839-4.\nThornton, Stephen T.; Marion, Jerry B. (2003). Classical Dynamics of Particles and Systems (5th ed.). Brooks Cole. ISBN 978-0-534-40896-1."
      },
      {
        "heading": "External links",
        "level": 1,
        "content": "\nCrowell, Benjamin. Light and Matter (an introductory text, uses algebra with optional sections involving calculus)\nFitzpatrick, Richard. Classical Mechanics (uses calculus)\nHoiland, Paul (2004). Preferred Frames of Reference & Relativity\nHorbatsch, Marko, \"Classical Mechanics Course Notes\".\nRosu, Haret C., \"Classical Mechanics\". Physics Education. 1999. [arxiv.org : physics/9909035]\nShapiro, Joel A. (2003). Classical Mechanics\nSussman, Gerald Jay & Wisdom, Jack &  Mayer, Meinhard E. (2001). Structure and Interpretation of Classical Mechanics\nTong, David. Classical Dynamics (Cambridge lecture notes on Lagrangian and Hamiltonian formalism)\nKinematic Models for Design Digital Library (KMODDL) Movies and photos of hundreds of working mechanical-systems models at Cornell University. Also includes an e-book library of classic texts on mechanical design and engineering.\nMIT OpenCourseWare 8.01: Classical Mechanics Free videos of actual course lectures with links to lecture notes, assignments and exams.\nAlejandro A. Torassa, On Classical Mechanics"
      }
    ],
    "summary": "Classical mechanics is a physical theory describing the motion of objects such as projectiles, parts of machinery, spacecraft, planets, stars, and galaxies. The development of classical mechanics involved substantial change in the methods and philosophy of physics. The qualifier classical distinguishes this type of mechanics from physics developed after the revolutions in physics of the early 20th century, all of which revealed limitations in classical mechanics.\nThe earliest formulation of classical mechanics is often referred to as Newtonian mechanics. It consists of the physical concepts based on the 17th century foundational works of Sir Isaac Newton, and the mathematical methods invented by Newton, Gottfried Wilhelm Leibniz, Leonhard Euler and others to describe the motion of bodies under the influence of forces. Later, methods based on energy were developed by Euler, Joseph-Louis Lagrange, William Rowan Hamilton and others, leading to the development of analytical mechanics (which includes Lagrangian mechanics and Hamiltonian mechanics). These advances, made predominantly in the 18th and 19th centuries, extended beyond earlier works; they are, with some modification, used in all areas of modern physics.\nIf the present state of an object that obeys the laws of classical mechanics is known, it is possible to determine how it will move in the future, and how it has moved in the past. Chaos theory shows that the long term predictions of classical mechanics are not reliable. Classical mechanics provides accurate results when studying objects that are not extremely massive and have speeds not approaching the speed of light. With objects  about the size of an atom's diameter, it becomes necessary to use quantum mechanics. To describe velocities approaching the speed of light, special relativity is needed. In cases where objects become extremely massive, general relativity becomes applicable. Some modern sources include relativistic mechanics in classical physics, as representing the field in its most developed and accurate form."
  },
  {
    "title": "Mecánica clásica",
    "source": "https://es.wikipedia.org/wiki/Mec%C3%A1nica_cl%C3%A1sica",
    "language": "es",
    "chunks": [
      {
        "heading": "Historia",
        "level": 1,
        "content": "El estudio del movimiento de los cuerpos es muy antiguo, lo que convierte a la mecánica clásica en una de las materias más antiguas y extensas de la ciencia, la ingeniería y la tecnología.\nAlgunos  filósofos griegos de la antigüedad, entre ellos Aristóteles, fundador de la física aristotélica, pueden haber sido los primeros en mantener la idea de que \"todo sucede por una razón\" y que los principios teóricos pueden ayudar a la comprensión de la naturaleza. Mientras que para un lector moderno, muchas de estas ideas conservadas se presentan como eminentemente razonables, hay una llamativa falta tanto de teoría matemática como de experimento controlado, tal y como lo conocemos. Estos se convirtieron más tarde en factores decisivos en la formación de la ciencia moderna, y su aplicación temprana llegó a conocerse como mecánica clásica. En su Elementa super demonstrationem ponderum, el matemático medieval Jordanus Nemorarius introdujo el concepto de \"gravedad posicional\" y el uso de las fuerzas componentes.\nLa primera explicación  causal publicada sobre los movimientos de los planetas fue la Astronomia nova de Johannes Kepler, publicada en 1609. Concluyó, basándose en las observaciones de Tycho Brahe sobre la órbita de  Marte, que las órbitas de los planetas eran elípticas. Esta ruptura con la pensamiento antiguo se produjo en la misma época en que Galileo proponía leyes matemáticas abstractas para el movimiento de los objetos. Es posible (o no) que realizara el famoso experimento de dejar caer dos balas de cañón de distinto peso desde la torre de Pisa, demostrando que ambas caían al suelo al mismo tiempo. La realidad de ese experimento en particular es discutida, pero realizó experimentos cuantitativos haciendo rodar bolas sobre un plano inclinado. Su teoría del movimiento acelerado se derivó de los resultados de tales experimentos y constituye una piedra angular de la mecánica clásica.\n\nNewton fundó sus principios de filosofía natural en  tres leyes del movimiento propuestas: la «ley de la inercia», su «segunda ley de aceleración» (mencionada anteriormente) y la «ley de acción y reacción»; y de ahí sentó las bases de la mecánica clásica. Tanto la segunda como la tercera ley de Newton recibieron el tratamiento científico y matemático adecuado en la Philosophiæ naturalis principia mathematica de Newton. Aquí se distinguen de los intentos anteriores de explicar fenómenos similares, que eran incompletos, incorrectos o tenían una expresión matemática poco precisa. Newton también enunció los principios de  conservación del momento y el momento angular. En mecánica, Newton también fue el primero en proporcionar la primera formulación científica y matemática correcta de la gravedad en la  ley de gravitación universal de Newton. La combinación de las leyes del movimiento y la gravitación de Newton proporciona la descripción más completa y precisa de la mecánica clásica. Demostró que estas leyes se aplican tanto a los objetos cotidianos como a los celestes. En particular, obtuvo una explicación teórica de las  leyes de movimiento de los planetas de Kepler.\n\nNewton había inventado previamente el cálculo de las matemáticas y lo utilizó para realizar los cálculos matemáticos. Para la aceptabilidad, su libro, los Principia, fue formulado enteramente en términos de métodos geométricos establecidos desde hace mucho tiempo, que pronto fueron eclipsados por su cálculo. Sin embargo, fue Leibniz quien desarrolló la notación de la derivada y la integral preferida en la actualidad. Newton, y la mayoría de sus contemporáneos, con la notable excepción de Huygens, trabajaron sobre el supuesto de que la mecánica clásica sería capaz de explicar todos los fenómenos, incluida la luz, en forma de óptica geométrica. Incluso cuando descubrió los llamados anillos de Newton (un fenómeno de interferencia de ondas) mantuvo su propia teoría corpuscular de la luz.\n\nDespués de Newton, la mecánica clásica se convirtió en un campo de estudio principal tanto en matemáticas como en física. Las formulaciones matemáticas permitieron progresivamente encontrar soluciones a un número mucho mayor de problemas. El primer tratamiento matemático notable fue en 1788 por Joseph Louis Lagrange. La mecánica lagrangiana fue a su vez reformulada en 1833 por William Rowan Hamilton.\n\nSe descubrieron algunas dificultades a finales del siglo XIX que solo podrían resolverse con una física más moderna. Algunas de estas dificultades se relacionaron con la compatibilidad con la teoría electromagnética y el famoso experimento de Michelson-Morley. La resolución de estos problemas condujo a la teoría especial de la relatividad, que a menudo todavía se considera parte de la mecánica clásica.\nUn segundo conjunto de dificultades se relacionó con la termodinámica. Cuando se combina con la termodinámica, la mecánica clásica conduce a la paradoja de Gibbs de la mecánica estadística clásica, en la que la entropía no es una cantidad bien definida. La radiación de cuerpo negro no se explica sin la introducción de cuantos. Cuando los experimentos alcanzaron el nivel atómico, la mecánica clásica no logró explicar, ni siquiera aproximadamente, cosas tan básicas como los niveles y tamaños de energía de los átomos y el efecto fotoeléctrico. El esfuerzo por resolver estos problemas condujo al desarrollo de la mecánica cuántica.\nDesde finales del siglo XX, la mecánica clásica en física ya no ha sido una teoría independiente. En cambio, la mecánica clásica ahora se considera una teoría aproximada a la mecánica cuántica más general. El énfasis se ha desplazado hacia la comprensión de las fuerzas fundamentales de la naturaleza como en el modelo estándar y sus extensiones más modernas en una teoría unificada del todo.[4]​ La mecánica clásica es una teoría útil para el estudio del movimiento de partículas que se mueven a velocidades lejanas a la de la luz en campos gravitacionales débiles. Además, se ha extendido al dominio complejo donde la mecánica clásica compleja exhibe comportamientos muy similares a la mecánica cuántica.[5]​\nUn temprano método científico matemático y experimental fue introducido en la mecánica en el siglo XI por al-Biruni, quien junto con al-Jazini en el siglo XII, unificó la  estática y la dinámica en la ciencia de la mecánica, y combinó los campos de la hidrostática con la dinámica para crear el campo de la hidrodinámica.[6]​ Los conceptos relacionados con las leyes del movimiento de Newton también fueron enunciados por varios otros físicos musulmanes durante la Edad Media. Las primeras versiones de la ley de la inercia, conocida como la primera ley del movimiento de Newton, y el concepto relativo al  momento, parte de la segunda ley del movimiento de Newton, fueron descritos por Ibn al-Haytham (Alhazen)[7]​[8]​ y Avicena.[9]​[10]​ La proporcionalidad entre la fuerza y la aceleración, un principio importante en la mecánica clásica, fue enunciada por primera vez por Abu'l-Barakat,[11]​ y Ibn Bajjah también desarrolló el concepto de una fuerza de reacción.[12]​ Las teorías sobre la gravedad fueron desarrolladas por Banū Mūsā,[13]​ Alhazen,[14]​ y al-Khazini.[15]​\nSe sabe que el tratamiento matemático de Galileo Galilei y su concepto de ímpetus[16]​surgió a partir de análisis medievales anteriores de movimiento, especialmente los de Avicena,[9]​Ibn Bajjah,[17]​y Jean Buridan.[18]​\nLa primera explicación causal publicada sobre los movimientos de los planetas fue la Astronomia nova de Johannes Kepler, publicada en 1609. Concluyó, basándose en las observaciones de Tycho Brahe sobre la órbita de  Marte, que las órbitas de los planetas eran elipses. Esta ruptura con la pensamiento antiguo se produjo en la misma época en que Galileo proponía leyes matemáticas abstractas para el movimiento de los objetos. Es posible (o no) que realizara el famoso experimento de dejar caer dos balas de cañón de distinto peso desde la torre de Pisa, demostrando que ambas caían al suelo al mismo tiempo. La realidad de ese experimento en particular es discutida, pero realizó experimentos cuantitativos haciendo rodar bolas sobre un plano inclinado. Su teoría del movimiento acelerado se derivó de los resultados de tales experimentos y constituye una piedra angular de la mecánica clásica."
      },
      {
        "heading": "Ramas principales",
        "level": 1,
        "content": "La mecánica clásica se dividió tradicionalmente en tres ramas principales:\n\n Estática, el estudio del Equilibrio mecánico y su relación con las fuerzas\nDinámica, el estudio del movimiento y su relación con las fuerzas\nCinemática, que trata de las implicaciones de los movimientos observados sin tener en cuenta las circunstancias que los causan\nOtra división se basa en la elección del formalismo matemático:\n\nMecánica newtoniana\nMecánica lagrangiana\nMecánica hamiltoniana\nAlternativamente, se puede hacer una división por región de aplicación:\n\nMecánica celeste, relacionada con las estrellas, los planetas y otros cuerpos celestes.\nMecánica continua, para materiales modelados como un continuo, por ejemplo, sólidos y fluidos (es decir, líquidos y gases).\nMecánica relativista (es decir, incluyendo las teorías de la especial y general), para cuerpos cuya velocidad es cercana a la de la luz.\nMecánica estadística, que proporciona un marco para relacionar las propiedades microscópicas de los átomos y moléculas individuales con las propiedades macroscópicas o termodinámicas de los materiales."
      },
      {
        "heading": "Aproximaciones de la mecánica clásica",
        "level": 1,
        "content": "La mecánica clásica fue concebida como un sistema que permitiera explicar adecuadamente el movimiento de los cuerpos relacionándolo con las causas que los originan, es decir, las fuerzas. La mecánica clásica busca hacer una descripción tanto cualitativa (¿qué y cómo ocurre?), como cuantitativa (¿en qué cantidad ocurre?) del fenómeno en cuestión. En este sentido, la ciencia mecánica podría ser construida desde dos aproximaciones alternativas: \n\nAproximación empírica\nAproximación analítica"
      },
      {
        "heading": "Aproximación empírica",
        "level": 2,
        "content": "Es aquella fundamentada en la experimentación, esto es, en la observación controlada de un aspecto previamente elegido del medio físico. Un ejemplo puede ayudar a entender este punto: si dejamos caer una pelota de golf desde cierta altura y partiendo del reposo, podemos medir experimentalmente la velocidad que adquiere la pelota para diferentes instantes. Si despreciamos los efectos de la fricción del aire, podremos constatar que, dentro de las inevitables incertidumbres inherentes a las mediciones, la relación de velocidad (v) contra tiempo (t) se ajusta bastante bien a la función lineal de la forma:\n\n  \n    \n      \n        v\n        =\n        \n          g\n        \n        t\n        \n      \n    \n    {\\displaystyle v={\\text{g}}t\\,}\n  \n \n\ndonde «g» representa el valor de la aceleración de la gravedad (9,81 m/s² a nivel del mar y 45 grados de latitud). Así, esta es la aproximación empírica o experimental al fenómeno mecánico estudiado, es decir, la caída libre de un cuerpo."
      },
      {
        "heading": "Aproximación analítica",
        "level": 2,
        "content": "En este caso se parte de una premisa básica (experimentalmente verificable) y, con la ayuda de las herramientas aportadas por cálculo infinitesimal, se deducen ecuaciones y relaciones entre las variables implicadas. Si volvemos al ejemplo anterior: es un hecho de naturaleza experimental, que cuando se deja caer un cuerpo, la aceleración con la que desciende (si se ignora la fricción del aire) es constante e igual a g = 9,81 m/s². Por otra parte, se sabe que la aceleración (en este caso, g) se define matemáticamente como la derivada de la velocidad respecto del tiempo:\n\n  \n    \n      \n        g\n        =\n        \n          \n            \n              d\n              v\n            \n            \n              d\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle g={\\frac {dv}{dt}}}\n  \n\nPor tanto, si se integra esta ecuación diferencial, sabiendo que en el inicio del movimiento (t = 0) la velocidad es nula (v = 0 ), se llega de nuevo a la expresión:\n\n  \n    \n      \n        v\n        =\n        g\n        t\n      \n    \n    {\\displaystyle v=gt}\n  \n\nAsí, esta es la aproximación analítica o teórica al tema en discusión."
      },
      {
        "heading": "Ambas aproximaciones",
        "level": 2,
        "content": "La aproximación empírica establece relaciones entre variables de interés mediante la búsqueda de dependencias o relaciones matemáticas, a partir de resultados experimentales. La aproximación analítica establece relaciones entre variables de interés a partir de premisas y de las herramientas que proporciona el cálculo.\nAsí, se busca derivar conclusiones y expresiones útiles a partir del razonamiento deductivo y el formalismo matemático. Si se extrema este argumento, la Mecánica Racional podría ser considerada una rama de las matemáticas, donde se juega con relaciones entre variables físicas, y se obtienen a partir de ellas ecuaciones útiles y aplicaciones prácticas."
      },
      {
        "heading": "Principios básicos e invariantes",
        "level": 1,
        "content": "Los principios básicos de la mecánica clásica son los siguientes:\n\nEl Principio de Hamilton o principio de mínima acción (del cual las leyes de Newton son una consecuencia).\nLa existencia de un tiempo absoluto, cuya medida es igual para cualquier observador con independencia de su grado de movimiento.\nEl estado de una partícula queda completamente determinado si se conoce su cantidad de movimiento y posición siendo estas simultáneamente medibles. Indirectamente, este enunciado puede ser reformulado por el principio de causalidad. En este caso se habla de predictibilidad teóricamente infinita: matemáticamente si en un determinado instante se conocieran (con precisión infinita) las posiciones y velocidades de un sistema finito de N partículas teóricamente pueden ser conocidas las posiciones y velocidades futuras, ya que en principio existen las funciones vectoriales \n  \n    \n      \n        \n          {\n          \n            \n              \n                \n                  r\n                  →\n                \n              \n            \n            \n              i\n            \n          \n          =\n          \n            \n              \n                \n                  r\n                  →\n                \n              \n            \n            \n              i\n            \n          \n          (\n          t\n          ;\n          \n            \n              \n                \n                  r\n                  →\n                \n              \n            \n            \n              i\n              ,\n              0\n            \n          \n          ,\n          \n            \n              \n                \n                  v\n                  →\n                \n              \n            \n            \n              i\n              ,\n              0\n            \n          \n          \n            }\n            \n              i\n              =\n              1\n            \n            \n              N\n            \n          \n        \n      \n    \n    {\\displaystyle \\displaystyle \\{{\\vec {r}}_{i}={\\vec {r}}_{i}(t;{\\vec {r}}_{i,0},{\\vec {v}}_{i,0}\\}_{i=1}^{N}}\n  \n que proporcionan las posiciones de las partículas en cualquier instante de tiempo. Estas funciones se obtienen de unas ecuaciones generales denominadas ecuaciones de movimiento que se manifiestan de forma diferencial relacionando magnitudes y sus derivadas. Las funciones \n  \n    \n      \n        \n          {\n          \n            \n              \n                \n                  r\n                  \n                    i\n                  \n                \n                →\n              \n            \n          \n          (\n          t\n          )\n          \n            }\n            \n              i\n              =\n              1\n            \n            \n              N\n            \n          \n        \n      \n    \n    {\\displaystyle \\displaystyle \\{{\\vec {r_{i}}}(t)\\}_{i=1}^{N}}\n  \n se obtienen por integración, una vez conocida la naturaleza física del problema y las condiciones iniciales.\nEs interesante notar que en mecánica relativista el supuesto (2) es inaceptable aunque sí son aceptables los supuestos (1) y (3). Por otro lado, en mecánica cuántica no es aceptable el supuesto (3) (en la mecánica cuántica relativista ni el supuesto (2) ni el (3) son aceptables).\n\nAunque la mecánica clásica y en particular la mecánica newtoniana es adecuada para describir la experiencia diaria (con eventos que suceden a velocidades muchísimo menores que la velocidad de la luz y a escala macroscópica), debido a la aceptación de estos tres supuestos tan restrictivos como (1), (2) y (3), no puede describir adecuadamente fenómenos electromagnéticos con partículas en rápido movimiento, ni fenómenos físicos microscópicos que suceden a escala atómica.\nSin embargo, esto no es un demérito de la teoría ya que la simplicidad de la misma se combina con la adecuación descriptiva para sistemas cotidianos como: cohetes, movimiento de planetas, moléculas orgánicas, trompos, trenes y trayectorias de móviles macroscópicos en general. Para estos sistemas cotidianos es muy complicado siquiera describir sus movimientos en términos de las teorías más generales como:\n\nLa mecánica relativista, que va más allá de la mecánica clásica y trata con objetos moviéndose a velocidades relativamente cercanas a la velocidad de la luz. En mecánica relativista siguen siendo válidos los supuestos básicos 1 y 3 aunque no el 2.\nLa mecánica cuántica, que trata con sistemas de reducidas dimensiones (a escala semejante a la atómica), y la teoría cuántica de campos (ver tb. campo), que trata con sistemas que exhiben ambas propiedades. En mecánica cuántica son válidos los supuestos básicos 1 y 2, pero no el 3. Mientras que en teoría cuántica de campos solo se mantiene el supuesto 1."
      },
      {
        "heading": "Mecánica newtoniana",
        "level": 1,
        "content": "La mecánica newtoniana o mecánica vectorial es una formulación específica de la mecánica clásica que estudia el movimiento de partículas y sólidos en un espacio euclídeo tridimensional. Aunque la teoría es generalizable, la formulación básica de la misma se hace en sistemas de referencia inerciales donde las ecuaciones básicas del movimiento se reducen a las Leyes de Newton, en honor a Isaac Newton quien hizo contribuciones fundamentales a esta teoría.\nEn mecánica vectorial precisamos de tres ecuaciones escalares, o una ecuación vectorial, para el caso más simple de una sola partícula:\n\n  \n    \n      \n        \n          \n            \n              \n                p\n              \n              ˙\n            \n          \n        \n        =\n        m\n        \n          \n            \n              \n                \n              \n              \n                \n                  d\n                  \n                    v\n                  \n                \n              \n            \n            \n              \n                \n              \n              \n                \n                  d\n                  t\n                \n              \n            \n          \n        \n        =\n        \n          F\n        \n        ,\n        \n        \n          \n            {\n            \n              \n                \n                  \n                    \n                      \n                        \n                          p\n                          ˙\n                        \n                      \n                    \n                    \n                      x\n                    \n                  \n                  =\n                  m\n                  \n                    \n                      \n                        \n                          \n                        \n                        \n                          \n                            d\n                            \n                              v\n                              \n                                x\n                              \n                            \n                          \n                        \n                      \n                      \n                        \n                          \n                        \n                        \n                          \n                            d\n                            t\n                          \n                        \n                      \n                    \n                  \n                  =\n                  \n                    F\n                    \n                      x\n                    \n                  \n                \n              \n              \n                \n                  \n                    \n                      \n                        \n                          p\n                          ˙\n                        \n                      \n                    \n                    \n                      y\n                    \n                  \n                  =\n                  m\n                  \n                    \n                      \n                        \n                          \n                        \n                        \n                          \n                            d\n                            \n                              v\n                              \n                                y\n                              \n                            \n                          \n                        \n                      \n                      \n                        \n                          \n                        \n                        \n                          \n                            d\n                            t\n                          \n                        \n                      \n                    \n                  \n                  =\n                  \n                    F\n                    \n                      y\n                    \n                  \n                \n              \n              \n                \n                  \n                    \n                      \n                        \n                          p\n                          ˙\n                        \n                      \n                    \n                    \n                      z\n                    \n                  \n                  =\n                  m\n                  \n                    \n                      \n                        \n                          \n                        \n                        \n                          \n                            d\n                            \n                              v\n                              \n                                z\n                              \n                            \n                          \n                        \n                      \n                      \n                        \n                          \n                        \n                        \n                          \n                            d\n                            t\n                          \n                        \n                      \n                    \n                  \n                  =\n                  \n                    F\n                    \n                      z\n                    \n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\dot {\\mathbf {p} }}=m{\\cfrac {d\\mathbf {v} }{dt}}=\\mathbf {F} ,\\qquad {\\begin{cases}{\\dot {p}}_{x}=m{\\cfrac {dv_{x}}{dt}}=F_{x}\\\\{\\dot {p}}_{y}=m{\\cfrac {dv_{y}}{dt}}=F_{y}\\\\{\\dot {p}}_{z}=m{\\cfrac {dv_{z}}{dt}}=F_{z}\\end{cases}}}\n  \n\ny en el caso de sistemas formados por N partículas puntuales, el número de ecuaciones escalares es igual a 3N. En mecánica newtoniana también pueden tratarse los sólidos rígidos mediante una ecuación vectorial para el movimiento de traslación del sólido y otra ecuación vectorial para el movimiento de rotación del sólido:\n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    \n                      \n                        \n                          p\n                        \n                        ˙\n                      \n                    \n                  \n                  =\n                  \n                    \n                      \n                        \n                          \n                        \n                        \n                          \n                            d\n                          \n                        \n                      \n                      \n                        \n                          \n                        \n                        \n                          \n                            d\n                            t\n                          \n                        \n                      \n                    \n                  \n                  (\n                  m\n                  \n                    v\n                  \n                  )\n                  =\n                  \n                    \n                      F\n                    \n                    \n                      R\n                    \n                  \n                \n              \n              \n                \n                  \n                    \n                      \n                        \n                          L\n                        \n                        ˙\n                      \n                    \n                  \n                  =\n                  \n                    \n                      \n                        \n                          \n                        \n                        \n                          \n                            d\n                          \n                        \n                      \n                      \n                        \n                          \n                        \n                        \n                          \n                            d\n                            t\n                          \n                        \n                      \n                    \n                  \n                  (\n                  \n                    I\n                  \n                  \n                    ω\n                  \n                  )\n                  =\n                  \n                    \n                      M\n                    \n                    \n                      R\n                    \n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}{\\dot {\\mathbf {p} }}={\\cfrac {d}{dt}}(m\\mathbf {v} )=\\mathbf {F} _{R}\\\\{\\dot {\\mathbf {L} }}={\\cfrac {d}{dt}}(\\mathbf {I} {\\boldsymbol {\\omega }})=\\mathbf {M} _{R}\\end{cases}}}\n  \n\nEstas ecuaciones constituyen la base de partida de la mecánica del sólido rígido."
      },
      {
        "heading": "Mecánica analítica",
        "level": 1,
        "content": "La mecánica analítica es una formulación más abstracta y general, que permite el uso en igualdad de condiciones de sistemas inerciales o no inerciales sin que, a diferencia de las leyes de Newton, la forma básica de las ecuaciones cambie. La mecánica analítica tiene, básicamente dos formulaciones: la formulación lagrangiana y la formulación hamiltoniana. Las dos llegan básicamente a los mismos resultados físicos, aunque la elección del enfoque puede depender del tipo de problema.\nEl germen de la mecánica analítica puede encontrarse en los trabajos de Leibniz y en la definición de dos magnitudes escalares básicas: la energía cinética y el trabajo. Estas magnitudes están relacionadas de forma diferencial por la ecuación del principio de fuerzas vivas:\n\n  \n    \n      \n        \n          d\n          \n            E\n            \n              c\n            \n          \n          =\n          δ\n          W\n          ,\n        \n      \n    \n    {\\displaystyle \\displaystyle dE_{c}=\\delta W,}\n  \n\nUna propiedad notable de este principio es que siendo el movimiento general un fenómeno en varias dimensiones, parece misterioso que con dos magnitudes escalares relacionadas mediante una sola ecuación diferencial, podamos predecir la evolución de los sistemas mecánicos (en la mecánica vectorial precisamos de \n  \n    \n      \n        3\n        N\n      \n    \n    {\\displaystyle 3N}\n  \n ecuaciones siendo \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n el número de partículas). \nAunque las formulaciones lagrangiana y hamiltoniana son esencialmente equivalentes, siendo más conveniente un enfoque u otro según el objeto del análisis. Formalmente cabe señalar que la mecánica lagrangiana describe el movimiento de un conjunto de N partículas puntuales mediante coordenadas generales sobre el fibrado tangente del llamado espacio de configuración mediante un sistema de N ecuaciones diferenciales ordinarias de segundo orden. En cambio en mecánica hamiltoniana el movimiento se describe mediante 2N ecuaciones diferenciales de primer orden sobre una variedad simpléctica formada a partir del fibrado tangente mencionado. El conjunto de transformaciones de coordenadas que permitan resolver el problema es más amplio en mecánica hamiltoniana."
      },
      {
        "heading": "Mecánica lagrangiana",
        "level": 2,
        "content": "La mecánica lagrangiana tiene la ventaja de ser suficientemente general como para que las ecuaciones de movimiento sean invariantes respecto a cualquier cambio de coordenadas. Eso permite trabajar con sistema de referencia inerciales o no-inerciales en pie de igualdad.\nPara un sistema de n grados de libertad, la mecánica lagrangiana proporciona un sistema de n ecuaciones diferenciales ordinarias de segundo orden, llamadas ecuaciones del movimiento que permiten conocer como evolucionará el sistema. La forma explícita de las ecuaciones tiene la forma:\n\n(*)\n  \n    \n      \n        \n          \n            d\n            \n              d\n              t\n            \n          \n        \n        \n          \n            \n              ∂\n              L\n            \n            \n              ∂\n              \n                \n                  \n                    \n                      q\n                      ˙\n                    \n                  \n                \n                \n                  i\n                \n              \n            \n          \n        \n        −\n        \n          \n            \n              ∂\n              L\n            \n            \n              ∂\n              \n                q\n                \n                  i\n                \n              \n            \n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle {\\frac {d}{dt}}{\\frac {\\partial L}{\\partial {\\dot {q}}_{i}}}-{\\frac {\\partial L}{\\partial q_{i}}}=0}\n  \n\nDonde \n  \n    \n      \n        L\n        (\n        \n          q\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          q\n          \n            n\n          \n        \n        ,\n        \n          \n            \n              \n                q\n                ˙\n              \n            \n          \n          \n            i\n          \n        \n        ,\n        …\n        ,\n        \n          \n            \n              \n                q\n                ˙\n              \n            \n          \n          \n            n\n          \n        \n        ,\n        t\n        )\n      \n    \n    {\\displaystyle L(q_{1},\\ldots ,q_{n},{\\dot {q}}_{i},\\ldots ,{\\dot {q}}_{n},t)}\n  \n es la expresión de lagrangiano en el sistema de coordenadas generalizadas \n  \n    \n      \n        (\n        \n          q\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          q\n          \n            n\n          \n        \n        ,\n        \n          \n            \n              \n                q\n                ˙\n              \n            \n          \n          \n            i\n          \n        \n        ,\n        …\n        ,\n        \n          \n            \n              \n                q\n                ˙\n              \n            \n          \n          \n            n\n          \n        \n        )\n        ∈\n        \n          \n            R\n          \n          \n            2\n            n\n          \n        \n      \n    \n    {\\displaystyle (q_{1},\\ldots ,q_{n},{\\dot {q}}_{i},\\ldots ,{\\dot {q}}_{n})\\in \\mathbb {R} ^{2n}}\n  \n. Aunque en general la integración del sistema de ecuaciones (*) no es sencilla, resulta de gran ayuda reducir el número de coordenadas del problema buscando magnitudes conservadas, es decir, magnitudes que no varían a lo largo del tiempo. Las magnitudes conservadas también se suelen llamar integrales del movimiento y suelen estar asociadas a leyes de conservación comunes.\nEn mecánica lagrangiana existe un modo muy elegante de buscar integrales de movimiento a partir del teorema de Noether. De acuerdo con este teorema cuando un lagrangiano es invariante bajo un grupo de simetría uniparamétrico entonces cualquier generador del álgebra de Lie asociada a ese grupo uniparmétrico es proporcional a una magnitud conservada:\n\nAsí cuando un problema físico tiene algún tipo de simetría rotacional, su lagrangiano es invariante bajo algún grupo de rotación y tenemos que se conserva el momento angular.\nCuando un problema físico presenta simetría traslacional, es decir, cuando las fuerzas que actúan sobre un sistema de partículas son idénticas en cualquier posición a lo largo de una línea, tenemos que en esa dirección se conserva el momento lineal.\nLa ley de conservación de la energía está asociada a una simetría de traslación en el tiempo. Cuando las ecuaciones básicas de un sistema son iguales en todos los instantes del tiempo y los parámetros que determinan el problema no dependen del tiempo, entonces la energía de dicho sistema se conserva.\nLa mecánica lagrangiana puede generalizarse de forma muy abstracta e incluso ser usada en problemas fuera de la física (como en el problema de determinar las geodésicas de una variedad de Riemann). En esa forma abstracta la mecánica lagrangina se construye como un sistema dinámico sobre el fibrado tangente de cierto espacio de configuración aplicándose diversos teoremas y temas de la geometría diferencial."
      },
      {
        "heading": "Mecánica hamiltoniana",
        "level": 2,
        "content": "La mecánica hamiltoniana es similar, en esencia, a la mecánica lagrangiana, aunque describe la evolución temporal de un sistema mediante ecuaciones diferenciales de primer orden, lo cual permite integrar más fácilmente las ecuaciones de movimiento. En su forma canónica las ecuaciones de Hamilton tienen la forma:\n\n  \n    \n      \n        \n          \n            \n              ∂\n              H\n            \n            \n              ∂\n              \n                q\n                \n                  i\n                \n              \n            \n          \n        \n        =\n        −\n        \n          \n            \n              \n                p\n                \n                  i\n                \n              \n              ˙\n            \n          \n        \n        ,\n        \n        \n          \n            \n              ∂\n              H\n            \n            \n              ∂\n              \n                p\n                \n                  i\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                q\n                \n                  i\n                \n              \n              ˙\n            \n          \n        \n      \n    \n    {\\displaystyle {\\partial H \\over \\partial q_{i}}=-{\\dot {p_{i}}},\\qquad {\\partial H \\over \\partial p_{i}}={\\dot {q_{i}}}}\n  \n\nDonde H es la función de Hamilton o hamiltoniano, y \n  \n    \n      \n        (\n        \n          q\n          \n            i\n          \n        \n        ,\n        \n          p\n          \n            i\n          \n        \n        \n          )\n          \n            i\n            =\n            1...\n            n\n          \n        \n        \n      \n    \n    {\\displaystyle (q_{i},p_{i})_{i=1...n}\\,}\n  \n son los pares de coordenadas canónicas conjugadas del problema. Usualmente las variables tipo qi se interpretan como coordenadas generalizadas de posición y las pi como momentos asociados a las velocidades.\nSin embargo, una característica notable de la mecánica hamiltoniana es que trata en pie de igualdad los grados de libertad asociados a la posición y a la velocidad de una partícula. De hecho en mecánica hamiltoniana no podemos distinguir formalmente entre coordenadas generalizadas de posición y coordenadas generalizadas de momento. De hecho se puede hacer un cambio de coordenadas en que las posiciones queden convertidas en momentos y los momentos en posiciones. Como resultado de esta descripción igualitaria entre momentos y posiciones la mecánica hamiltoniana admite transformaciones de coordenadas mucho más generales que la mecánica lagrangiana. Esa mayor libertad en escoger coordenadas generalizadas se traduce en una mayor capacidad para poder integrar las ecuaciones de movimiento y determinar propiedades de las trayectorias de partículas.\nUna generalización de la mecánica hamiltoniana es la geometría simpléctica, en esa forma la mecánica hamiltoniana es usada para resolver problemas no físicos, incluso para la matemática básica. Algunas generalizaciones y regeneralizaciones de la mecánica hamiltoniana son:\n\nLa geometría simpléctica.\nLa geometría de contacto que propiamente es una generalización de la anterior.\nLa mecánica de Nambu que es una especie de mecánica hamiltoniana con varios hamiltonianos simultáneos.[42]​"
      },
      {
        "heading": "Rango de validez de la mecánica clásica",
        "level": 1,
        "content": "Las distintas formulaciones de la mecánica clásica son aproximaciones a leyes más fundamentales (o más precisas) de la naturaleza. El dominio que posee la mecánica clásica es caracterizado por:\n\nTamaños mucho mayores a 1 nm.\nVelocidades mucho menores a la de la luz.\nLa primera de estas características delimita el dominio de la mecánica cuántica por sobre las leyes clásicas. Las ecuaciones de Newton, Lagrange o Hamilton necesitan un cambio fundamental para tratar objetos microscópicos y esto se puede conseguir usando la mecánica cuántica en sus distintas formulaciones. En el formalismo de Schrödinger, las variables dinámicas pasan a ser operadores y los estados de una partícula son descritos completamente por la función de onda, que puede evolucionar en el tiempo. Sin embargo, la mecánica cuántica también está separada en dos grandes dominios, que son dependientes de la velocidad de las partículas: la mecánica cuántica no-relativista y la mecánica cuántica relativista.\nPor otra parte, la segunda de estas características demarca el límite entre la mecánica clásica y la mecánica relativista. Para velocidades comparables a la de la luz y objetos macroscópicos, la teoría más precisa pasa a ser la relatividad general, que está basada en el principio de equivalencia, la curvatura del espacio-tiempo y el principio de covarianza generalizado. \nPor último, dentro del régimen de la mecánica cuántica relativista con muchos grados de libertad, el uso de teorías cuánticas de campo se vuelve de primera necesidad, mientras que, al tratar grandes cantidades de grados de libertad en el nivel macroscópico, suele ser útil el uso de la mecánica estadística relativista."
      },
      {
        "heading": "Aproximación a la relatividad especial",
        "level": 2,
        "content": "En relatividad especial, el momentum de una partícula está dado por\n\n  \n    \n      \n        \n          p\n        \n        =\n        γ\n        m\n        \n          v\n        \n        =\n        \n          \n            \n              m\n              \n                v\n              \n            \n            \n              1\n              −\n              \n                \n                  \n                    \n                      v\n                    \n                    \n                      2\n                    \n                  \n                  \n                    c\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {p} =\\gamma m\\mathbf {v} ={\\frac {m\\mathbf {v} }{\\sqrt {1-{\\frac {\\mathbf {v} ^{2}}{c^{2}}}}}}}\n  \n\ndonde \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n es la masa de la partícula, \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  \n su velocidad, \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n la velocidad de la luz y \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n es el factor de Lorentz. A velocidades bajas, \n  \n    \n      \n        v\n        ≪\n        c\n      \n    \n    {\\displaystyle v\\ll c}\n  \n, el factor de Lorentz puede ser aproximado por el primer término de su expansión en serie,\n\n  \n    \n      \n        γ\n        ≈\n        1\n        +\n        \n          \n            \n              v\n              \n                2\n              \n            \n            \n              2\n              \n                c\n                \n                  2\n                \n              \n            \n          \n        \n        +\n        \n          \n            \n              3\n              \n                v\n                \n                  4\n                \n              \n            \n            \n              8\n              \n                c\n                \n                  4\n                \n              \n            \n          \n        \n        +\n        …\n        ,\n      \n    \n    {\\displaystyle \\gamma \\approx 1+{\\frac {v^{2}}{2c^{2}}}+{\\frac {3v^{4}}{8c^{4}}}+\\dots ,}\n  \n\npor lo que el momentum se puede escribir como\n\n  \n    \n      \n        \n          p\n        \n        ≈\n        m\n        \n          v\n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {p} \\approx m\\mathbf {v} ,}\n  \n\nque es la forma usual de momentum en la mecánica Newtoniana."
      },
      {
        "heading": "Aproximación a la mecánica cuántica",
        "level": 2,
        "content": "Los límites de la mecánica clásica se muestran aproximadamente cuando la longitud de onda de Broglie de la partícula en cuestión es menor que el tamaño característico del sistema. Por ejemplo, si \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n es la longitud característica que describe el movimiento de un cuerpo con momentum \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n (como puede ser la dimensión lineal de un obstáculo en su camino), el aspecto ondulatorio de la materia se mantendrá oculto si\n\n  \n    \n      \n        \n          \n            λ\n            x\n          \n        \n        =\n        \n          \n            h\n            \n              x\n              p\n            \n          \n        \n        ≪\n        1\n      \n    \n    {\\displaystyle {\\frac {\\lambda }{x}}={\\frac {h}{xp}}\\ll 1}\n  \n\ndonde \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n es la constante de Planck. Dicho de otra forma, si el cuanto de acción \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n es despreciable respecto a \n  \n    \n      \n        x\n        p\n      \n    \n    {\\displaystyle xp}\n  \n, la mecánica clásica es aplicable.\nDe hecho, la transición gradual desde el nivel microscópico, en el que rigen las leyes cuánticas, al nivel macróscopico, que obedece las leyes clásicas, sugiere que la mecánica cuántica es consistente con la mecánica clásica dentro de la aproximación mencionada. Este requisito también se conoce como el principio de correspondencia."
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": "Relatividad especial de Einstein\nMecánica cuántica\nTeoría cuántica de campos\nMecánica puntual\nMecánica de medios continuos"
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bibliografía",
        "level": 1,
        "content": "Feynman, Richard (1996). Six Easy Pieces. Perseus Publishing. ISBN 0-201-40825-2. \nFeynman, Richard; Phillips, Richard (1998). Six Easy Pieces. Perseus Publishing. ISBN 0-201-32841-0. \nFeynman, Richard (1999). Lectures on Physics. Perseus Publishing. ISBN 0-7382-0092-1. \nLandau, L. D.; Lifshitz, E. M. (1972). Mechanics and Electrodynamics, Vol. 1. Franklin Book Company, Inc. ISBN 0-08-016739-X. \nKleppner, D. and Kolenkow, R. J., An Introduction to Mechanics, McGraw-Hill (1973). ISBN 0-07-035048-5\nGerald Jay Sussman and Jack Wisdom, Structure and Interpretation of Classical Mechanics, MIT Press (2001). ISBN 0-262-019455-4\nHerbert Goldstein, Charles P. Poole, John L. Safko, Classical Mechanics (3rd Edition), Addison Wesley; ISBN 0-201-65702-3\nRobert Martin Eisberg, Fundamentals of Modern Physics, John Wiley and Sons, 1961\nM. Alonso, J. Finn, \"Fundamental university physics\", Addison-Wesley\nAlonso, M.; Finn, J. (1992). Fundamental University Physics. Addison-Wesley. \nFeynman, Richard (1999). The Feynman Lectures on Physics. Perseus Publishing. ISBN 978-0-7382-0092-7. \nGoldstein, Herbert; Charles P. Poole; John L. Safko (2002). Classical Mechanics (3rd edición). Addison Wesley. ISBN 978-0-201-65702-9. \nKibble, Tom W.B.; Berkshire, Frank H. (2004). Classical Mechanics (5th ed.). Imperial College Press. ISBN 978-1-86094-424-6. \nKleppner, D.; Kolenkow, R.J. (1973). An Introduction to Mechanics. McGraw-Hill. ISBN 978-0-07-035048-9. \nLandau, L.D.; Lifshitz, E.M. (1972). Course of Theoretical Physics, Vol. 1 – Mechanics. Franklin Book Company. ISBN 978-0-08-016739-8. \nMorin, David (2008). Introduction to Classical Mechanics: With Problems and Solutions (1st edición). Cambridge: Cambridge University Press. ISBN 978-0-521-87622-3. \nGerald Jay Sussman; Jack Wisdom (2001). Structure and Interpretation of Classical Mechanics. MIT Press. ISBN 978-0-262-19455-6. \nO'Donnell, Peter J. (2015). Essential Dynamics and Relativity. CRC Press. ISBN 978-1-4665-8839-4. \nThornton, Stephen T.; Marion, Jerry B. (2003). Classical Dynamics of Particles and Systems (5th ed.). Brooks Cole. ISBN 978-0-534-40896-1."
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": " Wikimedia Commons alberga una categoría multimedia sobre Mecánica clásica.\nCrowell, Benjamin. Light and Matter (an introductory text, uses algebra with optional sections involving calculus)\nFitzpatrick, Richard. Classical Mechanics (uses calculus)\nHoiland, Paul (2004). Preferred Frames of Reference & Relativity\nHorbatsch, Marko, \"Classical Mechanics Course Notes\".\nRosu, Haret C., \"Classical Mechanics\". Physics Education. 1999. [arxiv.org : physics/9909035]\nShapiro, Joel A. (2003). Classical Mechanics\nSussman, Gerald Jay & Wisdom, Jack &  Mayer, Meinhard E. (2001). Structure and Interpretation of Classical Mechanics\nTong, David. Classical Dynamics (Cambridge lecture notes on Lagrangian and Hamiltonian formalism)\nKinematic Models for Design Digital Library (KMODDL) Movies and photos of hundreds of working mechanical-systems models at Cornell University. Also includes an e-book library of classic texts on mechanical design and engineering.\nMIT OpenCourseWare 8.01: Classical Mechanics Free videos of actual course lectures with links to lecture notes, assignments and exams.\nAlejandro A. Torassa, On Classical Mechanics"
      }
    ],
    "summary": "La mecánica clásica es la rama de la física que estudia las leyes del comportamiento de cuerpos físicos macroscópicos (a diferencia de la mecánica cuántica) en reposo y a velocidades pequeñas comparadas con la velocidad de la luz. En la mecánica clásica en general existen tres aspectos invariantes: el tiempo es absoluto, la evolución temporal de los sistemas se realiza de acuerdo  con el principio de mínima acción y las leyes físicas son deterministas.\nEl primer desarrollo de la mecánica clásica suele denominarse mecánica newtoniana. Consiste en los conceptos físicos basados en los trabajos fundacionales de Sir Isaac Newton, y en los métodos matemáticos inventados por Gottfried Wilhelm Leibniz, Joseph-Louis Lagrange, Leonhard Euler, y otros contemporáneos, en el siglo XVII para describir el movimiento de los cuerpos físicos bajo la influencia de un sistema de fuerzas.  Posteriormente, se desarrollaron métodos más abstractos que dieron lugar a las reformulaciones de la mecánica clásica conocidas como mecánica lagrangiana y mecánica hamiltoniana. Estos avances, realizados predominantemente en los siglos XVIII y XIX, van sustancialmente más allá de los trabajos anteriores, sobre todo por su uso de la mecánica analítica. También se utilizan, con algunas modificaciones, en todas las áreas de la física moderna.\nLa mecánica clásica proporciona resultados extremadamente precisos cuando se estudian objetos grandes que no son extremadamente masivos y velocidades que no se acercan a la velocidad de la luz. Cuando los objetos que se examinan tienen el tamaño del diámetro de un átomo, se hace necesario introducir el otro gran subcampo de la mecánica: la mecánica cuántica. Para describir las velocidades que no son pequeñas en comparación con la velocidad de la luz, se necesita la relatividad especial. En los casos en los que los objetos se vuelven extremadamente masivos, se aplica la relatividad general. Sin embargo, algunas fuentes modernas incluyen la mecánica relativista en la física clásica, que en su opinión representa la mecánica clásica en su forma más desarrollada y precisa.\nExisten varias formulaciones diferentes, en mecánica clásica, para describir un mismo fenómeno natural que, independientemente de los aspectos formales y metodológicos que utilizan, llegan a la misma conclusión.\nLa mecánica vectorial, que deviene directamente de las leyes de Newton, por lo que también se le conoce como «mecánica newtoniana», llega, a partir de las tres ecuaciones formuladas por Newton y mediante el cálculo diferencial e integral, a una muy exacta aproximación de los fenómenos físicos. Es aplicable a cuerpos que se mueven en relación con un observador a velocidades pequeñas comparadas con la de la luz. Fue construida en un principio para una sola partícula moviéndose en un campo gravitatorio. Se basa en el tratamiento de dos magnitudes vectoriales bajo una relación causal: la fuerza y la acción de la fuerza, medida por la variación del momentum (cantidad de movimiento). El análisis y síntesis de fuerzas y momentos constituye el método básico de la mecánica vectorial. Requiere del uso privilegiado de sistemas de referencia inercial.[2]​\nLa mecánica analítica (analítica en el sentido matemático de la palabra, no en el sentido filosófico) es una formulación matemática abstracta sobre la mecánica; permite desligarse de esos sistemas de referencia privilegiados y tener conceptos más generales al momento de describir un movimiento con el uso del cálculo de variaciones. Sus métodos son poderosos y trascienden de la mecánica a otros campos de la física. Se puede encontrar el germen de la mecánica analítica en la obra de Leibniz, quien propone que para solucionar problemas en mecánica, magnitudes escalares (menos oscuras, según Leibniz que la fuerza y el momento de Newton), como energía cinética y el trabajo, son suficientes y menos oscuras que las cantidades vectoriales, como la fuerza y el momento, propuestos por Newton. Existen dos formulaciones equivalentes: la llamada mecánica lagrangiana es una reformulación de la mecánica realizada por Joseph Louis Lagrange que se basa en la ahora llamada ecuación de Euler-Lagrange (ecuaciones diferenciales de segundo orden) y el principio de mínima acción; la otra, llamada mecánica hamiltoniana, es una reformulación más teórica basada en una funcional llamada hamiltoniano realizada por William Hamilton.[2]​ Las mecánicas hamiltoniana y lagrangiana son ejemplos de mecánicas analíticas, donde las magnitudes se relacionan entre sí por ecuaciones diferenciales parciales, que son equivalentes a las ecuaciones de Newton, por ejemplo las ecuaciones canónicas de Hamilton.[3]​"
  },
  {
    "title": "Electromagnetism",
    "source": "https://en.wikipedia.org/wiki/Electromagnetism",
    "language": "en",
    "chunks": [
      {
        "heading": "History",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Ancient world",
        "level": 2,
        "content": "Investigation into electromagnetic phenomena began about 5,000 years ago. There is evidence that the ancient Chinese, Mayan, and potentially even Egyptian civilizations knew that the naturally magnetic mineral magnetite had attractive properties, and many incorporated it into their art and architecture. Ancient people were also aware of lightning and static electricity, although they had no idea of the mechanisms behind these phenomena. The Greek philosopher Thales of Miletus discovered around 600 B.C.E. that amber could acquire an electric charge when it was rubbed with cloth, which allowed it to pick up light objects such as pieces of straw. Thales also experimented with the ability of magnetic rocks to attract one other, and hypothesized that this phenomenon might be connected to the attractive power of amber, foreshadowing the deep connections between electricity and magnetism that would be discovered over 2,000 years later. Despite all this investigation, ancient civilizations had no understanding of the mathematical basis of electromagnetism, and often analyzed its impacts through the lens of religion rather than science (lightning, for instance, was considered to be a creation of the gods in many cultures)."
      },
      {
        "heading": "19th century",
        "level": 2,
        "content": "Electricity and magnetism were originally considered to be two separate forces. This view changed with the publication of James Clerk Maxwell's 1873 A Treatise on Electricity and Magnetism in which the interactions of positive and negative charges were shown to be mediated by one force. There are four main effects resulting from these interactions, all of which have been clearly demonstrated by experiments:\n\nElectric charges attract or repel one another with a force inversely proportional to the square of the distance between them: opposite charges attract, like charges repel.\nMagnetic poles (or states of polarization at individual points) attract or repel one another in a manner similar to positive and negative charges and always exist as pairs: every north pole is yoked to a south pole.\nAn electric current inside a wire creates a corresponding circumferential magnetic field outside the wire. Its direction (clockwise or counter-clockwise) depends on the direction of the current in the wire.\nA current is induced in a loop of wire when it is moved toward or away from a magnetic field, or a magnet is moved towards or away from it; the direction of current depends on that of the movement.\nIn April 1820, Hans Christian Ørsted observed that an electrical current in a wire caused a nearby compass needle to move. At the time of discovery, Ørsted did not suggest any satisfactory explanation of the phenomenon, nor did he try to represent the phenomenon in a mathematical framework. However, three months later he began more intensive investigations. Soon thereafter he published his findings, proving that an electric current produces a magnetic field as it flows through a wire. The CGS unit of magnetic induction (oersted) is named in honor of his contributions to the field of electromagnetism.\nHis findings resulted in intensive research throughout the scientific community in electrodynamics. They influenced French physicist André-Marie Ampère's developments of a single mathematical form to represent the magnetic forces between current-carrying conductors. Ørsted's discovery also represented a major step toward a unified concept of energy.\nThis unification, which was observed by Michael Faraday, extended by James Clerk Maxwell, and partially reformulated by Oliver Heaviside and Heinrich Hertz, is one of the key accomplishments of 19th-century mathematical physics. It has had far-reaching consequences, one of which was the understanding of the nature of light. Unlike what was proposed by the electromagnetic theory of that time, light and other electromagnetic waves are at present seen as taking the form of quantized, self-propagating oscillatory electromagnetic field disturbances called photons. Different frequencies of oscillation give rise to the different forms of electromagnetic radiation, from radio waves at the lowest frequencies, to visible light at intermediate frequencies, to gamma rays at the highest frequencies.\nØrsted was not the only person to examine the relationship between electricity and magnetism. In 1802, Gian Domenico Romagnosi, an Italian legal scholar, deflected a magnetic needle using a Voltaic pile. The factual setup of the experiment is not completely clear, nor if current flowed across the needle or not. An account of the discovery was published in 1802 in an Italian newspaper, but it was largely overlooked by the contemporary scientific community, because Romagnosi seemingly did not belong to this community.\n\nAn earlier (1735), and often neglected, connection between electricity and magnetism was reported by a Dr. Cookson. The account stated:A tradesman at Wakefield in Yorkshire, having put up a great number of knives and forks in a large box ...  and having placed the box in the corner of a large room, there happened a sudden storm of thunder, lightning, &c.  ... The owner emptying the box on a counter where some nails lay, the persons who took up the knives, that lay on the nails, observed that the knives took up the nails. On this the whole number was tried, and found to do the same, and that, to such a degree as to take up large nails, packing needles, and other iron things of considerable weight ... E. T. Whittaker suggested in 1910 that this particular event was responsible for lightning to be \"credited with the power of magnetizing steel; and it was doubtless this which led Franklin in 1751 to attempt to magnetize a sewing-needle by means of the discharge of Leyden jars.\""
      },
      {
        "heading": "A fundamental force",
        "level": 1,
        "content": "The electromagnetic force is the second strongest of the four known fundamental forces and has unlimited range.\nAll other forces, known as non-fundamental forces. (e.g., friction, contact forces) are derived from the four fundamental forces. At high energy, the weak force and electromagnetic force are unified as a single interaction called the electroweak interaction.\nMost of the forces involved in interactions between atoms are explained by electromagnetic forces between electrically charged atomic nuclei and electrons. The electromagnetic force is also involved in all forms of chemical phenomena.\nElectromagnetism explains how materials carry momentum despite being composed of individual particles and empty space. The forces we experience when \"pushing\" or \"pulling\" ordinary material objects result from intermolecular forces between individual molecules in our bodies and in the objects.\nThe effective forces generated by the momentum of electrons' movement is a necessary part of understanding atomic and intermolecular interactions. As electrons move between interacting atoms, they carry momentum with them. As a collection of electrons becomes more confined, their minimum momentum necessarily increases due to the Pauli exclusion principle. The behavior of matter at the molecular scale, including its density, is determined by the balance between the electromagnetic force and the force generated by the exchange of momentum carried by the electrons themselves."
      },
      {
        "heading": "Classical electrodynamics",
        "level": 1,
        "content": "In 1600, William Gilbert proposed, in his De Magnete, that electricity and magnetism, while both capable of causing attraction and repulsion of objects, were distinct effects. Mariners had noticed that lightning strikes had the ability to disturb a compass needle. The link between lightning and electricity was not confirmed until Benjamin Franklin's proposed experiments in 1752 were conducted on 10 May 1752 by Thomas-François Dalibard of France using a 40-foot-tall (12 m) iron rod instead of a kite and he successfully extracted electrical sparks from a cloud.\nOne of the first to discover and publish a link between human-made electric current and magnetism was Gian Romagnosi, who in 1802 noticed that connecting a wire across a voltaic pile deflected a nearby compass needle. However, the effect did not become widely known until 1820, when Ørsted performed a similar experiment. Ørsted's work influenced Ampère to conduct further experiments, which eventually gave rise to a new area of physics: electrodynamics. By determining a force law for the interaction between elements of electric current, Ampère placed the subject on a solid mathematical foundation.\nA theory of electromagnetism, known as classical electromagnetism, was developed by several physicists during the period between 1820 and 1873, when James Clerk Maxwell's treatise was published, which unified previous developments into a single theory, proposing that light was an electromagnetic wave propagating in the luminiferous ether. In classical electromagnetism, the behavior of the electromagnetic field is described by a set of equations known as Maxwell's equations, and the electromagnetic force is given by the Lorentz force law.\nOne of the peculiarities of classical electromagnetism is that it is difficult to reconcile with classical mechanics, but it is compatible with special relativity. According to Maxwell's equations, the speed of light in vacuum is a universal constant that is dependent only on the electrical permittivity and magnetic permeability of free space. This violates Galilean invariance, a long-standing cornerstone of classical mechanics. One way to reconcile the two theories (electromagnetism and classical mechanics) is to assume the existence of a luminiferous aether through which the light propagates. However, subsequent experimental efforts failed to detect the presence of the aether. After important contributions of Hendrik Lorentz and Henri Poincaré, in 1905, Albert Einstein solved the problem with the introduction of special relativity, which replaced classical kinematics with a new theory of kinematics compatible with classical electromagnetism. (For more information, see History of special relativity.)\nIn addition, relativity theory implies that in moving frames of reference, a magnetic field transforms to a field with a nonzero electric component and conversely, a moving electric field transforms to a nonzero magnetic component, thus firmly showing that the phenomena are two sides of the same coin. Hence the term \"electromagnetism\". (For more information, see Classical electromagnetism and special relativity and Covariant formulation of classical electromagnetism.)\nToday few problems in electromagnetism remain unsolved. These include: the lack of magnetic monopoles, Abraham–Minkowski controversy, the location in space of the electromagnetic field energy, and the mechanism by which some organisms can sense electric and magnetic fields."
      },
      {
        "heading": "Extension to nonlinear phenomena",
        "level": 1,
        "content": "The Maxwell equations are linear, in that a change in the sources (the charges and currents) results in a proportional change of the fields. Nonlinear dynamics can occur when electromagnetic fields couple to matter that follows nonlinear dynamical laws. This is studied, for example, in the subject of magnetohydrodynamics, which combines Maxwell theory with the Navier–Stokes equations. Another branch of electromagnetism dealing with nonlinearity is nonlinear optics."
      },
      {
        "heading": "Quantities and units",
        "level": 1,
        "content": "Here is a list of common units related to electromagnetism:\n\nIn the electromagnetic CGS system, electric current is a fundamental quantity defined via Ampère's law and takes the permeability as a dimensionless quantity (relative permeability) whose value in vacuum is unity. As a consequence, the square of the speed of light appears explicitly in some of the equations interrelating quantities in this system.\n\nFormulas for physical laws of electromagnetism (such as Maxwell's equations) need to be adjusted depending on what system of units one uses. This is because there is no one-to-one correspondence between electromagnetic units in SI and those in CGS, as is the case for mechanical units. Furthermore, within CGS, there are several plausible choices of electromagnetic units, leading to different unit \"sub-systems\", including Gaussian, \"ESU\", \"EMU\", and Heaviside–Lorentz. Among these choices, Gaussian units are the most common today, and in fact the phrase \"CGS units\" is often used to refer specifically to CGS-Gaussian units."
      },
      {
        "heading": "Applications",
        "level": 1,
        "content": "The study of electromagnetism informs electric circuits, magnetic circuits, and semiconductor devices' construction."
      },
      {
        "heading": "See also",
        "level": 1,
        "content": ""
      },
      {
        "heading": "References",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Further reading",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Web sources",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Textbooks",
        "level": 2,
        "content": ""
      },
      {
        "heading": "General coverage",
        "level": 2,
        "content": ""
      },
      {
        "heading": "External links",
        "level": 1,
        "content": "\nMagnetic Field Strength Converter\nElectromagnetic Force – from Eric Weisstein's World of Physics"
      }
    ],
    "summary": "In physics, electromagnetism is an interaction that occurs between particles with electric charge via electromagnetic fields. The electromagnetic force is one of the four fundamental forces of nature. It is the dominant force in the interactions of atoms and molecules. Electromagnetism can be thought of as a combination of electrostatics and magnetism, which are distinct but closely intertwined phenomena. Electromagnetic forces occur between any two charged particles. Electric forces cause an attraction between particles with opposite charges and repulsion between particles with the same charge, while magnetism is an interaction that occurs between charged particles in relative motion. These two forces are described in terms of electromagnetic fields. Macroscopic charged objects are described in terms of Coulomb's law for electricity and Ampère's force law for magnetism; the Lorentz force describes microscopic charged particles.\nThe electromagnetic force is responsible for many of the chemical and physical phenomena observed in daily life. The electrostatic attraction between atomic nuclei and their electrons holds atoms together. Electric forces also allow different atoms to combine into molecules, including the macromolecules such as proteins that form the basis of life. Meanwhile, magnetic interactions between the spin and angular momentum magnetic moments of electrons also play a role in chemical reactivity; such relationships are studied in spin chemistry. Electromagnetism also plays several crucial roles in modern technology: electrical energy production, transformation and distribution; light, heat, and sound production and detection; fiber optic and wireless communication; sensors; computation; electrolysis; electroplating; and mechanical motors and actuators.\nElectromagnetism has been studied since ancient times. Many ancient civilizations, including the Greeks and the Mayans, created wide-ranging theories to explain lightning, static electricity, and the attraction between magnetized pieces of iron ore. However, it was not until the late 18th century that scientists began to develop a mathematical basis for understanding the nature of electromagnetic interactions. In the 18th and 19th centuries, prominent scientists and mathematicians such as Coulomb, Gauss and Faraday developed namesake laws which helped to explain the formation and interaction of electromagnetic fields. This process culminated in the 1860s with the discovery of Maxwell's equations, a set of four partial differential equations which provide a complete description of classical electromagnetic fields. Maxwell's equations provided a sound mathematical basis for the relationships between electricity and magnetism that scientists had been exploring for centuries, and predicted the existence of self-sustaining electromagnetic waves. Maxwell postulated that such waves make up visible light, which was later shown to be true. Gamma-rays, x-rays, ultraviolet, visible, infrared radiation, microwaves and radio waves were all determined to be electromagnetic radiation differing only in their range of frequencies.\nIn the modern era, scientists continue to refine the theory of electromagnetism to account for the effects of modern physics, including quantum mechanics and relativity. The theoretical implications of electromagnetism, particularly the requirement that observations remain consistent when viewed from various moving frames of reference (relativistic electromagnetism) and the establishment of the speed of light based on properties of the medium of propagation (permeability and permittivity), helped inspire Einstein's theory of special relativity in 1905. Quantum electrodynamics (QED) modifies Maxwell's equations to be consistent with the quantized nature of matter. In QED, changes in the electromagnetic field are expressed in terms of discrete excitations, particles known as photons, the quanta of light."
  },
  {
    "title": "Electromagnetismo",
    "source": "https://es.wikipedia.org/wiki/Electromagnetismo",
    "language": "es",
    "chunks": [
      {
        "heading": "Historia",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Historia de la teoría",
        "level": 1,
        "content": "Originalmente, la electricidad y el magnetismo se consideraban dos fenómenos independientes entre sí. Este punto de vista cambió, sin embargo, con la publicación en 1873 del Tratado de electricidad y magnetismo de James Maxwell , que mostró que la interacción de cargas positivas y negativas está gobernada por una sola fuerza. Hay cuatro efectos principales, resultantes de estas interacciones, que han sido claramente demostrados por experimentos:\n\nLas cargas eléctricas son atraídas o repelidas entre sí con una fuerza inversamente proporcional al cuadrado de la distancia entre ellas: las cargas diferentes se atraen, las cargas iguales se repelen.\nLos polos magnéticos (o estados de polarización en puntos separados) se atraen o repelen entre sí de manera similar y siempre van en pares: cada polo norte no existe por separado del polo sur.\nLa corriente eléctrica en un cable crea un campo magnético circular alrededor del cable, dirigido (en sentido horario o antihorario) según el flujo de corriente.\nSe induce una corriente en el bucle del cable cuando se acerca o aleja con relación al campo magnético, o cuando el imán se acerca o aleja del bucle del cable; la dirección de la corriente depende de la dirección de estos movimientos.\n\nEn preparación para la conferencia, la noche del 21 de abril de 1820, Hans Christian Oersted hizo una observación asombrosa. Cuando estaba compilando el material, notó que la aguja de la brújula se desviaba del polo norte magnético cuando se encendía y apagaba la corriente eléctrica de la batería que estaba usando. Esta desviación lo llevó a creer que los campos magnéticos emanan de todos los lados de un cable a través del cual fluye una corriente eléctrica, al igual que la luz y el calor se propagan en el espacio, y esa experiencia indica una conexión directa entre la electricidad y el magnetismo.\n\nEn el momento del descubrimiento, Oersted no ofreció una explicación satisfactoria de este fenómeno y no intentó presentar el fenómeno en cálculos matemáticos. Sin embargo, tres meses después, comenzó a realizar investigaciones más intensivas. Poco después, publicó los resultados de su investigación, demostrando que una corriente eléctrica crea un campo magnético cuando fluye a través de cables. En el sistema CGS , la unidad de inducción electromagnética, Oe, recibió su nombre de su contribución al campo del electromagnetismo.\n\nLas conclusiones de Oersted llevaron a un estudio intensivo de electrodinámica por parte de la comunidad científica mundial. Las obras de Dominique François Arago también se remontan a 1820 , quien advirtió que un cable por el que fluye una corriente eléctrica atrae limaduras de hierro . También magnetizó por primera vez alambres de hierro y acero, colocándolos dentro de una bobina de alambres de cobre por donde pasaba la corriente. También logró magnetizar la aguja colocándola en una bobina y descargando la Botella de Leyden a través de la bobina. Independientemente de Arago, Davy descubrió la magnetización del acero y el hierro por la corriente . Las primeras definiciones cuantitativas de la acción de una corriente sobre un imán de la misma forma se remontan a 1820 y pertenecen a científicos franceses Jean-Baptiste Biot y Félix Savart.[7]​ Los experimentos de Oersted también influyeron en el físico francés André-Marie Ampere , quien presentó la ley electromagnética entre un conductor y una corriente en forma matemática. El descubrimiento de Oersted también representa un paso importante hacia un concepto de campo unificado.\nEsta unidad, que fue descubierta por Michael Faraday , completada por James Clerk Maxwell , y también refinada por Oliver Heaviside y Heinrich Hertz, es uno de los logros clave del siglo XIX en física matemática . Este descubrimiento tuvo implicaciones de gran alcance, una de las cuales fue comprender la naturaleza de la luz. La luz y otras ondas electromagnéticas toman la forma de fenómenos oscilatorios autopropagantes cuantificados del campo electromagnético llamados fotones. Diferentes frecuencias de vibración conducen a diferentes formas de radiación electromagnética: desde ondas de radio a bajas frecuencias, a luz visible a frecuencias medias, a rayos gamma a altas frecuencias.\nOersted no fue la única persona que descubrió la conexión entre la electricidad y el magnetismo. En 1802, Giovanni Domenico Romagnosi , un jurista italiano, desvió una aguja magnética con descargas electrostáticas. Pero, de hecho, la investigación de Romagnosi no utilizó una celda galvánica y no había corriente continua como tal. El informe del descubrimiento se publicó en 1802 en un periódico italiano, pero la comunidad científica apenas lo notó en ese momento.[8]​"
      },
      {
        "heading": "Ramas",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Electrostática",
        "level": 2,
        "content": "La electrostática es el estudio de los fenómenos asociados a los cuerpos cargados en reposo. Como describe la ley de Coulomb, estos cuerpos ejercen fuerzas entre sí. Su comportamiento se puede analizar en términos de la idea de un campo eléctrico que rodea cualquier cuerpo cargado, de manera que otro cuerpo cargado colocado dentro del campo estará sujeto a una fuerza proporcional a la magnitud de su carga y de la magnitud del campo en su ubicación. El que la fuerza sea atractiva o repulsiva depende de la polaridad de la carga. La electrostática tiene muchas aplicaciones, que van desde el análisis de fenómenos como tormentas eléctricas hasta el estudio del comportamiento de los tubos electrónicos.\nCuando hablamos de electrostática nos referimos a los fenómenos que ocurren debido a una propiedad intrínseca y discreta de la materia, la carga, cuando es estacionaria o no depende del tiempo. La unidad de carga elemental, es decir, la más pequeña observable, es la carga que tiene el electrón.[9]​ Se dice que un cuerpo está cargado eléctricamente cuando tiene exceso o falta de electrones en los átomos que lo componen. Por definición el defecto de electrones se la denomina carga positiva y al exceso carga negativa.[10]​ La relación entre los dos tipos de carga es de atracción cuando son diferentes y de repulsión cuando son iguales.\nLa carga elemental es una unidad muy pequeña para cálculos prácticos, por eso en el Sistema Internacional la unidad de carga eléctrica, el culombio, se define como la cantidad de carga transportada en un segundo por una corriente de un amperio de intensidad de corriente eléctrica. \n\n  \n    \n      \n        1\n         \n        \n          C\n        \n        =\n        1\n         \n        \n          A\n        \n        ⋅\n        \n          s\n        \n      \n    \n    {\\displaystyle 1\\ \\mathrm {C} =1\\ \\mathrm {A} \\cdot \\mathrm {s} }\n  \n\nque equivale a la carga de 6,25 x 1018 electrones.[9]​ El movimiento de electrones por un conductor se denomina corriente eléctrica y la cantidad de carga eléctrica que pasa por unidad de tiempo se define como la intensidad de corriente. Se pueden introducir más conceptos como el de diferencia de potencial o el de resistencia, que nos conducirían ineludiblemente al área de circuitos eléctricos, y todo eso se puede ver con más detalle en el artículo principal.\nEl nombre de la unidad de carga se debe a Coulomb, quien en 1785 llegó a una relación matemática de la fuerza eléctrica entre cargas puntuales, que ahora se la conoce como ley de Coulomb:\n\n  \n    \n      \n        \n          F\n        \n        =\n        \n          \n            1\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n            \n          \n        \n        \n          \n            \n              \n                q\n                \n                  1\n                \n              \n              \n                q\n                \n                  2\n                \n              \n            \n            \n              r\n              \n                2\n              \n            \n          \n        \n        \n          \n            e\n          \n          \n            r\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {F} ={\\frac {1}{4\\pi \\varepsilon _{0}}}{\\frac {q_{1}q_{2}}{r^{2}}}\\mathbf {e} _{r}}\n  \n\nEntre dos cargas puntuales \n  \n    \n      \n        \n          q\n          \n            1\n          \n        \n        \n      \n    \n    {\\displaystyle q_{1}\\;}\n  \n y \n  \n    \n      \n        \n          q\n          \n            2\n          \n        \n        \n      \n    \n    {\\displaystyle q_{2}\\;}\n  \n existe una fuerza de atracción o repulsión \n  \n    \n      \n        \n          F\n        \n      \n    \n    {\\displaystyle \\mathbf {F} }\n  \n que varía de acuerdo con el cuadrado de la distancia \n  \n    \n      \n        \n          r\n          \n            2\n          \n        \n        \n      \n    \n    {\\displaystyle r^{2}\\;}\n  \n entre ellas y de dirección radial \n  \n    \n      \n        \n          \n            e\n          \n          \n            r\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {e} _{r}}\n  \n; y \n  \n    \n      \n        \n          ε\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{0}}\n  \n es una constante conocida como permitividad eléctrica.\nLas cargas elementales al no encontrarse solas se las debe tratar como una distribución de ellas. Por eso debe implementarse el concepto de campo, definido como una región del espacio donde existe una magnitud escalar o vectorial dependiente o independiente del tiempo. Así el campo eléctrico \n  \n    \n      \n        \n          \n            \n              E\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {E}}}\n  \n está definido como la región del espacio donde actúan las fuerzas eléctricas. Su intensidad se define como el límite al que tiende la fuerza de una distribución de carga sobre una carga positiva que tiende a cero, así:\n\n  \n    \n      \n        \n          E\n        \n        =\n        \n          lim\n          \n            Δ\n            q\n            →\n            0\n          \n        \n        \n          \n            \n              \n                F\n              \n              \n                Δ\n                q\n              \n            \n            \n              Δ\n              q\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} =\\lim _{\\Delta q\\to 0}{\\frac {\\mathbf {F} _{\\Delta q}}{\\Delta q}}}\n  \n\nY así finalmente llegamos a la expresión matemática que define el campo eléctrico:\n\n  \n    \n      \n        \n          E\n        \n        =\n        \n          \n            q\n            \n              4\n              π\n              \n                ε\n                \n                  0\n                \n              \n              \n                r\n                \n                  2\n                \n              \n            \n          \n        \n        \n          \n            e\n          \n          \n            r\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {E} ={\\frac {q}{4\\pi \\varepsilon _{0}r^{2}}}\\mathbf {e} _{r}}\n  \n\nEs importante conocer el alcance de este concepto de campo eléctrico: nos brinda la oportunidad de conocer cuál es su intensidad y qué ocurre con una carga en cualquier parte de dicho campo sin importar el conocimiento de qué lo provoca.[11]​\nUna forma de obtener qué cantidad de fuerza eléctrica pasa por cierto punto o superficie del campo eléctrico es usar el concepto de flujo eléctrico. Este flujo eléctrico \n  \n    \n      \n        Φ\n      \n    \n    {\\displaystyle \\Phi }\n  \n se define como la suma de la cantidad de campo que atraviesa un área determinada, así:\n\n  \n    \n      \n        Φ\n        =\n        ∑\n        \n          E\n        \n        ⋅\n        Δ\n        \n          S\n        \n        =\n        \n          ∮\n          \n            s\n          \n        \n        \n          E\n        \n        ⋅\n        \n          d\n        \n        \n          S\n        \n      \n    \n    {\\displaystyle \\Phi =\\sum \\mathbf {E} \\cdot \\Delta \\mathbf {S} =\\oint _{s}\\mathbf {E} \\cdot {\\text{d}}\\mathbf {S} }\n  \n\nEl matemático y físico, Carl Friedrich Gauss, demostró que la cantidad de flujo eléctrico en un campo es igual al cociente entre la carga encerrada por la superficie en la que se calcula el flujo, \n  \n    \n      \n        \n          q\n          \n            e\n            n\n            c\n          \n        \n        \n      \n    \n    {\\displaystyle q_{enc}\\;}\n  \n, y la permitividad eléctrica,\n  \n    \n      \n        \n          ε\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{0}}\n  \n. Esta relación se conoce como ley de Gauss:\n\n(1)\n  \n    \n      \n        Φ\n        =\n        \n          ∮\n          \n            s\n          \n        \n        \n          E\n        \n        ⋅\n        \n          d\n        \n        \n          S\n        \n        =\n        \n          \n            \n              q\n              \n                e\n                n\n                c\n              \n            \n            \n              ε\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\Phi =\\oint _{s}\\mathbf {E} \\cdot {\\text{d}}\\mathbf {S} ={\\frac {q_{enc}}{\\varepsilon _{0}}}}"
      },
      {
        "heading": "Magnetostática",
        "level": 2,
        "content": "No fue sino hasta el año de 1820, cuando Hans Christian Ørsted descubrió que el fenómeno magnético estaba ligado al eléctrico, que se obtuvo una teoría científica para el magnetismo.[12]​ La presencia de una corriente eléctrica, o sea, de un flujo de carga debido a una diferencia de potencial, genera una fuerza magnética que no varía en el tiempo. Si tenemos una carga q a una velocidad \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n  \n, en un campo magnético \n  \n    \n      \n        \n          B\n        \n      \n    \n    {\\displaystyle \\mathbf {B} }\n  \n aparecerá una fuerza magnética inducida por el movimiento en esta carga, así:\n\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        \n          v\n        \n        ×\n        \n          B\n        \n      \n    \n    {\\displaystyle \\mathbf {F} =q\\mathbf {v} \\times \\mathbf {B} }\n  \n\nPara determinar el valor de ese campo magnético, Jean Baptiste Biot en 1820,[13]​ dedujo una relación para corrientes estacionarias, ahora conocida como ley de Biot-Savart:\n\n  \n    \n      \n        \n          B\n        \n        =\n        \n          \n            \n              \n                μ\n                \n                  0\n                \n              \n              I\n            \n            \n              4\n              π\n            \n          \n        \n        \n          ∮\n          \n            c\n          \n        \n        \n          \n            \n              \n                d\n              \n              \n                l\n              \n              ×\n              \n                r\n              \n            \n            \n              r\n              \n                3\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {B} ={\\frac {\\mu _{0}I}{4\\pi }}\\oint _{c}{\\frac {{\\text{d}}\\mathbf {l} \\times \\mathbf {r} }{r^{3}}}}\n  \n\nDonde \n  \n    \n      \n        \n          μ\n          \n            0\n          \n        \n        \n      \n    \n    {\\displaystyle \\mu _{0}\\,}\n  \n es un coeficiente de proporcionalidad conocido como permeabilidad magnética, \n  \n    \n      \n        I\n        \n      \n    \n    {\\displaystyle I\\,}\n  \n es la intensidad de corriente, el \n  \n    \n      \n        \n          d\n        \n        \n          l\n        \n      \n    \n    {\\displaystyle {\\text{d}}\\mathbf {l} }\n  \n es el diferencial de longitud por el que circula la corriente y \n  \n    \n      \n        \n          r\n        \n      \n    \n    {\\displaystyle \\mathbf {r} }\n  \n es la distancia de este elemento de longitud el punto donde se evalúa la inducción magnética. De manera más estricta, \n  \n    \n      \n        \n          B\n        \n      \n    \n    {\\displaystyle \\mathbf {B} }\n  \n es la inducción magnética, dicho en otras palabras, es el flujo magnético por unidad de área. Experimentalmente se llegó a la conclusión que las líneas de fuerza de campos magnéticos eran cerradas, eliminando la posibilidad de un monopolo magnético. La relación matemática se la conoce como ley de Gauss para el campo magnético:\n\n(2)\n  \n    \n      \n        \n          ∮\n          \n            S\n          \n        \n        \n          B\n        \n        ⋅\n        \n          d\n        \n        \n          S\n        \n        =\n        0\n      \n    \n    {\\displaystyle \\oint _{S}\\mathbf {B} \\cdot {\\text{d}}\\mathbf {S} =0}\n  \n\nAdemás en la magnetostática existe una ley comparable a la de Gauss en la electrostática, la ley de Ampère. Esta ley nos dice que la circulación en un campo magnético es igual a la densidad de corriente que exista en una superficie cerrada:\n\n  \n    \n      \n        \n          ∮\n          \n            c\n          \n        \n        \n          B\n        \n        ⋅\n        \n          d\n        \n        \n          l\n        \n        =\n        \n          μ\n          \n            0\n          \n        \n        I\n      \n    \n    {\\displaystyle \\oint _{c}\\mathbf {B} \\cdot {\\text{d}}\\mathbf {l} =\\mu _{0}I}\n  \n\nCabe indicar que esta ley de Gauss es una generalización de la ley de Biot-Savart. Además que las fórmulas expresadas aquí son para cargas en el vacío, para más información consúltese los artículos principales."
      },
      {
        "heading": "Electrodinámica clásica",
        "level": 2,
        "content": "La electrodinámica es el estudio de los fenómenos asociados a los cuerpos cargados en movimiento y a los campos eléctricos y magnéticos variables. Dado que una carga en movimiento produce un campo magnético, la electrodinámica se refiere a efectos tales como el magnetismo, la radiación electromagnética, y la inducción electromagnética, incluyendo las aplicaciones prácticas, tales como el generador eléctrico y el motor eléctrico. Esta área de la electrodinámica, conocida como electrodinámica clásica, fue sistemáticamente explicada por James Clerk Maxwell, y las ecuaciones de Maxwell describen los fenómenos de esta área con gran generalidad. Una novedad desarrollada más reciente es la electrodinámica cuántica, que incorpora las leyes de la teoría cuántica a fin de explicar la interacción de la radiación electromagnética con la materia. Paul Dirac, Heisenberg y Wolfgang Pauli fueron pioneros en la formulación de la electrodinámica cuántica. La electrodinámica es inherentemente relativista y da unas correcciones que se introducen en la descripción de los movimientos de las partículas cargadas cuando sus velocidades se acercan a la velocidad de la luz. Se aplica a los fenómenos involucrados con aceleradores de partículas y con tubos electrónicos funcionando a altas tensiones y corrientes.\nEn las secciones anteriores se han descrito campos eléctricos y magnéticos que no variaban con el tiempo. Pero los físicos a finales del siglo XIX descubrieron que ambos campos estaban ligados y así un campo eléctrico en movimiento, una corriente eléctrica que varíe, genera un campo magnético y un campo magnético de por sí implica la presencia de un campo eléctrico. Entonces, lo primero que debemos definir es la fuerza que tendría una partícula cargada que se mueva en un campo magnético y así llegamos a la unión de las dos fuerzas anteriores, lo que hoy conocemos como la fuerza de Lorentz:\n\n(3)\n  \n    \n      \n        \n          F\n        \n        =\n        q\n        (\n        \n          E\n        \n        +\n        \n          v\n        \n        ×\n        \n          B\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {F} =q(\\mathbf {E} +\\mathbf {v} \\times \\mathbf {B} )}\n  \n\nEntre 1890 y 1900 Liénard y Wiechert calcularon el campo electromagnético asociado a cargas en movimiento arbitrario, resultado que se conoce hoy como potenciales de Liénard-Wiechert.\nPor otro lado, para generar una corriente eléctrica en un circuito cerrado debe existir una diferencia de potencial entre dos puntos del circuito, a esta diferencia de potencial se la conoce como fuerza electromotriz o «fem». Esta fuerza electromotriz es proporcional a la rapidez con que el flujo magnético varía en el tiempo, esta ley fue encontrada por Michael Faraday y es la interpretación de la inducción electromagnética, así un campo magnético que varía en el tiempo induce a un campo eléctrico, a una fuerza electromotriz. Matemáticamente se representa como:\n\n(4)\n  \n    \n      \n        \n          ∮\n          \n            C\n          \n        \n        \n          E\n        \n        ⋅\n        \n          d\n        \n        \n          l\n        \n        =\n        −\n        \n          \n            d\n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n          ∫\n          \n            S\n          \n        \n        \n          B\n        \n        ⋅\n        \n          d\n        \n        \n          S\n        \n      \n    \n    {\\displaystyle \\oint _{C}\\mathbf {E} \\cdot {\\text{d}}\\mathbf {l} =-{\\frac {\\text{d}}{{\\text{d}}t}}\\int _{S}\\mathbf {B} \\cdot {\\text{d}}\\mathbf {S} }\n  \n\nEl físico James Clerk Maxwell de 1861 relacionó las anteriormente citadas ecuaciones para la ley de Gauss ((1)), ley de Gauss para el campo magnético ((2)), ley de Faraday ((4)) e introdujo el concepto de una corriente de desplazamiento como una densidad de corriente efectiva para llegar a la ley de Ampère generalizada (5):\n\n(5)\n  \n    \n      \n        \n          ∮\n          \n            C\n          \n        \n        \n          B\n        \n        ⋅\n        \n          d\n        \n        \n          l\n        \n        =\n        \n          μ\n          \n            0\n          \n        \n        \n          ∫\n          \n            S\n          \n        \n        \n          j\n        \n        ⋅\n        \n          d\n        \n        \n          S\n        \n        +\n        \n          μ\n          \n            0\n          \n        \n        \n          ϵ\n          \n            0\n          \n        \n        \n          \n            d\n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n          ∫\n          \n            S\n          \n        \n        \n          E\n        \n        ⋅\n        \n          d\n        \n        \n          S\n        \n      \n    \n    {\\displaystyle \\oint _{C}\\mathbf {B} \\cdot {\\text{d}}\\mathbf {l} =\\mu _{0}\\int _{S}\\mathbf {j} \\cdot {\\text{d}}\\mathbf {S} +\\mu _{0}\\epsilon _{0}{\\frac {\\text{d}}{{\\text{d}}t}}\\int _{S}\\mathbf {E} \\cdot {\\text{d}}\\mathbf {S} }\n  \n\nLas cuatro ecuaciones, tanto en su forma diferencial como en la integral aquí descritas, son fruto de la reformulación del trabajo de Maxwell realizada por  Oliver Heaviside y Heinrich Rudolf Hertz. Pero el verdadero poder de estas ecuaciones, más la fuerza de Lorentz (3), se centra en que juntas son capaces de describir cualquier fenómeno electromagnético, además de las consecuencias físicas que posteriormente se describirán.[14]​\n\nLa genialidad del trabajo de Maxwell es que sus ecuaciones describen un campo eléctrico que va ligado inequívocamente a un campo magnético perpendicular a este y a la dirección de su propagación, este campo es ahora llamado campo electromagnético. Dichos campos podían ser derivados de un potencial escalar (\n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n) y un potencial vectorial (\n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbf {A} }\n  \n) dados por las ecuaciones:\n\n(6)\n  \n    \n      \n        \n          E\n        \n        =\n        \n          \n            1\n            c\n          \n        \n        \n          \n            \n              ∂\n              \n                A\n              \n            \n            \n              ∂\n              t\n            \n          \n        \n        −\n        ∇\n        ϕ\n      \n    \n    {\\displaystyle \\mathbf {E} ={\\frac {1}{c}}{\\frac {\\partial \\mathbf {A} }{\\partial t}}-\\nabla \\phi }\n  \n\n  \n    \n      \n        \n          B\n        \n        =\n        ∇\n        ×\n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbf {B} =\\nabla \\times \\mathbf {A} }\n  \n\nLa solución de las ecuaciones de Maxwell implicaba la existencia de una onda que se propagaba a la velocidad de la luz, con lo que además de unificar los fenómenos eléctricos y magnéticos la teoría formulada por Maxwell predecía con absoluta certeza los fenómenos ópticos.[15]​\nAsí la teoría predecía a una onda que, contraria a las ideas de la época, no necesitaba un medio de propagación; la onda electromagnética se podía propagar en el vacío debido a la generación mutua de los campos magnéticos y eléctricos. Esta onda a pesar de tener una velocidad constante, la velocidad de la luz c, puede tener diferente longitud de onda y consecuentemente dicha onda transporta energía. La radiación electromagnética recibe diferentes nombres al variar su longitud de onda, como rayos gamma, rayos X, espectro visible, etc.; pero en su conjunto recibe el nombre de espectro electromagnético."
      },
      {
        "heading": "Electrodinámica relativista",
        "level": 2,
        "content": "Clásicamente, al fijar un sistema de referencia, se puede descomponer los campos eléctricos y magnéticos del campo electromagnético. Pero, en la teoría de la relatividad especial, al tener a un observador con movimiento relativo respecto al sistema de referencia, este medirá efectos eléctricos y magnéticos diferentes de un mismo fenómeno electromagnético. El campo eléctrico y la inducción magnética a pesar de ser elementos vectoriales no se comportan como magnitudes físicas vectoriales, por el contrario la unión de ambos constituye otro ente físico llamado tensor y en este caso el tensor de campo electromagnético.[16]​\nAsí, la expresión para el campo electromagnético es:\n\n  \n    \n      \n        \n          F\n        \n        =\n        \n          F\n          \n            μ\n            ν\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  0\n                \n                \n                  \n                    E\n                    \n                      x\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n                \n                  \n                    E\n                    \n                      y\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n                \n                  \n                    E\n                    \n                      z\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n              \n              \n                \n                  −\n                  \n                    E\n                    \n                      x\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n                \n                  0\n                \n                \n                  −\n                  \n                    B\n                    \n                      z\n                    \n                  \n                \n                \n                  \n                    B\n                    \n                      y\n                    \n                  \n                \n              \n              \n                \n                  −\n                  \n                    E\n                    \n                      y\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n                \n                  \n                    B\n                    \n                      z\n                    \n                  \n                \n                \n                  0\n                \n                \n                  −\n                  \n                    B\n                    \n                      x\n                    \n                  \n                \n              \n              \n                \n                  −\n                  \n                    E\n                    \n                      z\n                    \n                  \n                  \n                    /\n                  \n                  c\n                \n                \n                  −\n                  \n                    B\n                    \n                      y\n                    \n                  \n                \n                \n                  \n                    B\n                    \n                      x\n                    \n                  \n                \n                \n                  0\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {F} =F_{\\mu \\nu }={\\begin{pmatrix}0&E_{x}/c&E_{y}/c&E_{z}/c\\\\-E_{x}/c&0&-B_{z}&B_{y}\\\\-E_{y}/c&B_{z}&0&-B_{x}\\\\-E_{z}/c&-B_{y}&B_{x}&0\\end{pmatrix}}}\n  \n\nEsta representación se conoce como formulación covariante tetradimensional del electromagnetismo. Las expresiones covariantes para las ecuaciones de Maxwell (7) y la fuerza de Lorentz (6) se reducen a:\n\n(6)\n  \n    \n      \n         \n        \n          f\n          \n            α\n          \n        \n        =\n        \n          ∑\n          \n            β\n          \n        \n        e\n         \n        \n          F\n          \n            α\n            β\n          \n        \n         \n        \n          u\n          \n            β\n          \n        \n        \n      \n    \n    {\\displaystyle \\ f_{\\alpha }=\\sum _{\\beta }e\\ F_{\\alpha \\beta }\\ u^{\\beta }\\,}\n  \n\n(7)\n  \n    \n      \n         \n        \n          ∂\n          \n            μ\n          \n        \n        \n          F\n          \n            μ\n            ν\n          \n        \n        =\n        \n          μ\n          \n            0\n          \n        \n        \n          J\n          \n            ν\n          \n        \n      \n    \n    {\\displaystyle \\ \\partial _{\\mu }F^{\\mu \\nu }=\\mu _{0}J^{\\nu }}\n  \n\n  \n    \n      \n        \n          ∂\n          \n            μ\n          \n        \n        ⋅\n        \n          F\n          \n            μ\n            ν\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle \\partial _{\\mu }\\cdot F^{\\mu \\nu }=0}\n  \n\nDada la forma de las ecuaciones anteriores, si el dominio sobre el que se extiende el campo electromagnético es simplemente conexo el campo electromagnético puede expresarse como la derivada exterior de un cuadrivector llamado potencial vector, relacionado con los potenciales del electromagnetismo clásico de la siguiente manera:\n\n  \n    \n      \n        \n          A\n        \n        =\n        (\n        \n          A\n          \n            0\n          \n        \n        ;\n        \n          A\n          \n            1\n          \n        \n        ,\n        \n          A\n          \n            2\n          \n        \n        ,\n        \n          A\n          \n            3\n          \n        \n        )\n        =\n        (\n        ϕ\n        ;\n        \n          A\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {A} =(A_{0};A_{1},A_{2},A_{3})=(\\phi ;\\mathbf {A} )}\n  \n\nDonde:\n\n  \n    \n      \n        ϕ\n        \n      \n    \n    {\\displaystyle \\phi \\;}\n  \n es el potencial electroestático.\n\n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbf {A} }\n  \n es el potencial vector clásico.\nLa relación entre el cuadrivector potencial y el tensor de campo electromanético resulta ser:\n\n  \n    \n      \n        \n          F\n        \n        =\n        \n          d\n        \n        \n          A\n        \n        =\n        \n          \n            1\n            \n              2\n              !\n            \n          \n        \n        \n          \n            \n              ∂\n              \n                A\n                \n                  β\n                \n              \n            \n            \n              ∂\n              \n                x\n                \n                  α\n                \n              \n            \n          \n        \n        −\n        \n          \n            \n              ∂\n              \n                A\n                \n                  α\n                \n              \n            \n            \n              ∂\n              \n                x\n                \n                  β\n                \n              \n            \n          \n        \n        \n          d\n        \n        \n          x\n          \n            α\n          \n        \n        ∧\n        \n          d\n        \n        \n          x\n          \n            β\n          \n        \n        =\n        \n          \n            1\n            \n              2\n              !\n            \n          \n        \n        \n          F\n          \n            α\n            β\n          \n        \n        \n          d\n        \n        \n          x\n          \n            α\n          \n        \n        ∧\n        \n          d\n        \n        \n          x\n          \n            β\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {F} =\\mathrm {d} \\mathbf {A} ={\\frac {1}{2!}}{\\frac {\\partial A_{\\beta }}{\\partial x^{\\alpha }}}-{\\frac {\\partial A_{\\alpha }}{\\partial x^{\\beta }}}{\\text{d}}x^{\\alpha }\\land {\\text{d}}x^{\\beta }={\\frac {1}{2!}}F_{\\alpha \\beta }{\\text{d}}x^{\\alpha }\\land {\\text{d}}x^{\\beta }}\n  \n\nEl hecho de que la interacción electromagnética pueda representarse por un (cuadri)vector que define completamente el campo electromagnético es la razón por la que se afirma en el tratamiento moderno que la interacción electromagnética es un campo vectorial.\nEn relatividad general el tratamiento del campo electromagnético en un espacio-tiempo curvo es similar al presentado aquí para el espacio-tiempo de Minkowski, solo que las derivadas parciales respecto a las coordenadas deben substituirse por derivadas covariantes."
      },
      {
        "heading": "Electrodinámica cuántica",
        "level": 2,
        "content": "Posteriormente a la revolución cuántica de inicios del siglo XX, los físicos se vieron forzados a buscar una teoría cuántica de la interacción electromagnética. El trabajo de Einstein con el efecto fotoeléctrico y la posterior formulación de la mecánica cuántica sugerían que la interacción electromagnética se producía mediante el intercambio de partículas elementales llamadas fotones. La nueva formulación cuántica lograda en la década de 1940 describe la interacción entre los bosones, o partículas portadoras de la interacción, y las otras partículas portadoras de materia (los fermiones).[17]​\nLa electrodinámica cuántica es principalmente una teoría cuántica de campos renormalizada. Su desarrollo fue obra de Sinitiro Tomonaga, Julian Schwinger, Richard Feynman y Freeman Dyson alrededor de los años 1947 a 1949.[18]​ En la electrodinámica cuántica, la interacción entre partículas viene descrita por un lagrangiano que posee simetría local, concretamente simetría de gauge. Para la electrodinámica cuántica, el campo de gauge donde los fermiones interactúan es el campo electromagnético, descrito en esta teoría como los estados de bosones (fotones, en este caso) portadores de la interacción.[18]​\nMatemáticamente, el lagrangiano para la interacción entre fermiones mediante intercambio de fotones viene dado por:\n\n  \n    \n      \n        \n          \n            L\n          \n        \n        =\n        \n          \n            \n              ψ\n              ¯\n            \n          \n        \n        (\n        i\n        \n          γ\n          \n            μ\n          \n        \n        \n          D\n          \n            μ\n          \n        \n        −\n        m\n        )\n        ψ\n        −\n        \n          \n            1\n            4\n          \n        \n        \n          F\n          \n            μ\n            ν\n          \n        \n        \n          F\n          \n            μ\n            ν\n          \n        \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}={\\bar {\\psi }}(i\\gamma ^{\\mu }D_{\\mu }-m)\\psi -{\\frac {1}{4}}F_{\\mu \\nu }F^{\\mu \\nu }\\,}\n  \n\nDonde el significado de los términos son:\n\n  \n    \n      \n        \n          γ\n          \n            μ\n          \n        \n        \n        \n      \n    \n    {\\displaystyle \\gamma _{\\mu }\\,\\!}\n  \n son las matrices de Dirac.\n\n  \n    \n      \n         \n        ψ\n      \n    \n    {\\displaystyle \\ \\psi }\n  \n y \n  \n    \n      \n        \n          \n            \n              ψ\n              ¯\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {\\psi }}}\n  \n son los campos o espinores de Dirac que representan las partículas cargadas eléctricamente.\n\n  \n    \n      \n        \n          D\n          \n            μ\n          \n        \n        =\n        \n          ∂\n          \n            μ\n          \n        \n        +\n        i\n        e\n        \n          A\n          \n            μ\n          \n        \n        \n        \n      \n    \n    {\\displaystyle D_{\\mu }=\\partial _{\\mu }+ieA_{\\mu }\\,\\!}\n  \n es la derivada covariante asociada a la simetría gauge.\n\n  \n    \n      \n         \n        \n          A\n          \n            μ\n          \n        \n      \n    \n    {\\displaystyle \\ A_{\\mu }}\n  \n el operador asociado al potencial vector covariante del campo electromagnético y\n\n  \n    \n      \n        \n          F\n          \n            μ\n            ν\n          \n        \n        =\n        \n          ∂\n          \n            μ\n          \n        \n        \n          A\n          \n            ν\n          \n        \n        −\n        \n          ∂\n          \n            ν\n          \n        \n        \n          A\n          \n            μ\n          \n        \n        \n        \n      \n    \n    {\\displaystyle F_{\\mu \\nu }=\\partial _{\\mu }A_{\\nu }-\\partial _{\\nu }A_{\\mu }\\,\\!}\n  \n el operador de campo asociado tensor de campo electromagnético."
      },
      {
        "heading": "Unidades de electromagnetismo del SI",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": "Interacciones fundamentales\nElectricidad\nMagnetismo\nHistoria de la electricidad\nSuperfuerza\nTeoría del absorbedor de Wheeler-Feynman"
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bibliografía",
        "level": 1,
        "content": "Alonso, Marcelo y Edward J. Finn (1976). Física. Fondo Educativo Interamericano. ISBN 84-03-20234-2. \nFeynman, Richard (1974). Feynman lectures on Physics Volume 2 (en inglés). Addison Wesley Longman. ISBN 0-201-02115-3. \nBaumgart K.K . , . Electromagnetismo // Diccionario enciclopédico Brockhaus y Efron  : en 86 volúmenes (82 volúmenes y 4 adicionales). - SPb. , 1890-1907.\nLandau L. D., Lifshits E. M. Un curso corto de física teórica . En 2 volúmenes - M .: Nauka, 1972 .-- T. II. Mecánica cuántica. - 368 p."
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": " Wikimedia Commons alberga una categoría multimedia sobre Electromagnetismo.\n Wikiquote alberga frases célebres de o sobre Electromagnetismo.\n Wikilibros alberga un libro o manual sobre Electromagnetismo.\n Wikiversidad alberga proyectos de aprendizaje sobre Electromagnetismo."
      }
    ],
    "summary": "El electromagnetismo es la rama de la física que estudia y unifica los fenómenos eléctricos y magnéticos en una sola teoría. El electromagnetismo describe la interacción de partículas cargadas con campos eléctricos y magnéticos. La interacción electromagnética es una de las cuatro fuerzas fundamentales del universo conocido. \nEl electromagnetismo es una rama de la física que estudia los efectos producidos por el magnetismo, lo cual surge a partir de la corriente eléctrica. Por su parte, el magnetismo es la disciplina que examina los fenómenos asociados a los imanes. Su nombre proviene de Magnesia, un distrito en Asia Menor (actual Turquía), donde se descubrieron por primera vez las piedras llamadas magnetitas, que tienen la capacidad de atraer ciertos metales.[1]​ \nEl electromagnetismo abarca diversos fenómenos del mundo real, como por ejemplo la luz. La luz es un campo electromagnético oscilante que se irradia desde partículas cargadas aceleradas. Aparte de la gravedad, la mayoría de las fuerzas en la experiencia cotidiana son consecuencia de electromagnetismo.\nLos principios del electromagnetismo encuentran aplicaciones en diversas disciplinas afines, tales como las microondas, antenas, máquinas eléctricas, comunicaciones por satélite, bioelectromagnetismo, plasmas, investigación nuclear, la fibra óptica, la interferencia y la compatibilidad electromagnéticas, la conversión de energía electromecánica, la meteorología por radar, y la observación remota. Los dispositivos electromagnéticos incluyen transformadores, relés, radio/TV, teléfonos, motores eléctricos, líneas de transmisión, guías de onda y láseres.\nLos fundamentos de la teoría electromagnética fueron presentados por Michael Faraday y formulados por primera vez de modo completo por James Clerk Maxwell en 1865. La formulación consiste en cuatro ecuaciones diferenciales vectoriales que relacionan el campo eléctrico, el campo magnético y sus respectivas fuentes materiales (corriente eléctrica, polarización eléctrica y polarización magnética), conocidas como ecuaciones de Maxwell, lo que ha sido considerada como la «segunda gran unificación de la física», siendo la primera realizada por Isaac Newton.\nEl estudio de los campos electromagnéticos se puede dividir en electrostática —el estudio de las interacciones entre cargas en reposo— y la electrodinámica —el estudio de las interacciones entre cargas en movimiento y la radiación—. La teoría clásica del electromagnetismo se basa en la fuerza de Lorentz y en las ecuaciones de Maxwell. Muchas propiedades ópticas y físicas de la materia también son explicados por la teoría electromagnética.\nEl electromagnetismo es una teoría de campos; es decir, las explicaciones y predicciones que provee se basan en magnitudes físicas vectoriales o tensoriales dependientes de la posición en el espacio y del tiempo. El electromagnetismo describe los fenómenos físicos macroscópicos en los cuales intervienen cargas eléctricas en reposo y en movimiento, usando para ello campos eléctricos y magnéticos y sus efectos sobre las sustancias sólidas, líquidas y gaseosas. Por ser una teoría macroscópica, es decir, aplicable a un número muy grande de partículas y a distancias grandes respecto de las dimensiones de estas, el electromagnetismo no describe los fenómenos atómicos y moleculares. La electrodinámica cuántica proporciona la descripción cuántica de esta interacción, que puede ser unificada con la interacción nuclear débil según el modelo electrodébil."
  },
  {
    "title": "Quantum mechanics",
    "source": "https://en.wikipedia.org/wiki/Quantum_mechanics",
    "language": "en",
    "chunks": [
      {
        "heading": "Overview and fundamental concepts",
        "level": 1,
        "content": "Quantum mechanics allows the calculation of properties and behaviour of physical systems. It is typically applied to microscopic systems: molecules, atoms and subatomic particles. It has been demonstrated to hold for complex molecules with thousands of atoms, but its application to human beings raises philosophical problems, such as Wigner's friend, and its application to the universe as a whole remains speculative. Predictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy. For example, the refinement of quantum mechanics for the interaction of light and matter, known as quantum electrodynamics (QED), has been shown to agree with experiment to within 1 part in 1012 when predicting the magnetic properties of an electron.\nA fundamental feature of the theory is that it usually cannot predict with certainty what will happen, but only give probabilities. Mathematically, a probability is found by taking the square of the absolute value of a complex number, known as a probability amplitude. This is known as the Born rule, named after physicist Max Born. For example, a quantum particle like an electron can be described by a wave function, which associates to each point in space a probability amplitude. Applying the Born rule to these amplitudes gives a probability density function for the position that the electron will be found to have when an experiment is performed to measure it. This is the best the theory can do; it cannot say for certain where the electron will be found. The Schrödinger equation relates the collection of probability amplitudes that pertain to one moment of time to the collection of probability amplitudes that pertain to another.: 67–87 \nOne consequence of the mathematical rules of quantum mechanics is a tradeoff in predictability between measurable quantities. The most famous form of this uncertainty principle says that no matter how a quantum particle is prepared or how carefully experiments upon it are arranged, it is impossible to have a precise prediction for a measurement of its position and also at the same time for a measurement of its momentum.: 427–435 \n\nAnother consequence of the mathematical rules of quantum mechanics is the phenomenon of quantum interference, which is often illustrated with the double-slit experiment. In the basic version of this experiment, a coherent light source, such as a laser beam, illuminates a plate pierced by two parallel slits, and the light passing through the slits is observed on a screen behind the plate.: 102–111 : 1.1–1.8  The wave nature of light causes the light waves passing through the two slits to interfere, producing bright and dark bands on the screen – a result that would not be expected if light consisted of classical particles. However, the light is always found to be absorbed at the screen at discrete points, as individual particles rather than waves; the interference pattern appears via the varying density of these particle hits on the screen. Furthermore, versions of the experiment that include detectors at the slits find that each detected photon passes through one slit (as would a classical particle), and not through both slits (as would a wave).: 109  However, such experiments demonstrate that particles do not form the interference pattern if one detects which slit they pass through.  This behavior is known as wave–particle duality. In addition to light, electrons, atoms, and molecules are all found to exhibit the same dual behavior when fired towards a double slit.\n\nAnother non-classical phenomenon predicted by quantum mechanics is quantum tunnelling: a particle that goes up against a potential barrier can cross it, even if its kinetic energy is smaller than the maximum of the potential. In classical mechanics this particle would be trapped. Quantum tunnelling has several important consequences, enabling radioactive decay, nuclear fusion in stars, and applications such as scanning tunnelling microscopy, tunnel diode and tunnel field-effect transistor.\nWhen quantum systems interact, the result can be the creation of quantum entanglement: their properties become so intertwined that a description of the whole solely in terms of the individual parts is no longer possible. Erwin Schrödinger called entanglement \"...the characteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought\". Quantum entanglement enables quantum computing and is part of quantum communication protocols, such as quantum key distribution and superdense coding. Contrary to popular misconception, entanglement does not allow sending signals faster than light, as demonstrated by the no-communication theorem.\nAnother possibility opened by entanglement is testing for \"hidden variables\", hypothetical properties more fundamental than the quantities addressed in quantum theory itself, knowledge of which would allow more exact predictions than quantum theory provides. A collection of results, most significantly Bell's theorem, have demonstrated that broad classes of such hidden-variable theories are in fact incompatible with quantum physics. According to Bell's theorem, if nature actually operates in accord with any theory of local hidden variables, then the results of a Bell test will be constrained in a particular, quantifiable way. Many Bell tests have been performed and they have shown results incompatible with the constraints imposed by local hidden variables.\nIt is not possible to present these concepts in more than a superficial way without introducing the mathematics involved; understanding quantum mechanics requires not only manipulating complex numbers, but also linear algebra, differential equations, group theory, and other more advanced subjects. Accordingly, this article will present a mathematical formulation of quantum mechanics and survey its application to some useful and oft-studied examples."
      },
      {
        "heading": "Mathematical formulation",
        "level": 1,
        "content": "In the mathematically rigorous formulation of quantum mechanics, the state of a quantum mechanical system is a vector \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n belonging to a (separable) complex Hilbert space \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n  \n. This vector is postulated to be normalized under the Hilbert space inner product, that is, it obeys \n  \n    \n      \n        ⟨\n        ψ\n        ,\n        ψ\n        ⟩\n        =\n        1\n      \n    \n    {\\displaystyle \\langle \\psi ,\\psi \\rangle =1}\n  \n, and it is well-defined up to a complex number of modulus 1 (the global phase), that is, \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n and \n  \n    \n      \n        \n          e\n          \n            i\n            α\n          \n        \n        ψ\n      \n    \n    {\\displaystyle e^{i\\alpha }\\psi }\n  \n represent the same physical system. In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system – for example, for describing position and momentum the Hilbert space is the space of complex square-integrable functions \n  \n    \n      \n        \n          L\n          \n            2\n          \n        \n        (\n        \n          C\n        \n        )\n      \n    \n    {\\displaystyle L^{2}(\\mathbb {C} )}\n  \n, while the Hilbert space for the spin of a single proton is simply the space of two-dimensional complex vectors \n  \n    \n      \n        \n          \n            C\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {C} ^{2}}\n  \n with the usual inner product.\nPhysical quantities of interest – position, momentum, energy, spin – are represented by observables, which are Hermitian (more precisely, self-adjoint) linear operators acting on the Hilbert space. A quantum state can be an eigenvector of an observable, in which case it is called an eigenstate, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. More generally, a quantum state will be a linear combination of the eigenstates, known as a quantum superposition. When an observable is measured, the result will be one of its eigenvalues with probability given by the Born rule: in the simplest case the eigenvalue \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n is non-degenerate and the probability is given by \n  \n    \n      \n        \n          |\n        \n        ⟨\n        \n          \n            \n              λ\n              →\n            \n          \n        \n        ,\n        ψ\n        ⟩\n        \n          \n            |\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle |\\langle {\\vec {\\lambda }},\\psi \\rangle |^{2}}\n  \n, where \n  \n    \n      \n        \n          \n            \n              λ\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {\\lambda }}}\n  \n is its associated unit-length eigenvector. More generally, the eigenvalue is degenerate and the probability is given by \n  \n    \n      \n        ⟨\n        ψ\n        ,\n        \n          P\n          \n            λ\n          \n        \n        ψ\n        ⟩\n      \n    \n    {\\displaystyle \\langle \\psi ,P_{\\lambda }\\psi \\rangle }\n  \n, where \n  \n    \n      \n        \n          P\n          \n            λ\n          \n        \n      \n    \n    {\\displaystyle P_{\\lambda }}\n  \n is the projector onto its associated eigenspace. In the continuous case, these formulas give instead the probability density.\nAfter the measurement, if result \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n was obtained, the quantum state is postulated to collapse to \n  \n    \n      \n        \n          \n            \n              λ\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {\\lambda }}}\n  \n, in the non-degenerate case, or to \n  \n    \n      \n        \n          P\n          \n            λ\n          \n        \n        ψ\n        \n          \n            /\n          \n        \n        \n        \n          \n            ⟨\n            ψ\n            ,\n            \n              P\n              \n                λ\n              \n            \n            ψ\n            ⟩\n          \n        \n      \n    \n    {\\textstyle P_{\\lambda }\\psi {\\big /}\\!{\\sqrt {\\langle \\psi ,P_{\\lambda }\\psi \\rangle }}}\n  \n, in the general case. The probabilistic nature of quantum mechanics thus stems from the act of measurement. This is one of the most difficult aspects of quantum systems to understand. It was the central topic in the famous Bohr–Einstein debates, in which the two scientists attempted to clarify these fundamental principles by way of thought experiments. In the decades after the formulation of quantum mechanics, the question of what constitutes a \"measurement\" has been extensively studied. Newer interpretations of quantum mechanics have been formulated that do away with the concept of \"wave function collapse\" (see, for example, the many-worlds interpretation). The basic idea is that when a quantum system interacts with a measuring apparatus, their respective wave functions become entangled so that the original quantum system ceases to exist as an independent entity (see Measurement in quantum mechanics)."
      },
      {
        "heading": "Time evolution of a quantum state",
        "level": 2,
        "content": "The time evolution of a quantum state is described by the Schrödinger equation:\n\n  \n    \n      \n        i\n        ℏ\n        \n          \n            ∂\n            \n              ∂\n              t\n            \n          \n        \n        ψ\n        (\n        t\n        )\n        =\n        H\n        ψ\n        (\n        t\n        )\n        .\n      \n    \n    {\\displaystyle i\\hbar {\\frac {\\partial }{\\partial t}}\\psi (t)=H\\psi (t).}\n  \n\nHere \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n denotes the Hamiltonian, the observable corresponding to the total energy of the system, and \n  \n    \n      \n        ℏ\n      \n    \n    {\\displaystyle \\hbar }\n  \n is the reduced Planck constant. The constant \n  \n    \n      \n        i\n        ℏ\n      \n    \n    {\\displaystyle i\\hbar }\n  \n is introduced so that the Hamiltonian is reduced to the classical Hamiltonian in cases where the quantum system can be approximated by a classical system; the ability to make such an approximation in certain limits is called the correspondence principle.\nThe solution of this differential equation is given by\n\n  \n    \n      \n        ψ\n        (\n        t\n        )\n        =\n        \n          e\n          \n            −\n            i\n            H\n            t\n            \n              /\n            \n            ℏ\n          \n        \n        ψ\n        (\n        0\n        )\n        .\n      \n    \n    {\\displaystyle \\psi (t)=e^{-iHt/\\hbar }\\psi (0).}\n  \n\nThe operator \n  \n    \n      \n        U\n        (\n        t\n        )\n        =\n        \n          e\n          \n            −\n            i\n            H\n            t\n            \n              /\n            \n            ℏ\n          \n        \n      \n    \n    {\\displaystyle U(t)=e^{-iHt/\\hbar }}\n  \n is known as the time-evolution operator, and has the crucial property that it is unitary. This time evolution is deterministic in the sense that – given an initial quantum state \n  \n    \n      \n        ψ\n        (\n        0\n        )\n      \n    \n    {\\displaystyle \\psi (0)}\n  \n – it makes a definite prediction of what the quantum state \n  \n    \n      \n        ψ\n        (\n        t\n        )\n      \n    \n    {\\displaystyle \\psi (t)}\n  \n will be at any later time.\n\nSome wave functions produce probability distributions that are independent of time, such as eigenstates of the Hamiltonian.: 133–137  Many systems that are treated dynamically in classical mechanics are described by such \"static\" wave functions. For example, a single electron in an unexcited atom is pictured classically as a particle moving in a circular trajectory around the atomic nucleus, whereas in quantum mechanics, it is described by a static wave function surrounding the nucleus. For example, the electron wave function for an unexcited hydrogen atom is a spherically symmetric function known as an s orbital (Fig. 1).\nAnalytic solutions of the Schrödinger equation are known for very few relatively simple model Hamiltonians including the quantum harmonic oscillator, the particle in a box, the dihydrogen cation, and the hydrogen atom. Even the helium atom – which contains just two electrons – has defied all attempts at a fully analytic treatment, admitting no solution in closed form.\nHowever, there are techniques for finding approximate solutions. One method, called perturbation theory, uses the analytic result for a simple quantum mechanical model to create a result for a related but more complicated model by (for example) the addition of a weak potential energy.: 793  Another approximation method applies to systems for which quantum mechanics produces only small deviations from classical behavior. These deviations can then be computed based on the classical motion.: 849"
      },
      {
        "heading": "Uncertainty principle",
        "level": 2,
        "content": "One consequence of the basic quantum formalism is the uncertainty principle. In its most familiar form, this states that no preparation of a quantum particle can imply simultaneously precise predictions both for a measurement of its position and for a measurement of its momentum. Both position and momentum are observables, meaning that they are represented by Hermitian operators. The position operator \n  \n    \n      \n        \n          \n            \n              X\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {X}}}\n  \n and momentum operator \n  \n    \n      \n        \n          \n            \n              P\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {P}}}\n  \n do not commute, but rather satisfy the canonical commutation relation:\n\n  \n    \n      \n        [\n        \n          \n            \n              X\n              ^\n            \n          \n        \n        ,\n        \n          \n            \n              P\n              ^\n            \n          \n        \n        ]\n        =\n        i\n        ℏ\n        .\n      \n    \n    {\\displaystyle [{\\hat {X}},{\\hat {P}}]=i\\hbar .}\n  \n\nGiven a quantum state, the Born rule lets us compute expectation values for both \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n and \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n, and moreover for powers of them. Defining the uncertainty for an observable by a standard deviation, we have\n\n  \n    \n      \n        \n          σ\n          \n            X\n          \n        \n        =\n        \n          \n            \n              \n                \n                  ⟨\n                  \n                    X\n                    \n                      2\n                    \n                  \n                  ⟩\n                \n                −\n                \n                  \n                    ⟨\n                    X\n                    ⟩\n                  \n                  \n                    2\n                  \n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\sigma _{X}={\\textstyle {\\sqrt {\\left\\langle X^{2}\\right\\rangle -\\left\\langle X\\right\\rangle ^{2}}}},}\n  \n\nand likewise for the momentum:\n\n  \n    \n      \n        \n          σ\n          \n            P\n          \n        \n        =\n        \n          \n            \n              ⟨\n              \n                P\n                \n                  2\n                \n              \n              ⟩\n            \n            −\n            \n              \n                ⟨\n                P\n                ⟩\n              \n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\sigma _{P}={\\sqrt {\\left\\langle P^{2}\\right\\rangle -\\left\\langle P\\right\\rangle ^{2}}}.}\n  \n\nThe uncertainty principle states that\n\n  \n    \n      \n        \n          σ\n          \n            X\n          \n        \n        \n          σ\n          \n            P\n          \n        \n        ≥\n        \n          \n            ℏ\n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle \\sigma _{X}\\sigma _{P}\\geq {\\frac {\\hbar }{2}}.}\n  \n\nEither standard deviation can in principle be made arbitrarily small, but not both simultaneously. This inequality generalizes to arbitrary pairs of self-adjoint operators \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n. The commutator of these two operators is\n\n  \n    \n      \n        [\n        A\n        ,\n        B\n        ]\n        =\n        A\n        B\n        −\n        B\n        A\n        ,\n      \n    \n    {\\displaystyle [A,B]=AB-BA,}\n  \n\nand this provides the lower bound on the product of standard deviations:\n\n  \n    \n      \n        \n          σ\n          \n            A\n          \n        \n        \n          σ\n          \n            B\n          \n        \n        ≥\n        \n          \n            \n              1\n              2\n            \n          \n        \n        \n          |\n          \n            \n              \n                ⟨\n              \n            \n            [\n            A\n            ,\n            B\n            ]\n            \n              \n                ⟩\n              \n            \n          \n          |\n        \n        .\n      \n    \n    {\\displaystyle \\sigma _{A}\\sigma _{B}\\geq {\\tfrac {1}{2}}\\left|{\\bigl \\langle }[A,B]{\\bigr \\rangle }\\right|.}\n  \n\nAnother consequence of the canonical commutation relation is that the position and momentum operators are Fourier transforms of each other, so that a description of an object according to its momentum is the Fourier transform of its description according to its position. The fact that dependence in momentum is the Fourier transform of the dependence in position means that the momentum operator is equivalent (up to an \n  \n    \n      \n        i\n        \n          /\n        \n        ℏ\n      \n    \n    {\\displaystyle i/\\hbar }\n  \n factor) to taking the derivative according to the position, since in Fourier analysis differentiation corresponds to multiplication in the dual space. This is why in quantum equations in position space, the momentum \n  \n    \n      \n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p_{i}}\n  \n is replaced by \n  \n    \n      \n        −\n        i\n        ℏ\n        \n          \n            ∂\n            \n              ∂\n              x\n            \n          \n        \n      \n    \n    {\\displaystyle -i\\hbar {\\frac {\\partial }{\\partial x}}}\n  \n, and in particular in the non-relativistic Schrödinger equation in position space the momentum-squared term is replaced with a Laplacian times \n  \n    \n      \n        −\n        \n          ℏ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle -\\hbar ^{2}}\n  \n."
      },
      {
        "heading": "Composite systems and entanglement",
        "level": 2,
        "content": "When two different quantum systems are considered together, the Hilbert space of the combined system is the tensor product of the Hilbert spaces of the two components. For example, let A and B be two quantum systems, with Hilbert spaces \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}_{A}}\n  \n and \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            B\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}_{B}}\n  \n, respectively. The Hilbert space of the composite system is then\n\n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            A\n            B\n          \n        \n        =\n        \n          \n            \n              H\n            \n          \n          \n            A\n          \n        \n        ⊗\n        \n          \n            \n              H\n            \n          \n          \n            B\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\mathcal {H}}_{AB}={\\mathcal {H}}_{A}\\otimes {\\mathcal {H}}_{B}.}\n  \n\nIf the state for the first system is the vector \n  \n    \n      \n        \n          ψ\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle \\psi _{A}}\n  \n and the state for the second system is \n  \n    \n      \n        \n          ψ\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle \\psi _{B}}\n  \n, then the state of the composite system is\n\n  \n    \n      \n        \n          ψ\n          \n            A\n          \n        \n        ⊗\n        \n          ψ\n          \n            B\n          \n        \n        .\n      \n    \n    {\\displaystyle \\psi _{A}\\otimes \\psi _{B}.}\n  \n\nNot all states in the joint Hilbert space \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            A\n            B\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}_{AB}}\n  \n can be written in this form, however, because the superposition principle implies that linear combinations of these \"separable\" or \"product states\" are also valid. For example, if \n  \n    \n      \n        \n          ψ\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle \\psi _{A}}\n  \n and \n  \n    \n      \n        \n          ϕ\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle \\phi _{A}}\n  \n are both possible states for system \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, and likewise \n  \n    \n      \n        \n          ψ\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle \\psi _{B}}\n  \n and \n  \n    \n      \n        \n          ϕ\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle \\phi _{B}}\n  \n are both possible states for system \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n, then\n\n  \n    \n      \n        \n          \n            \n              1\n              \n                2\n              \n            \n          \n        \n        \n          (\n          \n            \n              ψ\n              \n                A\n              \n            \n            ⊗\n            \n              ψ\n              \n                B\n              \n            \n            +\n            \n              ϕ\n              \n                A\n              \n            \n            ⊗\n            \n              ϕ\n              \n                B\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\tfrac {1}{\\sqrt {2}}}\\left(\\psi _{A}\\otimes \\psi _{B}+\\phi _{A}\\otimes \\phi _{B}\\right)}\n  \n\nis a valid joint state that is not separable. States that are not separable are called entangled.\nIf the state for a composite system is entangled, it is impossible to describe either component system A or system B by a state vector. One can instead define reduced density matrices that describe the statistics that can be obtained by making measurements on either component system alone. This necessarily causes a loss of information, though: knowing the reduced density matrices of the individual systems is not enough to reconstruct the state of the composite system. Just as density matrices specify the state of a subsystem of a larger system, analogously, positive operator-valued measures (POVMs) describe the effect on a subsystem of a measurement performed on a larger system. POVMs are extensively used in quantum information theory.\nAs described above, entanglement is a key feature of models of measurement processes in which an apparatus becomes entangled with the system being measured. Systems interacting with the environment in which they reside generally become entangled with that environment, a phenomenon known as quantum decoherence. This can explain why, in practice, quantum effects are difficult to observe in systems larger than microscopic."
      },
      {
        "heading": "Equivalence between formulations",
        "level": 2,
        "content": "There are many mathematically equivalent formulations of quantum mechanics. One of the oldest and most common is the \"transformation theory\" proposed by Paul Dirac, which unifies and generalizes the two earliest formulations of quantum mechanics – matrix mechanics (invented by Werner Heisenberg) and wave mechanics (invented by Erwin Schrödinger). An alternative formulation of quantum mechanics is Feynman's path integral formulation, in which a quantum-mechanical amplitude is considered as a sum over all possible classical and non-classical paths between the initial and final states. This is the quantum-mechanical counterpart of the action principle in classical mechanics."
      },
      {
        "heading": "Symmetries and conservation laws",
        "level": 2,
        "content": "The Hamiltonian \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n is known as the generator of time evolution, since it defines a unitary time-evolution operator \n  \n    \n      \n        U\n        (\n        t\n        )\n        =\n        \n          e\n          \n            −\n            i\n            H\n            t\n            \n              /\n            \n            ℏ\n          \n        \n      \n    \n    {\\displaystyle U(t)=e^{-iHt/\\hbar }}\n  \n for each value of \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n. From this relation between \n  \n    \n      \n        U\n        (\n        t\n        )\n      \n    \n    {\\displaystyle U(t)}\n  \n and \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n, it follows that any observable \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n that commutes with \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n will be conserved: its expectation value will not change over time.: 471  This statement generalizes, as mathematically, any Hermitian operator \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n can generate a family of unitary operators parameterized by a variable \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n. Under the evolution generated by \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, any observable \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n that commutes with \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n will be conserved. Moreover, if \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is conserved by evolution under \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, then \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n is conserved under the evolution generated by \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n. This implies a quantum version of the result proven by Emmy Noether in classical (Lagrangian) mechanics: for every differentiable symmetry of a Hamiltonian, there exists a corresponding conservation law."
      },
      {
        "heading": "Examples",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Free particle",
        "level": 2,
        "content": "The simplest example of a quantum system with a position degree of freedom is a free particle in a single spatial dimension. A free particle is one which is not subject to external influences, so that its Hamiltonian consists only of its kinetic energy:\n\n  \n    \n      \n        H\n        =\n        \n          \n            1\n            \n              2\n              m\n            \n          \n        \n        \n          P\n          \n            2\n          \n        \n        =\n        −\n        \n          \n            \n              ℏ\n              \n                2\n              \n            \n            \n              2\n              m\n            \n          \n        \n        \n          \n            \n              d\n              \n                2\n              \n            \n            \n              d\n              \n                x\n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle H={\\frac {1}{2m}}P^{2}=-{\\frac {\\hbar ^{2}}{2m}}{\\frac {d^{2}}{dx^{2}}}.}\n  \n\nThe general solution of the Schrödinger equation is given by\n\n  \n    \n      \n        ψ\n        (\n        x\n        ,\n        t\n        )\n        =\n        \n          \n            1\n            \n              2\n              π\n            \n          \n        \n        \n          ∫\n          \n            −\n            ∞\n          \n          \n            ∞\n          \n        \n        \n          \n            \n              ψ\n              ^\n            \n          \n        \n        (\n        k\n        ,\n        0\n        )\n        \n          e\n          \n            i\n            (\n            k\n            x\n            −\n            \n              \n                \n                  ℏ\n                  \n                    k\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  m\n                \n              \n            \n            t\n            )\n          \n        \n        \n          d\n        \n        k\n        ,\n      \n    \n    {\\displaystyle \\psi (x,t)={\\frac {1}{\\sqrt {2\\pi }}}\\int _{-\\infty }^{\\infty }{\\hat {\\psi }}(k,0)e^{i(kx-{\\frac {\\hbar k^{2}}{2m}}t)}\\mathrm {d} k,}\n  \n\nwhich is a superposition of all possible plane waves \n  \n    \n      \n        \n          e\n          \n            i\n            (\n            k\n            x\n            −\n            \n              \n                \n                  ℏ\n                  \n                    k\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  m\n                \n              \n            \n            t\n            )\n          \n        \n      \n    \n    {\\displaystyle e^{i(kx-{\\frac {\\hbar k^{2}}{2m}}t)}}\n  \n, which are eigenstates of the momentum operator with momentum \n  \n    \n      \n        p\n        =\n        ℏ\n        k\n      \n    \n    {\\displaystyle p=\\hbar k}\n  \n. The coefficients of the superposition are \n  \n    \n      \n        \n          \n            \n              ψ\n              ^\n            \n          \n        \n        (\n        k\n        ,\n        0\n        )\n      \n    \n    {\\displaystyle {\\hat {\\psi }}(k,0)}\n  \n, which is the Fourier transform of the initial quantum state \n  \n    \n      \n        ψ\n        (\n        x\n        ,\n        0\n        )\n      \n    \n    {\\displaystyle \\psi (x,0)}\n  \n.\nIt is not possible for the solution to be a single momentum eigenstate, or a single position eigenstate, as these are not normalizable quantum states. Instead, we can consider a Gaussian wave packet:\n\n  \n    \n      \n        ψ\n        (\n        x\n        ,\n        0\n        )\n        =\n        \n          \n            1\n            \n              \n                π\n                a\n              \n              \n                4\n              \n            \n          \n        \n        \n          e\n          \n            −\n            \n              \n                \n                  x\n                  \n                    2\n                  \n                \n                \n                  2\n                  a\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\psi (x,0)={\\frac {1}{\\sqrt[{4}]{\\pi a}}}e^{-{\\frac {x^{2}}{2a}}}}\n  \n\nwhich has Fourier transform, and therefore momentum distribution\n\n  \n    \n      \n        \n          \n            \n              ψ\n              ^\n            \n          \n        \n        (\n        k\n        ,\n        0\n        )\n        =\n        \n          \n            \n              a\n              π\n            \n            \n              4\n            \n          \n        \n        \n          e\n          \n            −\n            \n              \n                \n                  a\n                  \n                    k\n                    \n                      2\n                    \n                  \n                \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\hat {\\psi }}(k,0)={\\sqrt[{4}]{\\frac {a}{\\pi }}}e^{-{\\frac {ak^{2}}{2}}}.}\n  \n\nWe see that as we make \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n smaller the spread in position gets smaller, but the spread in momentum gets larger. Conversely, by making \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n larger we make the spread in momentum smaller, but the spread in position gets larger. This illustrates the uncertainty principle.\nAs we let the Gaussian wave packet evolve in time, we see that its center moves through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more and more uncertain. The uncertainty in momentum, however, stays constant."
      },
      {
        "heading": "Particle in a box",
        "level": 2,
        "content": "The particle in a one-dimensional potential energy box is the most mathematically simple example where restraints lead to the quantization of energy levels. The box is defined as having zero potential energy everywhere inside a certain region, and therefore infinite potential energy everywhere outside that region.: 77–78  For the one-dimensional case in the \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n direction, the time-independent Schrödinger equation may be written\n\n  \n    \n      \n        −\n        \n          \n            \n              ℏ\n              \n                2\n              \n            \n            \n              2\n              m\n            \n          \n        \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              ψ\n            \n            \n              d\n              \n                x\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        E\n        ψ\n        .\n      \n    \n    {\\displaystyle -{\\frac {\\hbar ^{2}}{2m}}{\\frac {d^{2}\\psi }{dx^{2}}}=E\\psi .}\n  \n\nWith the differential operator defined by\n\n  \n    \n      \n        \n          \n            \n              \n                p\n                ^\n              \n            \n          \n          \n            x\n          \n        \n        =\n        −\n        i\n        ℏ\n        \n          \n            d\n            \n              d\n              x\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {p}}_{x}=-i\\hbar {\\frac {d}{dx}}}\n  \nthe previous equation is evocative of the classic kinetic energy analogue,\n\n  \n    \n      \n        \n          \n            1\n            \n              2\n              m\n            \n          \n        \n        \n          \n            \n              \n                p\n                ^\n              \n            \n          \n          \n            x\n          \n          \n            2\n          \n        \n        =\n        E\n        ,\n      \n    \n    {\\displaystyle {\\frac {1}{2m}}{\\hat {p}}_{x}^{2}=E,}\n  \n\nwith state \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n in this case having energy \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n coincident with the kinetic energy of the particle.\nThe general solutions of the Schrödinger equation for the particle in a box are\n\n  \n    \n      \n        ψ\n        (\n        x\n        )\n        =\n        A\n        \n          e\n          \n            i\n            k\n            x\n          \n        \n        +\n        B\n        \n          e\n          \n            −\n            i\n            k\n            x\n          \n        \n        \n        \n        E\n        =\n        \n          \n            \n              \n                ℏ\n                \n                  2\n                \n              \n              \n                k\n                \n                  2\n                \n              \n            \n            \n              2\n              m\n            \n          \n        \n      \n    \n    {\\displaystyle \\psi (x)=Ae^{ikx}+Be^{-ikx}\\qquad \\qquad E={\\frac {\\hbar ^{2}k^{2}}{2m}}}\n  \n\nor, from Euler's formula,\n\n  \n    \n      \n        ψ\n        (\n        x\n        )\n        =\n        C\n        sin\n        ⁡\n        (\n        k\n        x\n        )\n        +\n        D\n        cos\n        ⁡\n        (\n        k\n        x\n        )\n        .\n        \n      \n    \n    {\\displaystyle \\psi (x)=C\\sin(kx)+D\\cos(kx).\\!}\n  \n\nThe infinite potential walls of the box determine the values of \n  \n    \n      \n        C\n        ,\n        D\n        ,\n      \n    \n    {\\displaystyle C,D,}\n  \n and \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n at \n  \n    \n      \n        x\n        =\n        0\n      \n    \n    {\\displaystyle x=0}\n  \n and \n  \n    \n      \n        x\n        =\n        L\n      \n    \n    {\\displaystyle x=L}\n  \n where \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n must be zero. Thus, at \n  \n    \n      \n        x\n        =\n        0\n      \n    \n    {\\displaystyle x=0}\n  \n,\n\n  \n    \n      \n        ψ\n        (\n        0\n        )\n        =\n        0\n        =\n        C\n        sin\n        ⁡\n        (\n        0\n        )\n        +\n        D\n        cos\n        ⁡\n        (\n        0\n        )\n        =\n        D\n      \n    \n    {\\displaystyle \\psi (0)=0=C\\sin(0)+D\\cos(0)=D}\n  \n\nand \n  \n    \n      \n        D\n        =\n        0\n      \n    \n    {\\displaystyle D=0}\n  \n. At \n  \n    \n      \n        x\n        =\n        L\n      \n    \n    {\\displaystyle x=L}\n  \n,\n\n  \n    \n      \n        ψ\n        (\n        L\n        )\n        =\n        0\n        =\n        C\n        sin\n        ⁡\n        (\n        k\n        L\n        )\n        ,\n      \n    \n    {\\displaystyle \\psi (L)=0=C\\sin(kL),}\n  \n\nin which \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n cannot be zero as this would conflict with the postulate that \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n has norm 1. Therefore, since \n  \n    \n      \n        sin\n        ⁡\n        (\n        k\n        L\n        )\n        =\n        0\n      \n    \n    {\\displaystyle \\sin(kL)=0}\n  \n, \n  \n    \n      \n        k\n        L\n      \n    \n    {\\displaystyle kL}\n  \n must be an integer multiple of \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n,\n\n  \n    \n      \n        k\n        =\n        \n          \n            \n              n\n              π\n            \n            L\n          \n        \n        \n        \n        n\n        =\n        1\n        ,\n        2\n        ,\n        3\n        ,\n        …\n        .\n      \n    \n    {\\displaystyle k={\\frac {n\\pi }{L}}\\qquad \\qquad n=1,2,3,\\ldots .}\n  \n\nThis constraint on \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n implies a constraint on the energy levels, yielding\n\n  \n    \n      \n        \n          E\n          \n            n\n          \n        \n        =\n        \n          \n            \n              \n                ℏ\n                \n                  2\n                \n              \n              \n                π\n                \n                  2\n                \n              \n              \n                n\n                \n                  2\n                \n              \n            \n            \n              2\n              m\n              \n                L\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                n\n                \n                  2\n                \n              \n              \n                h\n                \n                  2\n                \n              \n            \n            \n              8\n              m\n              \n                L\n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle E_{n}={\\frac {\\hbar ^{2}\\pi ^{2}n^{2}}{2mL^{2}}}={\\frac {n^{2}h^{2}}{8mL^{2}}}.}\n  \n\nA finite potential well is the generalization of the infinite potential well problem to potential wells having finite depth. The finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well. Another related problem is that of the rectangular potential barrier, which furnishes a model for the quantum tunneling effect that plays an important role in the performance of modern technologies such as flash memory and scanning tunneling microscopy."
      },
      {
        "heading": "Harmonic oscillator",
        "level": 2,
        "content": "As in the classical case, the potential for the quantum harmonic oscillator is given by: 234 \n\n  \n    \n      \n        V\n        (\n        x\n        )\n        =\n        \n          \n            1\n            2\n          \n        \n        m\n        \n          ω\n          \n            2\n          \n        \n        \n          x\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle V(x)={\\frac {1}{2}}m\\omega ^{2}x^{2}.}\n  \n\nThis problem can either be treated by directly solving the Schrödinger equation, which is not trivial, or by using the more elegant \"ladder method\" first proposed by Paul Dirac. The eigenstates are given by\n\n  \n    \n      \n        \n          ψ\n          \n            n\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            \n              1\n              \n                \n                  2\n                  \n                    n\n                  \n                \n                \n                n\n                !\n              \n            \n          \n        \n        ⋅\n        \n          \n            (\n            \n              \n                \n                  m\n                  ω\n                \n                \n                  π\n                  ℏ\n                \n              \n            \n            )\n          \n          \n            1\n            \n              /\n            \n            4\n          \n        \n        ⋅\n        \n          e\n          \n            −\n            \n              \n                \n                  m\n                  ω\n                  \n                    x\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  ℏ\n                \n              \n            \n          \n        \n        ⋅\n        \n          H\n          \n            n\n          \n        \n        \n          (\n          \n            \n              \n                \n                  \n                    m\n                    ω\n                  \n                  ℏ\n                \n              \n            \n            x\n          \n          )\n        \n        ,\n        \n      \n    \n    {\\displaystyle \\psi _{n}(x)={\\sqrt {\\frac {1}{2^{n}\\,n!}}}\\cdot \\left({\\frac {m\\omega }{\\pi \\hbar }}\\right)^{1/4}\\cdot e^{-{\\frac {m\\omega x^{2}}{2\\hbar }}}\\cdot H_{n}\\left({\\sqrt {\\frac {m\\omega }{\\hbar }}}x\\right),\\qquad }\n  \n\n  \n    \n      \n        n\n        =\n        0\n        ,\n        1\n        ,\n        2\n        ,\n        …\n        .\n      \n    \n    {\\displaystyle n=0,1,2,\\ldots .}\n  \n\nwhere Hn are the Hermite polynomials\n\n  \n    \n      \n        \n          H\n          \n            n\n          \n        \n        (\n        x\n        )\n        =\n        (\n        −\n        1\n        \n          )\n          \n            n\n          \n        \n        \n          e\n          \n            \n              x\n              \n                2\n              \n            \n          \n        \n        \n          \n            \n              d\n              \n                n\n              \n            \n            \n              d\n              \n                x\n                \n                  n\n                \n              \n            \n          \n        \n        \n          (\n          \n            e\n            \n              −\n              \n                x\n                \n                  2\n                \n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle H_{n}(x)=(-1)^{n}e^{x^{2}}{\\frac {d^{n}}{dx^{n}}}\\left(e^{-x^{2}}\\right),}\n  \n\nand the corresponding energy levels are\n\n  \n    \n      \n        \n          E\n          \n            n\n          \n        \n        =\n        ℏ\n        ω\n        \n          (\n          \n            n\n            +\n            \n              \n                1\n                2\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle E_{n}=\\hbar \\omega \\left(n+{1 \\over 2}\\right).}\n  \n\nThis is another example illustrating the discretization of energy for bound states."
      },
      {
        "heading": "Mach–Zehnder interferometer",
        "level": 2,
        "content": "The Mach–Zehnder interferometer (MZI) illustrates the concepts of superposition and interference with linear algebra in dimension 2, rather than differential equations. It can be seen as a simplified version of the double-slit experiment, but it is of interest in its own right, for example in the delayed choice quantum eraser, the Elitzur–Vaidman bomb tester, and in studies of quantum entanglement.\nWe can model a photon going through the interferometer by considering that at each point it can be in a superposition of only two paths: the \"lower\" path which starts from the left, goes straight through both beam splitters, and ends at the top, and the \"upper\" path which starts from the bottom, goes straight through both beam splitters, and ends at the right. The quantum state of the photon is therefore a vector \n  \n    \n      \n        ψ\n        ∈\n        \n          \n            C\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\psi \\in \\mathbb {C} ^{2}}\n  \n that is a superposition of the \"lower\" path \n  \n    \n      \n        \n          ψ\n          \n            l\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  1\n                \n              \n              \n                \n                  0\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle \\psi _{l}={\\begin{pmatrix}1\\\\0\\end{pmatrix}}}\n  \n and the \"upper\" path \n  \n    \n      \n        \n          ψ\n          \n            u\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  0\n                \n              \n              \n                \n                  1\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle \\psi _{u}={\\begin{pmatrix}0\\\\1\\end{pmatrix}}}\n  \n, that is, \n  \n    \n      \n        ψ\n        =\n        α\n        \n          ψ\n          \n            l\n          \n        \n        +\n        β\n        \n          ψ\n          \n            u\n          \n        \n      \n    \n    {\\displaystyle \\psi =\\alpha \\psi _{l}+\\beta \\psi _{u}}\n  \n for complex \n  \n    \n      \n        α\n        ,\n        β\n      \n    \n    {\\displaystyle \\alpha ,\\beta }\n  \n. In order to respect the postulate that \n  \n    \n      \n        ⟨\n        ψ\n        ,\n        ψ\n        ⟩\n        =\n        1\n      \n    \n    {\\displaystyle \\langle \\psi ,\\psi \\rangle =1}\n  \n we require that \n  \n    \n      \n        \n          |\n        \n        α\n        \n          \n            |\n          \n          \n            2\n          \n        \n        +\n        \n          |\n        \n        β\n        \n          \n            |\n          \n          \n            2\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle |\\alpha |^{2}+|\\beta |^{2}=1}\n  \n.\nBoth beam splitters are modelled as the unitary matrix \n  \n    \n      \n        B\n        =\n        \n          \n            1\n            \n              2\n            \n          \n        \n        \n          \n            (\n            \n              \n                \n                  1\n                \n                \n                  i\n                \n              \n              \n                \n                  i\n                \n                \n                  1\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle B={\\frac {1}{\\sqrt {2}}}{\\begin{pmatrix}1&i\\\\i&1\\end{pmatrix}}}\n  \n, which means that when a photon meets the beam splitter it will either stay on the same path with a probability amplitude of \n  \n    \n      \n        1\n        \n          /\n        \n        \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle 1/{\\sqrt {2}}}\n  \n, or be reflected to the other path with a probability amplitude of \n  \n    \n      \n        i\n        \n          /\n        \n        \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle i/{\\sqrt {2}}}\n  \n. The phase shifter on the upper arm is modelled as the unitary matrix \n  \n    \n      \n        P\n        =\n        \n          \n            (\n            \n              \n                \n                  1\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    e\n                    \n                      i\n                      Δ\n                      Φ\n                    \n                  \n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle P={\\begin{pmatrix}1&0\\\\0&e^{i\\Delta \\Phi }\\end{pmatrix}}}\n  \n, which means that if the photon is on the \"upper\" path it will gain a relative phase of \n  \n    \n      \n        Δ\n        Φ\n      \n    \n    {\\displaystyle \\Delta \\Phi }\n  \n, and it will stay unchanged if it is in the lower path.\nA photon that enters the interferometer from the left will then be acted upon with a beam splitter \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n, a phase shifter \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n, and another beam splitter \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n, and so end up in the state\n\n  \n    \n      \n        B\n        P\n        B\n        \n          ψ\n          \n            l\n          \n        \n        =\n        i\n        \n          e\n          \n            i\n            Δ\n            Φ\n            \n              /\n            \n            2\n          \n        \n        \n          \n            (\n            \n              \n                \n                  −\n                  sin\n                  ⁡\n                  (\n                  Δ\n                  Φ\n                  \n                    /\n                  \n                  2\n                  )\n                \n              \n              \n                \n                  cos\n                  ⁡\n                  (\n                  Δ\n                  Φ\n                  \n                    /\n                  \n                  2\n                  )\n                \n              \n            \n            )\n          \n        \n        ,\n      \n    \n    {\\displaystyle BPB\\psi _{l}=ie^{i\\Delta \\Phi /2}{\\begin{pmatrix}-\\sin(\\Delta \\Phi /2)\\\\\\cos(\\Delta \\Phi /2)\\end{pmatrix}},}\n  \n\nand the probabilities that it will be detected at the right or at the top are given respectively by\n\n  \n    \n      \n        p\n        (\n        u\n        )\n        =\n        \n          |\n        \n        ⟨\n        \n          ψ\n          \n            u\n          \n        \n        ,\n        B\n        P\n        B\n        \n          ψ\n          \n            l\n          \n        \n        ⟩\n        \n          \n            |\n          \n          \n            2\n          \n        \n        =\n        \n          cos\n          \n            2\n          \n        \n        ⁡\n        \n          \n            \n              Δ\n              Φ\n            \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle p(u)=|\\langle \\psi _{u},BPB\\psi _{l}\\rangle |^{2}=\\cos ^{2}{\\frac {\\Delta \\Phi }{2}},}\n  \n\n  \n    \n      \n        p\n        (\n        l\n        )\n        =\n        \n          |\n        \n        ⟨\n        \n          ψ\n          \n            l\n          \n        \n        ,\n        B\n        P\n        B\n        \n          ψ\n          \n            l\n          \n        \n        ⟩\n        \n          \n            |\n          \n          \n            2\n          \n        \n        =\n        \n          sin\n          \n            2\n          \n        \n        ⁡\n        \n          \n            \n              Δ\n              Φ\n            \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle p(l)=|\\langle \\psi _{l},BPB\\psi _{l}\\rangle |^{2}=\\sin ^{2}{\\frac {\\Delta \\Phi }{2}}.}\n  \n\nOne can therefore use the Mach–Zehnder interferometer to estimate the phase shift by estimating these probabilities.\nIt is interesting to consider what would happen if the photon were definitely in either the \"lower\" or \"upper\" paths between the beam splitters. This can be accomplished by blocking one of the paths, or equivalently by removing the first beam splitter (and feeding the photon from the left or the bottom, as desired). In both cases, there will be no interference between the paths anymore, and the probabilities are given by \n  \n    \n      \n        p\n        (\n        u\n        )\n        =\n        p\n        (\n        l\n        )\n        =\n        1\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle p(u)=p(l)=1/2}\n  \n, independently of the phase \n  \n    \n      \n        Δ\n        Φ\n      \n    \n    {\\displaystyle \\Delta \\Phi }\n  \n. From this we can conclude that the photon does not take one path or another after the first beam splitter, but rather that it is in a genuine quantum superposition of the two paths."
      },
      {
        "heading": "Applications",
        "level": 1,
        "content": "Quantum mechanics has had enormous success in explaining many of the features of our universe, with regard to small-scale and discrete quantities and interactions which cannot be explained by classical methods. Quantum mechanics is often the only theory that can reveal the individual behaviors of the subatomic particles that make up all forms of matter (electrons, protons, neutrons, photons, and others). Solid-state physics and materials science are dependent upon quantum mechanics.\nIn many aspects, modern technology operates at a scale where quantum effects are significant. Important applications of quantum theory include quantum chemistry, quantum optics, quantum computing, superconducting magnets, light-emitting diodes, the optical amplifier and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy. Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-molecule DNA."
      },
      {
        "heading": "Relation to other scientific theories",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Classical mechanics",
        "level": 2,
        "content": "The rules of quantum mechanics assert that the state space of a system is a Hilbert space and that observables of the system are Hermitian operators acting on vectors in that space – although they do not tell us which Hilbert space or which operators. These can be chosen appropriately in order to obtain a quantitative description of a quantum system, a necessary step in making physical predictions. An important guide for making these choices is the correspondence principle, a heuristic which states that the predictions of quantum mechanics reduce to those of classical mechanics in the regime of large quantum numbers. One can also start from an established classical model of a particular system, and then try to guess the underlying quantum model that would give rise to the classical model in the correspondence limit. This approach is known as quantization.: 299 \nWhen quantum mechanics was originally formulated, it was applied to models whose correspondence limit was non-relativistic classical mechanics. For instance, the well-known model of the quantum harmonic oscillator uses an explicitly non-relativistic expression for the kinetic energy of the oscillator, and is thus a quantum version of the classical harmonic oscillator.: 234 \nComplications arise with chaotic systems, which do not have good quantum numbers, and quantum chaos studies the relationship between classical and quantum descriptions in these systems.: 353 \nQuantum decoherence is a mechanism through which quantum systems lose coherence, and thus become incapable of displaying many typically quantum effects: quantum superpositions become simply probabilistic mixtures, and quantum entanglement becomes simply classical correlations.: 687–730  Quantum coherence is not typically evident at macroscopic scales, though at temperatures approaching absolute zero quantum behavior may manifest macroscopically.\nMany macroscopic properties of a classical system are a direct consequence of the quantum behavior of its parts. For example, the stability of bulk matter (consisting of atoms and molecules which would quickly collapse under electric forces alone), the rigidity of solids, and the mechanical, thermal, chemical, optical and magnetic properties of matter are all results of the interaction of electric charges under the rules of quantum mechanics."
      },
      {
        "heading": "Special relativity and electrodynamics",
        "level": 2,
        "content": "Early attempts to merge quantum mechanics with special relativity involved the replacement of the Schrödinger equation with a covariant equation such as the Klein–Gordon equation or the Dirac equation. While these theories were successful in explaining many experimental results, they had certain unsatisfactory qualities stemming from their neglect of the relativistic creation and annihilation of particles. A fully relativistic quantum theory required the development of quantum field theory, which applies quantization to a field (rather than a fixed set of particles). The first complete quantum field theory, quantum electrodynamics, provides a fully quantum description of the electromagnetic interaction. Quantum electrodynamics is, along with general relativity, one of the most accurate physical theories ever devised.\nThe full apparatus of quantum field theory is often unnecessary for describing electrodynamic systems. A simpler approach, one that has been used since the inception of quantum mechanics, is to treat charged particles as quantum mechanical objects being acted on by a classical electromagnetic field. For example, the elementary quantum model of the hydrogen atom describes the electric field of the hydrogen atom using a classical \n  \n    \n      \n        \n          −\n          \n            e\n            \n              2\n            \n          \n          \n            /\n          \n          (\n          4\n          π\n          \n            ϵ\n            \n              \n                \n                \n                  0\n                \n              \n            \n          \n          r\n          )\n        \n      \n    \n    {\\displaystyle \\textstyle -e^{2}/(4\\pi \\epsilon _{_{0}}r)}\n  \n Coulomb potential.: 285  Likewise, in a Stern–Gerlach experiment, a charged particle is modeled as a quantum system, while the background magnetic field is described classically.: 26  This \"semi-classical\" approach fails if quantum fluctuations in the electromagnetic field play an important role, such as in the emission of photons by charged particles.\nQuantum field theories for the strong nuclear force and the weak nuclear force have also been developed. The quantum field theory of the strong nuclear force is called quantum chromodynamics, and describes the interactions of subnuclear particles such as quarks and gluons. The weak nuclear force and the electromagnetic force were unified, in their quantized forms, into a single quantum field theory (known as electroweak theory), by the physicists Abdus Salam, Sheldon Glashow and Steven Weinberg."
      },
      {
        "heading": "Relation to general relativity",
        "level": 2,
        "content": "Even though the predictions of both quantum theory and general relativity have been supported by rigorous and repeated empirical evidence, their abstract formalisms contradict each other and they have proven extremely difficult to incorporate into one consistent, cohesive model. Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory of quantum gravity is an important issue in physical cosmology and the search by physicists for an elegant \"Theory of Everything\" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th- and 21st-century physics. This TOE would combine not only the models of subatomic physics but also derive the four fundamental forces of nature from a single force or phenomenon.\n\nOne proposal for doing so is string theory, which posits that the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In string theory, one of the many vibrational states of the string corresponds to the graviton, a quantum mechanical particle that carries gravitational force.\nAnother popular theory is loop quantum gravity (LQG), which describes quantum properties of gravity and is thus a theory of quantum spacetime. LQG is an attempt to merge and adapt standard quantum mechanics and standard general relativity. This theory describes space as an extremely fine fabric \"woven\" of finite loops called spin networks. The evolution of a spin network over time is called a spin foam. The characteristic length scale of a spin foam is the Planck length, approximately 1.616×10−35 m, and so lengths shorter than the Planck length are not physically meaningful in LQG."
      },
      {
        "heading": "Philosophical implications",
        "level": 1,
        "content": "Since its inception, the many counter-intuitive aspects and results of quantum mechanics have provoked strong philosophical debates and many interpretations. The arguments centre on the probabilistic nature of quantum mechanics, the difficulties with wavefunction collapse and the related measurement problem, and quantum nonlocality. Perhaps the only consensus that exists about these issues is that there is no consensus. Richard Feynman once said, \"I think I can safely say that nobody understands quantum mechanics.\" According to Steven Weinberg, \"There is now in my opinion no entirely satisfactory interpretation of quantum mechanics.\"\nThe views of Niels Bohr, Werner Heisenberg and other physicists are often grouped together as the \"Copenhagen interpretation\". According to these views, the probabilistic nature of quantum mechanics is not a temporary feature which will eventually be replaced by a deterministic theory, but is instead a final renunciation of the classical idea of \"causality\". Bohr in particular emphasized that any well-defined application of the quantum mechanical formalism must always make reference to the experimental arrangement, due to the complementary nature of evidence obtained under different experimental situations. Copenhagen-type interpretations were adopted by Nobel laureates in quantum physics, including Bohr, Heisenberg, Schrödinger, Feynman, and Zeilinger as well as 21st-century researchers in quantum foundations.\nAlbert Einstein, himself one of the founders of quantum theory, was troubled by its apparent failure to respect some cherished metaphysical principles, such as determinism and locality. Einstein's long-running exchanges with Bohr about the meaning and status of quantum mechanics are now known as the Bohr–Einstein debates. Einstein believed that underlying quantum mechanics must be a theory that explicitly forbids action at a distance. He argued that quantum mechanics was incomplete, a theory that was valid but not fundamental, analogous to how thermodynamics is valid, but the fundamental theory behind it is statistical mechanics. In 1935, Einstein and his collaborators Boris Podolsky and Nathan Rosen published an argument that the principle of locality implies the incompleteness of quantum mechanics, a thought experiment later termed the Einstein–Podolsky–Rosen paradox. In 1964, John Bell showed that EPR's principle of locality, together with determinism, was actually incompatible with quantum mechanics: they implied constraints on the correlations produced by distance systems, now known as Bell inequalities, that can be violated by entangled particles. Since then several experiments have been performed to obtain these correlations, with the result that they do in fact violate Bell inequalities, and thus falsify the conjunction of locality with determinism.\nBohmian mechanics shows that it is possible to reformulate quantum mechanics to make it deterministic, at the price of making it explicitly nonlocal. It attributes not only a wave function to a physical system, but in addition a real position, that evolves deterministically under a nonlocal guiding equation. The evolution of a physical system is given at all times by the Schrödinger equation together with the guiding equation; there is never a collapse of the wave function. This solves the measurement problem.\n\nEverett's many-worlds interpretation, formulated in 1956, holds that all the possibilities described by quantum theory simultaneously occur in a multiverse composed of mostly independent parallel universes. This is a consequence of removing the axiom of the collapse of the wave packet. All possible states of the measured system and the measuring apparatus, together with the observer, are present in a real physical quantum superposition. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we do not observe the multiverse as a whole, but only one parallel universe at a time. Exactly how this is supposed to work has been the subject of much debate. Several attempts have been made to make sense of this and derive the Born rule, with no consensus on whether they have been successful.\nRelational quantum mechanics appeared in the late 1990s as a modern derivative of Copenhagen-type ideas, and Quantum Bayesianism was developed some years later."
      },
      {
        "heading": "History",
        "level": 1,
        "content": "Quantum mechanics was developed in the early decades of the 20th century, driven by the need to explain phenomena that, in some cases, had been observed in earlier times. Scientific inquiry into the wave nature of light began in the 17th and 18th centuries, when scientists such as Robert Hooke, Christiaan Huygens and Leonhard Euler proposed a wave theory of light based on experimental observations. In 1803 English polymath Thomas Young described the famous double-slit experiment. This experiment played a major role in the general acceptance of the wave theory of light.\nDuring the early 19th century, chemical research by John Dalton and Amedeo Avogadro lent weight to the atomic theory of matter, an idea that James Clerk Maxwell, Ludwig Boltzmann and others built upon to establish the kinetic theory of gases. The successes of kinetic theory gave further credence to the idea that matter is composed of atoms, yet the theory also had shortcomings that would only be resolved by the development of quantum mechanics. While the early conception of atoms from Greek philosophy had been that they were indivisible units – the word \"atom\" deriving from the Greek for 'uncuttable' – the 19th century saw the formulation of hypotheses about subatomic structure. One important discovery in that regard was Michael Faraday's 1838 observation of a glow caused by an electrical discharge inside a glass tube containing gas at low pressure. Julius Plücker, Johann Wilhelm Hittorf and Eugen Goldstein carried on and improved upon Faraday's work, leading to the identification of cathode rays, which J. J. Thomson found to consist of subatomic particles that would be called electrons.\n\nThe black-body radiation problem was discovered by Gustav Kirchhoff in 1859. In 1900, Max Planck proposed the hypothesis that energy is radiated and absorbed in discrete \"quanta\" (or energy packets), yielding a calculation that precisely matched the observed patterns of black-body radiation. The word quantum derives from the Latin, meaning \"how great\" or \"how much\". According to Planck, quantities of energy could be thought of as divided into \"elements\" whose size (E) would be proportional to their frequency (ν):\n\n  \n    \n      \n        E\n        =\n        h\n        ν\n         \n      \n    \n    {\\displaystyle E=h\\nu \\ }\n  \n,\nwhere h is the Planck constant. Planck cautiously insisted that this was only an aspect of the processes of absorption and emission of radiation and was not the physical reality of the radiation. In fact, he considered his quantum hypothesis a mathematical trick to get the right answer rather than a sizable discovery. However, in 1905 Albert Einstein interpreted Planck's quantum hypothesis realistically and used it to explain the photoelectric effect, in which shining light on certain materials can eject electrons from the material. Niels Bohr then developed Planck's ideas about radiation into a model of the hydrogen atom that successfully predicted the spectral lines of hydrogen. Einstein further developed this idea to show that an electromagnetic wave such as light could also be described as a particle (later called the photon), with a discrete amount of energy that depends on its frequency. In his paper \"On the Quantum Theory of Radiation\", Einstein expanded on the interaction between energy and matter to explain the absorption and emission of energy by atoms. Although overshadowed at the time by his general theory of relativity, this paper articulated the mechanism underlying the stimulated emission of radiation, which became the basis of the laser.\n\nThis phase is known as the old quantum theory. Never complete or self-consistent, the old quantum theory was rather a set of heuristic corrections to classical mechanics. The theory is now understood as a semi-classical approximation to modern quantum mechanics. Notable results from this period include, in addition to the work of Planck, Einstein and Bohr mentioned above, Einstein and Peter Debye's work on the specific heat of solids, Bohr and Hendrika Johanna van Leeuwen's proof that classical physics cannot account for diamagnetism, and Arnold Sommerfeld's extension of the Bohr model to include special-relativistic effects.\nIn the mid-1920s quantum mechanics was developed to become the standard formulation for atomic physics. In 1923, the French physicist Louis de Broglie put forward his theory of matter waves by stating that particles can exhibit wave characteristics and vice versa. Building on de Broglie's approach, modern quantum mechanics was born in 1925, when the German physicists Werner Heisenberg, Max Born, and Pascual Jordan developed matrix mechanics and the Austrian physicist Erwin Schrödinger invented wave mechanics. Born introduced the probabilistic interpretation of Schrödinger's wave function in July 1926. Thus, the entire field of quantum physics emerged, leading to its wider acceptance at the Fifth Solvay Conference in 1927.\nBy 1930, quantum mechanics had been further unified and formalized by David Hilbert, Paul Dirac and John von Neumann with greater emphasis on measurement, the statistical nature of our knowledge of reality, and philosophical speculation about the 'observer'. It has since permeated many disciplines, including quantum chemistry, quantum electronics, quantum optics, and quantum information science. It also provides a useful framework for many features of the modern periodic table of elements, and describes the behaviors of atoms during chemical bonding and the flow of electrons in computer semiconductors, and therefore plays a crucial role in many modern technologies. While quantum mechanics was constructed to describe the world of the very small, it is also needed to explain some macroscopic phenomena such as superconductors and superfluids."
      },
      {
        "heading": "See also",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Explanatory notes",
        "level": 1,
        "content": ""
      },
      {
        "heading": "References",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Further reading",
        "level": 1,
        "content": ""
      },
      {
        "heading": "External links",
        "level": 1,
        "content": "\nJ. O'Connor and E. F. Robertson: A history of quantum mechanics.\nIntroduction to Quantum Theory at Quantiki.\nQuantum Physics Made Relatively Simple: three video lectures by Hans Bethe.\n Course material \n\nQuantum Cook Book and PHYS 201: Fundamentals of Physics II by Ramamurti Shankar, Yale OpenCourseware.\nModern Physics: With waves, thermodynamics, and optics – an online textbook.\nMIT OpenCourseWare: Chemistry and Physics. See 8.04, 8.05 and 8.06.\n⁠5+1/2⁠ Examples in Quantum Mechanics.\n Philosophy \n\nIsmael, Jenann. \"Quantum Mechanics\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\nZalta, Edward N. (ed.). \"Philosophical Issues in Quantum Theory\". Stanford Encyclopedia of Philosophy."
      }
    ],
    "summary": "Quantum mechanics is the fundamental physical theory that describes the behavior of matter and of light; its unusual characteristics typically occur at and below the scale of atoms.: 1.1  It is the foundation of all quantum physics, which includes quantum chemistry, quantum field theory, quantum technology, and quantum information science.\nQuantum mechanics can describe many systems that classical physics cannot. Classical physics can describe many aspects of nature at an ordinary (macroscopic and (optical) microscopic) scale, but is not sufficient for describing them at very small submicroscopic (atomic and subatomic) scales. Classical mechanics can be derived from quantum mechanics as an approximation that is valid at ordinary scales.\nQuantum systems have bound states that are quantized to discrete values of energy, momentum, angular momentum, and other quantities, in contrast to classical systems where these quantities can be measured continuously. Measurements of quantum systems show characteristics of both particles and waves (wave–particle duality), and there are limits to how accurately the value of a physical quantity can be predicted prior to its measurement, given a complete set of initial conditions (the uncertainty principle).\nQuantum mechanics arose gradually from theories to explain observations that could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and the correspondence between energy and frequency in Albert Einstein's 1905 paper, which explained the photoelectric effect. These early attempts to understand microscopic phenomena, now known as the \"old quantum theory\", led to the full development of quantum mechanics in the mid-1920s by Niels Bohr, Erwin Schrödinger, Werner Heisenberg, Max Born, Paul Dirac and others. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical entity called the wave function provides information, in the form of probability amplitudes, about what measurements of a particle's energy, momentum, and other physical properties may yield."
  },
  {
    "title": "Mecánica cuántica",
    "source": "https://es.wikipedia.org/wiki/Mec%C3%A1nica_cu%C3%A1ntica",
    "language": "es",
    "chunks": [
      {
        "heading": "Contexto histórico",
        "level": 1,
        "content": "La mecánica cuántica es, cronológicamente hablando, la última de las grandes ramas de la física. Se formuló a principios del siglo XX, casi al mismo tiempo que la teoría de la relatividad, aunque el grueso de la mecánica cuántica se desarrolló a partir de 1920 (siendo la teoría de la relatividad especial de 1905 y la teoría general de la relatividad de 1915).\nAdemás al advenimiento de la mecánica cuántica existían diversos problemas no resueltos en la electrodinámica clásica. El primero de estos problemas era la emisión de radiación de cualquier objeto en equilibrio, llamada radiación térmica, que es la que proviene de la vibración microscópica de las partículas que lo componen. Usando las ecuaciones de la electrodinámica clásica, la energía que emitía esta radiación térmica tendía al infinito, si se suman todas las frecuencias que emitía el objeto, con ilógico resultado para los físicos. También la estabilidad de los átomos no podía ser explicada por el electromagnetismo clásico, y la noción de que el electrón fuera o bien una partícula clásica puntual o bien una cáscara esférica de dimensiones finitas resultaban igualmente problemáticas para esto."
      },
      {
        "heading": "Radiación electromagnética",
        "level": 2,
        "content": "El problema de la radiación electromagnética de un cuerpo negro fue uno de los primeros problemas resueltos en el seno de la mecánica cuántica. Es en el seno de la mecánica estadística donde surgen por primera vez las ideas cuánticas en 1900. Al físico alemán Max Planck se le ocurrió un artificio matemático: si en el proceso aritmético se sustituía la integral de esas frecuencias por una suma no continua (discreta), se dejaba de obtener infinito como resultado, con lo que se eliminaba el problema; además, el resultado obtenido concordaba con lo que después era medido.\nFue Max Planck quien entonces enunció la hipótesis de que la radiación electromagnética es absorbida y emitida por la materia en forma de «cuantos» de luz o fotones de energía cuantizados introduciendo una constante estadística, que se denominó constante de Planck. Su historia es inherente al siglo XX, ya que la primera formulación cuántica de un fenómeno fue dada a conocer por el mismo Planck el 14 de diciembre de 1900 en una sesión de la Sociedad Física de la Academia de Ciencias de Berlín.[2]​\nLa idea de Planck habría permanecido muchos años solo como hipótesis sin verificar por completo si Albert Einstein no la hubiera retomado, proponiendo que la luz, en ciertas circunstancias, se comporta como partículas de energía (los cuantos de luz o fotones) en su explicación del efecto fotoeléctrico. Fue Albert Einstein quien completó en 1905 las correspondientes leyes del movimiento su teoría especial de la relatividad, demostrando que el electromagnetismo era una teoría esencialmente no mecánica. Culminaba así lo que se ha dado en llamar física clásica, es decir, la física no-cuántica.\nUsó este punto de vista llamado por él «heurístico», para desarrollar su teoría del efecto fotoeléctrico, publicando esta hipótesis en 1905, lo que le valió el Premio Nobel de Física de 1921. Esta hipótesis fue aplicada también para proponer una teoría sobre el calor específico, es decir, la que resuelve cuál es la cantidad de calor necesaria para aumentar en una unidad la temperatura de la unidad de masa de un cuerpo.\nEl siguiente paso importante se dio hacia 1925, cuando Louis De Broglie propuso que cada partícula material tiene una longitud de onda asociada, inversamente proporcional a su masa, y a su velocidad. Así quedaba establecida la dualidad onda/materia. Poco tiempo después Erwin Schrödinger formuló una ecuación de movimiento para las «ondas de materia», cuya existencia había propuesto De Broglie y varios experimentos sugerían que eran reales.\nLa mecánica cuántica introduce una serie de hechos contraintuitivos que no aparecían en los paradigmas físicos anteriores; con ella se descubre que el mundo atómico no se comporta como esperaríamos. Los conceptos de incertidumbre o cuantización son introducidos por primera vez aquí. Además la mecánica cuántica es la teoría científica que ha proporcionado las predicciones experimentales más exactas hasta el momento, a pesar de estar sujeta a las probabilidades."
      },
      {
        "heading": "Inestabilidad de los átomos clásicos",
        "level": 2,
        "content": "El segundo problema importante que la mecánica cuántica resolvió a través del modelo de Bohr, fue el de la estabilidad de los átomos. De acuerdo con la teoría clásica un electrón orbitando alrededor de un núcleo cargado positivamente debería emitir energía electromagnética perdiendo así velocidad hasta caer sobre el núcleo. La evidencia empírica era que esto no sucedía, y sería la mecánica cuántica la que resolvería este hecho primero mediante postulados ad hoc formulados por Bohr y más tarde mediante modelos como el modelo atómico de Schrödinger basados en supuestos más generales. A continuación se explica el fracaso del modelo clásico.\nEn mecánica clásica, un átomo de hidrógeno es un tipo de problema de los dos cuerpos en que el protón sería el primer cuerpo que tiene más del 99% de la masa del sistema y el electrón es el segundo cuerpo que es mucho más ligero. Para resolver el problema de los dos cuerpos es conveniente hacer la descripción del sistema, colocando el origen del sistema de referencia en el centro de masa de la partícula de mayor masa, esta descripción es correcta considerando como masa de la otra partícula la masa reducida que viene dada por\n\n  \n    \n      \n        μ\n        \n        =\n        \n        \n          \n            \n              \n                m\n                \n                  e\n                \n              \n              \n                m\n                \n                  p\n                \n              \n            \n            \n              \n                m\n                \n                  e\n                \n              \n              +\n              \n                m\n                \n                  p\n                \n              \n            \n          \n        \n        ≈\n        0\n        ,\n        999\n        \n          m\n          \n            e\n          \n        \n      \n    \n    {\\displaystyle \\mu \\,=\\,{\\frac {m_{e}m_{p}}{m_{e}+m_{p}}}\\approx 0,999m_{e}}\n  \n\nSiendo \n  \n    \n      \n        \n          \n            m\n            \n              p\n            \n          \n        \n      \n    \n    {\\displaystyle \\scriptstyle m_{p}}\n  \n la masa del protón y \n  \n    \n      \n        \n          \n            m\n            \n              e\n            \n          \n        \n      \n    \n    {\\displaystyle \\scriptstyle m_{e}}\n  \n la masa del electrón. En ese caso el problema del átomo de hidrógeno parece admitir una solución simple en la que el electrón se moviera en órbitas elípticas alrededor del núcleo atómico. Sin embargo, existe un problema con la solución clásica, de acuerdo con las predicciones del electromagnetismo una partícula eléctrica que sigue un movimiento acelerado, como sucedería al describir una elipse debería emitir radiación electromagnética, y por tanto perder energía cinética, la cantidad de energía radiada sería de hecho:\n\n  \n    \n      \n        \n          \n            \n              d\n              \n                E\n                \n                  r\n                \n              \n            \n            \n              d\n              t\n            \n          \n        \n        =\n        \n          \n            \n              \n                e\n                \n                  2\n                \n              \n              \n                a\n                \n                  2\n                \n              \n              \n                γ\n                \n                  4\n                \n              \n            \n            \n              6\n              π\n              \n                ϵ\n                \n                  0\n                \n              \n              \n                c\n                \n                  3\n                \n              \n            \n          \n        \n        ≈\n        \n          \n            π\n            96\n          \n        \n        \n          \n            \n              \n                e\n                \n                  14\n                \n              \n              \n                m\n                \n                  e\n                \n                \n                  2\n                \n              \n              \n                γ\n                \n                  4\n                \n              \n            \n            \n              \n                ϵ\n                \n                  0\n                \n                \n                  7\n                \n              \n              \n                h\n                \n                  8\n                \n              \n              \n                c\n                \n                  3\n                \n              \n            \n          \n        \n        ≥\n        5\n        ,\n        1\n        ⋅\n        \n          10\n          \n            −\n            8\n          \n        \n        \n          \n            watt\n          \n        \n      \n    \n    {\\displaystyle {\\frac {dE_{r}}{dt}}={\\frac {e^{2}a^{2}\\gamma ^{4}}{6\\pi \\epsilon _{0}c^{3}}}\\approx {\\frac {\\pi }{96}}{\\frac {e^{14}m_{e}^{2}\\gamma ^{4}}{\\epsilon _{0}^{7}h^{8}c^{3}}}\\geq 5,1\\cdot 10^{-8}{\\mbox{watt}}}\n  \n\nEse proceso acabaría con el colapso del átomo sobre el núcleo en un tiempo muy corto dadas las grandes aceleraciones existentes. A partir de los datos de la ecuación anterior el tiempo de colapso sería de 10-8 s, es decir, de acuerdo con la física clásica los átomos de hidrógeno no serían estables y no podrían existir más de una cienmillonésima de segundo.\nEsa incompatibilidad entre las predicciones del modelo clásico y la realidad observada llevó a buscar un modelo que explicara fenomenológicamente el átomo. El modelo atómico de Bohr era un modelo fenomenológico y provisorio que explicaba satisfactoriamente aunque de manera heurística algunos datos, como el orden de magnitud del radio atómico y los espectros de absorción del átomo, pero no explicaba cómo era posible que el electrón no emitiera radiación perdiendo energía. La búsqueda de un modelo más adecuado llevó a la formulación del modelo atómico de Schrödinger en el cual puede probarse que el valor esperado de la aceleración es nulo, y sobre esa base puede decirse que la energía electromagnética emitida debería ser también nula. Sin embargo, al contrario del modelo de Bohr, la representación cuántica de Schrödinger es difícil de entender en términos intuitivos."
      },
      {
        "heading": "Desarrollo histórico",
        "level": 2,
        "content": "La teoría cuántica fue desarrollada en su forma básica a lo largo de la primera mitad del siglo XX. El hecho de que la energía se intercambie de forma discreta se puso de relieve por hechos experimentales como los siguientes, inexplicables con las herramientas teóricas anteriores de la mecánica clásica o la electrodinámica:\n\nEspectro de la radiación del cuerpo negro, resuelto por Max Planck con la cuantización de la energía. La energía total del cuerpo negro resultó que tomaba valores discretos más que continuos. Este fenómeno se llamó cuantización, y los intervalos posibles más pequeños entre los valores discretos son llamados quanta (singular: quantum, de la palabra latina para «cantidad», de ahí el nombre de mecánica cuántica). La magnitud de un cuanto es un valor fijo llamado constante de Planck, y que vale: 6,626 ×10-34 J·s.\nBajo ciertas condiciones experimentales, los objetos microscópicos como los átomos o los electrones exhiben un comportamiento ondulatorio, como en la interferencia. Bajo otras condiciones, las mismas especies de objetos exhiben un comportamiento corpuscular, de partícula, («partícula» quiere decir un objeto que puede ser localizado en una región concreta del espacio), como en la dispersión de partículas. Este fenómeno se conoce como dualidad onda-partícula.\nLas propiedades físicas de objetos con historias asociadas pueden ser correlacionadas, en una amplitud prohibida para cualquier teoría clásica, solo pueden ser descritos con precisión si se hace referencia a ambos a la vez. Este fenómeno es llamado entrelazamiento cuántico y la desigualdad de Bell describe su diferencia con la correlación ordinaria. Las medidas de las violaciones de la desigualdad de Bell fueron algunas de las mayores comprobaciones de la mecánica cuántica.\nExplicación del efecto fotoeléctrico, dada por Albert Einstein, en que volvió a aparecer esa \"misteriosa\" necesidad de cuantizar la energía.\nEfecto Compton.\nEl desarrollo formal de la teoría fue obra de los esfuerzos conjuntos de varios físicos y matemáticos de la época como Schrödinger, Heisenberg, Einstein, Dirac, Bohr y Von Neumann entre otros (la lista es larga). Algunos de los aspectos fundamentales de la teoría están siendo aún estudiados activamente. La mecánica cuántica ha sido también adoptada como la teoría subyacente a muchos campos de la física y la química, incluyendo la física de la materia condensada, la química cuántica y la física de partículas.\nLa región de origen de la mecánica cuántica puede localizarse en la Europa central, en Alemania y Austria, y en el contexto histórico del primer tercio del siglo XX."
      },
      {
        "heading": "Suposiciones más importantes",
        "level": 2,
        "content": "Las suposiciones más importantes de esta teoría son las siguientes:\n\nAl ser imposible fijar a la vez la posición y el momento de una partícula, se renuncia al concepto de trayectoria, vital en mecánica clásica. En vez de eso, el movimiento de una partícula puede ser explicado por una función matemática que asigna, a cada punto del espacio y a cada instante, la probabilidad de que la partícula descrita se halle en tal posición en ese instante (al menos, en la interpretación de la Mecánica cuántica más usual, la probabilista o interpretación de Copenhague). A partir de esa función, o función de ondas, se extraen teóricamente todas las magnitudes del movimiento necesarias.\nExisten dos tipos de evolución temporal, si no ocurre ninguna medida el estado del sistema o función de onda evolucionan de acuerdo con la ecuación de Schrödinger, sin embargo, si se realiza una medida sobre el sistema, este sufre un «salto cuántico» hacia un estado compatible con los valores de la medida obtenida (formalmente el nuevo estado será una proyección ortogonal del estado original).\nExisten diferencias notorias entre los estados ligados y los que no lo están.\nLa energía no se intercambia de forma continua en un estado ligado, sino en forma discreta lo cual implica la existencia de paquetes mínimos de energía llamados cuantos, mientras en los estados no ligados la energía se comporta como un continuo."
      },
      {
        "heading": "Descripción de la teoría",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Interpretación de Copenhague",
        "level": 2,
        "content": "Para describir la teoría de forma general es necesario un tratamiento matemático riguroso, pero aceptando una de las tres interpretaciones de la mecánica cuántica (a partir de ahora la Interpretación de Copenhague), el marco se relaja. La mecánica cuántica describe el estado instantáneo de un sistema (estado cuántico) con una función de onda que codifica la distribución de probabilidad de todas las propiedades medibles, u observables. Algunos observables posibles sobre un sistema dado son la energía, posición, momento y momento angular. La mecánica cuántica no asigna valores definidos a los observables, sino que hace predicciones sobre sus distribuciones de probabilidad. Las propiedades ondulatorias de la materia son explicadas por la interferencia de las funciones de onda.\nEstas funciones de onda pueden variar con el transcurso del tiempo. Esta evolución es determinista si sobre el sistema no se realiza ninguna medida aunque esta evolución es estocástica y se produce mediante colapso de la función de onda cuando se realiza una medida sobre el sistema (Postulado IV de la MC). Por ejemplo, una partícula moviéndose sin interferencia en el espacio vacío puede ser descrita mediante una función de onda que es un paquete de ondas centrado alrededor de alguna posición media. Según pasa el tiempo, el centro del paquete puede trasladarse, cambiar, de modo que la partícula parece estar localizada más precisamente en otro lugar. La evolución temporal determinista de las funciones de onda es descrita por la ecuación de Schrödinger.\nAlgunas funciones de onda describen estados físicos con distribuciones de probabilidad que son constantes en el tiempo, estos estados se llaman estacionarios, son estados propios del operador hamiltoniano y tienen energía bien definida. Muchos sistemas que eran tratados dinámicamente en mecánica clásica son descritos mediante tales funciones de onda estáticas. Por ejemplo, un electrón en un átomo sin excitar se dibuja clásicamente como una partícula que rodea el núcleo, mientras que en mecánica cuántica es descrito por una nube de probabilidad estática que rodea al núcleo.\nCuando se realiza una medición en un observable del sistema, la función de ondas se convierte en una del conjunto de las funciones llamadas funciones propias o estados propios del observable en cuestión. Este proceso es conocido como colapso de la función de onda. Las probabilidades relativas de ese colapso sobre alguno de los estados propios posibles son descritas por la función de onda instantánea justo antes de la reducción. Considerando el ejemplo anterior sobre la partícula en el vacío, si se mide la posición de la misma, se obtendrá un valor impredecible x. En general, es imposible predecir con precisión qué valor de x se obtendrá, aunque es probable que se obtenga uno cercano al centro del paquete de ondas, donde la amplitud de la función de onda es grande. Después de que se ha hecho la medida, la función de onda de la partícula colapsa y se reduce a una que esté muy concentrada en torno a la posición observada x.\nLa ecuación de Schrödinger es determinista en el sentido de que, dada una función de onda a un tiempo inicial dado, la ecuación suministra una predicción concreta de qué función tendremos en cualquier tiempo posterior. Durante una medida, el eigen-estado al cual colapsa la función es probabilista y en este aspecto la mecánica cuántica es no determinista. Así que la naturaleza probabilista de la mecánica cuántica nace del acto de la medida. Esto conduce al problema de definir objetivamente en qué momento se produce la medida y la evolución pasa de lineal y determinista, a no-lineal y estocástica/aleatoria, cuestión que se conoce como problema de la medida y que, además de la interpretación de Copenhague, ha dado lugar a un número elevado de propuestas de resolución, conocidas como interpretaciones de la mecánica cuántica."
      },
      {
        "heading": "Formulación matemática",
        "level": 2,
        "content": "En la formulación matemática rigurosa, desarrollada por Dirac y von Neumann, los estados posibles de un sistema cuántico están representados por vectores unitarios (llamados estados) que pertenecen a un Espacio de Hilbert complejo separable (llamado el espacio de estados). Qué tipo de espacio de Hilbert es necesario en cada caso depende del sistema; por ejemplo, el espacio de estados para los estados de posición y momento es el espacio de funciones de cuadrado integrable \n  \n    \n      \n        \n          \n            L\n            \n              2\n            \n          \n          (\n          \n            \n              R\n            \n            \n              3\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\scriptstyle L^{2}(\\mathbb {R} ^{3})}\n  \n, mientras que la descripción de un sistema sin traslación pero con un espín \n  \n    \n      \n        \n          n\n          ℏ\n        \n      \n    \n    {\\displaystyle \\scriptstyle n\\hbar }\n  \n es el espacio \n  \n    \n      \n        \n          \n            \n              C\n            \n            \n              2\n              n\n              +\n              1\n            \n          \n        \n      \n    \n    {\\displaystyle \\scriptstyle \\mathbb {C} ^{2n+1}}\n  \n. La evolución temporal de un estado cuántico queda descrita por la ecuación de Schrödinger, en la que el hamiltoniano, el operador correspondiente a la energía total del sistema, tiene un papel central.\nCada magnitud observable queda representada por un operador lineal hermítico definido sobre un dominio denso del espacio de estados. Cada estado propio de un observable corresponde a un eigenvector del operador, y el valor propio o eigenvalor asociado corresponde al valor del observable en aquel estado propio. El espectro de un operador puede ser continuo o discreto. La medida de un observable representado por un operador con espectro discreto solo puede tomar un conjunto numerable de posibles valores, mientras que los operadores con espectro continuo presentan medidas posibles en intervalos reales completos. Durante una medida, la probabilidad de que un sistema colapse a uno de los eigenestados viene dada por el cuadrado del valor absoluto del producto interno entre el estado propio o auto-estado (que podemos conocer teóricamente antes de medir) y el vector estado del sistema antes de la medida. Podemos así encontrar la distribución de probabilidad de un observable en un estado dado computando la descomposición espectral del operador correspondiente. El principio de incertidumbre de Heisenberg se representa por la aseveración de que los operadores correspondientes a ciertos observables no conmutan."
      },
      {
        "heading": "Principio de Incertidumbre",
        "level": 2,
        "content": "Una de las consecuencias del formalismo cuántico es el principio de incertidumbre. En su forma más familiar, establece que ninguna medición de una partícula cuántica puede implicar simultáneamente predicciones precisas para la medición de su posición y la medición de su momento.[3]​[4]​ Tanto posición como momento son observables, esto significa que son representados por operadores hermíticos. El operador posición \n  \n    \n      \n        \n          \n            \n              X\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {X}}}\n  \n y el operador momento \n  \n    \n      \n        \n          \n            \n              P\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {P}}}\n  \n no conmutan, pero satisfacen la relación de conmutación canónica:\n\n  \n    \n      \n        [\n        \n          \n            \n              X\n              ^\n            \n          \n        \n        ,\n        \n          \n            \n              P\n              ^\n            \n          \n        \n        ]\n        =\n        i\n        ℏ\n        .\n      \n    \n    {\\displaystyle [{\\hat {X}},{\\hat {P}}]=i\\hbar .}\n  \n\nDado un estado cuántico, la regla de Born nos permite encontrar valores para \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n y \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n, así como sus cuadrados. Definiendo la incertidumbre para un observable usando desviación estándar, obteniendo\n\n  \n    \n      \n        \n          σ\n          \n            X\n          \n        \n        =\n        \n          \n            ⟨\n            \n              \n                X\n              \n              \n                2\n              \n            \n            ⟩\n            −\n            ⟨\n            \n              X\n            \n            \n              ⟩\n              \n                2\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\sigma _{X}={\\sqrt {\\langle {X}^{2}\\rangle -\\langle {X}\\rangle ^{2}}},}\n  \n\ny de la misma manera para el momento:\n\n  \n    \n      \n        \n          σ\n          \n            P\n          \n        \n        =\n        \n          \n            ⟨\n            \n              \n                P\n              \n              \n                2\n              \n            \n            ⟩\n            −\n            ⟨\n            \n              P\n            \n            \n              ⟩\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\sigma _{P}={\\sqrt {\\langle {P}^{2}\\rangle -\\langle {P}\\rangle ^{2}}}.}\n  \n\nEl principio de incertidumbre establece que\n\n  \n    \n      \n        \n          σ\n          \n            X\n          \n        \n        \n          σ\n          \n            P\n          \n        \n        ≥\n        \n          \n            ℏ\n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle \\sigma _{X}\\sigma _{P}\\geq {\\frac {\\hbar }{2}}.}\n  \n\nEn principio, cualquiera de las desviaciones estándar puede hacerse arbitrariamente pequeña, pero no ambas simultáneamente .[5]​ Esta desigualdad se generaliza a pares arbitrarios de operadores autoadjuntos \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n y \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n. El conmutador de estos dos operadores es \n\n  \n    \n      \n        [\n        A\n        ,\n        B\n        ]\n        =\n        A\n        B\n        −\n        B\n        A\n        ,\n      \n    \n    {\\displaystyle [A,B]=AB-BA,}\n  \n\ny proporciona el límite inferior en el producto de las desviaciones estándar:\n\n  \n    \n      \n        \n          σ\n          \n            A\n          \n        \n        \n          σ\n          \n            B\n          \n        \n        ≥\n        \n          \n            1\n            2\n          \n        \n        \n          |\n          \n            ⟨\n            [\n            A\n            ,\n            B\n            ]\n            ⟩\n          \n          |\n        \n        .\n      \n    \n    {\\displaystyle \\sigma _{A}\\sigma _{B}\\geq {\\frac {1}{2}}\\left|\\langle [A,B]\\rangle \\right|.}\n  \n\nOtra consecuencia de la relación de conmutación canónica es que los operadores posición y momento son la transformada de Fourier del otro, de modo que una descripción de un objeto según su momento es la transformada de Fourier de su descripción según su posición. El hecho de que la dependencia en cantidad de movimiento sea la transformada de Fourier de la dependencia en posición significa que el operador de cantidad de movimiento es equivalente (hasta un factor de \n  \n    \n      \n        i\n        \n          /\n        \n        ℏ\n      \n    \n    {\\displaystyle i/\\hbar }\n  \n) al derivar respecto a su posición, ya que en análisis de Fourier la derivación corresponde a la multiplicación en el espacio dual. Esta es la razón por la que en las ecuaciones cuánticas en el espacio de posición, el momento \n  \n    \n      \n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p_{i}}\n  \n es reemplazado por \n  \n    \n      \n        −\n        i\n        ℏ\n        \n          \n            ∂\n            \n              ∂\n              x\n            \n          \n        \n      \n    \n    {\\displaystyle -i\\hbar {\\frac {\\partial }{\\partial x}}}\n  \n, y en particular en la Ecuación de Schrödinger no relativista en el espacio de posiciones el momento al cuadrado es reemplazado por el laplaciano al cuadrado \n  \n    \n      \n        −\n        \n          ℏ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle -\\hbar ^{2}}\n  \n.[3]​"
      },
      {
        "heading": "Aplicaciones",
        "level": 1,
        "content": "En muchos aspectos, la tecnología moderna opera a una escala en la que los efectos cuánticos son significativos. Las aplicaciones importantes de la teoría cuántica incluyen la química cuántica, la óptica cuántica, la computación cuántica, los imanes superconductores, los diodos emisores de luz, el amplificador óptico y el láser, el transistor y semiconductores como el microprocesador, imágenes médicas y de investigación como la resonancia magnética y el microscopio electrónico.[6]​ Las explicaciones de muchos fenómenos biológicos y físicos tienen su origen en la naturaleza del enlace químico, sobre todo la macromolécula del ADN."
      },
      {
        "heading": "Relatividad y la mecánica cuántica",
        "level": 1,
        "content": "El mundo moderno de la física se funda notablemente en dos teorías principales, la relatividad general y la mecánica cuántica, aunque ambas teorías usan principios aparentemente incompatibles. Los postulados que definen la teoría de la relatividad de Einstein y la teoría del quántum están apoyados por rigurosa y repetida evidencia empírica. Sin embargo, ambas se resisten a ser incorporadas dentro de un mismo modelo coherente. Desde mediados del siglo XX, aparecieron teorías cuánticas relativistas del campo electromagnético (electrodinámica cuántica) y las fuerzas nucleares (modelo electrodébil, cromodinámica cuántica), pero no se tiene una teoría cuántica relativista del campo gravitatorio que sea plenamente consistente y válida para campos gravitatorios intensos (existen aproximaciones en espacios asintóticamente planos). Todas las teorías cuánticas relativistas consistentes usan los métodos de la teoría cuántica de campos.\nEn su forma ordinaria, la teoría cuántica abandona algunos de los supuestos básicos de la teoría de la relatividad, como por ejemplo el principio de localidad usado en la descripción relativista de la causalidad. El mismo Einstein había considerado absurda la violación del principio de localidad a la que parecía abocar la mecánica cuántica. La postura de Einstein fue postular que la mecánica cuántica si bien era consistente era incompleta. Para justificar su argumento y su rechazo a la falta de localidad y la falta de determinismo, Einstein y varios de sus colaboradores postularon la llamada paradoja de Einstein-Podolsky-Rosen (EPR), la cual demuestra que medir el estado de una partícula puede instantáneamente cambiar el estado de su socio enlazado, aunque las dos partículas pueden estar a una distancia arbitrariamente grande. Modernamente el paradójico resultado de la paradoja EPR se sabe es una consecuencia perfectamente consistente del llamado entrelazamiento cuántico. Es un hecho conocido que si bien la existencia del entrelazamiento cuántico efectivamente viola el principio de localidad, en cambio no viola la causalidad definido en términos de información, puesto que no hay transferencia posible de información. Si bien en su tiempo, parecía que la paradoja EPR suponía una dificultad empírica para la mecánica cuántica, y Einstein consideró que la mecánica cuántica en la interpretación de Copenhague podría ser descartada por experimento, décadas más tarde los experimentos de Alain Aspect (1981) revelaron que efectivamente la evidencia experimental parece apuntar en contra del principio de localidad.[7]​ Y por tanto, el resultado paradójico que Einstein rechazaba como «sin sentido» parece ser lo que sucede precisamente en el mundo real."
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": "Personalidades"
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Notas",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Bibliografía",
        "level": 2,
        "content": "Born, M.; Jordan, P. (1925). «Hacia la Mecánica Cuántica (Zur Quantenmechanik)». Z. Phys 34: 858-888.  (Texto en español)\nAndrade e Silva, J.; Lochak, Georges (1969). Los cuantos. Ediciones Guadarrama. ISBN 978-84-250-3040-6. \nOtero Carvajal, Luis Enrique: \"Einstein y la revolución científica del siglo XX\" Cuadernos de Historia Contemporánea, n.º 27, 2005, INSS 0214-400-X\nOtero Carvajal, Luis Enrique: \"La teoría cuántica y la discontinuidad en la física\", Umbral, Facultad de Estudios Generales de la Universidad de Puerto Rico, recinto de Río Piedras\nde la Peña, Luis (2006). Introducción a la mecánica cuántica (3 edición). México DF: Fondo de Cultura Económica. ISBN 968-16-7856-7. \nGalindo, A. y Pascual P.: Mecánica cuántica, Ed. Eudema, Barcelona, 1989, ISBN 84-7754-042-X.\nGreene, B. (2016). El tejido del cosmos. Editorial Crítica. ISBN 8498929822"
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": " Wikimedia Commons alberga una categoría multimedia sobre mecánica cuántica.\n Wikiversidad alberga proyectos de aprendizaje sobre mecánica cuántica.\n Wikiquote alberga frases célebres de o sobre mecánica cuántica.\n Wikilibros alberga un libro o manual sobre mecánica cuántica.\n Wikcionario  tiene definiciones y otra información sobre mecánica cuántica.\nJuan Carlos López Vieyra. Introducción a la mecánica cuántica, Instituto de Ciencias Nucleares de la UNAM.\nDavid P. Stern. Mecánica de Ondas, NASA.\nQuántica, exposición del CCCB"
      }
    ],
    "summary": "La mecánica cuántica es la rama de la física que estudia la naturaleza a escalas espaciales pequeñas, los sistemas atómicos, subatómicos, sus interacciones con la radiación electromagnética y otras fuerzas, en términos de cantidades observables. Se basa en la observación de que todas las formas de energía se liberan en unidades discretas o paquetes llamados cuantos. Las partículas con esta propiedad pueden pertenecer a dos tipos distintos: fermiones o bosones. Algunos de estos últimos están ligados a una -interacción fundamental (por ejemplo, el fotón pertenece a la electromagnética). Sorprendentemente, la teoría cuántica solo permite normalmente cálculos probabilísticos o estadísticos de las características observadas de las partículas elementales, entendidos en términos de funciones de onda. La ecuación de Schrödinger desempeña, en la mecánica cuántica, el papel que las leyes de Newton y la conservación de la energía desempeñan en la mecánica clásica. Es decir, la predicción del comportamiento futuro de un sistema dinámico y es una ecuación de onda en términos de una función de onda la que predice analíticamente la probabilidad precisa de los eventos o resultados.\nEn teorías anteriores de la física clásica, la energía era tratada únicamente como un fenómeno continuo, en tanto que la materia se supone que ocupa una región muy concreta del espacio y que se mueve de manera continua. Según la teoría cuántica, la energía se emite y se absorbe en cantidades discretas y minúsculas. Un paquete individual de energía, llamado cuanto, en algunas situaciones se comporta como una partícula de materia. Por otro lado, se encontró que las partículas exponen algunas propiedades ondulatorias cuando están en movimiento y ya no son vistas como localizadas en una región determinada, sino más bien extendidas en cierta medida. La luz u otra radiación emitida o absorbida por un átomo solo tiene ciertas frecuencias (o longitudes de onda), como puede verse en la línea del espectro asociado al elemento químico representado por tal átomo. La teoría cuántica demuestra que tales frecuencias corresponden a niveles definidos de los cuantos de luz, o fotones, y es el resultado del hecho de que los electrones del átomo solo pueden tener ciertos valores de energía permitidos. Cuando un electrón pasa de un nivel permitido a otro, una cantidad de energía es emitida o absorbida, cuya frecuencia es directamente proporcional a la diferencia de energía entre los dos niveles.\nLa mecánica cuántica surge tímidamente en los inicios del siglo XX dentro de las tradiciones más profundas de la física para dar una solución a problemas para los que las teorías conocidas hasta el momento habían agotado su capacidad de explicar, como la llamada catástrofe ultravioleta en la radiación de cuerpo negro predicha por la física estadística clásica y la inestabilidad de los átomos en el modelo atómico de Rutherford. La primera propuesta de un principio propiamente cuántico se debe a Max Planck en 1900, para resolver el problema de la radiación de cuerpo negro, que fue duramente cuestionado, hasta que Albert Einstein lo convierte en el principio que exitosamente pueda explicar el efecto fotoeléctrico. Las primeras formulaciones matemáticas completas de la mecánica cuántica no se alcanzan hasta mediados de la década de 1920, sin que hasta el día de hoy se tenga una interpretación coherente de la teoría, en particular del problema de la medición.\nEl formalismo de la mecánica cuántica se desarrolló durante la década de 1920. En 1924, Louis de Broglie propuso que, al igual que las ondas de luz presentan propiedades de partículas, como ocurre en el efecto fotoeléctrico, las partículas, también presentan propiedades ondulatorias. Dos formulaciones diferentes de la mecánica cuántica se presentaron después de la sugerencia de Broglie. En 1926, la mecánica ondulatoria de Erwin Schrödinger implica la utilización de una entidad matemática, la función de onda, que está relacionada con la probabilidad de encontrar una partícula en un punto dado en el espacio. En 1925, la mecánica matricial de Werner Heisenberg no hace mención alguna de las funciones de onda o conceptos similares, pero ha demostrado ser matemáticamente equivalente a la teoría de Schrödinger. Un descubrimiento importante de la teoría cuántica es el principio de incertidumbre, enunciado por Heisenberg en 1927, que pone un límite teórico absoluto en la precisión de ciertas mediciones. Como resultado de ello, la asunción clásica de los científicos de que el estado físico de un sistema podría medirse exactamente y utilizarse para predecir los estados futuros tuvo que ser abandonada. Esto supuso una revolución filosófica y dio pie a numerosas discusiones entre los más grandes físicos de la época.\nLa mecánica cuántica propiamente dicha no incorpora a la relatividad en su formulación matemática. La parte de la mecánica cuántica que incorpora elementos relativistas de manera formal para abordar diversos problemas se conoce como mecánica cuántica relativista o ya, en forma más correcta y acabada, teoría cuántica de campos (que incluye a su vez a la electrodinámica cuántica, cromodinámica cuántica y teoría electrodébil dentro del modelo estándar)[1]​ y más generalmente, la teoría cuántica de campos en espacio-tiempo curvo. La única interacción elemental que no se ha podido cuantizar hasta el momento ha sido la interacción gravitatoria. Este problema constituye entonces uno de los mayores desafíos de la física del siglo XXI. La mecánica cuántica se combinó con la teoría de la relatividad en la formulación de Paul Dirac de 1928, lo que, además, predijo la existencia de antipartículas. Otros desarrollos de la teoría incluyen la estadística cuántica, presentada en una forma por Einstein y Bose (la estadística de Bose-Einstein) y en otra forma por Dirac y Enrico Fermi (la estadística de Fermi-Dirac); la electrodinámica cuántica, interesada en la interacción entre partículas cargadas y los campos electromagnéticos, su generalización, la teoría cuántica de campos y la electrónica cuántica.\nLa mecánica cuántica proporciona el fundamento de la fenomenología del átomo, de su núcleo y de las partículas elementales (lo cual requiere necesariamente el enfoque relativista). También su impacto en teoría de la información, criptografía y química ha sido decisivo entre esta misma."
  },
  {
    "title": "Thermodynamics",
    "source": "https://en.wikipedia.org/wiki/Thermodynamics",
    "language": "en",
    "chunks": [
      {
        "heading": "Introduction",
        "level": 1,
        "content": "A description of any thermodynamic system employs the four laws of thermodynamics that form an axiomatic basis. The first law specifies that energy can be transferred between physical systems as heat, as work, and with transfer of matter. The second law defines the existence of a quantity called entropy, that describes the direction, thermodynamically, that a system can evolve and quantifies the state of order of a system and that can be used to quantify the useful work that can be extracted from the system.\nIn thermodynamics, interactions between large ensembles of objects are studied and categorized. Central to this are the concepts of the thermodynamic system and its surroundings. A system is composed of particles, whose average motions define its properties, and those properties are in turn related to one another through equations of state. Properties can be combined to express internal energy and thermodynamic potentials, which are useful for determining conditions for equilibrium and spontaneous processes.\nWith these tools, thermodynamics can be used to describe how systems respond to changes in their environment. This can be applied to a wide variety of topics in science and engineering, such as engines, phase transitions, chemical reactions, transport phenomena, and even black holes. The results of thermodynamics are essential for other fields of physics and for chemistry, chemical engineering, corrosion engineering, aerospace engineering, mechanical engineering, cell biology, biomedical engineering, materials science, and economics, to name a few.\nThis article is focused mainly on classical thermodynamics which primarily studies systems in thermodynamic equilibrium. Non-equilibrium thermodynamics is often treated as an extension of the classical treatment, but statistical mechanics has brought many advances to that field."
      },
      {
        "heading": "History",
        "level": 1,
        "content": "The history of thermodynamics as a scientific discipline generally begins with Otto von Guericke who, in 1650, built and designed the world's first vacuum pump and demonstrated a vacuum using his Magdeburg hemispheres. Guericke was driven to make a vacuum in order to disprove Aristotle's long-held supposition that 'nature abhors a vacuum'. Shortly after Guericke, the Anglo-Irish physicist and chemist Robert Boyle had learned of Guericke's designs and, in 1656, in coordination with English scientist Robert Hooke, built an air pump. Using this pump, Boyle and Hooke noticed a correlation between pressure, temperature, and volume. In time, Boyle's Law was formulated, which states that pressure and volume are inversely proportional. Then, in 1679, based on these concepts, an associate of Boyle's named Denis Papin built a steam digester, which was a closed vessel with a tightly fitting lid that confined steam until a high pressure was generated.\nLater designs implemented a steam release valve that kept the machine from exploding. By watching the valve rhythmically move up and down, Papin conceived of the idea of a piston and a cylinder engine. He did not, however, follow through with his design. Nevertheless, in 1697, based on Papin's designs, engineer Thomas Savery built the first engine, followed by Thomas Newcomen in 1712. Although these early engines were crude and inefficient, they attracted the attention of the leading scientists of the time.\nThe fundamental concepts of heat capacity and latent heat, which were necessary for the development of thermodynamics, were developed by Professor Joseph Black at the University of Glasgow, where James Watt was employed as an instrument maker. Black and Watt performed experiments together, but it was Watt who conceived the idea of the external condenser which resulted in a large increase in steam engine efficiency. Drawing on all the previous work led Sadi Carnot, the \"father of thermodynamics\", to publish Reflections on the Motive Power of Fire (1824), a discourse on heat, power, energy and engine efficiency. The book outlined the basic energetic relations between the Carnot engine, the Carnot cycle, and motive power. It marked the start of thermodynamics as a modern science.\nThe first thermodynamic textbook was written in 1859 by William Rankine, originally trained as a physicist and a civil and mechanical engineering professor at the University of Glasgow. The first and second laws of thermodynamics emerged simultaneously in the 1850s, primarily out of the works of William Rankine, Rudolf Clausius, and William Thomson (Lord Kelvin).\nThe foundations of statistical thermodynamics were set out by physicists such as James Clerk Maxwell, Ludwig Boltzmann, Max Planck, Rudolf Clausius and J. Willard Gibbs.\nClausius, who first stated the basic ideas of the second law in his paper \"On the Moving Force of Heat\", published in 1850, and is called \"one of the founding fathers of thermodynamics\", introduced the concept of entropy in 1865.\nDuring the years 1873–76 the American mathematical physicist Josiah Willard Gibbs published a series of three papers, the most famous being On the Equilibrium of Heterogeneous Substances, in which he showed how thermodynamic processes, including chemical reactions, could be graphically analyzed, by studying the energy, entropy, volume, temperature and pressure of the thermodynamic system in such a manner, one can determine if a process would occur spontaneously. Also Pierre Duhem in the 19th century wrote about chemical thermodynamics. During the early 20th century, chemists such as Gilbert N. Lewis, Merle Randall, and E. A. Guggenheim applied the mathematical methods of Gibbs to the analysis of chemical processes."
      },
      {
        "heading": "Etymology",
        "level": 1,
        "content": "Thermodynamics has an intricate etymology.\nBy a surface-level analysis, the word consists of two parts that can be traced back to Ancient Greek. Firstly, thermo- (\"of heat\"; used in words such as thermometer) can be traced back to the root θέρμη therme, meaning \"heat\". Secondly, the word dynamics (\"science of force [or power]\") can be traced back to the root δύναμις dynamis, meaning \"power\".\nIn 1849, the adjective thermo-dynamic is used by William Thomson.\nIn 1854, the noun thermo-dynamics is used by Thomson and William Rankine to represent the science of generalized heat engines.\nPierre Perrot claims that the term thermodynamics was coined by James Joule in 1858 to designate the science of relations between heat and power, however, Joule never used that term, but used instead the term perfect thermo-dynamic engine in reference to Thomson's 1849 phraseology."
      },
      {
        "heading": "Branches of thermodynamics",
        "level": 1,
        "content": "The study of thermodynamical systems has developed into several related branches, each using a different fundamental model as a theoretical or experimental basis, or applying the principles to varying types of systems."
      },
      {
        "heading": "Classical thermodynamics",
        "level": 2,
        "content": "Classical thermodynamics is the description of the states of thermodynamic systems at near-equilibrium, that uses macroscopic, measurable properties. It is used to model exchanges of energy, work and heat based on the laws of thermodynamics. The qualifier classical reflects the fact that it represents the first level of understanding of the subject as it developed in the 19th century and describes the changes of a system in terms of macroscopic empirical (large scale, and measurable) parameters. A microscopic interpretation of these concepts was later provided by the development of statistical mechanics."
      },
      {
        "heading": "Statistical mechanics",
        "level": 2,
        "content": "Statistical mechanics, also known as statistical thermodynamics, emerged with the development of atomic and molecular theories in the late 19th century and early 20th century, and supplemented classical thermodynamics with an interpretation of the microscopic interactions between individual particles or quantum-mechanical states. This field relates the microscopic properties of individual atoms and molecules to the macroscopic, bulk properties of materials that can be observed on the human scale, thereby explaining classical thermodynamics as a natural result of statistics, classical mechanics, and quantum theory at the microscopic level."
      },
      {
        "heading": "Chemical thermodynamics",
        "level": 2,
        "content": "Chemical thermodynamics is the study of the interrelation of energy with chemical reactions or with a physical change of state within the confines of the laws of thermodynamics. The primary objective of chemical thermodynamics is determining the spontaneity of a given transformation."
      },
      {
        "heading": "Equilibrium thermodynamics",
        "level": 2,
        "content": "Equilibrium thermodynamics is the study of transfers of matter and energy in systems or bodies that, by agencies in their surroundings, can be driven from one state of thermodynamic equilibrium to another. The term 'thermodynamic equilibrium' indicates a state of balance, in which all macroscopic flows are zero; in the case of the simplest systems or bodies, their intensive properties are homogeneous, and their pressures are perpendicular to their boundaries. In an equilibrium state there are no unbalanced potentials, or driving forces, between macroscopically distinct parts of the system. A central aim in equilibrium thermodynamics is: given a system in a well-defined initial equilibrium state, and given its surroundings, and given its constitutive walls, to calculate what will be the final equilibrium state of the system after a specified thermodynamic operation has changed its walls or surroundings."
      },
      {
        "heading": "Non-equilibrium thermodynamics",
        "level": 2,
        "content": "Non-equilibrium thermodynamics is a branch of thermodynamics that deals with systems that are not in thermodynamic equilibrium. Most systems found in nature are not in thermodynamic equilibrium because they are not in stationary states, and are continuously and discontinuously subject to flux of matter and energy to and from other systems. The thermodynamic study of non-equilibrium systems requires more general concepts than are dealt with by equilibrium thermodynamics. Many natural systems still today remain beyond the scope of currently known macroscopic thermodynamic methods."
      },
      {
        "heading": "Laws of thermodynamics",
        "level": 1,
        "content": "Thermodynamics is principally based on a set of four laws which are universally valid when applied to systems that fall within the constraints implied by each. In the various theoretical descriptions of thermodynamics these laws may be expressed in seemingly differing forms, but the most prominent formulations are the following."
      },
      {
        "heading": "Zeroth law",
        "level": 2,
        "content": "The zeroth law of thermodynamics states: If two systems are each in thermal equilibrium with a third, they are also in thermal equilibrium with each other.\nThis statement implies that thermal equilibrium is an equivalence relation on the set of thermodynamic systems under consideration. Systems are said to be in equilibrium if the small, random exchanges between them (e.g. Brownian motion) do not lead to a net change in energy. This law is tacitly assumed in every measurement of temperature. Thus, if one seeks to decide whether two bodies are at the same temperature, it is not necessary to bring them into contact and measure any changes of their observable properties in time. The law provides an empirical definition of temperature, and justification for the construction of practical thermometers.\nThe zeroth law was not initially recognized as a separate law of thermodynamics, as its basis in thermodynamical equilibrium was implied in the other laws. The first, second, and third laws had been explicitly stated already, and found common acceptance in the physics community before the importance of the zeroth law for the definition of temperature was realized. As it was impractical to renumber the other laws, it was named the zeroth law."
      },
      {
        "heading": "First law",
        "level": 2,
        "content": "The first law of thermodynamics states: In a process without transfer of matter, the change in internal energy, \n  \n    \n      \n        Δ\n        U\n      \n    \n    {\\displaystyle \\Delta U}\n  \n, of a thermodynamic system is equal to the energy gained as heat, \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n, less the thermodynamic work, \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n, done by the system on its surroundings.\n\n  \n    \n      \n        Δ\n        U\n        =\n        Q\n        −\n        W\n      \n    \n    {\\displaystyle \\Delta U=Q-W}\n  \n.\nwhere \n  \n    \n      \n        Δ\n        U\n      \n    \n    {\\displaystyle \\Delta U}\n  \n denotes the change in the internal energy of a closed system (for which heat or work through the system boundary are possible, but matter transfer is not possible), \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n denotes the quantity of energy supplied to the system as heat, and \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n denotes the amount of thermodynamic work done by the system on its surroundings. An equivalent statement is that perpetual motion machines of the first kind are impossible; work \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n done by a system on its surrounding requires that the system's internal energy \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n  \n decrease or be consumed, so that the amount of internal energy lost by that work must be resupplied as heat \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n by an external energy source or as work by an external machine acting on the system (so that \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n  \n is recovered) to make the system work continuously.\nFor processes that include transfer of matter, a further statement is needed: With due account of the respective fiducial reference states of the systems, when two systems, which may be of different chemical compositions, initially separated only by an impermeable wall, and otherwise isolated, are combined into a new system by the thermodynamic operation of removal of the wall, then\n\n  \n    \n      \n        \n          U\n          \n            0\n          \n        \n        =\n        \n          U\n          \n            1\n          \n        \n        +\n        \n          U\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle U_{0}=U_{1}+U_{2}}\n  \n,\nwhere U0 denotes the internal energy of the combined system, and U1 and U2 denote the internal energies of the respective separated systems.\nAdapted for thermodynamics, this law is an expression of the principle of conservation of energy, which states that energy can be transformed (changed from one form to another), but cannot be created or destroyed.\nInternal energy is a principal property of the thermodynamic state, while heat and work are modes of energy transfer by which a process may change this state. A change of internal energy of a system may be achieved by any combination of heat added or removed and work performed on or by the system. As a function of state, the internal energy does not depend on the manner, or on the path through intermediate steps, by which the system arrived at its state."
      },
      {
        "heading": "Second law",
        "level": 2,
        "content": "A traditional version of the second law of thermodynamics states: Heat does not spontaneously flow from a colder body to a hotter body.\nThe second law refers to a system of matter and radiation, initially with inhomogeneities in temperature, pressure, chemical potential, and other intensive properties, that are due to internal 'constraints', or impermeable rigid walls, within it, or to externally imposed forces. The law observes that, when the system is isolated from the outside world and from those forces, there is a definite thermodynamic quantity, its entropy, that increases as the constraints are removed, eventually reaching a maximum value at thermodynamic equilibrium, when the inhomogeneities practically vanish. For systems that are initially far from thermodynamic equilibrium, though several have been proposed, there is known no general physical principle that determines the rates of approach to thermodynamic equilibrium, and thermodynamics does not deal with such rates. The many versions of the second law all express the general irreversibility of the transitions involved in systems approaching thermodynamic equilibrium.\nIn macroscopic thermodynamics, the second law is a basic observation applicable to any actual thermodynamic process; in statistical thermodynamics, the second law is postulated to be a consequence of molecular chaos."
      },
      {
        "heading": "Third law",
        "level": 2,
        "content": "The third law of thermodynamics states: As the temperature of a system approaches absolute zero, all processes cease and the entropy of the system approaches a minimum value.\nThis law of thermodynamics is a statistical law of nature regarding entropy and the impossibility of reaching absolute zero of temperature. This law provides an absolute reference point for the determination of entropy. The entropy determined relative to this point is the absolute entropy. Alternate definitions include \"the entropy of all systems and of all states of a system is smallest at absolute zero,\" or equivalently \"it is impossible to reach the absolute zero of temperature by any finite number of processes\".\nAbsolute zero, at which all activity would stop if it were possible to achieve, is −273.15 °C (degrees Celsius), or −459.67 °F (degrees Fahrenheit), or 0 K (kelvin), or 0° R (degrees Rankine)."
      },
      {
        "heading": "System models",
        "level": 1,
        "content": "An important concept in thermodynamics is the thermodynamic system, which is a precisely defined region of the universe under study. Everything in the universe except the system is called the surroundings. A system is separated from the remainder of the universe by a boundary which may be a physical or notional, but serve to confine the system to a finite volume. Segments of the boundary are often described as walls; they have respective defined 'permeabilities'. Transfers of energy as work, or as heat, or of matter, between the system and the surroundings, take place through the walls, according to their respective permeabilities.\nMatter or energy that pass across the boundary so as to effect a change in the internal energy of the system need to be accounted for in the energy balance equation. The volume contained by the walls can be the region surrounding a single atom resonating energy, such as Max Planck defined in 1900; it can be a body of steam or air in a steam engine, such as Sadi Carnot defined in 1824. The system could also be just one nuclide (i.e. a system of quarks) as hypothesized in quantum thermodynamics. When a looser viewpoint is adopted, and the requirement of thermodynamic equilibrium is dropped, the system can be the body of a tropical cyclone, such as Kerry Emanuel theorized in 1986 in the field of atmospheric thermodynamics, or the event horizon of a black hole.\nBoundaries are of four types: fixed, movable, real, and imaginary. For example, in an engine, a fixed boundary means the piston is locked at its position, within which a constant volume process might occur. If the piston is allowed to move that boundary is movable while the cylinder and cylinder head boundaries are fixed. For closed systems, boundaries are real while for open systems boundaries are often imaginary. In the case of a jet engine, a fixed imaginary boundary might be assumed at the intake of the engine, fixed boundaries along the surface of the case and a second fixed imaginary boundary across the exhaust nozzle.\nGenerally, thermodynamics distinguishes three classes of systems, defined in terms of what is allowed to cross their boundaries:\n\nAs time passes in an isolated system, internal differences of pressures, densities, and temperatures tend to even out. A system in which all equalizing processes have gone to completion is said to be in a state of thermodynamic equilibrium.\nOnce in thermodynamic equilibrium, a system's properties are, by definition, unchanging in time. Systems in equilibrium are much simpler and easier to understand than are systems which are not in equilibrium. Often, when analysing a dynamic thermodynamic process, the simplifying assumption is made that each intermediate state in the process is at equilibrium, producing thermodynamic processes which develop so slowly as to allow each intermediate step to be an equilibrium state and are said to be reversible processes."
      },
      {
        "heading": "States and processes",
        "level": 1,
        "content": "When a system is at equilibrium under a given set of conditions, it is said to be in a definite thermodynamic state. The state of the system can be described by a number of state quantities that do not depend on the process by which the system arrived at its state. They are called intensive variables or extensive variables according to how they change when the size of the system changes. The properties of the system can be described by an equation of state which specifies the relationship between these variables. State may be thought of as the instantaneous quantitative description of a system with a set number of variables held constant.\nA thermodynamic process may be defined as the energetic evolution of a thermodynamic system proceeding from an initial state to a final state. It can be described by process quantities. Typically, each thermodynamic process is distinguished from other processes in energetic character according to what parameters, such as temperature, pressure, or volume, etc., are held fixed; Furthermore, it is useful to group these processes into pairs, in which each variable held constant is one member of a conjugate pair.\nSeveral commonly studied thermodynamic processes are:\n\nAdiabatic process: occurs without loss or gain of energy by heat\nIsenthalpic process: occurs at a constant enthalpy\nIsentropic process: a reversible adiabatic process, occurs at a constant entropy\nIsobaric process: occurs at constant pressure\nIsochoric process: occurs at constant volume (also called isometric/isovolumetric)\nIsothermal process: occurs at a constant temperature\nSteady state process: occurs without a change in the internal energy"
      },
      {
        "heading": "Instrumentation",
        "level": 1,
        "content": "There are two types of thermodynamic instruments, the meter and the reservoir. A thermodynamic meter is any device which measures any parameter of a thermodynamic system. In some cases, the thermodynamic parameter is actually defined in terms of an idealized measuring instrument. For example, the zeroth law states that if two bodies are in thermal equilibrium with a third body, they are also in thermal equilibrium with each other. This principle, as noted by James Maxwell in 1872, asserts that it is possible to measure temperature. An idealized thermometer is a sample of an ideal gas at constant pressure. From the ideal gas law pV=nRT, the volume of such a sample can be used as an indicator of temperature; in this manner it defines temperature. Although pressure is defined mechanically, a pressure-measuring device, called a barometer may also be constructed from a sample of an ideal gas held at a constant temperature. A calorimeter is a device which is used to measure and define the internal energy of a system.\nA thermodynamic reservoir is a system which is so large that its state parameters are not appreciably altered when it is brought into contact with the system of interest. When the reservoir is brought into contact with the system, the system is brought into equilibrium with the reservoir. For example, a pressure reservoir is a system at a particular pressure, which imposes that pressure upon the system to which it is mechanically connected. The Earth's atmosphere is often used as a pressure reservoir. The ocean can act as temperature reservoir when used to cool power plants."
      },
      {
        "heading": "Conjugate variables",
        "level": 1,
        "content": "The central concept of thermodynamics is that of energy, the ability to do work. By the First Law, the total energy of a system and its surroundings is conserved. Energy may be transferred into a system by heating, compression, or addition of matter, and extracted from a system by cooling, expansion, or extraction of matter. In mechanics, for example, energy transfer equals the product of the force applied to a body and the resulting displacement.\nConjugate variables are pairs of thermodynamic concepts, with the first being akin to a \"force\" applied to some thermodynamic system, the second being akin to the resulting \"displacement\", and the product of the two equaling the amount of energy transferred. The common conjugate variables are:\n\nPressure-volume (the mechanical parameters);\nTemperature-entropy (thermal parameters);\nChemical potential-particle number (material parameters)."
      },
      {
        "heading": "Potentials",
        "level": 1,
        "content": "Thermodynamic potentials are different quantitative measures of the stored energy in a system. Potentials are used to measure the energy changes in systems as they evolve from an initial state to a final state. The potential used depends on the constraints of the system, such as constant temperature or pressure. For example, the Helmholtz and Gibbs energies are the energies available in a system to do useful work when the temperature and volume or the pressure and temperature are fixed, respectively. Thermodynamic potentials cannot be measured in laboratories, but can be computed using molecular thermodynamics.\nThe five most well known potentials are:\n\nwhere \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n is the temperature, \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n the entropy, \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n the pressure, \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n the volume, \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n the chemical potential, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n the number of particles in the system, and \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n is the count of particles types in the system.\nThermodynamic potentials can be derived from the energy balance equation applied to a thermodynamic system. Other thermodynamic potentials can also be obtained through Legendre transformation."
      },
      {
        "heading": "Axiomatic thermodynamics",
        "level": 1,
        "content": "Axiomatic thermodynamics is a mathematical discipline that aims to describe thermodynamics in terms of rigorous axioms, for example by finding a mathematically rigorous way to express the familiar laws of thermodynamics.\nThe first attempt at an axiomatic theory of thermodynamics was Constantin Carathéodory's 1909 work Investigations on the Foundations of Thermodynamics, which made use of Pfaffian systems and the concept of adiabatic accessibility, a notion that was introduced by Carathéodory himself. In this formulation, thermodynamic concepts such as heat, entropy, and temperature are derived from quantities that are more directly measurable. Theories that came after, differed in the sense that they made assumptions regarding thermodynamic processes with arbitrary initial and final states, as opposed to considering only neighboring states."
      },
      {
        "heading": "Applied fields",
        "level": 1,
        "content": ""
      },
      {
        "heading": "See also",
        "level": 1,
        "content": "Thermodynamic process path"
      },
      {
        "heading": "Lists and timelines",
        "level": 2,
        "content": "List of important publications in thermodynamics\nList of textbooks on thermodynamics and statistical mechanics\nList of thermal conductivities\nList of thermodynamic properties\nTable of thermodynamic equations\nTimeline of thermodynamics\nThermodynamic equations"
      },
      {
        "heading": "Notes",
        "level": 1,
        "content": ""
      },
      {
        "heading": "References",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Further reading",
        "level": 1,
        "content": "Goldstein, Martin & Inge F. (1993). The Refrigerator and the Universe. Harvard University Press. ISBN 978-0-674-75325-9. OCLC 32826343. A nontechnical introduction, good on historical and interpretive matters.\nKazakov, Andrei; Muzny, Chris D.; Chirico, Robert D.; Diky, Vladimir V.; Frenkel, Michael (2008). \"Web Thermo Tables – an On-Line Version of the TRC Thermodynamic Tables\". Journal of Research of the National Institute of Standards and Technology. 113 (4): 209–220. doi:10.6028/jres.113.016. ISSN 1044-677X. PMC 4651616. PMID 27096122.\nGibbs J.W. (1928). The Collected Works of J. Willard Gibbs Thermodynamics. New York: Longmans, Green and Co. Vol. 1, pp. 55–349.\nGuggenheim E.A. (1933). Modern thermodynamics by the methods of Willard Gibbs. London: Methuen & co. ltd.\nDenbigh K. (1981). The Principles of Chemical Equilibrium: With Applications in Chemistry and Chemical Engineering. London: Cambridge University Press.\nStull, D.R., Westrum Jr., E.F. and Sinke, G.C. (1969). The Chemical Thermodynamics of Organic Compounds. London: John Wiley and Sons, Inc.{{cite book}}:  CS1 maint: multiple names: authors list (link)\nBazarov I.P. (2010). Thermodynamics: Textbook. St. Petersburg: Lan publishing house. p. 384. ISBN 978-5-8114-1003-3. 5th ed. (in Russian)\nBawendi Moungi G., Alberty Robert A. and Silbey Robert J. (2004). Physical Chemistry. J. Wiley & Sons, Incorporated.\nAlberty Robert A. (2003). Thermodynamics of Biochemical Reactions. Wiley-Interscience.\nAlberty Robert A. (2006). Biochemical Thermodynamics: Applications of Mathematica. Vol. 48. John Wiley & Sons, Inc. pp. 1–458. ISBN 978-0-471-75798-6. PMID 16878778. {{cite book}}: |journal= ignored (help)\nDill Ken A., Bromberg Sarina (2011). Molecular Driving Forces: Statistical Thermodynamics in Biology, Chemistry, Physics, and Nanoscience. Garland Science. ISBN 978-0-8153-4430-8.\nM. Scott Shell (2015). Thermodynamics and Statistical Mechanics: An Integrated Approach. Cambridge University Press. ISBN 978-1107656789.\nDouglas E. Barrick (2018). Biomolecular Thermodynamics: From Theory to Applications. CRC Press. ISBN 978-1-4398-0019-5.\nThe following titles are more technical:\n\nBejan, Adrian (2016). Advanced Engineering Thermodynamics (4 ed.). Wiley. ISBN 978-1-119-05209-8.\nCengel, Yunus A., & Boles, Michael A. (2002). Thermodynamics – an Engineering Approach. McGraw Hill. ISBN 978-0-07-238332-4. OCLC 45791449.{{cite book}}:  CS1 maint: multiple names: authors list (link)\nDunning-Davies, Jeremy (1997). Concise Thermodynamics: Principles and Applications. Horwood Publishing. ISBN 978-1-8985-6315-0. OCLC 36025958.\nKroemer, Herbert & Kittel, Charles (1980). Thermal Physics. W.H. Freeman Company. ISBN 978-0-7167-1088-2. OCLC 32932988."
      },
      {
        "heading": "External links",
        "level": 1,
        "content": " Media related to Thermodynamics at Wikimedia Commons\n\nCallendar, Hugh Longbourne (1911). \"Thermodynamics\" . Encyclopædia Britannica. Vol. 26 (11th ed.). pp. 808–814.\nThermodynamics Data & Property Calculation Websites\nThermodynamics Educational Websites\nBiochemistry Thermodynamics\nThermodynamics and Statistical Mechanics\nEngineering Thermodynamics – A Graphical Approach\nThermodynamics and Statistical Mechanics by Richard Fitzpatrick"
      }
    ],
    "summary": "Thermodynamics is a branch of physics that deals with heat, work, and temperature, and their relation to energy, entropy, and the physical properties of matter and radiation. The behavior of these quantities is governed by the four laws of thermodynamics, which convey a quantitative description using measurable macroscopic physical quantities but may be explained in terms of microscopic constituents by statistical mechanics. Thermodynamics applies to various topics in science and engineering, especially physical chemistry, biochemistry, chemical engineering, and mechanical engineering, as well as other complex fields such as meteorology.\nHistorically, thermodynamics developed out of a desire to increase the efficiency of early steam engines, particularly through the work of French physicist Sadi Carnot (1824) who believed that engine efficiency was the key that could help France win the Napoleonic Wars. Scots-Irish physicist Lord Kelvin was the first to formulate a concise definition of thermodynamics in 1854 which stated, \"Thermo-dynamics is the subject of the relation of heat to forces acting between contiguous parts of bodies, and the relation of heat to electrical agency.\"  German physicist and mathematician Rudolf Clausius restated Carnot's principle known as the Carnot cycle and gave the theory of heat a truer and sounder basis. His most important paper, \"On the Moving Force of Heat\", published in 1850, first stated the second law of thermodynamics. In 1865 he introduced the concept of entropy. In 1870 he introduced the virial theorem, which applied to heat.\nThe initial application of thermodynamics to mechanical heat engines was quickly extended to the study of chemical compounds and chemical reactions. Chemical thermodynamics studies the nature of the role of entropy in the process of chemical reactions and has provided the bulk of expansion and knowledge of the field. Other formulations of thermodynamics emerged. Statistical thermodynamics, or statistical mechanics, concerns itself with statistical predictions of the collective motion of particles from their microscopic behavior. In 1909, Constantin Carathéodory presented a purely mathematical approach in an axiomatic formulation, a description often referred to as geometrical thermodynamics."
  },
  {
    "title": "Termodinámica",
    "source": "https://es.wikipedia.org/wiki/Termodin%C3%A1mica",
    "language": "es",
    "chunks": [
      {
        "heading": "Historia",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Etimología",
        "level": 1,
        "content": "La etimología de termodinámica tiene una historia intrincada.[12]​ Se escribió por primera vez como adjetivo con un guion intercalado (termo-dinámica) y desde 1854 hasta 1868 como el sustantivo termodinámica para representar la ciencia de los motores térmicos generalizados.[12]​ El biofísico estadounidense Donald Haynie afirma que termodinámica fue acuñada en 1840 a partir de la raíz griega θέρμη therme, que significa «calor», y δύναμις dynamis, que significa «poder».[13]​\nPierre Perrot afirma que el término termodinámica fue acuñado por James Joule en 1858 para designar la ciencia de las relaciones entre el calor y la energía,[14]​ sin embargo, Joule nunca utilizó ese término, sino que utilizó en su lugar el término motor termodinámico perfecto en referencia a la fraseología de Thomson de 1849.[15]​\nPara 1858, termodinámica, como término funcional, se utilizó en el artículo de William Thomson \"Un relato de la teoría de Carnot sobre la potencia motriz del calor\".[15]​"
      },
      {
        "heading": "Ramas de la termodinámica",
        "level": 1,
        "content": "El estudio de los sistemas termodinámicos se ha desarrollado en varias ramas relacionadas, cada una de las cuales utiliza un modelo fundamental diferente como base teórica o experimental, o aplica los principios a distintos tipos de sistemas."
      },
      {
        "heading": "Termodinámica clásica",
        "level": 2,
        "content": "La termodinámica clásica es la descripción de los estados de los sistemas termodinámicos en situación de casi equilibrio, que utiliza propiedades macroscópicas y medibles. Se utiliza para modelar los intercambios de energía, trabajo y calor basándose en los principios de termodinámica. El calificativo clásico refleja el hecho de que representa el primer nivel de comprensión del tema tal y como se desarrolló en el siglo XIX y describe los cambios de un sistema en términos de parámetros empíricos macroscópicos (a gran escala y medibles). [16]​Una interpretación microscópica de estos conceptos fue proporcionada posteriormente por el desarrollo de la física estadística."
      },
      {
        "heading": "Física estadística",
        "level": 2,
        "content": "La física estadística, también conocida como termodinámica estadística, surgió con el desarrollo de las teorías atómicas y moleculares a finales del siglo XIX y principios del XX, y complementó la termodinámica clásica con una interpretación de las interacciones microscópicas entre partículas individuales o estados mecánicos cuánticos. Este campo relaciona las propiedades microscópicas de los átomos y las moléculas individuales con las propiedades macroscópicas de los materiales que pueden observarse a escala humana, explicando así la termodinámica clásica como un resultado natural de la estadística, la mecánica clásica y la teoría cuántica a nivel microscópico.[17]​"
      },
      {
        "heading": "Termodinámica química",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Termodinámica del equilibrio",
        "level": 2,
        "content": "La termodinámica del equilibrio es el estudio de las transferencias de materia y energía en sistemas o cuerpos que, por medio de organismos de su entorno, pueden pasar de un estado de equilibrio termodinámico a otro. El concepto «equilibrio termodinámico» indica un macroestado de equilibrio, en el que todos los flujos macroscópicos son nulos; en el caso de los sistemas o cuerpos más simples, sus propiedades intensivas son homogéneas y sus presiones son perpendiculares a sus límites. En un estado de equilibrio no hay potenciales desequilibrados, o fuerzas impulsoras, entre partes macroscópicas distintas del sistema. Un objetivo central de la termodinámica del equilibrio es: dado un sistema en un estado de equilibrio inicial bien definido, y dado su entorno, y dadas sus paredes constitutivas, calcular cuál será el estado de equilibrio final del sistema después de que una operación termodinámica específica haya cambiado sus paredes o su entorno.\nLa termodinámica de no equilibrio es una rama de la termodinámica que se ocupa de los sistemas que no están en equilibrio termodinámico. La mayoría de los sistemas que se encuentran en la naturaleza no están en equilibrio termodinámico porque no están en estados estacionarios, y están sujetos de forma continua y discontinua a flujos de materia y energía hacia y desde otros sistemas. El estudio termodinámico de los sistemas que no están en equilibrio requiere conceptos más generales que los tratados por la termodinámica del equilibrio. Muchos sistemas naturales siguen estando hoy en día fuera del alcance de los métodos termodinámicos macroscópicos actualmente conocidos."
      },
      {
        "heading": "Principios",
        "level": 1,
        "content": "Es importante remarcar que los principios de la termodinámica son válidos siempre para los sistemas macroscópicos, pero inaplicables a nivel microscópico. La idea del demonio de Maxwell ayuda a comprender los límites del segundo principio de termodinámica jugando con las propiedades microscópicas de las partículas que componen un gas."
      },
      {
        "heading": "Principio cero",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Primer principio",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Segundo principio",
        "level": 2,
        "content": "Este principio marca la dirección en la que deben llevarse a cabo los procesos termodinámicos y, por lo tanto, la imposibilidad de que ocurran en el sentido contrario (por ejemplo, una mancha de tinta dispersada en el agua no puede volver a concentrarse en un pequeño volumen). El sentido de evolución de los procesos reales es único ya que son irreversibles. Este hecho viene caracterizado por el aumento de una magnitud física, S, la entropía del sistema termodinámico, con el llamado principio de aumento de entropía, que es una forma de enunciar el segundo principio de la termodinámica. También establece, en algunos casos, la imposibilidad de convertir completamente toda la energía de un tipo a otro sin pérdidas. De esta forma, el segundo principio impone restricciones para las transferencias de energía que hipotéticamente pudieran llevarse a cabo teniendo en cuenta solo el primer principio. Esta ley apoya todo su contenido aceptando la existencia de una magnitud física llamada entropía, de tal manera que, para un sistema aislado (que no intercambia materia ni energía con su entorno), la variación de la entropía siempre debe ser mayor que cero.\nDebido a esta ley también se tiene que el flujo espontáneo de calor siempre es unidireccional, desde los cuerpos de mayor temperatura hacia los de menor temperatura, hasta lograr un equilibrio térmico.\nLa aplicación más conocida es la de las máquinas térmicas, que obtienen trabajo mecánico mediante aporte de calor de una fuente o foco caliente, para ceder parte de este calor a la fuente o foco o sumidero frío. La diferencia entre los dos calores tiene su equivalente en el trabajo mecánico obtenido.\nExisten numerosos enunciados equivalentes para definir este principio, destacándose el de Clausius y el de Kelvin."
      },
      {
        "heading": "Enunciado de Clausius",
        "level": 3,
        "content": "En palabras de Sears es: «No es posible ningún proceso cuyo único resultado sea la extracción de calor de un recipiente a una cierta temperatura y la absorción de una cantidad igual de calor por un recipiente a temperatura más elevada»."
      },
      {
        "heading": "Enunciado de Kelvin-Planck",
        "level": 3,
        "content": "Es imposible construir una máquina térmica que, operando en un ciclo, no produzca otro efecto que la absorción de energía desde un depósito, con la realización de una cantidad igual de trabajo. Sería correcto decir que «es imposible construir una máquina que, operando cíclicamente, produzca como único efecto la extracción de calor de un foco y la realización equivalente de trabajo». Varía con el primero, dado que en él, se puede deducir que la máquina transforma todo el trabajo en calor, y, que el resto, para otras funciones… Este enunciado afirma la imposibilidad de construir una máquina que convierta todo el calor en trabajo. Siempre es necesario intercambiar calor con un segundo foco (el foco frío), de forma que parte del calor absorbido se expulsa como calor de desecho al ambiente. Ese calor desechado no puede reutilizarse para aumentar el calor (inicial) producido por el sistema (en este caso la máquina), es a lo que llamamos entropía."
      },
      {
        "heading": "Otra interpretación",
        "level": 3,
        "content": "Es imposible construir una máquina térmica cíclica que transforme calor en trabajo sin aumentar la energía termodinámica del ambiente. Debido a esto podemos concluir, que el rendimiento energético de una máquina térmica cíclica que convierte calor en trabajo, siempre será menor a la unidad, y esta estará más próxima a la unidad, cuanto mayor sea el rendimiento energético de la misma. Es decir, cuanto mayor sea el rendimiento energético de una máquina térmica, menor será el impacto en el ambiente, y viceversa."
      },
      {
        "heading": "Refrigeradores y bombas de calor",
        "level": 2,
        "content": "Un refrigerador es un equipo que extrae energía calorífica de un cuerpo o sistema a baja temperatura para cederlo a otro que se encuentre a una temperatura mayor, el cual es considerado como sumidero.\nEn cambio, una bomba de calor, si bien tiene el mismo funcionamiento que un refrigerador, tiene como fuente principal el foco caliente, el cual absorbe energía calorífica de una fuente fría que para este caso es considerado como el sumidero\nPor lo tanto, ambos son equipos que requieren una entrada de energía (Trabajo) para poder transferir energía calorífica desde un foco frío a otro caliente. Para estos equipos la dirección de los procesos que participan en el desarrollo de la transferencia de calor por medio del ciclo de carnot, se da de manera inversa a la que conocemos regularmente en las máquinas térmicas.\nEl análisis de estos equipos viene dado por medio de la ecuación que relaciona a los focos tanto frío como caliente, así como al trabajo necesario requerido para poner en funcionamiento ya sea el refrigerador o la bomba de calor, ya que la ecuación de relación es la misma para ambas, donde Qh=Qc+W"
      },
      {
        "heading": "Tercer principio",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Sistema",
        "level": 2,
        "content": "Se puede definir un sistema como un conjunto de materia, que está limitado por unas paredes, reales o imaginarias, impuestas por el observador. Si en el sistema no entra ni sale materia, se dice que se trata de un sistema cerrado o sistema aislado si no hay intercambio de materia y energía, dependiendo del caso. En la naturaleza, encontrar un sistema estrictamente aislado es, por lo que se sabe, imposible, pero sí pueden hacerse aproximaciones. Un sistema del que sale y/o entra materia recibe el nombre de abierto. Algunos ejemplos:\n\nUn sistema abierto se da cuando existe un intercambio de masa y de energía con los alrededores; es por ejemplo, un coche. Le echamos combustible y él desprende diferentes gases y calor.\nUn sistema cerrado se da cuando no existe un intercambio de masa con el medio circundante, solo se puede dar un intercambio de energía; un reloj de cuerda, no introducimos ni sacamos materia de él. Solo precisa un aporte de energía que emplea para medir el tiempo.\nUn sistema aislado se da cuando no existe el intercambio ni de masa y energía con los alrededores; ¿Cómo encontrarlo si no es posible interactuar con él? Sin embargo, un termo lleno de comida caliente es una aproximación, ya que el envase no permite el intercambio de materia e intenta impedir que la energía (calor) salga de él. El universo es un sistema aislado, ya que la variación de energía es cero.\n  \n    \n      \n        Δ\n        E\n        =\n        0.\n      \n    \n    {\\displaystyle \\Delta E=0.}"
      },
      {
        "heading": "Medio externo",
        "level": 2,
        "content": "Se llama medio externo o ambiente a todo aquello que no está en el sistema pero que puede influir en él. Por ejemplo, considérese una taza con agua, que está siendo calentada por un mechero: en un sistema formado por la taza y el agua, el medio está formado por el mechero, el aire, etc."
      },
      {
        "heading": "Equilibrio térmico",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Variables termodinámicas",
        "level": 2,
        "content": "Las variables que tienen relación con el estado interno de un sistema se llaman variables termodinámicas o coordenadas termodinámicas, y entre ellas las más importantes en el estudio de la termodinámica son:\n\nla masa\nel volumen\nla densidad\nla presión\nla temperatura\nEn termodinámica, es muy importante estudiar sus propiedades, las cuales pueden clasificarse en dos tipos:\n\npropiedades intensivas: son aquellas que no dependen de la cantidad de sustancia o del tamaño de un sistema, por lo que su valor permanece inalterado al subdividir el sistema inicial en varios subsistemas, por este motivo no son propiedades aditivas.\npropiedades extensivas: son las que dependen de la cantidad de sustancia del sistema, y son recíprocamente equivalentes a las intensivas. Una propiedad extensiva depende por tanto del «tamaño» del sistema. Una propiedad extensiva tiene la propiedad de ser aditiva en el sentido de que si se divide el sistema en dos o más partes, el valor de la magnitud extensiva para el sistema completo es la suma de los valores de dicha magnitud para cada una de las partes.\nAlgunos ejemplos de propiedades extensivas son la masa, el volumen, el peso, cantidad de sustancia, energía, entropía, entalpía, etc. En general, el cociente entre dos magnitudes extensivas nos da una magnitud intensiva; por ejemplo, la división entre masa y volumen genera la densidad."
      },
      {
        "heading": "Estado de un sistema",
        "level": 2,
        "content": "Un sistema que puede describirse en función de coordenadas termodinámicas se llama sistema termodinámico y la situación en la que se encuentra definido por dichas coordenadas se llama estado del sistema."
      },
      {
        "heading": "Equilibrio térmico",
        "level": 2,
        "content": "Un estado en el cual dos coordenadas termodinámicas independientes X e Y permanecen constantes mientras no se modifican las condiciones externas se dice que se encuentra en equilibrio térmico. Si dos sistemas se encuentran en equilibrio térmico se dice que tienen la misma temperatura. Entonces se puede definir la temperatura como una propiedad que permite determinar si un sistema se encuentra o no en equilibrio térmico con otro sistema.\nEl equilibrio térmico se presenta cuando dos cuerpos con temperaturas diferentes se ponen en contacto, y el que tiene mayor temperatura cede energía térmica en forma de calor al que tiene más baja, hasta que ambos alcanzan la misma temperatura.\nAlgunas definiciones útiles en termodinámica son las siguientes."
      },
      {
        "heading": "Foco térmico",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Contacto térmico",
        "level": 2,
        "content": "Se dice que dos sistemas están en contacto térmico cuando puede existir transferencia de calor de un sistema a otro."
      },
      {
        "heading": "Procesos termodinámicos",
        "level": 1,
        "content": "Se dice que un sistema pasa por un proceso termodinámico, o transformación termodinámica, cuando al menos una de las coordenadas termodinámicas no cambia. Los procesos más importantes son:\n\nProcesos isotérmicos: son procesos en los que la temperatura no cambia.\nProcesos isobáricos: son procesos en los cuales la presión no varía.\nProcesos isocóricos: son procesos en los que el volumen permanece constante.\nProcesos adiabáticos: son procesos en los que no hay transferencia de calor alguna.\nProcesos diatérmicos: son procesos que dejan pasar el calor fácilmente.\nProcesos isoentrópicos: procesos adiabáticos y reversibles. Procesos en los que la entropía no varía.\nPor ejemplo, dentro de un termo donde se colocan agua caliente y cubos de hielo, ocurre un proceso adiabático, ya que el agua caliente se empezará a enfriar debido al hielo, y al mismo tiempo el hielo se empezará a calentar y posteriormente fundir hasta que ambos estén en equilibrio térmico, sin embargo no hubo transferencia de calor del exterior del termo al interior por lo que se trata de un proceso adiabático."
      },
      {
        "heading": "Rendimiento termodinámico o eficiencia",
        "level": 1,
        "content": "Un concepto importante en la ingeniería térmica es el de rendimiento. El rendimiento de una máquina térmica se define como:\n\n  \n    \n      \n        η\n        =\n        \n          \n            \n              \n                |\n              \n              \n                E\n                \n                  \n                    s\n                    a\n                    l\n                    i\n                    d\n                    a\n                  \n                \n              \n              \n                |\n              \n            \n            \n              \n                |\n              \n              \n                E\n                \n                  \n                    e\n                    n\n                    t\n                    r\n                    a\n                    d\n                    a\n                  \n                \n              \n              \n                |\n              \n            \n          \n        \n        \n      \n    \n    {\\displaystyle \\eta ={\\frac {|E_{\\rm {salida}}|}{|E_{\\rm {entrada}}|}}\\,}\n  \n\ndonde, dependiendo del tipo de máquina térmica, estas energías serán el calor o el trabajo que se transfieran en determinados subsistemas de la máquina."
      },
      {
        "heading": "Teorema de Carnot",
        "level": 2,
        "content": "Nicolas Léonard Sadi Carnot en 1824 demostró que el rendimiento de alguna máquina térmica que tuviese la máxima eficiencia posible (a las que en la actualidad se denotan con su nombre) y que operase entre dos termostatos (focos con temperatura constante), dependería solo de las temperaturas de dichos focos. Por ejemplo, el rendimiento para un motor térmico de Carnot viene dado por:\n\n  \n    \n      \n        \n          η\n          \n            m\n            C\n          \n        \n        =\n        1\n        −\n        \n          \n            \n              T\n              \n                f\n              \n            \n            \n              T\n              \n                c\n              \n            \n          \n        \n        \n      \n    \n    {\\displaystyle \\eta _{mC}=1-{\\frac {T_{f}}{T_{c}}}\\,}\n  \n\ndonde \n  \n    \n      \n        \n          T\n          \n            c\n          \n        \n      \n    \n    {\\displaystyle T_{c}}\n  \n y \n  \n    \n      \n        \n          T\n          \n            f\n          \n        \n      \n    \n    {\\displaystyle T_{f}}\n  \n son las temperaturas del termostato caliente y del termostato frío, respectivamente, medidas en Kelvin.\nEste rendimiento máximo es el correspondiente al de una máquina térmica reversible, la cual es solo una idealización, por lo que cualquier máquina térmica construida tendrá un rendimiento menor que el de una máquina reversible operando entre los mismos focos, de manera que:\n\n  \n    \n      \n        \n          η\n          \n            \n              m\n              .\n              t\n              .\n              r\n              e\n              v\n              e\n              r\n              s\n              i\n              b\n              l\n              e\n            \n          \n        \n        >\n        \n          η\n          \n            \n              m\n              .\n              t\n              .\n              i\n              r\n              r\n              e\n              v\n              e\n              r\n              s\n              i\n              b\n              l\n              e\n            \n          \n        \n        \n      \n    \n    {\\displaystyle \\eta _{\\rm {m.t.reversible}}>\\eta _{\\rm {m.t.irreversible}}\\,}"
      },
      {
        "heading": "Instrumentación",
        "level": 1,
        "content": "Existen dos tipos de instrumentos termodinámicos, el medidor y el depósito. Un medidor termodinámico es cualquier dispositivo que mide cualquier parámetro de un sistema termodinámico. En algunos casos, el parámetro termodinámico se define realmente en términos de un instrumento de medida idealizado. Por ejemplo, el principio cero establece que si dos cuerpos están en equilibrio térmico con un tercer cuerpo, también están en equilibrio térmico entre ellos. Este principio, tal y como señaló James Maxwell en 1872, afirma que es posible medir la temperatura. Un termómetro idealizado es una muestra de un gas ideal a presión constante. A partir de la ley de los gases ideales pV=nRT, el volumen de dicha muestra puede utilizarse como indicador de la temperatura; de esta manera define la temperatura. Aunque la presión se define mecánicamente, también se puede construir un aparato para medir la presión, llamado barómetro, a partir de una muestra de un gas ideal mantenida a temperatura constante. Un calorímetro es un dispositivo que se utiliza para medir y definir la energía interna de un sistema.\nUn depósito termodinámico es un sistema que es tan grande que sus parámetros de estado no se alteran de forma apreciable cuando se pone en contacto con el sistema de interés. Cuando el depósito se pone en contacto con el sistema, el sistema entra en equilibrio con el depósito. Por ejemplo, un depósito de presión es un sistema a una presión determinada, que impone esa presión al sistema al que está conectado mecánicamente. La atmósfera terrestre suele utilizarse como depósito de presión. El océano puede actuar como depósito de temperatura cuando se utiliza para refrigerar centrales eléctricas."
      },
      {
        "heading": "Variables conjugadas",
        "level": 1,
        "content": "El concepto central de la termodinámica es el de energía, la capacidad de hacer trabajo. Por el Primer Principio, la energía total de un sistema y su entorno se conserva. La energía puede ser transferida a un sistema por calentamiento, compresión o adición de materia, y extraída de un sistema por enfriamiento, expansión o extracción de materia. En mecánica, por ejemplo, la transferencia de energía es igual al producto de la fuerza aplicada a un cuerpo y el desplazamiento resultante.\nLas variables conjugadas son pares de conceptos termodinámicos, siendo la primera similar a una «fuerza» aplicada a algún sistema termodinámico, la segunda similar al «desplazamiento» resultante, y el producto de las dos igual a la cantidad de energía transferida. Las variables conjugadas comunes son:\n\nPresión-volumen (los parámetros mecánicos);\nTemperatura-entropía (parámetros térmicos);\nPotencial químico-número de partículas (parámetros materiales)."
      },
      {
        "heading": "Campos de aplicación",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Diagramas termodinámicos",
        "level": 1,
        "content": "Diagrama PVT\nDiagrama de fase\nDiagrama p-v\nDiagrama temperatura-entropía"
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": "Ludwig Boltzmann\nCalor y temperatura (continuación del estudio de la termodinámica)\nCaos\nConstante de Boltzmann\nEnergía\nEntalpía\nEntropía\nExergía\nNeguentropía\nSistémica\nTermoquímica\nTransmisión de calor\nFluctuación cuántica"
      },
      {
        "heading": "Notas",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bibliografía",
        "level": 1,
        "content": "Boltzmann, Ludwig (1986). Escritos de mecánica y termodinámica. Alianza Editorial. ISBN 842060173X. \nPérez Cruz, Justo R. (2005). La Termodinámica de Galileo a Gibbs. Fundación Canaria Orotava de Historia de la Ciencia. ISBN 978-84-609-7580-9. Archivado desde el original el 16 de octubre de 2013. Consultado el 18 de marzo de 2010. \nPlanck, Max (1990). Treatise on Thermodynamics. Dover Publications. ISBN 048666371X. \nZemansky, Mark W. (1985). «Calor y termodinámica». Madrid: McGraw-Hill. ISBN 84-85240-85-5. \nCallen, Herbert B. (1985). «Thermodynamics and an Introduction to Thermostatistics». John Wiley & Sons. \nReif, Federick (1985). «Fundamentals of Statistical and Thermal Physics». McGraw-Hill. \nMasanes, Lluís & Oppenheim, Jonathan (2017) A general derivation and quantification of the third law of thermodynamics. Nature.\nGoldstein, Martin; Inge F. (1993). The Refrigerator and the Universe. Harvard University Press. ISBN 978-0-674-75325-9. OCLC 32826343. (requiere registro).  A nontechnical introduction, good on historical and interpretive matters.\nKazakov, Andrei; Muzny, Chris D.; Chirico, Robert D.; Diky, Vladimir V.; Frenkel, Michael (2008). «Web Thermo Tables – an On-Line Version of the TRC Thermodynamic Tables». Journal of Research of the National Institute of Standards and Technology 113 (4): 209-220. ISSN 1044-677X. PMC 4651616. PMID 27096122. doi:10.6028/jres.113.016. \nGibbs J.W. (1928). The Collected Works of J. Willard Gibbs Thermodynamics. New York: Longmans, Green and Co.  Vol. 1, pp. 55–349.\nGuggenheim E.A. (1933). Modern thermodynamics by the methods of Willard Gibbs. London: Methuen & co. ltd. \nDenbigh K. (1981). The Principles of Chemical Equilibrium: With Applications in Chemistry and Chemical Engineering. London: Cambridge University Press. \nStull, D.R., Westrum Jr., E.F. and Sinke, G.C. (1969). The Chemical Thermodynamics of Organic Compounds. London: John Wiley and Sons, Inc. \nBazarov I.P. (2010). Thermodynamics: Textbook. St. Petersburg: Lan publishing house. p. 384. ISBN 978-5-8114-1003-3.  5th ed. (in Russian)\nBawendi Moungi G., Alberty Robert A. and Silbey Robert J. (2004). Physical Chemistry. J. Wiley & Sons, Incorporated. \nAlberty Robert A. (2003). Thermodynamics of Biochemical Reactions. Wiley-Interscience. \nAlberty Robert A. (2006). Biochemical Thermodynamics: Applications of Mathematica. John Wiley & Sons, Inc. ISBN 978-0-471-75798-6. PMID 16878778. \nDill Ken A., Bromberg Sarina (2011). Molecular Driving Forces: Statistical Thermodynamics in Biology, Chemistry, Physics, and Nanoscience. Garland Science. ISBN 978-0-8153-4430-8. \nM. Scott Shell (2015). Thermodynamics and Statistical Mechanics: An Integrated Approach. Cambridge University Press. ISBN 978-1107656789. \nDouglas E. Barrick (2018). Biomolecular Thermodynamics: From Theory to Applications. CRC Press. ISBN 978-1-4398-0019-5. \nLos siguientes títulos son más técnicos:\n\nBejan, Adrian (2016). Advanced Engineering Thermodynamics (4 edición). Wiley. ISBN 978-1-119-05209-8. \nCengel, Yunus A., & Boles, Michael A. (2002). Thermodynamics – an Engineering Approach. McGraw Hill. ISBN 978-0-07-238332-4. OCLC 45791449. (requiere registro). \nDunning-Davies, Jeremy (1997). Concise Thermodynamics: Principles and Applications. Horwood Publishing. ISBN 978-1-8985-6315-0. OCLC 36025958. \nKroemer, Herbert; Kittel, Charles (1980). Thermal Physics. W.H. Freeman Company. ISBN 978-0-7167-1088-2. OCLC 32932988."
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": "\n Wikimedia Commons alberga una galería multimedia sobre Termodinámica.\n\n Wikcionario  tiene definiciones y otra información sobre termodinámica.\n\n Wikiversidad alberga proyectos de aprendizaje sobre Termodinámica."
      }
    ],
    "summary": "La termodinámica es la rama de la física que describe los estados de equilibrio termodinámico a nivel macroscópico.  Constituye una teoría fenomenológica que estudia sistemas reales a partir de razonamientos deductivos, sin modelizar y siguiendo un método experimental.[1]​ Los estados de equilibrio se estudian y definen por medio de magnitudes extensivas tales como la energía interna, la entropía, el volumen o la composición molar del sistema,[2]​ o por medio de magnitudes no-extensivas derivadas de las anteriores como la temperatura, presión y el potencial químico; otras magnitudes, tales como la imanación, la fuerza electromotriz y las asociadas con la mecánica de los medios continuos en general también se pueden tratar por medio de la termodinámica.[3]​\nLa termodinámica trata los procesos de transferencia de calor, que es una de las formas de energía, y cómo se puede realizar un trabajo con ella. En esta área se describe cómo la materia en cualquiera de sus fases (sólido, líquido, gaseoso) va transformándose. Desde un punto de vista macroscópico de la materia, se estudia cómo esta reacciona a cambios en su volumen, presión y temperatura, entre otras magnitudes. La termodinámica se basa en cuatro principios fundamentales: el equilibrio termodinámico (o principio cero), el principio de conservación de la energía (primer principio), el aumento de la entropía con el tiempo (segundo principio) y la imposibilidad del cero absoluto (tercer principio).[4]​\nUna consecuencia de la termodinámica es lo que hoy se conoce como física estadística. Esta rama estudia, al igual que la termodinámica, los procesos de transferencia de calor, pero, al contrario a la anterior, desde un punto de vista molecular. La materia, como se conoce, está compuesta por moléculas, pero intentar deducir y extrapolar el comportamiento de una sola de sus moléculas al conjunto de todas ellas nos llevaría a medidas erróneas. Por eso se debe tratar como un conjunto de elementos aleatorios y utilizar el lenguaje estadístico y consideraciones mecánicas para describir el comportamiento macroscópico resultante de este conjunto molecular microscópico.[5]​\nLa termodinámica ofrece un aparato formal aplicable únicamente a estados de equilibrio,[6]​ definidos como aquel estado hacia «el que todo sistema tiende a evolucionar y caracterizado porque en el mismo todas las propiedades del sistema quedan determinadas por factores intrínsecos y no por influencias externas previamente aplicadas».[2]​ Tales estados terminales de equilibrio son, por definición, independientes del tiempo, y todo el aparato formal de la termodinámica —todas las leyes y variables termodinámicas— se definen de tal modo que se podría decir que un sistema está en equilibrio si sus propiedades se pueden describir consistentemente empleando la teoría termodinámica.[2]​ Los estados de equilibrio son necesariamente coherentes con los contornos del sistema y las restricciones a las que esté sometido. Por medio de los cambios producidos en estas restricciones (esto es, al retirar limitaciones tales como impedir la expansión del volumen del sistema, impedir el flujo de calor, etc.), el sistema tenderá a evolucionar de un estado de equilibrio a otro;[7]​ comparando ambos estados de equilibrio, la termodinámica permite estudiar los procesos de intercambio de masa y energía térmica entre sistemas térmicos diferentes.\nComo ciencia fenomenológica, la termodinámica no se ocupa de ofrecer una interpretación física de sus magnitudes. La primera de ellas, la energía interna, se acepta como una manifestación macroscópica de las leyes de conservación de la energía a nivel microscópico, que permite caracterizar el estado energético del sistema macroscópico (macroestado).[8]​ El punto de partida para la mayor parte de las consideraciones termodinámicas son los que postulan que la energía se puede intercambiar entre sistemas en forma de calor o trabajo, y que solo se puede hacer de una determinada manera. También se introduce una magnitud llamada entropía,[9]​ que se define como aquella función extensiva de la energía interna, el volumen y la composición molar que toma valores máximos en equilibrio: el principio de maximización de la entropía define el sentido en el que el sistema evoluciona de un estado de equilibrio a otro.[10]​ Es la física estadística, íntimamente relacionada con la termodinámica, la que ofrece una interpretación física de ambas magnitudes: la energía interna se identifica con la suma de las energías individuales de los átomos y moléculas del sistema, y la entropía mide el grado de orden y el estado dinámico de los sistemas, y tiene una conexión muy fuerte con la teoría de información.[11]​ En la termodinámica se estudian y clasifican las interacciones entre diversos sistemas, lo que lleva a definir conceptos como sistema termodinámico y su contorno. Un sistema termodinámico se caracteriza por sus propiedades, relacionadas entre sí mediante las ecuaciones de estado. Estas se pueden combinar para expresar la energía interna y los potenciales termodinámicos, útiles para determinar las condiciones de equilibrio entre sistemas y los procesos espontáneos.\nCon estas herramientas, la termodinámica describe cómo los sistemas reaccionan a los cambios en su entorno. Esto se puede aplicar a una amplia variedad de ramas de la ciencia y de la ingeniería, tales como motores, cambios de fase, reacciones químicas, fenómenos de transporte e incluso agujeros negros."
  },
  {
    "title": "Batman",
    "source": "https://en.wikipedia.org/wiki/Batman",
    "language": "en",
    "chunks": [
      {
        "heading": "Publication history",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Creation and early history",
        "level": 2,
        "content": "In early 1939, following the success of Superman, DC Comics' editors requested more superheroes. Bob Kane created Batman, initially drawing a character with red tights, bat wings, and a domino mask. Bill Finger, a collaborator, made significant contributions by suggesting a cowl, cape, gloves, and a darker costume. The character's alter ego, Bruce Wayne, was inspired by historical figures Robert the Bruce and Mad Anthony Wayne. Batman's early adventures drew inspiration from contemporary pulp fiction and characters like Zorro and the Shadow, establishing Batman as a master detective with a dark, brooding persona driven by the murder of his parents."
      },
      {
        "heading": "Golden, Silver and Bronze Ages",
        "level": 2,
        "content": "Batman debuted in Detective Comics #27 in 1939. Early stories were dark, featuring a Batman who did not shy away from killing. The character quickly became popular, leading to his own solo title in 1940. Robin, Batman's sidekick, was introduced in 1940, lightening the tone and boosting sales. Over the next few years, Batman's rogues' gallery expanded with iconic villains like the Joker and Catwoman.\nThe 1950s saw Batman in lighter, science fiction-influenced stories. However, declining sales led to a 1964 revamp by editor Julius Schwartz, who returned Batman to his detective roots and updated his appearance. The 1966 Batman TV series introduced a campy, humorous tone, which was reflected in the comics until its cancellation in 1968. In the 1970s, writers Dennis O'Neil and Neal Adams restored Batman's dark, gritty nature, a trend that continued despite fluctuating sales."
      },
      {
        "heading": "Modern Age and reboots",
        "level": 2,
        "content": "In the Modern Age of Comic Books Batman comics have undergone significant transformations, reflecting changing storytelling trends and audience interests. Beginning with seminal works like The Dark Knight Returns in the 1980s, which reintroduced Batman in a grittier, more mature context, the character's narrative evolved to explore deeper themes and darker tones. This period also saw the exploration of Batman's origins and psyche through works like Batman: Year One, and Batman: The Killing Joke, which delved into the complexities of heroism and villainy. In the 1990s, storylines such as \"Knightfall\" introduced new adversaries like Bane, who physically and mentally challenged Batman, leading to a temporary replacement by Jean-Paul Valley. The aftermath of an earthquake in \"No Man's Land\" depicted Gotham City in chaos, further pushing Batman to new limits of heroism and survival. Entering the 21st century, Grant Morrison's influential run introduced Damian Wayne as Batman's son and heir, bringing familial dynamics and a new generation of challenges to the forefront. Morrison's storytelling also delved into surreal and existential themes, such as in Batman R.I.P. and Final Crisis, which tested Batman's resolve and sanity against cosmic threats and personal demons. The New 52 reboot in 2011 refreshed Batman's continuity while preserving core elements of his character. This era introduced modern interpretations of classic storylines, like Night of the Owls, where Batman confronts the Court of Owls, a clandestine society controlling Gotham for centuries. The chilling return of the Joker in \"Death of the Family\" explored the intricate relationships within Batman's extended family of allies and adversaries. More recent developments under DC Rebirth and Infinite Frontier have continued to evolve Batman's universe, exploring new characters like Gotham and Gotham Girl, and tackling contemporary issues within the context of Gotham City's ever-evolving landscape of crime and heroism."
      },
      {
        "heading": "Characterization",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bruce Wayne",
        "level": 2,
        "content": "Batman's secret identity is Bruce Wayne, a wealthy American industrialist. As a child, Bruce witnessed the murder of his parents, Dr. Thomas Wayne and Martha Wayne, which ultimately led him to craft the Batman persona and seek justice against criminals. He resides on the outskirts of Gotham City in his personal residence, Wayne Manor. Wayne averts suspicion by acting the part of a superficial playboy idly living off his family's fortune and the profits of Wayne Enterprises, his inherited conglomerate. He supports philanthropic causes through his nonprofit Wayne Foundation, which in part addresses social issues encouraging crime as well as assisting victims of it, but is more widely known as a celebrity socialite. In public, he frequently appears in the company of high-status women, which encourages tabloid gossip while feigning near-drunkenness with consuming large quantities of disguised ginger ale since Wayne is actually a strict teetotaler to maintain his physical and mental prowess. Although Bruce Wayne leads an active romantic life, his vigilante activities as Batman account for most of his time. While Bruce Wayne is never depicted as being especially religious, he is ethnically Jewish on his mother's side; his maternal cousin Batwoman (Kate Kane) is practising. His father, Thomas, raised Bruce as a Christian, but as an adult he doesn't follow any religion.\nVarious modern stories have portrayed the extravagant, playboy image of Bruce Wayne as a facade. This is in contrast to the Post-Crisis Superman, whose Clark Kent persona is the true identity, while the Superman persona is the facade. In Batman Unmasked, a television documentary about the psychology of the character, behavioral scientist Benjamin Karney notes that Batman's personality is driven by Bruce Wayne's inherent humanity; that \"Batman, for all its benefits and for all of the time Bruce Wayne devotes to it, is ultimately a tool for Bruce Wayne's efforts to make the world better\". Bruce Wayne's principles include the desire to prevent future harm and a vow not to kill. Bruce Wayne believes that our actions define us, we fail for a reason, and anything is possible.\nWriters of Batman and Superman stories have often compared and contrasted the two. Interpretations vary depending on the writer, the story, and the timing. Grant Morrison notes that both heroes \"believe in the same kind of things\" despite the day/night contrast their heroic roles display. Morrison notes an equally stark contrast in their real identities. Bruce Wayne and Clark Kent belong to different social classes: \"Bruce has a butler, Clark has a boss.\" T. James Musler's book Unleashing the Superhero in Us All explores the extent to which Bruce Wayne's vast personal wealth is important in his life story, and the crucial role it plays in his efforts as Batman.\nWill Brooker notes in his book Batman Unmasked that \"the confirmation of the Batman's identity lies with the young audience ...he doesn't have to be Bruce Wayne; he just needs the suit and gadgets, the abilities, and most importantly the morality, the humanity. There's just a sense about him: 'they trust him ...and they're never wrong.\""
      },
      {
        "heading": "Personality",
        "level": 3,
        "content": "Batman's primary character traits can be summarized as \"wealth; physical prowess; deductive abilities and obsession\". The details and tone of Batman comic books have varied over the years with different creative teams. Dennis O'Neil noted that character consistency was not a major concern during early editorial regimes: \"Julie Schwartz did a Batman in Batman and Detective and Murray Boltinoff did a Batman in the Brave and the Bold and apart from the costume they bore very little resemblance to each other. Julie and Murray did not want to coordinate their efforts, nor were they asked to do so. Continuity was not important in those days.\"\nThe driving force behind Bruce Wayne's character is his parents' murder and their absence. Bob Kane and Bill Finger discussed Batman's background and decided that \"there's nothing more traumatic than having your parents murdered before your eyes\". Despite his trauma, he sets his mind on studying to become a scientist and to train his body into physical perfection to fight crime in Gotham City as Batman, an inspired idea from Wayne's insight into the criminal mind. He also speaks over 40 languages.\nAnother of Batman's characterizations is that of a vigilante; in order to stop evil that started with the death of his parents, he must sometimes break the law himself. Although manifested differently by being re-told by different artists, it is nevertheless that the details and the prime components of Batman's origin have never varied at all in the comic books, the \"reiteration of the basic origin events holds together otherwise divergent expressions\". The origin is the source of the character's traits and attributes, which play out in many of the character's adventures.\nBatman is often treated as a vigilante by other characters in his stories. Frank Miller views the character as \"a dionysian figure, a force for anarchy that imposes an individual order\". Dressed as a bat, Batman deliberately cultivates a frightening persona in order to aid him in crime-fighting, a fear that originates from the criminals' own guilty conscience. Miller is often credited with reintroducing anti-heroic traits into Batman's characterization, such as his brooding personality, willingness to use violence and torture, and increasingly alienated behavior. Batman, shortly a year after his debut and the introduction of Robin, was changed in 1940 after DC editor Whitney Ellsworth felt the character would be tainted by his lethal methods and DC established their own ethical code, subsequently he was retconned to have a stringent moral code, which has stayed with the character of Batman ever since. Miller's Batman was closer to the original pre-Robin version, who was willing to kill criminals if necessary."
      },
      {
        "heading": "Others",
        "level": 2,
        "content": "On several occasions former Robin Dick Grayson has served as Batman; most notably in 2009 while Wayne was believed dead, and served as a second Batman even after Wayne returned in 2010. As part of DC's 2011 continuity relaunch, Grayson returned to being Nightwing following the Flashpoint crossover event.\nIn an interview with IGN, Morrison detailed that having Dick Grayson as Batman and Damian Wayne as Robin represented a \"reverse\" of the normal dynamic between Batman and Robin, with, \"a more light-hearted and spontaneous Batman and a scowling, badass Robin\". Morrison explained their intentions for the new characterization of Batman: \"Dick Grayson is kind of this consummate superhero. The guy has been Batman's partner since he was a kid, he's led the Teen Titans, and he's trained with everybody in the DC Universe. So he's a very different kind of Batman. He's a lot easier; He's a lot looser and more relaxed.\"\nOver the years, there have been numerous others to assume the name of Batman, or to officially take over for Bruce during his leaves of absence. Jean-Paul Valley, also known as Azrael, assumed the cowl after the events of the Knightfall saga. Jim Gordon donned a mecha-suit after the events of Batman: Endgame, and served as Batman in 2015 and 2016. In 2021, as part of the Fear State crossover event, Lucius Fox's son Jace Fox succeeds Bruce as Batman in a 2021 storyline, depicted in the series I Am Batman, after Batman was declared dead.\nAdditionally, members of the group Batman Incorporated, Bruce Wayne's experiment at franchising his brand of vigilantism, have at times stood in as the official Batman in cities around the world. Various others have also taken up the role of Batman in stories set in alternative universes and possible futures, including, among them, various former proteges of Bruce Wayne."
      },
      {
        "heading": "Supporting characters",
        "level": 1,
        "content": "Batman's interactions with both villains and cohorts have, over time, developed a strong supporting cast of characters."
      },
      {
        "heading": "Enemies",
        "level": 2,
        "content": "Batman faces a variety of foes ranging from common criminals to outlandish supervillains. Many of them mirror aspects of the Batman's character and development, often having tragic origin stories that lead them to a life of crime. These foes are commonly referred to as Batman's rogues gallery. Batman's \"most implacable foe\" is the Joker, a homicidal maniac with a clown-like appearance. The Joker is considered by critics to be his perfect adversary, since he is the antithesis of Batman in personality and appearance; the Joker has a maniacal demeanor with a colorful appearance, while Batman has a serious and resolute demeanor with a dark appearance. As a \"personification of the irrational\", the Joker represents \"everything Batman [opposes]\". Other long-time recurring foes that are part of Batman's rogues gallery include Catwoman (a cat burglar anti-heroine who is variously an ally and romantic interest), the Penguin, Ra's al Ghul, Two-Face (Harvey Dent), the Riddler, the Scarecrow, Mr. Freeze, Poison Ivy, Harley Quinn, Bane, Clayface, and Killer Croc, among others. Many of Batman's adversaries are often psychiatric patients at Arkham Asylum."
      },
      {
        "heading": "Allies",
        "level": 2,
        "content": "Alfred Pennyworth, Batman's loyal butler and father figure, first appeared in Batman #16 (1943). After Bruce Wayne's parents were killed, Alfred raised Bruce and became one of the few people to know his secret identity. He is often portrayed as a steadying presence in Bruce's life, offering both emotional support and practical assistance in Batman's crime-fighting endeavors. More than just a caretaker, Alfred is a trusted ally and sometimes sidekick, sharing Wayne Manor with Bruce and contributing to Batman's mission.\nOne of Batman's most crucial allies is Commissioner James Gordon. Their relationship is built on mutual respect and a shared commitment to justice in Gotham City. In Batman: Year One, Gordon and Batman learn to trust each other, which transforms their efforts against crime into a more effective partnership. Gordon's perspective as a police officer complements Batman's vigilantism, allowing them to tackle Gotham's challenges together. Another important ally is the Justice League, which further emphasizes the importance of collaboration. Batman's relationship with Superman showcases how their contrasting ideologies can complement each other. In stories like World's Finest, their friendship highlights how Batman's methods benefit from Superman's optimism and strength."
      },
      {
        "heading": "Sidekicks",
        "level": 2,
        "content": "Robin, Batman's vigilante partner, has been a widely recognized supporting character for many years; each iteration of the Robin character, of which there have been five in the mainstream continuity, function as members of the Batman family, but additionally, as Batman's \"central\" sidekick in various media. Bill Finger stated that he wanted to include Robin because \"Batman didn't have anyone to talk to, and it got a little tiresome always having him thinking.\" The first Robin, Dick Grayson, was introduced in 1940. In the 1970s he finally grew up, went off to college and became the hero Nightwing. A second Robin, Jason Todd was introduced in the 1980s, following Dick Grayson's departure from the role. Initially impulsive and rebellious, Jason's tenure as Robin was controversial among fans. In 1988, DC held a fan vote to determine his fate in the iconic A Death in the Family storyline, where the Joker brutally beat Jason with a crowbar and left him to die in an explosion. The fans voted for his death. However, Jason was later resurrected and returned as the antihero Red Hood.\nThe third Robin in the mainstream comics is Tim Drake, who first appeared in 1989. He went on to star in his own comic series, and goes by the name Red Robin, a variation on the traditional Robin persona. In the first decade of the new millennium, Stephanie Brown served as the fourth in-universe Robin between stints as her self-made vigilante identity the Spoiler, and later as Batgirl. After Brown's apparent death, Drake resumed the role of Robin for a time. The role eventually passed to Damian Wayne, the 10-year-old son of Bruce Wayne and Talia al Ghul, in the late 2000s. Damian's tenure as du jour Robin ended when the character was killed off in the pages of Batman Incorporated in 2013. Batman's next young sidekick is Harper Row, a streetwise young woman who avoids the name Robin but followed the ornithological theme nonetheless; she debuted the codename and identity of the Bluebird in 2014. Unlike the Robins, the Bluebird is willing and permitted to use a gun, albeit non-lethal; her weapon of choice is a modified rifle that fires taser rounds. In 2015, a new series began titled We Are...Robin, focused on a group of teenagers using the Robin persona to fight crime in Gotham City. The most prominent of these, Duke Thomas, later becomes Batman's crimefighting partner as The Signal."
      },
      {
        "heading": "Romantic interests",
        "level": 2,
        "content": "Batman's romantic history spans decades, filled with relationships that reflect his struggle between personal happiness and his duty as Gotham's protector. His first love interest was Julie Madison, introduced in Detective Comics #31 (1939). Though engaged to Bruce Wayne, she left due to his distant and playboy persona, highlighting the conflict between Bruce's dual life and his desire for a normal relationship.\nSelina Kyle, also known as Catwoman, is perhaps the most notable figure in Batman's romantic history. Debuting in Batman #1 (1940), their relationship is characterized by a blend of romance and rivalry. Over the years, they have shared intense connections, often navigating the fine line between love and conflict. Their relationship culminated in an engagement during the DC Rebirth.\nAnother important figure is Vicki Vale, a journalist introduced in Batman #49 (1948). Vicki's attempts to uncover Batman's true identity lead to a complicated romantic involvement that waxed and waned over the years, especially during the early 1980s when their relationship became more serious.\nTalia al Ghul, introduced in Detective Comics #411 (1971), is another key player in Batman's love life. Their relationship is fraught with conflict due to her father, Ra's al Ghul, and his criminal ambitions. Despite the challenges, their love story resulted in the birth of Damian Wayne, who would grow to become the latest Robin and add a new layer of complexity to Batman's character.\nAdditionally, Batman's relationship with Wonder Woman has been explored in various storylines, including a passionate kiss in JLA (2003) during a moment of crisis. However, their relationship remains largely unexplored, often overshadowed by their respective commitments.\nOne of the most controversial romantic pairings emerged from the animated adaptation of Batman: The Killing Joke (2016), which depicted a brief romantic involvement between Batman and Batgirl (Barbara Gordon). This portrayal sparked significant criticism among fans, reflecting the challenges and complexities of Batman's romantic entanglements over the years."
      },
      {
        "heading": "Abilities",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Skills and training",
        "level": 2,
        "content": "Batman has no inherent superhuman powers; he relies on \"his own scientific knowledge, detective skills, and athletic prowess\". Batman's inexhaustible wealth gives him access to advanced technologies, and as a proficient scientist, he is able to use and modify these technologies to his advantage. In the stories, Batman is regarded as one of the world's greatest detectives, if not the world's greatest crime solver. Batman has been repeatedly described as having a genius-level intellect, being one of the greatest martial artists in the DC Universe, and having peak human physical and mental conditioning. As a polymath, his knowledge and expertise in countless disciplines is nearly unparalleled by any other character in the DC Universe. He has shown prowess in assorted fields such as mathematics, biology, physics, chemistry, and several levels of engineering. He has traveled the world acquiring the skills needed to aid him in his endeavors as Batman. In the Superman: Doomed story arc, Superman considers Batman to be one of the most brilliant minds on the planet.\nBatman has trained extensively in various fighting styles, making him one of the best hand-to-hand fighters in the DC Universe. He possesses a photographic memory, and has fully utilized his photographic memory to master a total of 127 forms of martial arts. In terms of his physical condition, Batman is described as peak human and far beyond an Olympic-athlete-level condition, able to perform feats such as easily running across rooftops in a Parkour-esque fashion, pressing thousands of pounds regularly, and even bench pressing six hundred pounds of soil and coffin in a poisoned and starved state. Superman describes Batman as \"the most dangerous man on Earth\", able to defeat an entire team of superpowered extraterrestrials by himself in order to rescue his imprisoned teammates in Grant Morrison's first storyline in JLA.\nBatman is strongly disciplined, and he has the ability to function under great physical pain and resist most forms of telepathy and mind control. He is a master of disguise, multilingual, and an expert in espionage, often gathering information under the identity of a notorious gangster named Matches Malone. Batman is highly skilled in stealth movement and escapology, which allows him to appear and disappear at will and to break free of nearly inescapable deathtraps with little to no harm. He is also a master strategist, considered DC's greatest tactician, with numerous plans in preparation for almost any eventuality.\nBatman is an expert in interrogation techniques and his intimidating and frightening appearance alone is often all that is needed in getting information from suspects. Despite having the potential to harm his enemies, Batman's most defining characteristic is his strong commitment to justice and his reluctance to take a life. This unyielding moral rectitude has earned him the respect of several heroes in the DC Universe, most notably that of Superman and Wonder Woman.\nAmong physical and other crime fighting related training, he is also proficient at other types of skills. Some of these include being a licensed pilot (in order to operate the Batplane), as well as being able to operate other types of machinery. In some publications, he even underwent some magician training."
      },
      {
        "heading": "Technology",
        "level": 2,
        "content": "Batman utilizes a vast arsenal of specialized, high-tech vehicles and gadgets in his war against crime, the designs of which usually share a bat motif. Batman historian Les Daniels credits Gardner Fox with creating the concept of Batman's arsenal with the introduction of the utility belt in Detective Comics #29 (July 1939) and the first bat-themed weapons the batarang and the \"Batgyro\" in Detective Comics #31 and 32 (Sept. and October 1939).\n\nPersonal armor\n\nBatman's batsuit aids in his combat against enemies, having the properties of both Kevlar and Nomex. It protects him from gunfire and other significant impacts, and incorporates the imagery of a bat in order to frighten criminals.\nThe details of the Batman costume change repeatedly through various decades, stories, media and artists' interpretations, but the most distinctive elements remain consistent: a scallop-hem cape; a cowl covering most of the face; a pair of bat-like ears; a stylized bat emblem on the chest; and the ever-present utility belt. His gloves typically feature three scallops that protrude from long, gauntlet-like cuffs, although in his earliest appearances he wore short, plain gloves without the scallops. The overall look of the character, particularly the length of the cowl's ears and of the cape, varies greatly depending on the artist. Dennis O'Neil said, \"We now say that Batman has two hundred suits hanging in the Batcave so they don't have to look the same ...Everybody loves to draw Batman, and everybody wants to put their own spin on it.\"\nFinger and Kane originally conceptualized Batman as having a black cape and cowl and grey suit, but conventions in coloring called for black to be highlighted with blue. Hence, the costume's colors have appeared in the comics as dark blue and grey; as well as black and grey. In the Tim Burton's Batman and Batman Returns films, Batman has been depicted as completely black with a bat in the middle surrounded by a yellow background. Christopher Nolan's The Dark Knight Trilogy depicted Batman wearing high-tech gear painted completely black with a black bat in the middle. Ben Affleck's Batman in the DC Extended Universe films wears a suit grey in color with a black cowl, cape, and bat symbol. Seemingly following the suit of the DC Extended Universe outfit, Robert Pattinson's uniform in The Batman restores the more traditional gray bodysuit and black appendage design, notably different from prior iterations by mostly utilizing real world armor and apparel pieces from modern military and motorcycle gear.\n\nBatmobile\n\nBatman's primary vehicle is the Batmobile, which is usually depicted as an imposing black car, often with tailfins that suggest a bat's wings.\nBatman also has an aircraft called the Batplane (originally a relatively traditionally, but bat-motifed plane, later seen as the much more unique \"Batwing\" starting in the 1989 film), along with various other means of transportation.\nIn proper practice, the \"bat\" prefix (as in Batmobile or batarang) is rarely used by Batman himself when referring to his equipment, particularly after some portrayals (primarily the 1960s Batman live-action television show and the Super Friends animated series) stretched the practice to campy proportions. For example, the 1960s television show depicted a Batboat, Bat-Sub, and Batcycle, among other bat-themed vehicles. The 1960s television series Batman has an arsenal that includes such \"bat-\" names as the Bat-computer, Bat-scanner, bat-radar, bat-cuffs, bat-pontoons, bat-drinking water dispenser, bat-camera with polarized bat-filter, bat-shark repellent bat-spray, and Bat-rope. The storyline \"A Death in the Family\" suggests that given Batman's grim nature, he is unlikely to have adopted the \"bat\" prefix on his own. In The Dark Knight Returns, Batman tells Carrie Kelley that the original Robin came up with the name \"Batmobile\" when he was young, since that is what a kid would call Batman's vehicle.\nThe Batmobile, which was before frequently depicted to resemble a sports car, was redesigned in 2011 when DC Comics relaunched its entire line of comic books, with the Batmobile being given heavier armor and new aesthetics.\n\nUtility belt\n\nBatman keeps most of his field equipment in his utility belt. Over the years it has shown to contain an assortment of crime-fighting tools, weapons, and investigative and technological instruments. Different versions of the belt have these items stored in compartments, often as pouches or hard cylinders attached evenly around it.\nSince the 1989 film, Batman is often depicted as carrying a projectile which shoots a retractable grappling hook attached to a cable (before this, a he employed a traditionally thrown grappling hook.) This allows him to attach to distant objects, be propelled into the air, and thus swing from the rooftops of Gotham City.\nAn exception to the range of Batman's equipment are hand guns, which he refuses to use on principle, since a gun was used in his parents' murder. In modern stories in terms of his vehicles, Batman compromises on that principle to install weapon systems on them for the purpose of non-lethally disabling other vehicles, forcing entry into locations and attacking dangerous targets too large to defeat by other means.\n\nBat-Signal\n\nWhen Batman is needed, the Gotham City police activate a searchlight with a bat-shaped insignia over the lens called the Bat-Signal, which shines into the night sky, creating a bat-symbol on a passing cloud which can be seen from any point in Gotham. The origin of the signal varies, depending on the continuity and medium.\nIn various incarnations, most notably the 1960s Batman TV series, Commissioner Gordon also has a dedicated phone line, dubbed the Bat-Phone, connected to a bright red telephone (in the TV series) which sits on a wooden base and has a transparent top. The line connects directly to Batman's residence, Wayne Manor, specifically both to a similar phone sitting on the desk in Bruce Wayne's study and the extension phone in the Batcave.\n\nBatcave\n\nThe Batcave is Batman's secret headquarters, consisting of a series of caves beneath his mansion, Wayne Manor. As his command center, the Batcave serves multiple purposes; supercomputer, surveillance, redundant power-generators, forensics lab, medical infirmary, private study, training dojo, fabrication workshop, arsenal, hangar and garage. It houses the vehicles and equipment Batman uses in his campaign to fight crime. It is also a trophy room and storage facility for Batman's unique memorabilia collected over the years from various cases he has worked on.\nIn both the comic book Batman: Shadow of the Bat #45 and the 2005 film Batman Begins, the cave is said to have been part of the Underground Railroad."
      },
      {
        "heading": "Fictional character biography",
        "level": 1,
        "content": "Batman's history has undergone many retroactive continuity revisions, both minor and major. Elements of the character's history have varied greatly. Scholars William Uricchio and Roberta E. Pearson noted in the early 1990s, \"Unlike some fictional characters, the Batman has no primary urtext set in a specific period, but has rather existed in a plethora of equally valid texts constantly appearing over more than five decades.\""
      },
      {
        "heading": "20th century",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Origin",
        "level": 3,
        "content": "The central fixed event in the Batman stories is the character's origin story. As a young boy, Bruce Wayne was horrified and traumatized when he watched his parents, the physician Dr. Thomas Wayne and his wife Martha, murdered with a gun by a mugger named Joe Chill. Batman refuses to utilize any sort of gun on the principle that a gun was used to murder his parents. This event drove him to train his body to its peak condition and fight crime in Gotham City as Batman. Pearson and Uricchio also noted beyond the origin story and such events as the introduction of Robin, \"Until recently, the fixed and accruing and hence, canonized, events have been few in number\", a situation altered by an increased effort by later Batman editors such as Dennis O'Neil to ensure consistency and continuity between stories."
      },
      {
        "heading": "Golden Age",
        "level": 3,
        "content": "In Batman's first appearance in Detective Comics #27, he is already operating as a crime-fighter. Batman's origin is first presented in Detective Comics #33 (November 1939) and is later expanded upon in Batman #47. As these comics state, Bruce Wayne is born to Dr. Thomas Wayne and his wife Martha, two very wealthy and charitable Gotham City socialites. Bruce is brought up in Wayne Manor, and leads a happy and privileged existence until the age of 8, when his parents are killed by a small-time criminal named Joe Chill while on their way home from a movie theater. That night, Bruce Wayne swears an oath to spend his life fighting crime. He engages in intense intellectual and physical training; however, he realizes that these skills alone would not be enough. \"Criminals are a superstitious cowardly lot\", Wayne remarks, \"so my disguise must be able to strike terror into their hearts. I must be a creature of the night, black, terrible ...\" As if responding to his desires, a bat suddenly flies through the window, inspiring Bruce to craft the Batman persona.\nIn early strips, Batman's career as a vigilante earns him the ire of the police. During this period, Bruce Wayne has a fiancé named Julie Madison. In Detective Comics #38, Wayne takes in an orphaned circus acrobat, Dick Grayson, who becomes his vigilante partner, Robin. Batman also becomes a founding member of the Justice Society of America, although he, like Superman, is an honorary member, and thus only participates occasionally. Batman's relationship with the law thaws quickly, and he is made an honorary member of Gotham City's police department. During this time, Alfred Pennyworth arrives at Wayne Manor, and after deducing the Dynamic Duo's secret identities, joins their service as their butler."
      },
      {
        "heading": "Silver Age",
        "level": 3,
        "content": "The Silver Age of Comic Books in DC Comics is sometimes held to have begun in 1956 when the publisher introduced Barry Allen as a new, updated version of the Flash. Batman is not significantly changed by the late 1950s for the continuity which would be later referred to as Earth-One. The lighter tone Batman had taken in the period between the Golden and Silver Ages led to the stories of the late 1950s and early 1960s that often feature many science-fiction elements, and Batman is not significantly updated in the manner of other characters until Detective Comics #327 (May 1964), in which Batman reverts to his detective roots, with most science-fiction elements jettisoned from the series.\nAfter the introduction of DC Comics' Multiverse in the 1960s, DC established that stories from the Golden Age star the Earth-Two Batman, a character from a parallel world. This version of Batman partners with and marries the reformed Earth-Two Catwoman (Selina Kyle). The two have a daughter, Helena Wayne, who becomes the Huntress. She assumes the position as Gotham's protector along with Dick Grayson, the Earth-Two Robin, once Bruce Wayne retires to become police commissioner. Wayne holds the position of police commissioner until he is killed during one final adventure as Batman. Batman titles, however, often ignored that a distinction had been made between the pre-revamp and post-revamp Batmen (since unlike the Flash or Green Lantern, Batman comics had been published without interruption through the 1950s) and would occasionally make reference to stories from the Golden Age. Nevertheless, details of Batman's history were altered or expanded upon through the decades. Additions include meetings with a future Superman during his youth, his upbringing by his uncle Philip Wayne (introduced in Batman #208 (February 1969)) after his parents' death, and appearances of his father and himself as prototypical versions of Batman and Robin, respectively. In 1980, then-editor Paul Levitz commissioned the Untold Legend of the Batman miniseries to thoroughly chronicle Batman's origin and history.\nBatman meets and regularly works with other heroes during the Silver Age, most notably Superman, whom he began regularly working alongside in a series of team-ups in World's Finest Comics, starting in 1954 and continuing through the series' cancellation in 1986. Batman and Superman are usually depicted as close friends. As a founding member of the Justice League of America, Batman appears in its first story, in 1960's The Brave and the Bold #28. In the 1970s and 1980s, The Brave and the Bold became a Batman title, in which Batman teams up with a different DC Universe superhero each month."
      },
      {
        "heading": "Bronze Age",
        "level": 3,
        "content": "In 1969, Dick Grayson attends college as part of DC Comics' effort to revise the Batman comics. Additionally, Batman also moves from his mansion, Wayne Manor into a penthouse apartment atop the Wayne Foundation building in downtown Gotham City, in order to be closer to Gotham City's crime. In 1974's \"Night of the Stalker\" storyline, a diploma on the wall reveals Bruce Wayne as a graduate of Yale Law School. Batman spends the 1970s and early 1980s mainly working solo, with occasional team-ups with Robin or Batgirl. Batman's adventures also become somewhat darker and more grim during this period, depicting increasingly violent crimes, including the first appearance (since the early Golden Age) of the Joker as a homicidal psychopath, and the arrival of Ra's al Ghul, a centuries-old terrorist who knows Batman's secret identity. In the 1980s, Dick Grayson becomes Nightwing.\nIn the final issue of The Brave and the Bold in 1983, Batman quits the Justice League and forms a new group called the Outsiders. He serves as the team's leader until Batman and the Outsiders #32 (1986) and the comic subsequently changed its title."
      },
      {
        "heading": "Modern Age",
        "level": 3,
        "content": "After the 12-issue miniseries Crisis on Infinite Earths, DC Comics retconned the histories of some major characters in an attempt at updating them for contemporary audiences. Frank Miller retold Batman's origin in the storyline \"Year One\" from Batman #404–407, which emphasizes a grittier tone in the character. Though the Earth-Two Batman is erased from history, many stories of Batman's Silver Age/Earth-One career (along with an amount of Golden Age ones) remain canonical in the Post-Crisis universe, with his origins remaining the same in essence, despite alteration. For example, Gotham's police are mostly corrupt, setting up further need for Batman's existence. The guardian Phillip Wayne is removed, leaving young Bruce to be raised by Alfred Pennyworth. Additionally, Batman is no longer a founding member of the Justice League of America, although he becomes leader for a short time of a new incarnation of the team launched in 1987. To help fill in the revised backstory for Batman following Crisis, DC launched a new Batman title called Legends of the Dark Knight in 1989 and has published various miniseries and one-shot stories since then that largely take place during the \"Year One\" period.\nSubsequently, Batman begins exhibiting an excessive, reckless approach to his crimefighting, a result of the pain of losing Jason Todd. Batman works solo until the decade's close, when Tim Drake becomes the new Robin.\nMany of the major Batman storylines since the 1990s have been intertitle crossovers that run for a number of issues. In 1993, DC published \"Knightfall\". During the storyline's first phase, the new villain Bane paralyzes Batman, leading Wayne to ask Azrael to take on the role. After the end of \"Knightfall\", the storylines split in two directions, following both the Azrael-Batman's adventures, and Bruce Wayne's quest to become Batman once more. The story arcs realign in \"KnightsEnd\", as Azrael becomes increasingly violent and is defeated by a healed Bruce Wayne. Wayne hands the Batman mantle to Dick Grayson (then Nightwing) for an interim period, while Wayne trains for a return to the role.\nThe 1994 company-wide crossover storyline Zero Hour: Crisis in Time! changes aspects of DC continuity again, including those of Batman. Noteworthy among these changes is that the general populace and the criminal element now consider Batman an urban legend rather than a known force.\nBatman once again becomes a member of the Justice League during Grant Morrison's 1996 relaunch of the series, titled JLA. During this time, Gotham City faces catastrophe in the decade's closing crossover arc. In 1998's \"Cataclysm\" storyline, Gotham City is devastated by an earthquake and ultimately cut off from the United States. Deprived of many of his technological resources, Batman fights to reclaim the city from legions of gangs during 1999's \"No Man's Land\".\nMeanwhile, Batman's relationship with the Gotham City Police Department changed for the worse with the events of \"Batman: Officer Down\" and \"Batman: War Games/War Crimes\"; Batman's long-time law enforcement allies Commissioner Gordon and Harvey Bullock are forced out of the police department in \"Officer Down\", while \"War Games\" and \"War Crimes\" saw Batman become a wanted fugitive after a contingency plan of his to neutralize Gotham City's criminal underworld is accidentally triggered, resulting in a massive gang war that ends with the sadistic Black Mask the undisputed ruler of the city's criminal gangs. Lex Luthor arranges for the murder of Batman's on-again, off-again love interest Vesper Lynd (introduced in the mid-1990s) during the \"Bruce Wayne: Murderer?\" and \"Bruce Wayne: Fugitive\" story arcs. Though Batman is able to clear his name, he loses another ally in the form of his new bodyguard Sasha, who is recruited into the organization known as \"Checkmate\" while stuck in prison due to her refusal to turn state's evidence against her employer. While he was unable to prove that Luthor was behind the murder of Vesper, Batman does get his revenge with help from Talia al Ghul in Superman/Batman #1–6."
      },
      {
        "heading": "21st century",
        "level": 2,
        "content": ""
      },
      {
        "heading": "2000s",
        "level": 3,
        "content": "DC Comics' 2005 miniseries Identity Crisis reveals that JLA member Zatanna had edited Batman's memories to prevent him from stopping the Justice League from lobotomizing Dr. Light after he raped Sue Dibny. Batman later creates the Brother I satellite surveillance system to watch over and, if necessary, kill the other heroes after he remembered. The revelation of Batman's creation and his tacit responsibility for the Blue Beetle's death becomes a driving force in the lead-up to the Infinite Crisis miniseries, which again restructures DC continuity. Batman and a team of superheroes destroy Brother EYE and the OMACs, though, at the very end, Batman reaches his apparent breaking point when Alexander Luthor Jr. seriously wounds Nightwing. Picking up a gun, Batman nearly shoots Luthor in order to avenge his former sidekick, until Wonder Woman convinces him to not pull the trigger.\nFollowing Infinite Crisis, Bruce Wayne, Dick Grayson (having recovered from his wounds), and Tim Drake retrace the steps Bruce had taken when he originally left Gotham City, to \"rebuild Batman\". In the Face the Face storyline, Batman and Robin return to Gotham City after their year-long absence. Part of this absence is captured during Week 30 of the 52 series, which shows Batman fighting his inner demons. Later on in 52, Batman is shown undergoing an intense meditation ritual in Nanda Parbat. This becomes an important part of the regular Batman title, which reveals that Batman is reborn as a more effective crime fighter while undergoing this ritual, having \"hunted down and ate\" the last traces of fear in his mind. At the end of the \"Face the Face\" story arc, Bruce officially adopts Tim (who had lost both of his parents at various points in the character's history) as his son. The follow-up story arc in Batman, Batman and Son, introduces Damian Wayne, who is Batman's son with Talia al Ghul. Although originally, in Batman: Son of the Demon, Bruce's coupling with Talia was implied to be consensual, this arc retconned it into Talia forcing herself on Bruce.\nBatman, along with Superman and Wonder Woman, reforms the Justice League in the new Justice League of America series, and is leading the newest incarnation of the Outsiders.\nGrant Morrison's 2008 storyline, \"Batman R.I.P.\" featured Batman being physically and mentally broken by the enigmatic villain Doctor Hurt and attracted news coverage in advance of its highly promoted conclusion, which would speculated to feature the death of Bruce Wayne. However, though Batman is shown to possibly perish at the end of the arc, the two-issue arc \"Last Rites\", which leads into the crossover storyline \"Final Crisis\", shows that Batman survives his helicopter crash into the Gotham City River and returns to the Batcave, only to be summoned to the Hall of Justice by the JLA to help investigate the New God Orion's death. The story ends with Batman retrieving the god-killing bullet used to kill Orion, setting up its use in \"Final Crisis\". In the pages of Final Crisis Batman is reduced to a charred skeleton. In Final Crisis #7, Wayne is shown witnessing the passing of the first man, Anthro. Wayne's \"death\" sets up the three-issue Battle for the Cowl miniseries in which Wayne's ex-proteges compete for the \"right\" to assume the role of Batman, which concludes with Grayson becoming Batman, while Tim Drake takes on the identity of the Red Robin. Dick and Damian continue as Batman and Robin, and in the crossover storyline \"Blackest Night\", what appears to be Bruce's corpse is reanimated as a Black Lantern zombie, but is later shown that Bruce's corpse is one of Darkseid's failed Batman clones. Dick and Batman's other friends conclude that Bruce is alive."
      },
      {
        "heading": "2010s",
        "level": 3,
        "content": "Bruce subsequently returned in Morrison's miniseries Batman: The Return of Bruce Wayne, which depicted his travels through time from prehistory to present-day Gotham. Bruce's return set up Batman Incorporated, an ongoing series which focused on Wayne franchising the Batman identity across the globe, allowing Dick and Damian to continue as Gotham's Dynamic Duo. Bruce publicly announced that Wayne Enterprises will aid Batman on his mission, known as \"Batman, Incorporated\". However, due to rebooted continuity that occurred as part of DC Comics' 2011 relaunch of all of its comic books, The New 52, Dick Grayson was restored as Nightwing with Wayne serving as the sole Batman once again. The relaunch also interrupted the publication of Batman, Incorporated, which resumed its story in 2012–2013 with changes to suit the new status quo."
      },
      {
        "heading": "The New 52",
        "level": 2,
        "content": "During The New 52, all of DC's continuity was reset and the timeline was changed, making Batman the first superhero to emerge. This emergence took place during Zero Year, where Bruce Wayne returns to Gotham and becomes Batman, fighting the original Red Hood and the Riddler. In the present day, Batman discovers the Court of Owls, a secret organization operating in Gotham for decades. Batman somewhat defeats the Court by defeating Owlman, although the Court continues to operate on a smaller scale. The Joker returns after losing the skin on his face (as shown in the opening issue of the second volume of Detective Comics) and attempts to kill the Batman's allies, though he is stopped by Batman. After some time, Joker returns again, and both he and Batman die while fighting each other. Jim Gordon temporarily becomes Batman, using a high-tech suit, while it is revealed that an amnesiac Bruce Wayne is still alive. Gordon attempts to fight a new villain called Mr. Bloom, while Wayne, regains his memories with the help of Alfred Pennyworth and Julie Madison. Once with his memories, Wayne becomes Batman again and defeats Mr. Bloom with the help of Gordon."
      },
      {
        "heading": "DC Rebirth",
        "level": 2,
        "content": "The timeline was reset again during Rebirth, although no significant changes were made to the Batman mythos.  Batman meets two new superheroes operating in Gotham named Gotham and Gotham Girl. Psycho-Pirate gets into Gotham's head and turns against Batman, and is finally defeated when he is killed. This event is very traumatic for Gotham Girl and she begins to lose her sanity.\nBatman forms his own Suicide Squad, including Catwoman, and attempts to take down Bane. The mission is successful, and Batman breaks Bane's back. Batman proposes to Catwoman.\nAfter healing from his wounds, an angry Bane travels to Gotham, where he fights Batman and loses. Batman then tells Catwoman about the War of Jokes and Riddles, and she agrees to marry him. Bane takes control of Arkham Asylum and manipulates Catwoman into leaving Wayne before the wedding. This causes Wayne to become very angry, and, as Batman, lashes out against criminals, nearly killing Mr. Freeze.\nBatman learns of Bane's control over Arkham and teams up with the Penguin to stop him. Bane captures Batman, and Scarecrow causes him to hallucinate, although he eventually breaks free. Batman escapes and reunites with Catwoman, while Bane captures and kills Alfred Pennyworth. Batman returns and defeats Bane, although too late to save Alfred. Gotham Girl prompts him to marry Catwoman.\nIt is revealed that the Joker who was working for Bane was really Clayface in disguise. The real Joker has been plotting a master plan to take over Gotham. This plan comes to fruition during The Joker War, in which Joker takes over the city. Batman defeats the Joker who vanishes after an explosion. Ghost-Maker, an enemy from Batman's past, appears in Gotham, and, after a battle, becomes a sort of ally to Batman. A new group called the Magistrate rises up in Gotham, led by Simon Saint, whose goal is to outlaw vigilantes such as Batman. At the same time, Scarecrow returns, fighting Batman. During Fear State, Batman battles and defeats both Scarecrow and the Magistrate's Peacekeepers."
      },
      {
        "heading": "Other versions",
        "level": 1,
        "content": "The character of Batman has been portrayed in numerous alternative versions across various media since his debut in 1939. These adaptations explore different facets and interpretations of the character.\nIn the \"Smallville\" a television series, Bruce Wayne adopts the Batman persona in 2001, later teaming up with Superman and other superheroes. Frank Miller's influential series, \"The Dark Knight Returns\", reimagines Batman as an older, more hardened vigilante, coming out of retirement to fight crime in a dystopian future.\nIn the \"Injustice: Gods Among Us\" universe, Batman leads a resistance against a tyrannical Superman who has taken control of Earth.\nThe DC Bombshells series sets Batman in a World War II-era context, with Bruce Wayne taking inspiration from Batwoman to become the masked hero. The \"Dark Multiverse\" introduces various twisted versions of Batman, such as the Batman Who Laughs, a hybrid of Batman and the Joker, and Red Death, a fusion of Batman and the Flash.\nOther notable reimaginings include JLA/Avengers, where Batman appears in a crossover with Marvel's Avengers; Stan Lee's Just Imagine, which offers a completely different origin for Batman; and \"Kingdom Come\", where an older Batman operates in a dystopian future alongside other aged superheroes.\nIn \"Superman: American Alien\", Bruce Wayne's journey is retold with significant differences, and \"Batman: White Knight\" explores a reality where the Joker is cured of his insanity and seeks to expose Batman as the true villain of Gotham. These various adaptations and reinterpretations highlight the versatility and enduring appeal of Batman as a character, allowing for a rich exploration of his mythology across different narratives and settings."
      },
      {
        "heading": "In popular culture",
        "level": 1,
        "content": "Batman has ascended to the status of a global pop culture phenomenon, transcending his origins in comic books. His influence expanded notably with the release of the 1989 film, which propelled him to the forefront of public consciousness through widespread merchandising. The Guardian describes Batman as emblematic of the constant reinvention characteristic of modern mass culture, embodying both iconic status and commercial appeal, making him a quintessential cultural artifact of the 21st century."
      },
      {
        "heading": "Media appearances",
        "level": 2,
        "content": "Apart from comics, Batman's presence spans various mediums, including newspapers, radio dramas, television, stage, and film. From the 1940s serials to contemporary TV shows like \"Gotham\" and \"Titans\", Batman's legacy endures. Celebrating the character's 75th anniversary, Warner Bros released \"Batman: Strange Days\", showcasing his timeless appeal.\nIn September 2024, Batman become the first superhero to be given a star on the Hollywood Walk of Fame. It was the 2,790th star."
      },
      {
        "heading": "Criticism",
        "level": 3,
        "content": "Batman has been criticized by fans for the extreme changes in tone and style between different iterations of the character in the franchise."
      },
      {
        "heading": "Different interpretations",
        "level": 2,
        "content": "Gay interpretations of Batman have been studied academically since psychologist Fredric Wertham's claims in 1954. Andy Medhurst and Will Brooker have explored Batman's appeal to gay audiences and the validity of a queer reading. Meanwhile, in psychological interpretations, Dr. Travis Langley sees Batman as representing the \"shadow archetype\", confronting inner darkness to fight evil, according to Carl Jung and Joseph Campbell's theories. Langley's analysis adds depth to Batman's psychological complexity."
      },
      {
        "heading": "Notes",
        "level": 1,
        "content": ""
      },
      {
        "heading": "References",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Sources",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Further reading",
        "level": 1,
        "content": ""
      },
      {
        "heading": "External links",
        "level": 1,
        "content": "\nOfficial website \nBatman Bio at the Unofficial Guide to the DC Universe\nBatman on DC Database, a DC Comics wiki\nBatman (1940–present) Comics Inventory"
      }
    ],
    "summary": "Batman is a superhero who appears in American comic books published by DC Comics. Batman was created by the artist Bob Kane and writer Bill Finger, and debuted in the 27th issue of the comic book Detective Comics on March 30, 1939. In the DC Universe, Batman is the alias of Bruce Wayne, a wealthy American playboy, philanthropist, and industrialist who resides in Gotham City. His origin story features him swearing vengeance against criminals after witnessing the murder of his parents, Thomas and Martha, as a child, a vendetta tempered by the ideal of justice. He trains himself physically and intellectually, crafts a bat-inspired persona, and monitors the Gotham streets at night. Kane, Finger, and other creators accompanied Batman with supporting characters, including his sidekicks Robin and Batgirl; allies Alfred Pennyworth and James Gordon; love interest and occasional adversary Catwoman; as well as foes such as the Penguin, the Riddler, Two-Face, and his archenemy, the Joker, among others.\nKane conceived Batman in early 1939 to capitalize on the popularity of Superman; although Kane frequently claimed sole creation credit, Finger substantially developed the concept from a generic superhero into something more bat-like. They drew inspiration from pulp fiction characters like the Shadow and Sherlock Holmes. Batman received a spin-off publication, Batman, in 1940. Kane and Finger introduced Batman as a ruthless vigilante who frequently killed or maimed criminals, but he evolved into a just, tempered superhero with a stringent moral code that prohibits killing during the 1940s. Unlike most superheroes, Batman does not possess any superpowers, instead relying on his intellect, fighting skills, and wealth. The 1960s Batman television series used a camp aesthetic, which continued to be associated with Batman for years after it ended. Various creators worked to return Batman to his darker roots in the 1970s and 1980s, culminating with the 1986 miniseries The Dark Knight Returns by Frank Miller.\nDC has featured Batman in many comic books, including comics published under its imprints such as Vertigo and Black Label; he has been considered DC's flagship charactersince the 1990s. The longest-running Batman comic, Detective Comics, is the longest-running comic book in the United States. Batman is frequently depicted alongside other DC superheroes, such as Superman and Wonder Woman, as a member of organizations such as the Justice League and the Outsiders. In addition to Bruce Wayne, other characters used the Batman persona, such as Jean-Paul Valley / Azrael in the 1993–1994 \"Knightfall\" story arc; Dick Grayson, the first Robin, from 2009 to 2011; and Jace Fox, the son of Wayne's ally Lucius, since 2021. DC has also published comics featuring alternate versions of Batman, including the incarnation seen in The Dark Knight Returns and its successors, the incarnation from the Flashpoint (2011) event, and numerous interpretations in comics published under the Elseworlds label.\nBatman is one of the most iconic characters in popular culture and has been listed among the greatest comic book superheroes and characters ever created. He is one of the most commercially successful superheroes, and his likeness has been licensed and featured in various media and merchandise sold around the world; this includes toy lines such as Lego Batman and video games such as the Batman: Arkham series. Batman has been adapted in many live-action and animated television series and films. Adam West portrayed him in the 1960s Batman television series, and he has been portrayed in film by Michael Keaton, Val Kilmer, George Clooney, Christian Bale, Ben Affleck, and Robert Pattinson. Many actors, most prolifically Kevin Conroy, have provided Batman's voice in animation and video games. In September 2024, Batman was given a star on the Hollywood Walk of Fame, being the first superhero to receive the honor."
  },
  {
    "title": "Batman",
    "source": "https://es.wikipedia.org/wiki/Batman",
    "language": "es",
    "chunks": [
      {
        "heading": "Creación y desarrollo",
        "level": 1,
        "content": "El éxito alcanzado por Superman en Action Comics impulsó a los editores de DC Comics —que en ese entonces, 1939, constituía una división de National Publications— a crear nuevos superhéroes para sus libros de historietas. Como resultado, Bob Kane (1915-1998) creó un personaje llamado «The Bat-Man».[22]​ Su colaborador Bill Finger (1914-1974), a quien recurrió Kane para mostrarle su esbozo original del personaje con la intención de pensar en más elementos para su diseño, comentó al respecto:\n\n \n\nFinger sugirió cambiar la máscara por una capucha, dibujarle una capa en vez de alas, proporcionarle guantes y retirar las secciones rojas y brillantes del traje. Mientras Kane realizaba el diseño y concepto visual, Finger se encargaba de definir la personalidad, contexto e historia del personaje.[24]​[25]​ No obstante, Finger también sugirió varias cosas sobre el aspecto físico del personaje y su vestimenta. En su autobiografía de 1989, Kane explicó lo siguiente:\n\n \n\nLa cultura popular de los años 1930 influyó de manera importante en la concepción de Batman. Kane y Finger se inspiraron en películas, cómics, libros, titulares de prensa y dibujos animados de esa década.[27]​ El diseño sombrío y la personalidad misteriosa del personaje provinieron de otros personajes populares de publicaciones pulp tales como Doc Savage, Black Bat y La Sombra.[28]​ Una de las principales influencias de Kane fue el personaje de El Zorro, en especial aquel personificado por el actor Douglas Fairbanks en la película La Marca del Zorro (1920). A su vez, el filme The Bat Whispers (1930) lo ayudó a establecer al murciélago como el concepto principal del personaje: en aquella producción, aparece un personaje al que se refieren como «The Bat», vestido con una máscara negra y un manto para ocultar su identidad y perseguir a sus víctimas. A continuación Finger sugirió que Batman tuviese habilidades extraordinarias como detective, de forma similar a Sherlock Holmes. Esto se volvería un rasgo característico del héroe en sus aventuras.[29]​ Una vez que definieron que Batman tendría una identidad secreta, Finger también creó el mote «Bruce Wayne» a partir de los nombres del rey escocés Robert Bruce y del soldado estadounidense Anthony Wayne.[30]​\nKane cedió los derechos de propiedad sobre el personaje a cambio de una mención como autor en todos los cómics de Batman, entre otros requerimientos. Al principio esta mención no decía «Batman creado por Bob Kane»; su nombre simplemente se escribía encima del título de cada ejemplar. A mediados de los años 1960, el nombre desapareció siendo reemplazado por los créditos de los dibujantes y escritores responsables de cada nuevo relato. A finales de la siguiente década, cuando Jerry Siegel y Joe Shuster comenzaron a recibir sus respectivos créditos por las historietas de Superman, y William Moulton Marston recibió reconocimiento por la creación de la Mujer Maravilla, las publicaciones de Batman empezaron a incluir la línea «Creado por Bob Kane», en adición a los demás créditos.[30]​\nFinger no recibió el mismo reconocimiento. A pesar de que había obtenido crédito por sus anteriores trabajos en la empresa, primordialmente durante la década de 1960, el contrato de Kane con la editorial únicamente le garantizó las correspondientes «gratitudes» por su contribución; por ejemplo, en Batman n.º 169 (febrero de 1965), el editor Julius Schwartz lo mencionó como el creador de Riddler, uno de los villanos recurrentes en la franquicia. Esto se debió a que el convenio estipulaba que solo habría de ser referido como escritor de cada historieta y no como su creador. Kane comentó: «Finger estaba desalentado por los pocos logros [que había tenido] durante su trayectoria. Sintió que no había usado todo su potencial artístico y que el éxito le había dejado de lado».[30]​ Cuando Finger murió, en 1974, DC Comics aun no lo acreditaba como el cocreador de Batman.[31]​ Jerry Robinson, colaborador de Finger y Kane en Batman, criticó a este último por haberse negado a reconocer la contribución de Finger. En una entrevista con The Comics Journal, Robinson mencionó:\n\n \n\nEn 1965 Kane rechazó a Finger como uno de los creadores del personaje escribiendo una carta a los seguidores, en la que decía: «[...] me parece que Bill Finger ha dado la impresión de que fue él, y no yo, quien creó a Batman, así como a Robin y todos los villanos y personajes principales. Esto es un anuncio fraudulento. Una mentira». Acerca de la ausencia de Finger en los créditos, agregó: «El problema de ser un escritor 'fantasma' es que debes permanecer en el anonimato y sin 'reconocimiento' alguno. Pero, si uno de ellos desea tener 'crédito', entonces debe dejar de ser un 'fantasma' y convertirse en un líder o innovador».[33]​ A pesar de su renuencia, en una entrevista de 1989 citó una vez más la situación de Finger desde una perspectiva totalmente diferente:\n\n \n\nTras concretar un acuerdo con los familiares de Finger, DC Entertainment acreditó por primera vez a Bill como cocreador del hombre murciélago en la película Batman v Superman: Dawn of Justice (2016) y en la segunda temporada de la serie televisiva Gotham.[35]​ Este reconocimiento se hizo extensivo a los cómics a partir de octubre de 2015, con los tomos n.º 3 de Batman and Robin Eternal y n.º 3 de Batman: Arkahm Knight Genesis.[36]​"
      },
      {
        "heading": "Primeras publicaciones",
        "level": 2,
        "content": "La primera aparición de Batman ocurrió en «El caso del sindicato químico», uno de los relatos del libro de historietas Detective Comics n.° 27, publicado en mayo de 1939. Si bien Finger declaró que lo había escrito al estilo de las publicaciones pulp —historias de violencia o romance caracterizadas por entremezclar elementos de la vida real con la ficción, impresas en páginas de bajo costo y difundidas principalmente entre los años 1920 y 1940—,[37]​[38]​ lo cierto es que se trató de una réplica del relato «Partners of Peril», escrito por Theodore Tinsley para el cómic La Sombra n.° 113.[39]​ En este capítulo, el héroe se enfrenta a un empresario químico y sus esfuerzos por deshacerse de sus socios para apoderarse del negocio. Posee la peculiaridad de mostrar a un Batman más inhumano en comparación con los siguientes relatos del enmascarado; por ejemplo, cuando el villano muere en las últimas páginas del libro, el justiciero exclama: «Es un final adecuado para alguien de su tipo».[40]​\nVarios rasgos y elementos característicos del personaje se habrían de incorporar en los primeros seis ejemplares, específicamente en los n.° 27 a 33 de Detective Comics. Por ejemplo, en los n.° 29 y 31 (julio y septiembre de 1939) aparecieron el cinturón multiusos y el búmeran temáticos, así como el batplane, respectivamente. No fue sino hasta el tomo n.° 33 (noviembre de 1939) que se exploró por primera vez su origen:[b]​ en su infancia, Bruce Wayne es testigo del asesinato de sus padres por un asaltante, lo cual le lleva a combatir el crimen en Gotham City a manera de venganza, bajo la identidad de un murciélago.[41]​[42]​[43]​ Cabe señalar que el diseño de su aspecto físico también sufrió modificaciones en estos primeros relatos: por ejemplo, su barbilla dejó de ser tan pronunciada y, en cambio, las orejas puntiagudas de su máscara se alargaron. En opinión de Kane: «Casi un año después [de la primera aparición de Batman], por fin logré crear a mi Batman definitivo».[44]​\nA pesar de la predilección de Kane por un protagonismo solitario,[45]​ la introducción de Robin como aliado de Batman en el volumen n.° 38 (abril de 1940) no solo diluyó de forma notable el tono pulp de las publicaciones originales, sino que también marcó un parteaguas en la industria de las historietas al detonar el surgimiento de otros «aliados jóvenes» de superhéroes en sus respectivos ejemplares.[46]​ Se considera que este tomo también influyó en el incremento en las ventas de Detective Comics, cuyas cifras casi se duplicaron con respecto a las de los volúmenes originales.[47]​[48]​\nEse mismo año, National Publications publicó el primer número del libro de historietas protagonizado por Batman. Hasta entonces, la empresa era la más exitosa en ventas de cómics, e incluso se le consideraba como la más popular y reconocida en la industria gracias a personajes como el hombre murciélago y Superman.[49]​  La trama de Batman n.° 1 destacó por introducir a Catwoman y Joker como villanos recurrentes así como por ser la única ocasión en que Batman utiliza un arma de fuego para deshacerse de unos monstruos. La omisión de armas de fuego en las siguientes publicaciones del personaje se debió en gran medida a la intervención del editor Whitney Ellsworth.[50]​ A finales de 1940, Superman y Batman aparecieron juntos en la serie World's Finest Comics, en la que Jerry Robinson y Dick Sprang colaboraban. Como parte de una reestructura que incluyó el renombre de la compañía a DC Comics, en 1942 se adoptó una política editorial en la que las principales publicaciones del sello, entre las cuales se incluyó Batman, adoptaron historias más «alegres y coloridas» con la finalidad de atraer a una audiencia más juvenil.[51]​[52]​"
      },
      {
        "heading": "Controversia de los años 1950",
        "level": 2,
        "content": "En los años inmediatos a la Segunda Guerra Mundial, Batman era uno de los escasos personajes cuyas historias seguían siendo publicadas en medio de una crisis que enfrentó la industria, debido primordialmente a la crítica social que responsabilizó al género de superhéroes por desencadenar «crimen juvenil, brutalidad y violencia, sexualidad desviada y promiscuidad en los jóvenes lectores».[53]​ En junio de 1952, el hombre murciélago hizo su aparición en la serie Superman como coprotagonista de la edición n.° 76 titulada The Mightiest Team in the World. La trama explora una alianza entre ambos héroes, quienes comparten entre sí sus verdaderas identidades.[54]​ El éxito del tomo ocasionó que DC Comics evaluara la posibilidad de rediseñar World's Finest Comics, para desarrollar nuevos relatos protagonizados por los personajes más populares de la compañía.[55]​ Esto trajo como resultado «un éxito financiero durante una época en la que los héroes ya no abundaban, mientras que los existentes estaban destinados a un solo tiraje».[56]​ La última publicación de World's Finest Comics se distribuyó en 1986.\n\nLa crítica social se acentuó con la publicación de La seducción del inocente, por el psiquiatra Fredric Wertham, donde se enfatizó en el rol de los cómics en la corrupción de la moral de la juventud y su influencia en la ejecución de delitos. El autor señaló además en específico las historietas de Batman por su aparente contenido homosexual dada la relación entre el protagonista y Robin.[57]​[58]​ Lo anterior llevó al establecimiento de la Comics Code Authority como órgano censor de este tipo de publicaciones.[59]​ De acuerdo con ciertos autores, la inclusión de Batwoman y Batgirl, en 1956 y 1961 respectivamente, obedeció a la necesidad de contrarrestar la controversia desatada por la obra de Wertham.[60]​ DC Comics reconoció que la omisión de Catwoman en las publicaciones de dicha década se debió a que «lo problemático de su atuendo» iba en contra de los lineamientos de la Comics Code Authority.[61]​\nHacia finales de la década de 1950 las historias de Batman se caracterizaron por temáticas orientadas a la ciencia ficción, con la intención de replicar el resultado favorable de otros personajes de DC Comics.[62]​ Lo anterior dio lugar a nuevos personajes del universo de Batman, tales como Ace, el Bati-sabueso y Bat-Mite; así como aventuras que involucraban robots y elementos alienígenas.[61]​ En febrero de 1960, Batman apareció en el ejemplar n.° 28 de Brave and the Bold, como nuevo miembro de la Liga de la Justicia, apareciendo posteriormente en varias historietas de la misma serie ese mismo año.[63]​"
      },
      {
        "heading": "Período del declive (1964-1985)",
        "level": 2,
        "content": "En 1964 se registró una drástica disminución en las ventas de Batman; Kane dijo al respecto que DC estaba «planeando acabar del todo con el personaje».[64]​ Con el fin de mejorar las ventas, se contrató al editor Julius Schwartz para renovar totalmente la serie. Los cambios comenzaron a evidenciarse con el número 327 de Detective Comics (mayo de 1964) que se subtituló \"New Look\" (\"Nuevo aire\") y que pretendía dar una imagen más moderna y con relatos de una narrativa similar a la de las típicas novelas detectivescas. Para redefinir al personaje contrató a Carmine Infantino. Los cambios, tanto de la vestimenta de Batman (por ejemplo, la insignia del murciélago cambió de color) como del Batmobile, son algunos rasgos que definieron a las siguientes historietas del personaje. Asimismo, se suprimió a los seres alienígenas, a Batwoman, a Ace the Bat-Hound y a Bat-Mite, y el mayordomo Alfred desapareció en los siguientes ejemplares (después de haber sido asesinado), con el objetivo de incorporar a la Tía Harriet como la nueva integrante de la Mansión Wayne así como compañera de Bruce Wayne y Dick Grayson —la identidad secreta de Robin—.[65]​\nDos años después, en 1966, se estrenó la serie de televisión Batman cuyo éxito incrementó las ventas de la serie literaria, la cual alcanzó un flujo de circulación cercano a las 900.000 copias. Neal Hefti, Nelson Riddle, Billy May y Warren Barker compusieron la banda sonora.[66]​ Con un tono más «feminista» en comparación al sugerido por Detective Comics, debido en parte a la designación de Batgirl como coprotagonista de los capítulos de la tercera temporada, la serie fue cancelada en 1968, aun después de los buenos resultados alcanzados durante sus emisiones. A consecuencia de lo anterior, las historietas de Batman volvieron a perder interés; Schwartz comentó luego sobre dicha situación: «Cuando el programa se volvió tan notable, quedé impactado por su temática. Obviamente, con su desaparición, las historietas sufrieron un efecto similar».[67]​\n\nEn 1969, Dennis O'Neil y Neal Adams comenzaron a describir un Batman apegado a las historias originales, brindándole la distinción de «severo vengador de la noche».[68]​ O'Neil expresó que su idea era reinventar al personaje, basándose en los primeros relatos y alejándose de la imagen dejada por la serie televisiva. A manera de reseña, O'Neil recapituló: «Acudí a la biblioteca de DC y me puse a leer algunas de las primeras historietas. Mi idea era resolver cuál fue el sentido con el que Kane y Finger crearon a Batman».[69]​ La primera colaboración mutua de O'Neil y Adams fue en Detective Comics número 395 (enero de 1970). En lo sucesivo, aunque fueron pocas las asociaciones de ambos, la influencia de su reinvención fue catalogada como «apabullante».[70]​ Dick Giordano, uno de los colaboradores, mencionó al respecto: «Regresamos al Batman oscuro y siniestro, y creo que por eso las historietas funcionaron tan bien [...] Incluso hoy en día, continuamos usando la capa larga y las orejas grandes en nuestro diseño».[71]​ Mientras que el trabajo de O'Neil y Adams fue popular entre los seguidores, no consiguieron mejorar las ventas, y algo similar ocurrió con la versión de Steve Englehart y Marshall Rogers, que intentaron coordinarse e imitar el estilo interpretativo de Adams. Su colaboración, del número 471 al 476, ha sido considerada como la fuente de inspiración directa de la primera película y la serie animada del personaje.[72]​ Sin embargo, las ventas seguían sin mejorar en las décadas de 1970 y 1980, llegando a un nivel crítico a comienzos de 1985.[73]​ Poco después se recuperarían de este gran declive."
      },
      {
        "heading": "El ascenso del Caballero Oscuro",
        "level": 2,
        "content": "Una nueva reinvención surgió con la novela gráfica Batman: The Dark Knight Returns de Frank Miller, publicada en 1986. Convertida en uno de los mayores éxitos de la industria,[74]​ la novela describe el futuro alternativo de Batman a sus sesenta años de edad, tras retomar su oficio de superhéroe. Junto a su considerable nivel de ventas, resulta destacable su impacto en el resurgimiento popular de Batman.[75]​ Ese mismo año, O'Neil se convirtió en el editor responsable de Batman y, aprovechando las influencias de la obra de Miller y la miniserie Crisis on Infinite Earths, decidió constituir a Batman como un héroe «más legendario e incomparable». De esta forma, en su primera historia Batman: año uno (publicada en los números 404-407, 1987) Frank Miller y el artista gráfico David Mazzucchelli volvieron a considerar el origen del personaje, a partir de una nueva perspectiva.\nAl año siguiente, Alan Moore y el dibujante Brian Bolland siguieron estas pautas e hicieron su colaboración en la novela gráfica Batman: The Killing Joke, enfocándose en The Joker e inspirándose en los detalles visuales y narrativos de la edición año uno. La historia relata la hazaña de este criminal en su intento por volver loco al oficial James Gordon. Para ello, deja lisiada a su hija Barbara y termina secuestrándolo, para torturarlo física y mentalmente. Meses después, DC Comics, consciente de la impopularidad del personaje secundario Jason Todd (identidad secreta del segundo Robin) con los fanáticos e inspirada en las referencias a un Todd muerto en Batman: The Dark Knight Returns, habilitó una línea telefónica disponible para definir el futuro del personaje. Las votaciones se hicieron justo después de la publicación del relato en donde Todd y su madre se hallan secuestrados en un almacén, teniendo un efecto directo en los siguientes números (en el caso anterior, más de 10 000 llamadas se tomaron en cuenta y, por un breve margen de 28 votos, los lectores convinieron en su muerte ocasionada por The Joker, la cual se halla narrada en la edición Batman: Una muerte en la familia).[76]​ En Una muerte en la familia, resulta destacable añadir que, aun cuando El Joker ha sido responsable de varias muertes desde su introducción, Batman toma el homicidio de Todd como un asunto meramente personal por lo que considera incluso asesinar a The Joker, deslindándose así de sus códigos de moralidad que le han caracterizado desde siempre.\n\nEn 1989 se estrenó la película Batman, dirigida por Tim Burton y protagonizada por Michael Keaton. La adaptación logró recaudar una considerable cantidad de dinero a nivel internacional, incrementando a su vez la popularidad del personaje. Incluso, llevaría a la publicación de una nueva serie, Legends of the Dark Knight, cuyo primer tomo vendió alrededor de un millón de copias.[77]​ Un par de años después, Batman protagonizó el crossover Batman: Knightfall (1993). En su incursión, el villano Bane hiere de gravedad a Bruce Wayne, por lo cual Azrael (cuya identidad secreta es Jean-Paul Valley) debe sustituirlo como el nuevo guardián de Gotham City. En 1999 se lanzó el crossover Batman: No Man’s Land, en donde se describieron los efectos de un gran seísmo en Gotham City, y en 2003 Jeph Loeb y Jim Lee crearon Batman: Hush, donde durante el año que duró la edición se concentraron en el concepto del villano Hush, recuperando finalmente la alianza entre Batman y Robin. Su trabajo logró ser reconocido como la edición más vendida desde el volumen número 500 (octubre de 1993). En esa misma época, Jeph Loeb se destacó por su contribución en Batman junto a su colaborador Tim Sale, y fue el responsable de dos series limitadas (Batman: The Long Halloween y Batman: Dark Victory) en donde aparecerían algunos de los antagonistas más recurrentes en la franquicia (notablemente Dos Caras).\nPor otra parte, Jim Lee colaboró nuevamente con Miller en All Star Batman and Robin the Boy Wonder, referida como la más vendida de las publicaciones de DC en 2005. Sin embargo, la serie recibió duras críticas por parte de la prensa especializada debido al texto, en donde se presenta a Batman como un personaje violento, cruel y psicópata, excitado por su propio sadismo hacia los criminales y reflexivo sobre las lesiones que inflige.[78]​[79]​[80]​[81]​ Ese mismo año, Batman apareció también en las series limitadas Crisis de Identidad y Crisis Infinita. A principios de 2006, Grant Morrison y Paul Dini asumieron los roles de editores responsables de Batman y Detective Comics, respectivamente; el primero se ocupó de incorporar elementos previos del personaje (principalmente las historias de ciencia ficción que se publicaron en los años 1950), con lo que creó una nueva temática en torno al héroe que sufría alucinaciones bajo la influencia de gases que alteraban su conciencia y de una serie de restricciones sensoriales. Ciertamente, los trabajos de Morrison alcanzaron un punto clímax con la edición Batman R.I.P, en la que Batman se enfrenta a la organización Black Glove que, en última instancia, provoca la locura del superhéroe. La historia continuó con Final Crisis, en donde se supone que Batman muere a manos de Darkseid. En la serie de 2009, Batman: Battle for the Cowl, Dick Grayson se convierte en el nuevo Batman, mientras que Damian Wayne (hijo de Bruce) asume el papel de Robin.[82]​ En junio de 2009, se contrató a Judd Winick para escribir Batman, y Grant Morrison obtuvo su propia serie titulada Batman and Robin.[83]​\nEn 2010, Wayne viajó a través de la historia en la serie Batman: The Return of Bruce Wayne, hasta regresar a la época contemporánea. Tras su regreso, reclamó de nuevo su oficio como Batman, aunque le permitió a Grayson seguir ocupando su lugar como el Caballero Oscuro por un tiempo.[84]​[85]​ En este período, se ocupó de llevar a cabo sus acciones contra el crimen a un nivel más globalizado, el cual es el eje de Batman Inc.[86]​ DC Comics anunció luego que Grayson sería el protagonista de los títulos Batman, Detective Comics y Batman and Robin, mientras que Wayne ocuparía el papel de Batman en Batman Incorporated. Además de este último, Wayne aparece también en Batman: The Dark Knight.[87]​ En 2011, debutó la nueva línea The New 52 en donde DC Comics reinició varias de sus franquicias de historietas; en cuanto a Batman, la mayoría de la cronología y de su historia ha sido preservada sin modificaciones importantes, a no ser de que Wayne es desde entonces el único Batman oficialmente reconocido por la editorial.[88]​ A mediados de 2016, durante el denominado DC Rebirth, DC Comics relanzó todos sus títulos publicados hasta el momento. La editorial reinició la serie Batman cuyo primer ejemplar de un solo capítulo comenzó a distribuirse a partir de junio del mismo año. En esta edición colaboraron el escritor Tom King y los ilustradores David Finch y Mikel Janín. Los cambios supusieron también el reinicio en la numeración de las historietas publicadas hasta el momento.[89]​"
      },
      {
        "heading": "Los nuevos 52",
        "level": 2,
        "content": "En septiembre de 2011, la línea completa de historietas de superhéroes de DC Comics, incluida su franquicia Batman, fue cancelada y relanzada con nuevos números desde el #1 como parte del nuevo reinicio titulado Los nuevos 52. Bruce Wayne es el único personaje que se identifica como Batman y aparece en Batman, Detective Comics, Batman y Robin, y Batman: The Dark Knight. Dick Grayson vuelve al manto de Nightwing y aparece en su propia serie en curso. Mientras que muchos personajes tienen sus historias significativamente alteradas para atraer a nuevos lectores, la historia de Batman permanece casi intacta. Batman Incorporated fue relanzado en 2012-2013 para completar la historia de \"Leviatan\".\nDesde el comienzo de la nueva 52, Scott Snyder ha sido el escritor de la insignia del título Batman. Su primer arco importante fue \"Night of the Owls\", donde Batman se enfrenta a la Corte de los búhos, una sociedad secreta que ha controlado Gótica durante siglos. El segundo arco de la historia fue \"Muerte de la familia\" (2012), donde el Joker regresa a Gótica y simultáneamente ataca a cada miembro de la familia Batman. El tercer arco de la historia fue \"Batman: Zero Year\", que redefinió el origen de Batman en The New 52. Siguió Batman # 0, publicado en junio de 2012, que exploró los primeros años del personaje. El argumento final antes del evento Convergencia (2015) fue Batman: Endgame, que representa la supuesta batalla final entre Batman y el Joker cuando desata el mortal virus \"Endgame\" en Ciudad Gótica. La historia termina con Batman y la supuesta muerte del Joker.\nComenzando con Batman vol. 2, #41, el Comisionado James Gordon se hace cargo del manto de Bruce como un nuevo, sancionado por el estado, el mecha-Batman, debutando en la Divergencia cómica especial del Día Libre del Cómic. Sin embargo, se revela que Bruce Wayne está vivo, aunque ahora sufren amnesia casi total de su vida como Batman y solo recuerda su vida como Bruce Wayne a través de lo que ha aprendido de Alfred. Bruce Wayne encuentra felicidad y propone a su novia, Julie, pero el Sr. Bloom daña gravemente a Jim Gordon y toma el control de Gótica, luego amenaza con destruir la ciudad al energizar un reactor de partículas creando una \"estrella extraña\" para tragar la ciudad. Bruce Wayne descubre la verdad de que él era Batman y después de hablar con un extraño que sonríe mucho (está fuertemente implícito que este es el Joker amnésico) obliga a Alfred a implantar sus recuerdos como Batman, pero a costa de sus recuerdos como el Renacido Bruce Wayne. Él regresa y ayuda a Jim Gordon a derrotar al Sr. Bloom y a cerrar el reactor. Gordon consigue su trabajo como el comisionado.[90]​\nEn 2015, DC Comics lanzó The Dark Knight III: La carrera maestra, la secuela de Frank Miller de The Dark Knight Returns y The Dark Knight Strikes Again.[91]​"
      },
      {
        "heading": "DC Renacimiento",
        "level": 2,
        "content": "En junio de 2016, el evento DC Rebirth relanzó la línea completa de cómics de DC Comics. Batman fue reiniciado y comenzó a distribuirse dos veces al mes, comenzando con Batman vol. 3, #1 (junio de 2016). La serie fue escrita por Tom King, y las ilustraciones fueron proporcionadas por David Finch y Mikel Janín. La serie Batman presentó dos vigilantes, Gotham y Gotham Girl. Detective Comics reanudó su sistema de numeración original a partir de junio de 2016 # 934, y el nuevo 52 volumen 2 de la serie se añadió en el volumen 1.[92]​ El escritor James Tynion IV y los artistas Eddy Barrows y Álvaro Martínez trabajaron en Detective Comics #934, y la serie inicialmente contó con un equipo formado por Tim Drake, Stephanie Brown, Cassandra Cain y Clayface, liderados por Batman y Batwoman."
      },
      {
        "heading": "Biografía del personaje y cronología",
        "level": 1,
        "content": "La recopilación de los sucesos que integran la vida del personaje ha sido objeto de varias revisiones y cambios constantes. Los investigadores William Uricchio y Roberta E. Pearson concluyeron: «A diferencia de otros personajes, Batman no está ubicado en un período determinado, sino que ha prevalecido como ícono en una gran variedad de textos durante más de cinco décadas».[93]​ Para determinar las sucesivas relaciones del héroe, los escritores han tomado la historia y origen de Batman como «ejes principales»:[94]​ cuando Bruce era un niño pequeño de solo ocho años, quedó sumamente traumatizado tras presenciar la muerte de sus padres —el doctor Thomas Wayne y su esposa Martha— perpetrada por el delincuente Joe Chill durante un asalto mal terminado cuando los Wayne se opusieron al robo de sus pertenencias. La impresión lo condujo a tomar la decisión de convertirse en el «hombre murciélago» y combatir el delito en su ciudad. En Batman Confidential número catorce, se menciona que el asesinato aconteció un 26 de junio, fecha que Bruce anualmente conmemora visitando la «escena del crimen».[94]​\nAunque han surgido numerosos relatos derivados, algunos editores han intentado establecer vínculos entre los principales acontecimientos, con el fin de relacionarlos de manera consistente en el tiempo.[95]​"
      },
      {
        "heading": "La inspiración del murciélago",
        "level": 2,
        "content": "Inicialmente, Batman apareció como un solitario combatiente del crimen.[97]​ Su origen empezó a ser explorado a partir de Detective Comics número 33. Bruce Wayne es hijo del Dr. Thomas Wayne y su esposa Martha, dos empresarios exitosos y reconocidos en la sociedad de Gotham City. Su infancia transcurre en medio de privilegios y riquezas, predominantes durante su estancia en la mansión familiar. A los ocho años, cuando salían de una función de cine, sus padres son víctimas de un asalto en el que pierden la vida, asesinados por el delincuente Joe Chill; Bruce abrumado por un fuerte sentimiento de culpa, promete que hará todo lo posible por hacer de su ciudad un lugar más seguro, combatiendo el delito en cualquiera de sus formas. Con el afán de cumplir su sentencia, se somete a un riguroso entrenamiento físico y mental —aunque luego se percata de la necesidad de una identidad secreta, ya que según Wayne: «Los criminales son supersticiosos y cobardes, por lo que mis habilidades tienen que aprovechar sus temores para intimidarlos. Debo ser una criatura nocturna, oscura e impactante [...]»—. En ese mismo relato, la intromisión repentina de un murciélago que entra a través de la ventana de su cubículo influye en su idea de convertirse en el nuevo héroe: Batman.[96]​\nDebido a su repentina aparición como el «vengador de la noche», la policía de Gotham City piensa que se trata de un nuevo delincuente, por lo que comienza a perseguirlo en sus primeras aventuras. Además, Batman comienza su primera relación romántica con Julie Madison,[98]​ conociendo también a Robin (un acróbata de circo huérfano cuya identidad secreta es Dick Grayson) y afiliándose a la Liga de la Justicia.[99]​ En los siguientes ejemplares Batman tiene que demostrar su apego a las leyes y apoyo a la justicia, para convencer al departamento de policía de su verdadero propósito: combatir la delincuencia.[100]​ Poco después, la policía lo nombra miembro honorario del departamento.[101]​ En este mismo período el mayordomo Alfred Pennyworth llega a la mansión de los Wayne y, tras descubrir las verdaderas identidades de sus amos, acepta su nuevo trabajo.[102]​ Si bien, ciertamente se añade luego en el canon oficial que Alfred cuidó del joven Bruce al quedar este desamparado tras la muerte de sus padres, por lo que, en la cronología contemporánea, se considera que este personaje ha tenido un importante papel en la protección y cuidado de Bruce desde su infancia.[103]​"
      },
      {
        "heading": "Versión paralela y prototipos",
        "level": 2,
        "content": "En 1956 comenzó una nueva etapa editorial para DC Comics y, como consecuencia, sus editores se orientaron a la renovación de los personajes e historietas existentes hasta ese momento, a los cuales planeaban ubicar en un contexto más contemporáneo. Así, Flash se convirtió en el primer superhéroe en ser actualizado, adoptando la nueva identidad de Barry Allen. Sin embargo, y aun después de adoptar un tono menos oscuro, Batman no pasó por cambios significativos. A inicios de los años 1960, se comenzaron a añadir los elementos característicos de la ciencia ficción en Batman y Detective Comics —en el número 327 de este último, se muestra una mayor evidencia de lo anterior—, resultando en la creación de Batman de Tierra-2, sobre la base de la perspectiva de «versión paralela» en un «universo alternativo» al del Batman original. La versión consiguió ampliarse a algunos personajes más (Catwoman y Helena Wayne), quienes adoptaron una nueva concepción. La Cazadora (identidad secreta de Helena) se convirtió en la nueva guardián de Gotham City, uniéndose al Robin de Tierra-2, después de que Batman decidiera retirarse de su oficio como héroe para trabajar como oficial del departamento de policía. Los investigadores concluyeron en que las dos versiones refieren a dos personajes diferentes ubicados en relatos completamente distintos. Aun con esta resolución, algunos editores han ignorado —consciente o inconscientemente— las diferencias existentes entre ambas versiones.[c]​\nAlgunos de los nuevos elementos que empezaron a mencionarse como parte del canon fueron el encuentro de Batman con un Superman del futuro, la protección de Bruce por su tío Philip Wayne durante su orfandad (Batman número 208, enero de 1969) y la manifestación de sus ancestros como los prototipos originales de Batman y Robin.[104]​[105]​ El editor Paul Levitz se ocupó de incluir los anteriores detalles, vinculándolos con la versión paralela, en la serie The Untold Legend of the Batman publicada en 1980.[106]​[107]​"
      },
      {
        "heading": "La llegada de Ra's al Ghul",
        "level": 2,
        "content": "En World's Finest Comics Batman se une a Superman y otros héroes de DC para resolver en equipo diversas intrigas delictivas. En los años 1960, se convirtió en uno de los miembros fundadores de la Liga de la Justicia (The Brave and the Bold número 28), al mismo tiempo que Dick Grayson asiste a la universidad. Durante este período, los editores realizaron una revisión de las historietas del personaje con el objetivo de continuar la cronología descrita hasta entonces. Para finales de la década, Bruce se mudó a un ático, ubicado detrás de Industrias Wayne (en pleno centro comercial y financiero de la ciudad), buscando residir más cerca de los crímenes. Robin y Batgirl dejaron de aliarse con él, figurando nuevas aventuras con una mayor descripción violenta gracias a la introducción del villano Ra's al Ghul. También, en el aspecto contrario, sobresale la aparición de Nightwing, la nueva identidad de Grayson, el anterior Robin.[108]​\nEn las décadas de los 1970 y 1980 la serie The Brave and the Bold se convirtió en una versión derivada de Batman, por lo que el protagonista se unió con un superhéroe diferente en cada ejemplar mensual que se publicaba. En 1983 la Liga de la Justicia se disolvió para ser sustituida por la nueva asociación The Outsiders (Los Marginales) encabezada por Batman hasta 1986, año en que la revista Batman and the Outsiders, después de 32 ejemplares publicados, cambió de nombre a solamente The Outsiders.[109]​"
      },
      {
        "heading": "Renovación de datos",
        "level": 2,
        "content": "Al concluir la publicación de Crisis on Infinite Earths, los orígenes de Batman y algunos personajes se modificaron una vez más. DC consideró la actualización de Batman, algo que no había ocurrido en su momento durante la «Edad de Plata de los cómics» por lo que Frank Miller se encargó de una nueva edición sobre el origen del personaje, intentando brindar una imagen más «valerosa y venerable», la cual se publicó en el volumen Batman: año uno (Batman números 404-407).[110]​ Aunque la versión alternativa (Batman de Tierra-2) y los relatos de Levitz se eliminaron de la historia, muchas de las historias de la era de la «Edad de Plata de los cómics» y de Batman: año uno siguieron de forma canónica después de la era de Crisis on Infinite Earths, sin alterar significativamente el relato sobre los orígenes del personaje. Contrariamente, prevaleció la descripción de un departamento policiaco corrupto, mientras que detalles como el pasado de Jason Todd (que empezó a ser descrito como un hijo huérfano de un insignificante ladrón), la desaparición de Philip Wayne (sustituido por Alfred como el responsable de cuidar a Bruce tras su orfandad) y la pertenencia de Batman a la Liga de la Justicia sufrieron cambios notables durante esta nueva etapa.[111]​ En 1989, DC lanzó la nueva serie basada en el origen de Batman Legends of the Dark Knight, la cual fue seguida de miniseries y cuentos cortos inspirados en la edición año uno.[112]​"
      },
      {
        "heading": "La caída del héroe",
        "level": 2,
        "content": "Influido directamente por el asesinato de su segundo pupilo: Jason Todd, provocado por The Joker (Batman: Una muerte en la familia, 1988), Batman retoma su personalidad de «vengador despiadado» (Batman número 429) y comienza a trabajar solo —hasta que Tim Drake se convierte en el nuevo Robin en la saga \"Un lugar solitario para morir\"—.[113]​ En 2005 los guionistas resucitaron el personaje de Jason Todd para enfrentarse a su mentor.\nLos años 1990 se caracterizaron por la introducción de Batman en diversos crossovers de DC; en 1993 (mismo año en que se publicó La muerte de Superman) apareció Batman: KnightFall, relato en el que Bane hiere gravemente al héroe obligándolo a un retiro temporal. Azrael es entonces designado como el nuevo «guardián de Gotham City» mientras Bruce se recupera de sus heridas. La siguiente edición, KnightsEnd, retoma las aventuras de Azrael en su calidad de protector para referirlo como un héroe injusto e indigno para la sociedad. Por ello, Bruce se enfrenta a él y deja que Dick Grayson (en ese momento Nightwing) asuma el nuevo papel de Batman, mientras Wayne entrena para recuperarse.[114]​ En 1994, Hora Cero: Crisis en el tiempo creó una nueva interpretación de Batman al describirlo más como «leyenda urbana» que como un «héroe real», introduciendo además a Joe Chill (el asesino de los padres de Bruce, quien había desaparecido del canon) nuevamente en el contexto.[115]​"
      },
      {
        "heading": "Terremoto en Gotham City",
        "level": 2,
        "content": "En 1996 reapareció la Liga de la Justicia y sus aventuras se retomaron en la serie JLA de Grant Morrison.[116]​ En la mayoría de los casos, Batman asumía un papel indispensable en las victorias del equipo. Sin embargo, el grupo vuelve a disolverse después de suscitarse una gran devastación en Gotham City a causa de un terremoto de gran magnitud (Batman Cataclysm, 1998). En No Man's Land, de 1999, debido a la proliferación de alianzas criminales que intentan apoderarse del terreno devastado, Batman se ocupa de «limpiar» la ciudad sin la ayuda de muchos de sus recursos tecnológicos. Al final del tomo, Lex Luthor empieza la reconstrucción de Gotham City y culpa a Bruce del asesinato de Vesper Fairchilde, amante de Lex —aunque Batman logra salir airoso de dichas acusaciones— (Bruce Wayne: Murderer? y Bruce Wayne: Fugitive). Por otro lado pierde a un importante aliado, su guardaespaldas Sasha Bordeaux, quien es reclutada por la agencia de espionaje Jaque Mate mientras se halla en prisión por oponerse a inculpar a Bruce por los asesinatos anteriormente mencionados. Finalmente, logra vengarse de Lex (a quien no pudo relacionar con la muerte de Vesper, su amante) junto a Talia al Ghul en Superman/Batman números 1-6, ocasionando que pierda la oportunidad de convertirse en Presidente de los Estados Unidos así como su quiebra corporativa."
      },
      {
        "heading": "El renacer de Batman",
        "level": 2,
        "content": "En Crisis de Identidad (serie limitada de 2004) un miembro de la Liga de la Justicia, Zatanna, modificó la memoria de Batman para impedirle que detuviera a la Liga en su labor de lobotomizar a Dr. Light después de haber violado a Sue Dibney. Esto lleva a Batman a una verdadera crisis de identidad en la que el héroe sospecha de la comunidad universal de superhéroes. Sin concebirlo como un plan siniestro, crea el satélite Brother I para supervisar las funciones de los diversos héroes y, en caso de que fuera necesario, matarlos. El satélite pasa luego a manos de Maxwell Lord, quien asesina al superhéroe Blue Beetle, evitando que este pueda informar a la Liga de la Justicia de la existencia del objeto.[117]​ La revelación de su existencia y su vínculo directo con el asesinato de Blue Beetle se da en la miniserie Crisis Infinita (2005). En el número 7 de esta última, Alexander Luthor Jr. menciona que el asesino de los padres de Bruce había sido capturado,[118]​ contradiciendo los relatos de Hora Cero: Crisis en el tiempo.\nEn los siguientes ejemplares, Batman recupera su identidad y destruye el satélite junto a un equipo de superhéroes. Debido a que Lex Luthor Jr. hiere mortalmente a Nightwing, el héroe decide usar un arma de fuego contra el villano pero, a última hora, la Mujer Maravilla lo convence de no apretar el gatillo.[119]​ Tras la conclusión de Crisis Infinita, Batman retorna después de un año de ausencia a la recién reconstruida Gotham City, donde se encuentra esperándolo Robin para unirse nuevamente a él y combatir el crimen. Durante su ausencia, se sometió a un intenso ritual de meditación en Nanda Parbat, una ciudad ficticia que se halla oculta entre las montañas del Tíbet y que es conocida por sus poderes curativos divinos, para luchar eficientemente contra sus «demonios internos». Como resultado, se convierte en un combatiente del crimen más eficaz al atacar directamente los temores mentales que le acechaban constantemente.[120]​[121]​ Al final de Batman: Face the Face, el hombre murciélago adopta a Tim Drake como su primer hijo,[122]​ pues su segundo hijo nace a partir de su relación amorosa con Talia Al Ghul —su nombre es Damian (Batman and Son)—. Junto a la Mujer Maravilla, Batman constituye nuevamente a la Liga de la Justicia en la serie Justice League of America,[123]​ y se une al mismo tiempo a la asociación de The Outsiders (Los Marginales).[124]​"
      },
      {
        "heading": "La nueva identidad",
        "level": 2,
        "content": "En la edición de Grant Morrison, Batman R.I.P., Batman es torturado física y mentalmente por la enigmática organización Black Glove. Al respecto, la prensa creó una considerable cobertura para lo que habría de concluir con la muerte de Bruce Wayne.[125]​[126]​ Sin embargo, la intención original no era que Batman muriera en esa edición, sino que la trama continuara con Final Crisis en donde sí ocurriría el deceso. Así, apareció un relato de dos ediciones denominado Last Rites, en el que Batman sobrevive al choque de su helicóptero en el río de Gotham City y regresa a su Batcave. Poco después, es convocado al Salón de la Justicia por la Liga de la Justicia para ayudarlos a investigar la muerte de Orión. Lo anterior conduce finalmente a los eventos de Final Crisis, donde Batman es secuestrado por la Abuela Bondad. Tras ser torturado por los secuaces de Darkseid, en un intento por separar sus rasgos de personalidad y trasplantarlos a cuerpos clonados, el plan de Darkseid falla (los clones se matan a sí mismos, debido a que el sentido de la justicia de Batman les impide servir a Darkseid). Al final de Last Rites, se revela que Batman conserva la bala que se usó para matar a Orión.[127]​\nDe acuerdo con Final Crisis número seis, Batman muere aparentemente al enfrentarse a Darkseid. Previo a este suceso, el primero anuncia que habría de quebrantar su política consistente en no usar armas de fuego para vencer a Darkseid. Así, le dispara a Darkseid en el pecho con una bala hecha de Radion (la misma bala usada para matar a Orión). No obstante, al mismo tiempo el villano desata su Sanción Omega —también conocida como «la muerte que es vida»— sobre Batman.[128]​ Aunque este ataque no «mata» a su víctima, envía a su conciencia hacia mundos paralelos. Por ello, aun cuando el cadáver de Batman sugiere su muerte, al final del relato se revela que ha sido enviado a un pasado distante para observar la muerte de Anthro, un cro-magnon.[129]​[130]​ En la miniserie Battle for the Cowl los principales allegados a Wayne compiten por asumir el papel de Batman. Al final, Grayson acepta de mala gana sustituir a Wayne como el nuevo guardián de Gotham City.[131]​ Mientras tanto, Tim Drake asume la identidad de Red Robin, para buscar a Bruce Wayne, quien cree que todavía está vivo.[132]​\nPor otra parte, en Blackest Night, el villano Black Hand es visto desenterrando el cuerpo de Bruce Wayne, para robar su cráneo y llevarlo a la corporación Black Lantern.[133]​ Deadman, cuyo cuerpo se ha convertido en un Black Lantern, se apresura a ayudar al nuevo Batman y a Robin, junto con Red Robin, contra los villanos de Gotham City que han regresado como Black Lanterns, así como para enfrentarse contra los miembros de su propia familia que fueron resucitados.[134]​ El cráneo fue reanimado poco después como un Black Lantern, y se diseñó un cuerpo para él, en un proceso controlado directamente por Nekron, líder de Black Hand, para que este se enfrente a la Liga de la Justicia y los Titanes. Una vez que la versión Black Lantern de Batman crea varios anillos de poder oscuro para asesinar a la mayoría de los miembros de la Liga, el cráneo volvió a la normalidad. Nekron explicó que este había cumplido su propósito como un «lazo emocional». Además, el villano se refiere en la historia a dicho cráneo como «Bruce Wayne», aun cuando sabe que el cuerpo no es el del verdadero Batman.[135]​\nEn el tercer ejemplar de Batman and Robin, «Blackest Knight», se revela que el cuerpo que aparece al final de Final Crisis n.º 6 era en realidad un clon creado por Darkseid, en un intento fallido por reunir un ejército de hombres murciélago. Debido a esto, el cráneo que utilizó Black Lantern, y que fue reanimado por Nekron, se trata realmente de uno falso. Dick Grayson, pensando que era el cuerpo verdadero de Bruce Wayne, intenta resucitarlo solo para encontrarse con un combatiente violento e inconsciente de sus acciones. Fue entonces cuando se percató de que ese no era el verdadero cuerpo.[136]​[137]​ La historia de Morrison continúa con la miniserie Batman: The Return of Bruce Wayne (2010), en la cual Bruce viaja a través del tiempo desde la era prehistórica hasta la época contemporánea de Gotham City. Después, salió la serie Bruce Wayne: The Road Home, en donde Bruce se adapta a una nueva identidad, conocida como Question, y comprueba que Gotham City está bien protegida con los nuevos Batman y Robin. Ayuda a estos a resolver misterios, y también a Red Robin, Batgirl, y otros. Debido a que Darkseid fue quien lo envió a esa época para usarlo a manera de arma apocalíptica, los aliados de Bruce deben detenerlo.[138]​[139]​[140]​ Gracias al apoyo de sus compañeros, Batman logra ponerle fin al oscuro propósito de Darkseid y regresa a la época contemporánea.\nTras su retorno, asume nuevamente el papel de Caballero Oscuro, permitiéndole a Dick y a Damian continuar con su labor como el Dúo Dinámico de Gotham City (Batman, Inc.). En los ejemplares más recientes, se revela que Batman desea formar un ejército de héroes que le ayudarán a enfrentarse al mal en cada país del mundo. Para ello, Bruce anuncia públicamente que Wayne Enterprises financia a Batman en dicha misión, conocida como «Batman, Incorporated».[141]​ Asimismo, Batman protagoniza la nueva serie Batman: The Dark Knight, escrita por David Finch, donde el héroe debe investigar la desaparición de su amigo Dawn Golden en un contexto de magia y misticismo.[142]​"
      },
      {
        "heading": "Identidad secreta",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Batman",
        "level": 2,
        "content": "Los rasgos principales de Batman se resumen en «destreza física, habilidades deductivas y obsesión».[94]​ La mayor parte de las características básicas de los cómics han variado por las diferentes interpretaciones que le han dado al personaje. Esto es ejemplificado en la percepción de Dennis O'Neil, donde señala lo siguiente: «Julie Schwartz creó su propio Batman en las dos series de DC Comics [Batman y Detective Comics], mientras que Murray Boltinoff hizo su versión personal para The Brave and the Bold. Aparte de la vestimenta, las dos versiones tenían muy pocas semejanzas entre sí; Schwartz y Boltinoff nunca coordinaron sus trabajos —ni pretendían hacerlo en ningún momento—. Simple y sencillamente, la continuidad no era algo importante en ese entonces».[143]​\nUn elemento principal que define a Batman como personaje es su origen. Bob Kane dijo que tanto él como Bill Finger discutieron sobre los antecedentes del superhéroe, concluyendo finalmente: «No hay nada más traumático que ver cómo tus padres mueren delante de tus ojos».[144]​ Esta experiencia condujo a Bruce a convertirse en Batman, bajo la promesa de vengar la muerte de sus padres.[94]​ Algunos coinciden en que, aun con la aparición de nuevas tramas con cierta complejidad narrativa, «sus orígenes han mantenido unidas a todas las expresiones divergentes», en referencia a cada una de las diferentes interpretaciones y apariciones que ha tenido el personaje desde su creación.[145]​ Asimismo, este concepto es la fuente de varios rasgos y atributos característicos de Batman, las cuales se manifiestan en las diversas aventuras del mismo.[94]​\nGeneralmente, Batman es referido como el «vigilante de Gotham City» en sus historias. Frank Miller lo concibe como una «figura dionisíaca» así como «una especie de fuerza anárquica, encargada de imponer el orden y la justicia por su propia cuenta».[146]​ Su vestimenta de murciélago conserva su esencia «oscura y siniestra», la cual es de vital importancia para intimidar a sus enemigos y, en última instancia, devolver la paz y el orden a la ciudad.[147]​"
      },
      {
        "heading": "Bruce Wayne",
        "level": 2,
        "content": "La identidad secreta de Batman es Bruce Wayne, un exitoso empresario que vive en Gotham City. No obstante, los mismos habitantes de la ciudad lo ven como un playboy irresponsable y superficial que vive de la fortuna personal de su familia (acumulada a partir de las inversiones realizadas en inmuebles de Gotham justo antes de que la ciudad se convirtiera en una gran metrópolis)[148]​ y de las ganancias de Industrias Wayne, una firma tecnológica privada que heredó. A pesar de esto, Wayne es conocido también por sus contribuciones caritativas, especialmente por medio de su fundación.[149]​ La razón por la cual Bruce creó una imagen de playboy es para evitar sospechas sobre su verdadera identidad, a veces comportándose de forma torpe y egocéntrica.[150]​\nLos escritores de los relatos de Batman y Superman usualmente comparan a ambos personajes dentro del contexto de varias historias, para finalmente llegar a conclusiones distintas. Al igual que Superman, la consistencia de las identidades duales de Batman varía con el tiempo. En historias más recientes, se ha intentado hacer de Bruce Wayne la fachada, siendo entonces Batman la verdadera representación de su personalidad[151]​ (en contrapunto al Superman contemporáneo, donde Clark Kent es la personalidad real, y Superman solo es el disfraz).[152]​[153]​\nEn el documental de televisión Batman Unmasked, cuyo contenido aborda la psicología del personaje, el profesor asociado de psicología social en la Universidad de California en Los Ángeles, así como científico en la RAND, Benjamin Karney, percibe que la personalidad de Batman es guiada por la humanidad inherente de Bruce Wayne; en sus propias palabras: «Batman, por todos los beneficios que implica y el tiempo que Bruce Wayne le dedica, es finalmente una herramienta para los esfuerzos de Bruce Wayne de hacer del planeta un lugar mejor».\nTal como se observa en el libro de Will Brooker, Batman Unmasked: «La identidad de Batman ciertamente se conecta con las audiencias juveniles [...] no tiene que ser Bruce Wayne; sólo necesita el traje y los gadgets, las habilidades, y principalmente la humanidad y moralidad. Así, sólo existe un concepto sobre él: «Ellos confían en su persona [...] y nunca se equivocan».[154]​"
      },
      {
        "heading": "Otros",
        "level": 2,
        "content": "Tras la aparente muerte de Wayne, Dick Grayson se ha convertido en el nuevo Batman. Esta es la segunda ocasión en que sustituye a Bruce, ante su ausencia. Curiosamente, en ambas, aceptó asumir dicha identidad de mala gana. En una entrevista con IGN, Morrison detalla que, a partir de Grayson como Batman y Damian Wayne como Robin, se dará paso a una «forma reversiva» de la dinámica natural existente entre Batman y Robin, teniendo entonces a «un Batman más alegre y espontáneo, y a un Robin más ceñudo y combativo». Sabiendo esto, explica sus intenciones para la nueva concepción de Batman: «Dick Grayson es similar a este superhéroe consumado. El joven ha sido compañero de Batman desde que era un niño, ha formado parte de Los Jóvenes Titanes y entrenado con todos los personajes que forman parte del Universo DC. Por lo tanto, es un nuevo Batman muy diferente al original. De hecho, es mucho más sencillo; está mucho más suelto y más relajado».[155]​\nOtros que han asumido el papel del Caballero Oscuro son Azrael, tras los eventos de la serie Knightfall; el comisionado James Gordon, que utilizó un traje robótico después de los acontecimientos de Batman: Endgame;[156]​ y otros personajes en universos alternativos."
      },
      {
        "heading": "Habilidades",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Habilidades y entrenamiento",
        "level": 2,
        "content": "A diferencia de muchos otros superhéroes, Batman no posee superpoderes, por lo que hace uso de «sus conocimientos científicos, habilidades detectivescas y una gran destreza física».[157]​ así como tampoco utiliza armas de fuego, ya que manifiesta un rechazo a las mismas debido al asesinato de sus padres perpetrado con un arma de fuego. En las historias es considerado como uno de los mejores detectives del planeta. Sin embargo posee solo una regla, razón por la cual es conocido y temido por los criminales de la ciudad: Batman no mata, pero si sabe hacer daño, ya que se vale de cualquier método para atrapar a los criminales u obtener la información que necesita de ellos, incluyendo a veces la intimidación y la tortura física.[158]​ En la primera historia de Grant Morrison dentro de la serie JLA (abreviatura de Justice League of America, conocida como la «Liga de la Justicia», y título de un libro de historietas publicado por DC entre 1997 y 2006), Superman describe al hombre murciélago como «el hombre más peligroso en la Tierra», que es capaz de vencer solo a un equipo de alienígenas superpoderosos para rescatar a sus compañeros aprisionados.[159]​ Además, es catalogado como un maestro del disfraz así como de la prestidigitación, debido a que en ciertas ocasiones suele reunir información confidencial bajo la identidad del gánster Matches Malone."
      },
      {
        "heading": "Tecnología",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Vestimenta",
        "level": 3,
        "content": "La vestimenta de Batman incorpora la imaginería de un murciélago para asustar a los criminales.[160]​ Sus rasgos cambian con frecuencia a través de los relatos y medios en los que el personaje es introducido, aunque los elementos más característicos permanecen constantes: una capa festoneada, una máscara que cubre la mayor parte del rostro y que tiene un par de orejas en forma de murciélago, el emblema de este animal estilizado sobre el pecho, y el cinturón multiusos. Típicamente, la combinación de colores del vestuario es azul con gris, la cual surgió debido a la manera en que se ilustra un libro de historietas.[160]​ Originalmente, Finger y Kane concibieron a Batman solamente con una capa, una máscara de color negro y un traje gris, sin embargo se optó, por cuestiones de convencionalismo en el uso de los colores, por implementar una combinación de azul con negro.[160]​ Esto ha sido considerado por Larry Ford, en su obra Place, Power, Situation, and Spectacle: A Geography of Film, como una reversión del simbolismo convencional en torno a los códigos de los colores, aspecto que asocia a los «chicos malos» con los colores oscuros.[161]​ Los guantes de Batman muestran tres festones que sobresalen de largos manguitos similares a guanteletes, aunque al principio eran guantes cortos y llanos sin los mencionados festones. Una elipse amarilla alrededor del logo de murciélago en el pecho del personaje se añadió en 1964, convirtiéndose desde entonces en su símbolo tradicional, equiparable al ideograma amarillo y rojo de la letra «S» usado por Superman.[162]​ La apariencia general del personaje, en especial la longitud de sus orejas y de la capa, varía enormemente dependiendo del artista involucrado; Dennis O'Neil comentó sobre ello: «Podemos decir que si Batman tiene doscientos trajes guardados en la Batcave, estos no tienen que lucir exactamente igual [...] Todos adoran dibujar a Batman, de la misma forma en que todos quieren ponerle su propio toque personal».[163]​"
      },
      {
        "heading": "Batmobile",
        "level": 3,
        "content": "El vehículo principal de Batman es el Batmobile, que generalmente se representa como un imponente automóvil negro, a menudo con aletas traseras que sugieren las alas de un murciélago. Batman también tiene un avión llamado Batplane (más tarde llamado \"Batwing\"), junto con varios otros medios de transporte. En la práctica adecuada, el prefijo \"murciélago\" (como en Batmóvil o batarang) rara vez lo usa el propio Batman cuando se refiere a su equipo, particularmente después de algunas representaciones (principalmente el programa de televisión de acción en vivo Batman de los años 60 y la serie animada Súper amigos). La práctica de las proporciones campy. Por ejemplo, el programa de televisión de la década de 1960 representaba un Batboat, Bat-Sub y Batcycle, entre otros vehículos con temática de murciélagos. La serie de televisión de los años sesenta, Batman tiene un arsenal que incluye nombres de \"murciélagos\" como la computadora del murciélago, el escáner del murciélago, el radar del murciélago, los puños del murciélago, los pontones de murciélago, el dispensador de agua potable para murciélagos, la cámara con murciélago polarizado-filtro, repelente de murciélagos, aerosol de murciélago y cuerda de murciélago. La historia \"Una muerte en la familia\" sugiere que, dada la naturaleza sombría de Batman, es poco probable que haya adoptado el prefijo \"murciélago\" por su cuenta. En The Dark Knight Returns, Batman le dice a Carrie Kelley que el Robin original se le ocurrió el nombre de \"Batmobile\" cuando era joven, ya que eso es lo que un niño llamaría el vehículo de Batman. toda su línea de historietas, al Batmóvil se le da una armadura más pesada y una nueva estética."
      },
      {
        "heading": "Equipamiento",
        "level": 3,
        "content": "Batman utiliza un vasto arsenal de gadgets para combatir el crimen, los cuales comparten la peculiaridad de poseer un diseño inspirado en un murciélago. El investigador Les Daniels acreditó a Gardner Fox como el responsable de haber conceptualizado el arsenal de Batman a partir de la incorporación del cinturón multiusos en Detective Comics n.º 29 (julio de 1939) así como el Batarang y el Batgyro (Detective Comics n.° 31 y 32, septiembre y octubre de 1939), estas últimas dos catalogadas como las primeras armas con diseño en forma de murciélago.[164]​ El vehículo principal del personaje es el Batmobile, el cual es descrito como un imponente automóvil negro con aletas dorsales que asemejan a las alas de un quiróptero. Otros medios de transporte conocidos son un jet, una lancha, un helicóptero y una motocicleta.\nEl prefijo «bat» (término que en español significa «murciélago») raramente es usado por Batman para referirse a su equipamiento; hasta la aparición de la serie televisiva de los años 1960 y a las producciones animadas de Hanna-Barbera como los Súper Amigos en donde se dio paso a un nuevo contexto «camp» (humor del absurdo). En tal contexto humorístico, fue recurrente que cada objeto utilizado por Batman llevara el prefijo bat, como por ejemplo la Bat-computer, el Bat-scanner, Bat-radar, la Bat-camera, el Bat-rope, entre otros.\nBatman conserva la mayor parte de sus gadgets en un cinturón multiusos. Con el paso del tiempo, se percibe que dicho cinturón contiene una variedad virtualmente ilimitada de herramientas útiles para la guerra contra la delincuencia en Gotham City. Algunas versiones distintas del cinturón poseen bolsas o cilindros unidos a su alrededor que albergan en su interior cada dispositivo. Lo que ha perdurado en todas las versiones es que este cinturón (de color amarillo) es el único objeto de color que viste el héroe."
      },
      {
        "heading": "Bati-señal",
        "level": 3,
        "content": "Cuando se necesita a Batman, la policía de Gotham City activa un reflector con una insignia en forma de murciélago sobre la lente llamada Bat-Signal, que brilla en el cielo nocturno, creando un símbolo de murciélago en una nube que se puede ver desde cualquier punto en Gotham; el origen de la señal varía, dependiendo de la continuidad y el medio.\nEn varias encarnaciones, especialmente en la serie de televisión Batman de la década de 1960, el comisionado Gordon también tiene una línea telefónica dedicada, llamada Bat-Phone, conectada a un teléfono rojo brillante (en la serie de televisión) que se encuentra en una base de madera y tiene una tapa transparente. La línea se conecta directamente a la residencia de Batman, Mansión Wayne, específicamente a un teléfono similar que se encuentra en el escritorio del estudio de Bruce Wayne y al teléfono de extensión en la Baticueva."
      },
      {
        "heading": "Batcave",
        "level": 3,
        "content": "El cuartel secreto de Batman es la Batcave, una serie de cuevas subterráneas ubicadas en la parte inferior de la mansión donde reside. Desde su cuartel, es capaz de vigilar la ciudad en cualquier momento, sirviéndole también para guardar sus vehículos y equipamiento. Además, funciona a manera de depósito para conservar sus recuerdos. Tanto en la historieta Batman: Shadow of the Bat (edición número 45) como en el filme de 2005, Batman Begins, se menciona que la batcueva forma parte del Ferrocarril subterráneo o ferrocarril de la libertad. Aun cuando algunos héroes y villanos han tenido la oportunidad de verla, casi ninguno de ellos sabe dónde se encuentra exactamente."
      },
      {
        "heading": "Personajes secundarios",
        "level": 1,
        "content": "Las interacciones de Batman con los personajes que le rodean, tanto héroes como villanos, ayudan a definir al personaje.[94]​"
      },
      {
        "heading": "Aliados de Batman",
        "level": 2,
        "content": "El comisionado James Gordon, es uno de los aliados más importantes de Batman por su papel dentro del departamento policiaco de Gotham, debutó junto con el encapuchado en Detective Comics n.° 27, y mantuvo una presencia consistente desde entonces. Su relación se basa en el respeto mutuo y un compromiso compartido con la justicia en Ciudad Gótica. En Batman: Año Uno, Gordon y Batman aprenden a confiar el uno en el otro, lo que transforma sus esfuerzos contra el crimen en una asociación más efectiva.[165]​\nA veces, Batman es visto como integrante de superequipos como la Liga de la Justicia y Los Marginales. Regularmente, se une con su compañero de la Liga de la Justicia, Superman, protagonizando con él las series World's Finest y Superman/Batman. En la continuidad previa a la historieta Crisis, ambos son referidos como amigos íntimos; no obstante, en la cronología contemporánea, mantienen una relación respetuosa y un tanto intranquila, a causa de sus diferentes concepciones del combate al crimen y de la justicia.\nOtros personajes secundarios en el universo de Batman son Barbara Gordon, la Batgirl original, hija del comisionado Gordon que debido a una herida de bala producida por El Joker, usa una silla de ruedas y sirve a la comunidad de superhéroes como la hacker Oracle. Azrael, un presunto asesino que reemplaza a Bruce Wayne como Batman por un tiempo entre 1992 y 1993 en el comic Batman: Sword of Azrael, creado por Dennis O'Neil y Joe Quesada. Cassandra Cain, la hija de un asesino de ascedencia asisatica que se convierte en la nueva Batgirl, actualmente es conocida como Black Bat en el equipo de Batman Incorporated. Helena Bertinelli, la Cazadora, única sobreviviente de una familia de delincuentes que se convierte en vigilante de Gotham y ha trabajado con Batman en más de una ocasión. Stephanie Brown, hija del criminal Cluemaster que operó como Spoiler, temporalmente como Robin y actualmente como la más nueva Batgirl. Ace the Batisabueso, el compañero canino de Batman que debutó en Batman 92 (1955).[166]​ Bat-Mite, una criatura extradimensional que idolatra a Batman que apareció por primera vez en el comic Detective Comics #267 (1959).[166]​"
      },
      {
        "heading": "Enemigos de Batman",
        "level": 2,
        "content": "Batman se enfrenta a una variedad de enemigos que van desde criminales comunes hasta supervillanos estrambóticos. Al principio, Batman luchaba principalmente contra gánsteres, pero los supervillanos disfrazados aparecieron rápidamente, formando una de las galerías de enemigos recurrentes más memorables en la historia de los cómics de superhéroes. De acuerdo con el tono del superheroe, los adversarios destacados generalmente carecen de superpoderes, al igual que el justiciero enmascarado. Sin embargo, no son menos peligrosos debido a la violencia y la psicopatía que los caracterizan.[167]​\nMuchos de estos presentan aspectos contrarios a la personalidad y comportamientos del héroe, teniendo en común una serie de historias trágicas que los condujeron a la vida delictiva.[165]​ El «enemigo más implacable» de Batman es El Joker, un criminal sociópata con apariencia física de payaso y que, como «personificación de lo irracional», representa «todo lo opuesto a Batman». \nDos Caras muestra físicamente sus dos personalidades como dos caras de una misma moneda que también son Batman y Bruce Wayne. Ra's al Ghul, en la película de 2005 Batman Begins, está tan comprometido con el bien que está dispuesto a eliminar a todos los criminales, mientras que Batman solo piensa en detenerlos.[168]​  Además de estos coloridos personajes, Batman también tiene que lidiar con políticos corruptos.[168]​\nOtros antagonistas recurrentes son: Catwoman, Hiedra Venenosa, El Pingüino, Bane, El Espantapájaros, Harley Quinn, Riddler, Mr. Freeze, Clayface, Mascara Negra, Victor Zsasz, Killer Croc, Deadshot, Luciérnaga entre muchos otros más. Muchos de los enemigos de Batman suelen ser pacientes psiquiátricos en el Asilo Arkham.\nEl personaje ha incursionado con éxito en diversos géneros, entre los que se encuentran la ciencia ficción y el terror sobrenatural, un ejemplo de esto último es la novela gráfica de 2005 Batman: condado de Gotham, de Steve Niles y Scott Hampton, versatilidad de la que no disfrutan la gran mayoría de los superhéroes; de este modo ha debido enfrentar a conocidos xenomorfos consagrados por el cine y la literatura, como son el Depredador, Alien, y al mismísimo Conde Drácula, con este último, en un memorable encuentro llamado Batman & Drácula: lluvia roja, una novela gráfica de 1991 de Doug Moench y Kelley Jones."
      },
      {
        "heading": "Intereses románticos",
        "level": 2,
        "content": "Desde su creación, Batman ha tenido relaciones románticas con regularidad. Sus noviazgos suelen ser breves, ya que Batman, como Bruce Wayne, no se presenta como un personaje capaz de comprometerse seriamente. De hecho, las mujeres parecen más bien una posible distracción que impide al justiciero cumplir su misión. Batman, solo se preocupa por lo que puede ayudarle en su misión, y esto no puede incluir la presencia femenina ya que le podría distraer de su objetivo. Su primer interés amoroso fue Julie Madison, presentada en Detective Comics #31 (1939). Aunque estaba comprometida con Bruce Wayne, ella se fue debido a su personalidad distante y playboy, lo que es una fachada de la  la vida dual de Bruce Wayne.[169]​\nSelina Kyle, también conocida como Catwoman o Gatúbela, es quizás la figura más notable en la historia romántica de Batman. Debutando en Batman #1 (1940), su relación se caracteriza por una mezcla de romance y rivalidad. A lo largo de los años, han compartido conexiones intensas, a menudo navegando por la delgada línea entre el amor y el conflicto. Su relación culminó en un compromiso durante la era del Renacimiento.[169]​Engendraría una hija con Catwoman llamada Helena en la realidad paralela de Tierra-2 que apareció por primera vez en All Star Comics Nº69 de diciembre de 1977 y en DC Super-Stars Nº17 bajo su alter ego Cazadora.\nTalia al Ghul, presentada en Detective Comics #411 (1971), es otro personaje clave en la vida amorosa de Batman, su relación está plagada de conflictos debido a su padre, Ra's al Ghul , y sus ambiciones criminales. A pesar de los desafíos, su historia de amor resultó en el nacimiento de Damian Wayne, quien se convertiría en el último Robin.\nOtras figuras románticas en la vida de Batman son: Vicki Vale, una periodista que aparece en Batman #49 (1948), los intentos de Vicki por descubrir la verdadera identidad de Batman conducen a una complicada relación romántica que va y viene con el paso de los años, especialmente a principios de los 80, cuando su relación se vuelve más seria.[169]​Silver St. Cloud, una miembro de la alta sociedad de Gotham City que organiza fiestas para los multimillonarios, las apariciones iniciales de Silver también representan la primera vez que se reconoce explícitamente que Bruce Wayne o incluso Batman han tenido una relación sexual. Batwoman (Kathy Kane) una rica heredera que se inspira en Batman para convertirse en superheroina, durante la Edad de Plata se sintió atraída romanticamete por Batman, aunque actualmente se identifica como lesbiana. Además, la relación de Batman con Wonder Woman ha sido explorada en varias historias, incluido un beso apasionado en JLA (2003) durante un momento de crisis, sin embargo, su relación permanece en gran parte inexplorada, a menudo eclipsada por sus respectivos compromisos. \nUna de las parejas románticas más controvertidas surgió de la adaptación animada de Batman: The Killing Joke (2016), que mostraba un breveenamoramiento de Batgil no correspondido por Batman que se vuelve físico después de que una Batgirl frustrada inicia un altercado menor entre los dos. También fue confirmado por la propia Barbara en Batman Beyond que los dos habían salido brevemente algún tiempo después de que Dick Grayson dejó Gotham después de The New Batman Adventures, su atracción por él es evidente en la película Mystery of the Batwoman, con la que Bruce se siente incómodo y Alfred y Tim se burlan de él al respecto, pero la relación terminó mal y Barbara comenzó a guardar rencor hacia él en el futuro debido a esto, más tarde se revela en los cómics de Batman Beyond 2.0 que Barbara estaba embarazada de un hijo de Bruce, hasta que tuvo un aborto espontáneo después de un accidente relacionado con la lucha contra el crimen y cuando Dick descubrió la verdad de Bruce, empeoró su relación ya tensa con este último y cortó lazos con ella con Batman.\nTambién se ha relacionado románticamente entre otras mujeres con figuras como: la enfermera Linda Page, su guardaspaldas Sasha Bordeaux, la hija de Alfred Julia Pennyworth, la conductora de radio Vesper Fairchild, la supermodelo Jezebel Jet, Lois Lane en la continuidad previa a Flashpoint, las superheroinas Black Canary, Zatanna Zatra, Cheetah y Bekka, y las supervillanas Hiedra Venenosa, Nocturna, Midnight, White Rabbit, Penumbra, Madolyn Corbett entre otros muchos más."
      },
      {
        "heading": "Impacto cultural",
        "level": 1,
        "content": "Batman se ha convertido en un icono de la cultura popular, siendo reconocido en todo el mundo. La presencia del personaje se ha extendido más allá de sus orígenes en los cómics; sucesos como la «Batmanía» producto de la serie televisiva de 1966 masificó una imagen infantil y paródica respecto a los superhéroes y en específico hacia Batman. Luego, una segunda «Batmanía» ante el lanzamiento de la película Batman en 1989, y posteriormente una tercera gracias a la viral campaña publicitaria de la cinta The Dark Knight «han llevado a Batman hasta el primer plano de la conciencia pública».[77]​ En un artículo hecho para conmemorar el sexagésimo aniversario de la creación del personaje, el periódico británico The Guardian escribió: «Batman es una figura empañada por la interminable reinvención que conlleva la moderna cultura de masas. Es al mismo tiempo un icono y un artículo de consumo: el artefacto cultural perfecto para el siglo XXI».[170]​ Además, los medios de comunicación han utilizado a Batman en encuestas generales y de trivialidades — la revista Forbes estimó a Bruce Wayne como el noveno personaje ficticio más adinerado con su fortuna de 5,8 billones USD, siendo superado por Iron Man que ocupa el puesto número seis.[171]​ Asimismo, BusinessWeek consideró al personaje como uno de los diez superhéroes más inteligentes en la industria del cómic estadounidense.[172]​ En septiembre de 2024, Batman recibió una estrella en el paseo de la fama de Hollywood, siendo el primer superhéroe en obtener dicho reconocimiento.[173]​"
      },
      {
        "heading": "Apariciones alternativas",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Televisión",
        "level": 2,
        "content": ""
      },
      {
        "heading": "Acción en vivo",
        "level": 3,
        "content": "El personaje de Batman ha aparecido también en varios medios alternativos a las historietas. Incluso ha sido desarrollado con el fin de dar lugar a tiras de prensa de periódicos sindicados, libros, radio dramas, programas de televisión y varias adaptaciones cinematográficas. Su primera aparición alternativa fue en una tira de prensa publicada en un diario el 25 de octubre de 1943.[174]​ Ese mismo año, se adaptó un serial conformado de quince capítulos, Batman, con Lewis Wilson como el primer actor en interpretar a Batman en la pantalla. Aunque nunca se le ha dedicado una serie de radio, en 1945 el personaje realizó algunas apariciones como invitado en The Adventures of Superman, específicamente en las secuencias donde el actor de voz Bud Collyer requería tiempo libre.[175]​ Un segundo serial de cine, Batman and Robin, se estrenó en 1949, con el actor Robert Lowery tomando el papel de Batman. Todas las anteriores apariciones durante los años 1940, «ayudaron a hacer [de Batman] un nombre común para millones de personas que jamás habían adquirido una historieta».[175]​\nEn 1964, se publicó la colección de cuentos cortos de Donald Barthelme «Come Back, Dr. Caligari», donde el autor redactó el cuento \"The Joker's Greatest Triumph\". En el relato, Batman aparece como un millonario francés pretencioso a manera de parodia.[176]​\nLa serie de televisión Batman, protagonizada por Adam West, se estrenó en enero de 1966 como parte de la programación de la cadena estadounidense ABC. Creado con un sentido del humor «ostentoso», el programa se convirtió en un fenómeno popular. En su autobiografía Back to the Batcave, West revela su disgusto por el uso del término camp para calificar la serie, pues en su opinión era una farsa o sátira, siendo deliberada en ese aspecto. Finalmente, el show duró 120 episodios, concluyendo en 1968. Entre la primera y segunda temporada del programa, el mismo elenco participó en la película Batman, estrenada en 1966. Por otra parte, la popularidad de la serie televisiva resultó en la primera adaptación animada de Batman en la serie The Batman/Superman Hour;[177]​ los segmentos en los que aparece el hombre murciélago se reestrenaron comercialmente bajo el nombre Batman with Robin the Boy Wonder, el mismo que llegó a producir un total de 33 episodios entre 1968 y 1977."
      },
      {
        "heading": "Animación",
        "level": 3,
        "content": "De 1973 a 1986, Batman tuvo un papel estelar en la serie de ABC Súper Amigos, animada por los estudios Hanna-Barbera. En dichas series, el actor Olan Soule prestó su voz para el personaje, aunque habría de ser reemplazado después por Adam West en Súper Amigos; West también contribuyó en el mismo papel para la serie Las nuevas aventuras de Batman, producida por Filmation y transmitida en 1977.\nEn 1992, se estrenó el programa de televisión Batman: La Serie Animada, producida por Warner Bros. y transmitida por la cadena FOX; Les Daniels describió que la serie «[se está] aproximando a lo que cualquier declaración artística debe expresar para definir el aspecto de Batman durante los años 1990».[178]​ El éxito de Batman: La Serie Animada condujo a un spin-off cinematográfico titulado Batman: la máscara del fantasma (1993), así como a la creación de otras series situadas en la misma continuidad de la franquicia animada, entre las cuales sobresalieron Las nuevas aventuras de Batman, Batman del futuro y Liga de la Justicia. En cada una de estas producciones participó Kevin Conroy prestando su voz para el papel estelar. En 2004, se lanzó la serie animada The Batman, con Rino Romano como Batman, la cual fue reemplazada en 2008 por la nueva Batman: The Brave and the Bold, con Diedrich Bader como el héroe encapuchado. En 2013 se estrenó la serie Beware the Batman. En 2008, ocurrió el lanzamiento animado a manera de antología de la película Batman: Gotham Knight. Hizo apariciones en las series animadas de Static Shock, Young Justice y Justice League Action. Aparece en nuevas series como DC Super Hero Girls, Scooby-Doo y ¿quién crees tú? y Harley Quinn."
      },
      {
        "heading": "Cine",
        "level": 2,
        "content": "En 1989, Batman regresó a la pantalla grande con la película Batman del director Tim Burton, y fue personificado por el actor Michael Keaton; la cinta se convirtió en un éxito financiero, llegando a ser la película con mayores recaudaciones de ese año y, en aquel momento, la quinta más recaudadora de la historia del cine.[179]​ La buena recepción que tuvo la película dio lugar a tres secuelas: Batman Returns (1992), Batman Forever (1995) y Batman y Robin (1997), estas últimas dos dirigidas por Joel Schumacher y protagonizadas por Val Kilmer y George Clooney, respectivamente.\nEn 2005, Christopher Nolan dirigió la cinta Batman Begins, cuya trama significó un reinicio en la franquicia fílmica del personaje, y fue protagonizada por Christian Bale. Su continuación, The Dark Knight (2008), mantuvo el récord de la película con mayores recaudaciones en su primer fin de semana de exhibición en territorio estadounidense, llegando a obtener aproximadamente 158 millones USD,[180]​ y se convirtió en la película que más rápido ha logrado recaudar 400 millones USD en toda la historia de la industria fílmica estadounidense (logrando sobrepasar la cifra señalada en 18 días).[181]​ A partir de los anteriores récords, The Dark Knight pasó a ser la segunda película con mayores recaudaciones de todos los tiempos (533 millones USD) siendo solamente superada por Titanic.[182]​ La secuela de The Dark Knight, la última de la franquicia dirigida por Nolan, The Dark Knight Rises, se estrenó a mediados de 2012.\nTambién aparece como protagonista en The Lego Movie (2014) y The Lego Batman Movie (2017); en la serie de televisión Gotham, que está ambientada en la adolescencia de Bruce Wayne[183]​ interpretado por David Mazouz.[184]​ Como parte del universo extendido de DC Comics, Ben Affleck ha interpretado al personaje en la película Batman v Superman: Dawn of Justice (2016), Escuadrón Suicida (película) (2016), Liga de la Justicia (2017), Liga de la Justicia de Zack Snyder (2021) y volverá en The Flash (2023). Robert Pattinson ha sido el último en interpretarlo en la película The Batman."
      },
      {
        "heading": "Videojuegos",
        "level": 2,
        "content": "En 1986, apareció el videojuego de acción-aventura Batman con tecnología isométrica 3D, siendo distribuido por Ocean Software para los ordenadores Amstrad CPC, Amstrad PCW, Sinclair ZX Spectrum y MSX.[185]​[186]​ El objetivo del juego es rescatar a Robin mediante la recolección de las siete partes del aerodeslizador de Batman que se hallan distribuidas en la Batcueva. Tras su lanzamiento, obtuvo buenas evaluaciones por parte de la crítica especializada.[187]​[188]​ Tres años después, en 1989, se lanzó el juego de plataformas Batman inspirado en la película de Tim Burton. Su desarrollo corrió a cargo de Sunsoft, siendo finalmente distribuido para la consola Nintendo NES. Cabe señalarse que contaba con cinco niveles, concluyendo con el enfrentamiento entre Batman y The Joker en el campanario de la catedral de Gotham City. Al año siguiente, el hombre murciélago apareció en la adaptación homónima de Sunsoft para la consola Sega Mega Drive.\nEn 1990, Atari Games distribuyó el juego arcade tipo brawler Batman, en donde Batman debe enfrentarse a The Joker para salvar a Gotham. Al igual que sus predecesores, la trama se basó en la película de 1989. Batman Returns, perteneciente al sistema de plataformas, apareció a principios de 1993, estando basado en la película del mismo nombre dirigida por Tim Burton. Al año siguiente, se publicó el juego The Adventures of Batman & Robin, basado en la serie animada y distribuido para la consola Super Nintendo. En 1995, se lanzó el título Batman Forever, basado en la trama de la película homónima y en 1996 apareció la versión arcade (Batman Forever: The Arcade Game).\nRespecto a los años 2000, en 2005 se distribuyó el juego virtual Batman Begins inspirado en la trama de la cinta homónima dirigida Christopher Nolan. Asimismo, en 2008 debutó Lego Batman: The Video Game en las consolas Xbox 360, Playstation 3, PSP, Nintendo DS y Wii. Hasta julio de 2009, se habían vendido un total de 4,1 millones de copias de Lego Batman: The Video Game a nivel mundial.[189]​ Finalmente, en 2009 se comercializó el videojuego Batman: Arkham Asylum para los sistemas PlayStation 3, Xbox 360 y Microsoft Windows. Sin embargo, a diferencia de los demás juegos, Arkham Asylum se inspira en los cómics originales donde apareció, por vez primera, el héroe encapuchado. Este último ha sido aclamado por la prensa especializada, manteniendo un Récord Mundial Guiness como el 'videojuego de superhéroes más críticamente aclamado de todos los tiempos', y rompiendo la marca previamente establecida en esta categoría (lograda por el título Marvel vs. Capcom 2: New Age of Heroes) al alcanzar el puntaje promedio más alto de críticas positivas en todo el mundo (91,6).[190]​\nOtros juegos en los que ha aparecido Batman a partir de los años 2010 son Batman: Arkham City, Injustice: Dioses entre nosotros, Batman: Arkham Origins y Batman: Arkham Knight.\nEn 2019, el personaje de Batman hizo una colaboración en conjunto con el juego Fortnite: Battle Royale durante la temporada 10 o X del Capítulo 1, todo esto en conmemoración a su 80° aniversario desde su primera publicación de cómics.[191]​"
      },
      {
        "heading": "Cosplays y campaña solidaria",
        "level": 2,
        "content": "Batman al igual que algunos de los personajes secundarios de su tira como The Jocker o Harley Quinn son temáticas de las más buscadas por los cosplay en su participación dentro de los comic cons.[192]​\nA partir de 2013 y durante 10 años, Maximiliano Altavista, un ciudadano argentino de la ciudad de La Plata, llevó a cabo una campaña solidaria en su rol de cosplay de Batman donde colaboró repetidamente en donaciones y trabajo voluntario de mantenimiento en diversos hospitales de la Provincia de Buenos Aires, incluso impulsando la creación de un pequeño patio recreativo dentro de uno de los recintos, con la temática del personaje. Altavista comenzó con su desinteresada labor visitando a pacientes internados en hospitales y mantuvo su participación de manera desinteresada y sin fines de lucro durante 10 años hasta que el estrés psicofísico que conllevaba esa labor le impidió seguir llevando su campaña solidaria, aunque antes de su retiro se encargó de montar una asociación civil para que otros continúen con su legado. En sus apariciones como el hombre murciélago, el Batman platense mantenía una identidad reservada y solía acercarse hacia los establecimientos donde participaba como cosplay, en un automóvil modificado para que luciera como un Batmobil.[193]​[194]​"
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": "Anexo:Películas de Batman\nAsilo Arkham\nBatmania\nMasacre de Aurora de 2012\nBatman: City Of Scars\nBatman: The Last Arkham"
      },
      {
        "heading": "Notas",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Bibliografía",
        "level": 2,
        "content": "Beatty, Scott (2005). The Batman Handbook: The Ultimate Training Manual (en inglés). Quirk Books. ISBN 1-59474-023-2. \nBoichel, Bill (1991). Batman: Commodity as Myth (en inglés). Londres: Billboard Books/Watson-Guptill Publications. ISBN 0-85170-276-7. \nDaniels, Les (1999). Batman: The Complete History (en inglés). Chronicle Books. ISBN 0-8118-4232-0. \nDaniels, Les (1995). DC Comics: Sixty Years of the World's Favorite Comic Book Heroes (en inglés). Bulfinch. ISBN 0-8212-2076-4. \nGreenspan, Ezra; Rose, Jonathan (2000). Book History, Volumen 3 (en inglés). Penn State Press. ISBN 9780271020501. \nJones, Gerard (1995). Men of Tomorrow: Geeks, Gangsters, and the Birth of the Comic Book (en inglés). Basic Books. ISBN 0-465-03657-0. \nKane, Bob; Andrae, Tom (1989). Batman & Me: An Autobiography (en inglés). Eclipse Books. ISBN 1-56060-017-9. \nPearson, Roberta; Uricchio, William (1991). The Many Lives of the Batman: Critical Approaches to a Superhero and His Media (en inglés). Londres: Routledge. ISBN 0-85170-276-7. \nReinhart, Mark (2004). The Batman Filmography, 2d ed. (en inglés). McFarland. ISBN 9780786468911. \nWright, Bradford W. (2001). Comic Book Nation: The Transformation of Youth Culture in America (en inglés). Johns Hopkins. ISBN 0-8018-7450-5. \nYork, Christopher (2000). All in the Family: Homophobia and Batman Comics in the 1950s (en inglés). The International Journal of Comic Art."
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": " Wikiquote en inglés alberga frases célebres de y sobre Batman.\n\nPágina web oficial  (en inglés)"
      }
    ],
    "summary": "Batman[a]​ «en español: Hombre Murciélago»[15]​ es un superhéroe ficticio de cómic creado por los estadounidenses Bob Kane y Bill Finger,[16]​ y propiedad de DC Comics. Apareció por primera vez en la historia titulada «El caso del sindicato químico» de la revista Detective Comics N.º 27, lanzada por la editorial National Publications el 30 de marzo de 1939.\nLa identidad secreta de Batman es Bruce Wayne (Bruno Díaz en algunos países de habla hispana),[17]​[18]​[19]​ un multimillonario magnate empresarial y filántropo dueño de Empresas Wayne en Gotham City. Después de presenciar el asesinato de sus padres, el Dr. Thomas Wayne y Martha Wayne en un violento y fallido asalto cuando era niño, juró venganza contra los criminales, un juramento moderado por el sentido de la justicia. Bruce Wayne se entrena física e intelectualmente y crea un traje inspirado en los murciélagos para combatir el crimen, con sus gadgets de combate del batcinturón y sus vehículos.[20]​\nA diferencia de los superhéroes, no tiene superpoderes: recurre a su intelecto, así como a aplicaciones científicas y tecnológicas para crear armas y herramientas con las cuales lleva a cabo sus actividades. Vive en la mansión Wayne, en cuyos subterráneos se encuentra la Batcueva, el centro de operaciones de Batman. Recibe la ayuda constante de otros aliados, entre los cuales pueden mencionarse Robin, Batgirl (posteriormente Oráculo), Nightwing, el comisionado de la policía local, James Gordon, y su mayordomo Alfred Pennyworth. Una gran variedad de villanos conforman la galería de Batman, incluido su archienemigo, el Joker.\nSe trata del personaje más emblemático de DC Comics, junto con Superman. Dada su buena aceptación, obtuvo su propia revista en 1940. Tres años después, Columbia Pictures estrenó la primera adaptación para la televisión del personaje, a la cual le siguió la serie Batman y Robin, en 1949. A mediados de la década de 1960, se lanzó otra serie titulada Batman por Adam West, que utilizó un concepto más «camp» que terminó apartándolo de su tono sombrío con el que originalmente fue concebido. Más adelante, los escritores Dennis O'Neil, Neal Adams y Frank Miller produjeron nuevo material escrito sobre el universo de Batman entre los años 1970 y 1980, retomando el diseño y elementos originales de la franquicia. Se considera que la película homónima de Tim Burton, estrenada en 1989 y la secuela de 1992, tuvo un papel importante en la popularidad contemporánea del hombre murciélago interpretado por Michael Keaton y volverá también en The Flash (2023). También fue interpretado por Val Kilmer en Batman Forever (1995) y George Clooney en Batman y Robin (1997), al igual que la serie de filmes iniciada de Trilogía The Dark Knight (2005–2012), dirigidas por Christopher Nolan e interpretado por Christian Bale.[21]​ Además de las anteriores producciones, existen varias otras más en las que el personaje y sus elementos han sido incorporados. El actor Ben Affleck lo interpreta en las películas del Universo extendido de DC Batman v Superman: Dawn of Justice (2016), Escuadrón Suicida (2016), Liga de la Justicia (2017), la versión de Liga de la Justicia de Zack Snyder (2021) y The Flash (2023). En 2020 se confirmó que para la película: The Batman, el actor que interpretaría el papel del murciélago sería Robert Pattinson. Esta película que originalmente se estrenaría en 2021 terminó siendo atrasada hasta 2022."
  },
  {
    "title": "Dachshund",
    "source": "https://en.wikipedia.org/wiki/Dachshund",
    "language": "en",
    "chunks": [
      {
        "heading": "Etymology",
        "level": 1,
        "content": "The name dachshund is of German origin, and means 'badger dog', from Dachs ('badger') and Hund ('dog, hound'). The German word Dachshund is pronounced [ˈdaks.hʊnt] . The pronunciation varies in English: variations of the first and second syllables include ,  and , , . The first syllable may be incorrectly pronounced as  by some English speakers. Although Dachshund is a German word, in modern Germany, the dogs are more commonly known by the short name Dackel. Working dogs are less commonly known as Teckel.\nBecause of their long, narrow build, they are often nicknamed wiener or sausage dog."
      },
      {
        "heading": "Classification",
        "level": 1,
        "content": "While classified in the hound group or scent hound group in the United States and Great Britain, the breed has its own group in the countries which belong to the Fédération Cynologique Internationale (World Canine Federation). Many dachshunds, especially the wire-haired subtype, may exhibit behavior and appearance similar to the terrier group of dogs. An argument can be made for the scent (or hound) group classification because the breed was developed to use scent to trail and hunt animals, and probably descended from the Saint Hubert Hound like many modern scent hound breeds such as bloodhounds and Basset Hounds; but with the persistent personality and love for digging that probably developed from the terrier, it can also be argued that they could belong in the terrier, or \"earth dog\", group."
      },
      {
        "heading": "Characteristics",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Appearance",
        "level": 2,
        "content": "A typical dachshund is long-bodied and muscular with short stubby legs. Its front paws are disproportionately large, being paddle-shaped and particularly suitable for digging. Its skin is loose enough not to tear while tunneling in tight burrows to chase prey. Its snout is long."
      },
      {
        "heading": "Coat and color",
        "level": 3,
        "content": "There are three dachshund coat varieties: smooth coat (short hair), long-haired, and wire-haired. Longhaired dachshunds have a silky coat and short featherings on legs and ears. Wire-haired dachshunds are the least common coat variety in the United States (although it is the most common in Germany) and the most recent coat to appear in breeding standards. While short-haired dachshunds have two types of coats, silky and smooth. Silky short hairs have a very shiny, glossy, and soft to the touch coat, while smooth short hairs have more of a coarse and prickly coat. \nDachshunds have a wide variety of colors and patterns, the most common one being red. Their base coloration can be single-colored (either red, cream, or brown), tan pointed (black and tan, chocolate and tan, blue and tan, or isabella and tan), and in wire-haired dogs, a color referred to as wild boar. Patterns such as dapple (merle), sable, brindle and piebald also can occur on any of the base colors. Dachshunds in the same litter may be born in different coat colors depending on the genetic makeup of the parents.\nThe Dachshund Club of America (DCA) and the American Kennel Club (AKC) consider Double Dapple to be out of standard and a disqualifying color in the show ring. Piebald is now a recognized color in the Dachshund Club of America (DCA) breed standard.\nDogs that are double-dappled have the merle pattern of a dapple, but with distinct white patches that occur when the dapple gene expresses itself twice in the same area of the coat. The DCA excluded the wording \"double-dapple\" from the standard in 2007 and now strictly uses the wording \"dapple\" as the double dapple gene is commonly responsible for blindness and deafness."
      },
      {
        "heading": "Size",
        "level": 3,
        "content": "Dachshunds come in three sizes: standard, miniature, and kaninchen (German for \"rabbit\"). Although the standard and miniature sizes are recognized almost universally, the rabbit size is not recognized by clubs in the United States and the United Kingdom. The rabbit size is recognized by the Fédération Cynologique Internationale (World Canine Federation) (FCI), which contain kennel clubs from 83 countries all over the world. An increasingly common size for family pets falls between the miniature and the standard size; these are frequently referred to as \"tweenies,\" which is not an official classification.\nA full-grown standard dachshund typically weighs 7.5 to 14.5 kg (16 to 32 lb), while the miniature variety normally weighs less than 5.5 kg (12 lb). The kaninchen weighs 3.5 to 5 kg (8 to 11 lb). According to kennel club standards, the miniature (and kaninchen, where recognized) differs from the full-size only by size and weight, thus offspring from miniature parents must never weigh more than the miniature standard to be considered a miniature as well. While many kennel club size divisions use weight for classification, such as the American Kennel Club, other kennel club standards determine the difference between the miniature and standard by chest circumference; some kennel clubs, such as in Germany, even measure chest circumference in addition to height and weight."
      },
      {
        "heading": "Eye color",
        "level": 3,
        "content": "Light-colored dachshunds can sport amber, light brown, or green eyes; however, kennel club standards state that the darker the eye color, the better. Dapple and double dapple dachshunds can have multi-coloured \"wall\" eyes with fully blue, partially blue or patched irises owing to the effect of the dapple gene on eye pigmentation expression.  \"Wall\" eye is permissible according to DCA standards but undesirable by AKC standards."
      },
      {
        "heading": "Temperament",
        "level": 2,
        "content": "Dachshunds can be stubborn and refuse commands, especially if chasing a small animal which they have a propensity for. As dachshunds were originally used as badger hunters they have a keen sense for chasing smaller animals. Dachshunds are often stubborn, making them a challenge to train.\n\nBeing the owner of dachshunds, to me a book on dog discipline becomes a volume of inspired humor. Every sentence is a riot. Some day, if I ever get a chance, I shall write a book, or warning, on the character and temperament of the dachshund and why he can't be trained and shouldn't be. I would rather train a striped zebra to balance an Indian club than induce a dachshund to heed my slightest command. When I address Fred I never have to raise either my voice or my hopes. He even disobeys me when I instruct him in something he wants to do.\nDachshunds can be aggressive to strangers and other dogs. Despite this, they are rated in the intelligence of dogs as an average working dog with a persistent ability to follow trained commands 50% of the time or more.\nThey can have a loud bark. Some bark quite a lot and may need training to stop, while others will not bark much at all. Dachshunds can be standoffish toward strangers. A Japanese study found the Miniature Dachshund to have higher rates of refusing to move whilst on a walk, barking at outside noises whilst inside, barking at strangers visiting their home, separation anxiety, inappropriate elimination (faecal and urinary incontinence), hesitancy to approach unknown humans and canines, and aggression towards family members, highlighting the breed's stubbornness and aggression.\n\nAccording to the American Kennel Club's breed standards, \"the dachshund is clever, lively and courageous to the point of rashness, persevering in above and below ground work, with all the senses well-developed. Any display of shyness is a serious fault.\" Their temperament and body language give the impression that they do not know or care about their relatively small size. Like many small hunting dogs, they will challenge a larger dog. Indulged dachshunds may become snappy or extremely obstinate.\nA 2008 University of Pennsylvania study of 6,000 dog owners who were interviewed indicated that dogs of smaller breeds were more likely to be \"genetically predisposed toward aggressive behaviour\". Dachshunds were rated the most aggressive, with 20% having bitten strangers, as well as high rates of attacks on other dogs and their owners. The study noted that attacks by small dogs were unlikely to cause serious injuries and because of this were probably under-reported."
      },
      {
        "heading": "Health",
        "level": 1,
        "content": "The breed is prone to spinal problems, especially intervertebral disk disease (IVDD), due in part to an extremely long spinal column and short rib cage. The risk of injury may be worsened by obesity, jumping, rough handling, or intense exercise, which place greater strain on the vertebrae. About 20–25% of dachshunds will develop IVDD. Dachshunds with a number of calcified intervertebral discs at a young age have a higher risk of developing disc disease in later life. In addition, studies have shown that development of calcified discs is highly heritable in the breed. An appropriate screening programme for IVDD has been identified by Finnish researchers and a UK IVDD screening programme has been developed for breeders with the aim to reduce prevalence of spinal problems.\nTreatment consists of combinations of crate confinement and courses of anti-inflammatory medications (steroids and non-steroidal anti-inflammatory drugs like carprofen and meloxicam), or chronic pain medications, like tramadol. Serious cases may require surgery to remove the troublesome disk contents. A dog may need the aid of a cart to get around if paralysis occurs.\nA minimally invasive procedure called \"percutaneous laser disk ablation\" has been developed at the Oklahoma State University Veterinary Hospital. Originally, the procedure was used in clinical trials only on dachshunds that had suffered previous back incidents. Since dachshunds are prone to back issues, the goal is to expand this treatment to dogs in a normal population.\nIn addition to back problems, the breed is prone to patellar luxation where the kneecap can become dislodged. Dachshunds may also be affected by osteogenesis imperfecta (brittle bone disease). The condition seems to be mainly limited to wire-haired Dachshunds, with 17% being carriers. A genetic test is available to allow breeders to avoid breeding carriers to carriers. In such pairings, each puppy will have a 25% chance of being affected.\nIn some double dapples, there are varying degrees of vision and hearing loss, including reduced or absent eyes. Not all double dapples have problems with their eyes and/or ears, which may include degrees of hearing loss, full deafness, malformed ears, congenital eye defects, reduced or absent eyes, partial or full blindness, or varying degrees of both vision and hearing problems; but heightened problems can occur owing to the genetic process in which two dapple genes cross, particularly in certain breeding lines. Dapple genes, which are dominant genes, are considered \"dilution\" genes, meaning whatever color the dog would have originally carried is lightened, or diluted, randomly; two dominant \"dilution\" genes can cancel each other out, or \"cross\", removing all color and producing a white recessive gene, essentially a white mutation. When occurring genetically within the eyes or ears, this white mutation can be detrimental to development, causing hearing or vision problems.\nOther dachshund health problems include hereditary epilepsy, granulomatous meningoencephalitis, dental issues, Cushing's syndrome, thyroid and autoimmune problems, various allergies and atopies, and various eye conditions including cataracts, glaucoma, progressive retinal atrophy, corneal ulcers, nonulcerative corneal disease, sudden acquired retinal degeneration, and cherry eye. Dachshunds are also 2.5 times more likely than other breeds of dogs to develop patent ductus arteriosus, a congenital heart defect. Dilute color dogs (Blue, Isabella, and Cream) are very susceptible to color dilution alopecia, a skin disorder that can result in hair loss and extreme sensitivity to sun. Since the occurrence and severity of these health problems is largely hereditary, breeders are working to eliminate these.\nFactors influencing the litter size of puppies and the proportion of stillborn puppies per litter were analyzed in normally sized German dachshunds. The records analyzed contained data on 42,855 litters. It was found that as the inbreeding coefficient increased, litter size decreased and the percentage of stillborn puppies increased, thus indicating inbreeding depression. It was also found that young and older dams had smaller litter sizes and more stillborn puppies than middle-aged dams.\nA study in Japan found the Miniature Dachshund to have lower rates of glaucoma than other breeds. With 2.4% of cases belonging to the breed but the breed making up 10.1% of visits to the veterinary hospital.\nDachshunds are predisposed to hypothyroidism."
      },
      {
        "heading": "Life expectancy",
        "level": 2,
        "content": "A 2018 study in Japan of pet cemetery data put the Miniature Dachshund life expectancy at 13.9 years compared to 13.7 overall and 15.1 for crossbreeds. A 2024 study in the UK found a life expectancy of 13.2 years for the breed compared to an average of 12.7 for purebreeds and 12 for crossbreeds with the Miniature Dachshund found to have a life expectancy of 14 years. A 2024 Italian study found a life expectancy of 11.5 years for the breed compared to 10 years overall. A 2005 Swedish study of insurance data found 28% of Dachshund died by the age of 10, less than the overall rate of 35% of dogs dying by the age of 10."
      },
      {
        "heading": "History",
        "level": 1,
        "content": "The dachshund is a creation of German breeders and includes elements of German, French, and English hounds and terriers. Dachshunds have been kept by royal courts all over Europe, including that of Queen Victoria, who was particularly enamored of the breed.\nThe first verifiable references to the dachshund, originally named the \"Dachs Kriecher\" (\"badger crawler\") or \"Dachs Krieger\" (\"badger warrior\"), came from books written in the early 18th century. Prior to that, there exist references to \"badger dogs\" and \"hole dogs\", but these likely refer to purposes rather than to specific breeds. The original German dachshunds were larger than the modern full-size variety, weighing between 14 and 18 kg (31 and 40 lb), and originally came in straight-legged and crook-legged varieties (the modern dachshund is descended from the latter). Though the breed is famous for its use in exterminating badgers and badger-baiting, dachshunds were also commonly used for rabbit and fox hunting, for locating wounded deer, and in packs were known to hunt game as large as wild boar and as fierce as the wolverine.\nThere are huge differences of opinion as to when dachshunds were specifically bred for their purpose of hunting badger, as the American Kennel Club states the dachshund was bred in the 15th century, while the Dachshund Club of America states that foresters bred the dogs in the 18th or 19th century.\nDouble-dapple dachshunds, which are prone to eye disease, blindness, or hearing problems, are generally believed to have been introduced to the United States between 1879 and 1885.\nThe flap-down ears and famous curved tail of the dachshund have deliberately been bred into the dog. In the case of the ears, this is to keep grass seeds, dirt, and other matter from entering the ear canal. The curved tail is dual-purposed: to be seen more easily in long grass and, in the case of burrowing dachshunds, to help haul the dog out if it becomes stuck in a burrow.\nThe smooth-haired dachshund, the oldest style, may be a cross between the German Shorthaired Pointer, a Pinscher, and a Bracke (a type of bloodhound), or to have been produced by crossing a short Bruno Jura Hound with a pinscher. Others believe it was a cross from a miniature French pointer and a pinscher; others claim that it was developed from the St. Hubert Hound, also a bloodhound, in the 18th century, and still others believe that they were descended from Basset Hounds, based upon their scent abilities and general appearance.\nThe exact origins of the dachshund are therefore unknown. According to William Loeffler, from  The American Book of the Dog (1891), in the chapter on dachshunds: \"The origin of the Dachshund is in doubt, our best authorities disagreeing as to the beginning of the breed.\" What can be agreed on, however, is that the smooth dachshund gave rise to both the long-haired and the wire-haired varieties.\nThere are two theories about how the standard long-haired dachshund came about. One theory is that smooth dachshunds would occasionally produce puppies which had slightly longer hair than their parents. By selectively breeding these animals, breeders eventually produced a dog which consistently produced long-haired offspring, and the long-haired dachshund was born. Another theory is that the standard long-haired dachshund was developed by breeding smooth dachshunds with various land and water spaniels. The long-haired dachshund may be a cross among any of the small dog breeds in the spaniel group, including the German Stoeberhund, and the smooth dachshund.\nThe wire-haired dachshund, the last to develop, was bred in the late 19th century. There is a possibility the wire-haired dachshund was a cross between the smooth dachshund and various hard-coated terriers and wire-haired pinschers, such as the Schnauzer, the Dandie Dinmont Terrier, the German Wirehaired Pointer, or perhaps the Scottish Terrier."
      },
      {
        "heading": "Symbol of Germany",
        "level": 1,
        "content": "Dachshunds have traditionally been viewed as a symbol of Germany. Political cartoonists commonly used the image of the dachshund to ridicule Germany. During World War I, the dachshund's popularity in the United States plummeted because of this association. As a result, they were often called \"liberty hounds\", just as \"liberty cabbage\" became a term for sauerkraut mostly in North America. The stigma of the association was revived to a lesser extent during World War II, though it was comparatively short-lived. Kaiser Wilhelm II and German field marshal Erwin Rommel were known for keeping dachshunds.\nOwing to the association of the breed with Germany, as well as its particular popularity among dog keepers in Munich at the time, the dachshund was chosen as the first official mascot for the 1972 Summer Olympics in Munich, with the name Waldi.\nIn the German sitcom Hausmeister Krause, the main protagonist, Dieter Krause, is portrayed as a typical German square and fuddy-duddy, who embodies many German stereotypes, and his obsession for dachshunds is one of them. The important role of dachshunds in the series even led to a new rise in popularity of dachshunds in Germany."
      },
      {
        "heading": "Sports",
        "level": 1,
        "content": "Some people train and enter their dachshunds to compete in dachshund races, such as the Wiener Nationals. Several races across the United States routinely draw several thousand attendees.\nDespite the popularity of these events, the Dachshund Club of America opposes \"wiener racing\", as many greyhound tracks use the events to draw large crowds to their facilities. The DCA is also worried about potential injuries to dogs, owing to their predisposition to back injuries. Another favorite sport is earthdog trials, in which dachshunds enter tunnels with dead ends and obstacles attempting to locate either an artificial bait or live but caged (and thus protected) rats."
      },
      {
        "heading": "Dackel versus Teckel",
        "level": 1,
        "content": "In Germany, dachshunds are widely called Dackel (both singular and plural). Among hunters, they are mainly referred to as Teckel. There are kennels which specialize in breeding hunting dachshunds, the so-called jagdliche Leistungszucht (\"hunting-related performance breeding\") or Gebrauchshundezucht (\"working dog breeding\"), as opposed to breeding family dogs. Therefore, it is sometimes incorrectly believed that Teckel is either a name for the hunting breed or a mark for passing the test for a trained hunting dog (called \"VGP\", \"Verband-Gebrauchsprüfung\") in Germany."
      },
      {
        "heading": "Popularity",
        "level": 1,
        "content": "Dachshunds are one of the most popular dogs in the United States, ranking 12th in the 2018 AKC registration statistics. They are popular with urban and apartment dwellers, ranking among the top 10 most popular breeds in 76 of 190 major US cities surveyed by the AKC.\nThere are organized local dachshund clubs in most major American cities, including New York, New Orleans, Portland, Los Angeles, and Chicago. As of 2024, the American Kennel Club found the Dachshund to be the 6th most popular dog breed. \n\n\tIn Art"
      },
      {
        "heading": "Notable dogs and owners",
        "level": 1,
        "content": "John F. Kennedy bought a dachshund puppy while touring Europe in 1937 for his then-girlfriend Olivia. The puppy, named Dunker, never left Germany after Kennedy started to get allergic reactions.\nGrover Cleveland, the 22nd and 24th President, had a dachshund in the White House.\nWilliam Randolph Hearst was an avid lover of dachshunds. When his own dachshund Helena died, he eulogized her in his \"In The News\" column.\nFred, E. B. White's dachshund, appeared in many of his famous essays.\nLump (pronounced [lʊmp]; German for \"rascal\"), the pet of Pablo Picasso, who was thought to have inspired some of his artwork. Picasso & Lump tells the story of Picasso and Lump.\nJack Ruby, the killer of Lee Harvey Oswald, had a dachshund named Sheba, which he often referred to as his wife. At the time he murdered Oswald, he had four of them—although he once had as many as 10.\nAndy Warhol had a pair of dachshunds, Archie and Amos, whom he depicted in his paintings and mentioned frequently in his diaries.\nStanley and Boodgie, immortalized on canvas by owner David Hockney, and published in the book David Hockney's Dog Days.\nWadl and Hexl, Kaiser Wilhelm II's famous ferocious pair. Upon arriving at Archduke Franz Ferdinand's country seat, Konopiště castle, on a semi-official visit, they promptly proceeded to do away with one of the Austro-Hungarian heir presumptive's priceless golden pheasants, thereby almost causing an international incident. Another one of his beloved dachshunds, Senta, is currently buried at Huis Doorn, Wilhelm's manor in the Netherlands.\nIn Zelenogorsk, Russia, a parade of dachshunds pass by a dachshund monument every July 25 to commemorate the day the city was founded.\nJoe was the dachshund of General Claire Lee Chennault, commander of the Flying Tigers and then the China Air Task Force of the US Army Air Forces, and became the mascot of those organizations.\nMaxie, a dachshund owned by actress Marie Prevost, tried to awaken his dead mistress, who was found with small bites on her legs. Maxie's barking eventually summoned neighbors to the scene. The incident inspired the 1977 Nick Lowe song \"Marie Prevost\".\nLiliane Kaufmann, wife of Edgar J. Kaufmann who commissioned the home Fallingwater from Frank Lloyd Wright in 1935, was a well-known breeder and owner of long-haired dachshunds. At the Fallingwater bookstore, visitors are able to purchase a book titled Moxie, which is about one of the dachshunds who lived at Fallingwater.  Liliane raised long-haired dachshunds and they traveled from Pittsburgh to Bear Run with her.\nThe former Queen of Denmark, Margrethe II, is one of several Danish royals to keep dachshunds. Margrethe of Denmark has a particular fondness of dachshunds and has kept many throughout her life. Her 80th birthday, celebrated in 2020, was marked by posing with one of her favourites (Lilia) on the grounds of Fredensborg Castle.\nObie is a dachshund who became infamous for his obesity, weighing as much as 77 pounds (35 kilograms), more than twice a normal-weight standard dachshund. He reached his target weight of 28 lb (13 kg) in July 2013.\nCarole Lombard and Clark Gable had a dachshund named Commissioner.\nCrusoe the Celebrity Dachshund gained fame on social media. In 2015, Crusoe came out with his The New York Times best-selling book titled Crusoe: Adventures of the Wiener Dog Extraordinaire!. In 2018, Crusoe came out with another book titled Crusoe: The Worldly Wiener Dog. At the 9th annual Shorty Awards, Crusoe won the best animal category. In 2018, Crusoe won the People's Choice Awards Animal Star of 2018.\nEnglish singer-songwriter Adele has a dachshund named Louis Armstrong."
      },
      {
        "heading": "See also",
        "level": 1,
        "content": "List of dog breeds\nNintendogs: Dachshund and Friends"
      },
      {
        "heading": "References",
        "level": 1,
        "content": ""
      },
      {
        "heading": "Further reading",
        "level": 1,
        "content": "Dachshund Breed Standard Archived 30 August 2013 at the Wayback Machine Russian Kennel Club 13 March 2001\nDachshund Breed Standard Archived 2 April 2012 at the Wayback Machine Poland Kennel Club 9 May 2001"
      },
      {
        "heading": "External links",
        "level": 1,
        "content": "\nDachshund pronunciation"
      }
    ],
    "summary": "The dachshund  (UK:  DAKS-huund, -⁠ənd, -⁠huunt or US:  DAHKS-huunt, -⁠huund, -⁠ənt; German: 'badger dog'), also known as the wiener dog or sausage dog, badger dog and doxie, is a short-legged, long-bodied, hound-type dog breed. The dog may be smooth-haired, wire-haired, or long-haired, with varied coloration.\nThe dachshund was bred to scent, chase, and flush out badgers and other burrow-dwelling animals. The miniature dachshund was bred to hunt small animals such as rabbits.\nThe dachshund was ranked 9th in registrations with the American Kennel Club in 2022."
  },
  {
    "title": "Dachshund",
    "source": "https://es.wikipedia.org/wiki/Dachshund",
    "language": "es",
    "chunks": [
      {
        "heading": "Antecedentes",
        "level": 1,
        "content": "El primer club de la raza se fundó en Alemania en 1888, cuando se redactó el primer estándar, aunque ejemplares de esta raza ya se habían presentado años antes en exposiciones caninas en Inglaterra.\nLa popularidad de los dachshund se ha debido en gran medida a ser una de las razas preferidas por las monarquías europeas, incluida la de la reina Victoria, que era particularmente entusiasta de esta raza.\nLa FCI decidió crear el Grupo IV sólo para esta raza, ya que a pesar de ser un perro de madriguera, su carácter y constitución distan mucho de los basset hound."
      },
      {
        "heading": "Características",
        "level": 1,
        "content": "Su forma es alargada y baja, con una cola larga y hocico estirado, orejas largas y caídas, patas cortas y uñas negras. Estas patas cortas son quizás la característica más reconocida de la raza y las que facilitan la caza en madrigueras, que era su principal actividad dentro de la caza, donde se muestra como un rastreador y muy valiente, y que no dudará en enfrentarse a un fiero jabalí.\n\nHay tres variedades, según la circunferencia torácica medida a la edad de 15 meses:\n\nEstándar: peso máximo de 9 kg y una circunferencia torácica superior a 35 cm.\nMiniatura: circunferencia torácica entre 30 y 35 cm.\nDachshund para la caza del conejo: circunferencia menor de 30 cm.\nCada una de las variedades de tamaño puede presentar tres tipos de pelo:\n\nDe pelo corto.- El pelo debe ser corto, espeso, brillante, liso, bien pegado al cuerpo, fuerte y duro; no debe mostrar zonas sin pelo. La cola con pelo fino y tupido.\nDe pelo largo.- El pelaje está provisto de una capa externa de pelo liso, brillante y bien pegado al cuerpo, se alarga debajo del cuello y en la parte inferior del cuerpo, sobresale en las orejas y presenta en la parte posterior de las extremidades un pelo claramente más largo en forma de plumas. El pelo más largo se encuentra en la parte inferior de la cola, donde forma bandera.\nDe pelo duro.- Con excepción del hocico, las cejas y las orejas, tiene en todo el cuerpo - mezclado con la capa interna de pelo - una capa externa con el pelo pegado al cuerpo, espeso, áspero y grueso. En el hocico posee una barba bien marcada. Las cejas son tupidas. En las orejas, el pelo es más corto que en el cuerpo, casi liso. La cola con pelo bien desarrollado, uniforme y muy pegado.\nSon perros de carácter cariñoso, sociable y juguetones, les encanta pasar el tiempo jugando sin importar la edad y por esa razón no se es recomendable dejarlos mucho tiempo solos, ya que pueden causar daños como romper muebles u otros objetos. También al estar solos tienden a sentirse tristes, por lo cual, ya no quieren pasar tiempo con las demás personas y no quieren comer. Si se lo deja todos los días, el can puede desarrollar el síndrome del abandono, que se da mayormente en ejemplares jóvenes. El síndrome de ansiedad por separación en perros es un trastorno de comportamiento muy característico en canes que se quedan solos en casa o se separan de sus dueños tras haber pasado una larga temporada con ellos. Esto tiene tratamiento:\nModificación del entorno del perro: es necesario proporcionarle estimulación física y mental para bajar sus niveles de ansiedad.\nTerapia de modificación de conducta: hay que enseñar al perro a tolerar la separación.\n\n \n\nEn las cinco variedades de pelo se admiten los mismos colores:\n\nPerros unicolores: Rojo, rojo-amarillo, amarillo, con o sin moteado negro. El blanco no es deseable, pero en forma de unas pequeñas manchitas no es eliminatorio. La trufa (parte terminal de la nariz) y uñas son negras, aunque se permite el color rojizo-marrón.\nPerros bicolores: Negro profundo o marrón claro, todos con marcas de fuego de color rojo oxidado o amarillo encima de los ojos, a los lados del hocico y del labio inferior, en el borde interior de las orejas, en el antepecho, en las partes interiores y posteriores de las extremidades, en los pies, alrededor del ano y desde aquí hasta un tercio o la mitad de la parte inferior de la cola. La trufa y las uñas son negras en perros negros; en perros de color marrón, llevan este mismo color. El blanco no es deseable, pero se admiten pequeñas manchas. No son deseables marcas de fuego demasiado extendidas.\nPerros manchados (arlequines, atigrados): El color básico es siempre un color oscuro (negro, rojo o gris). Son deseables manchas irregulares grises o beige. Ni el color oscuro ni el claro deben predominar. El color atigrado tiene rayas más oscuras sobre un fondo rojo o amarillo. La trufa y uñas son como en el caso de los perros uni o bicolores."
      },
      {
        "heading": "Educación",
        "level": 2,
        "content": "Se exagera mucho al afirmar que el dachshund es testarudo y no se deja educar. Sin embargo, puede existir alguna razón para juzgarlo así, tal vez su pequeño tamaño lleva a cuidarlo más de lo debido y a transigir con sus caprichos, los mismos que tienen la mayoría de cachorros, lo que luego en el lenguaje popular se traduce en las proverbiales «cabezonerías» del dachshund, que se dan en casi todos los cachorros. Aparte de ello, la educación de un dachshund no es más difícil que la de cualquier perro, al contrario; es un can inteligente, que aprende rápido, y de cachorro, igual de cabezón que otras razas. Una vez que comprenden las reglas de convivencia y se les educa adecuadamente son perros cariñosos, protectores y a la par que muy juguetones, incluso llegando a colaborar en las tareas del hogar."
      },
      {
        "heading": "Consideraciones",
        "level": 2,
        "content": "El dachshund, de buenas líneas de trabajo, no es un sabueso en el estricto sentido del término, pero como le viene de este su excelente olfato, puede hacer el trabajo de un sabueso a su manera y de acuerdo con su pequeño tamaño. No es un terrier propiamente dicho, pero trabaja a la perfección bajo tierra en la madriguera, como lo haría un terrier, tampoco es un braco propiamente dicho, pero puede hacer el trabajo de este, de acuerdo con su tamaño.\nNo hay que igualar un perro de caza a otro, todos tienen sus ventajas y sus inconvenientes, sus especialidades según su tamaño, pelo, etc. El dachshund reúne cualidades de casi todos los perros de caza y en manos de un buen cazador que lo sepa adiestrar bien y que disponga de un terreno de caza de pelo y pluma y quizás alguna madriguera, zarzas, jaras, matojos, un charco cerca y algún paso de jabalíes, tendrá en el dachshund un pequeño perro de caza que le levantara cualquier pieza, pondrá algún zorro a tiro, dentro o fuera de la madriguera, rastreará el rastro de sangre de: jabalí, corzo, ciervo, u otra pieza de caza mayor durante kilómetros y kilómetros, por lo que conviene que, en este caso, vayan atados en largo, pues debido a su tozudez, el perro seguirá el rastro constantemente, con riesgo de perderse, aunque siempre vuelva al sitio de partida, volviendo sobre sus rastros, pero pueden pasar muchas horas o incluso días, cuando el amo ya no esté allí.\nConviene adiestrar al dachshund para caza mayor o menor (solo una de ellas), puesto que si vamos de caza menor y detecta un rastro de jabalíes, venados, corzos, ciervos, seguirá el rastro hasta que le perdamos de vista, despreocupándose de las piezas menores. Y viceversa; en caza mayor, no dudará en seguir el rastro de liebres, conejos y perdices hasta localizarlas. Dado que es un perro apto para ambas cazas y que el instinto le va a llevar a seguir cualquier rastro de ambas, se debe educar para que se especialice única y exclusivamente al tipo de caza que nos vamos a dedicar."
      },
      {
        "heading": "Salud",
        "level": 1,
        "content": "La raza es conocida por sus problemas de espina dorsal, debido a su columna vertebral extremadamente larga y a sus costillas cortas. El riesgo de lesiones puede empeorar en caso de sobrepeso. Para prevenir daños, es recomendable que estos perros no salten ni suban y bajen escaleras. Cada vez es más evidente que estos problemas son hereditarios, y los criadores trabajan en la mejora de la raza.\nSin embargo, es sabido que estos perros, con una correcta educación deportiva, no presentarían problemas en este ámbito. En especial si son animales que gozan de grandes espacios donde pueden correr y saltar libremente. Son perros muy ágiles de reflejos excepcionales que, sin embargo, por sus patas cortas no alcanzan altas velocidades. En general son canes poco propensos a enfermedades siempre y cuando el dueño tenga un buen cuidado al momento de una enfermedad, llevándolo al veterinario y siguiendo sus instrucciones.\nPor otro lado, la alimentación del dachshund o perro salchicha es de suma importancia. Si se le proporciona demasiada comida, el can engordará y tendrá dificultades para su movilidad, y como consecuencia presentaría dolores de espalda y con los años graves problemas de espina dorsal y hernias discales. Aunque esta raza consume carne como cualquiera otra canina no quiere decir esto que sólo debe alimentarse de esta. Es muy recomendable alimentarlos con alimentos equilibrados ricos en proteínas y vitaminas, las cuales lo ayudan a crecer de una manera saludable. Esta raza también se distingue por ser excelentes perros caseros, especialmente apropiados para vivir en residencias de reducido tamaño debido a su pequeña estatura y buen temperamento. Les gusta mucho la zanahoria, la manzana y el pepino.\nEl perro salchicha es una de las razas con mayor esperanza de vida, llegando a vivir de 12 a 16 años, siempre y cuando reciban los cuidados apropiados para propiciar su salud y bienestar.[8]​"
      },
      {
        "heading": "Símbolo en Alemania",
        "level": 1,
        "content": "Los dachshunds se han visto tradicionalmente como un símbolo de Alemania. Los caricaturistas políticos solían utilizar la imagen del perro salchicha para ridiculizar a Alemania. Durante la Primera Guerra Mundial, la popularidad de la raza de perro en Estados Unidos se desplomó debido a esta asociación. Como resultado, a menudo se los llamaba liberty hounds ('sabuesos de la libertad'). El estigma de la asociación revivió en menor medida durante la Segunda Guerra Mundial, aunque fue comparativamente de corta duración. El emperador Guillermo II de Alemania y el mariscal de campo alemán Erwin Rommel eran conocidos por tener dachshunds.\nDebido a la asociación de la raza con Alemania, así como a su particular popularidad entre los cuidadores de perros en Múnich en ese momento, el perro salchicha fue elegido como la mascota oficial de los Juegos Olímpicos de Múnich 1972, con el nombre de Waldi."
      },
      {
        "heading": "Véase también",
        "level": 1,
        "content": "Tejonero de Westfalia\n Portal:Perros. Contenido relacionado con Perros."
      },
      {
        "heading": "Referencias",
        "level": 1,
        "content": "\"Dachshund - Definition and More from the Free Merriam-Webster Dictionary\". Merriam-webster.com. [2]. Retrieved 2012-05-16.\nLangenscheidt's German–English English–German Dictionary. New York: Pocket Books. 1970. pp. 75, 145. ISBN 0-671-54107-2.\n\"Dachshund Dogs & Puppies – Miniature & Standard Dachshunds\". Dogpage.us. [3]. Retrieved 2009-06-16.\n\"The Dachshund\". Dog Owners Guide. [4]. Retrieved 2009-11-19.\nNicholas, Anna (1987). Dachshund. Neptune City: TFH Publications. p. 10. ISBN 0-86622-158-1.\n\"Fédération Cynologique Internationale Group 4 \"Dachshund Group\"\". Fédération Cynologique Internationale. [5]. Retrieved 2009-06-16.\n\"American Kennel Club Dachshund Breed Information\". American Kennel Club. [6]. Retrieved 2009-06-16.\n\"A Brief History of the Breed\". AlmostHomeRescue.org. [7] Archivado el 28 de junio de 2020 en Wayback Machine.. Retrieved 2009-06-16.\n\"Dachshund Breed Standard\". American Kennel Club. [8]. Retrieved 2009-02-03.\n\"Fédération Cynologique Internationale Official Website\". Fédération Cynologique Internationale. [9]. Retrieved 2009-06-16.\nHutchinson, Robert (2005). For the Love of Dachshunds. BrownTrout Publishers. p. 112. ISBN 1-56313-903-0. [10]. Retrieved 2009-06-16."
      },
      {
        "heading": "Enlaces externos",
        "level": 1,
        "content": " Wikimedia Commons alberga una galería multimedia sobre Dachshund.\nTeckel Club España Archivado el 5 de febrero de 2020 en Wayback Machine.\nTeckel Kanincken\nTeckel Miniatura\nTeckel Standard\nAlimentación para Dachshund\nDachshund Colores Archivado el 3 de julio de 2020 en Wayback Machine.\nTeckel de Pelo Largo Archivado el 1 de julio de 2020 en Wayback Machine."
      }
    ],
    "summary": "El dachshund, también conocido como perro salchicha o teckel, es una raza de perro originaria de Alemania. Su peculiar fisonomía se debe a una mutación genética conocida como bassetismo, que dota a los ejemplares de unas extremidades cortas en relación con el tamaño del cuerpo. \nSegún su tamaño y peso puede clasificarse en estándar (9 kg-15 kg), miniatura (5 kg-9 kg) y Kaninchen (3 kg-5 kg) este último no solo se caracteriza por un menor peso y tamaño, sino que presenta unos rasgos físicos distintos.\nAdemás el perro salchicha se diferencia en función de la tipología del pelo, puede ser de pelo duro (generalmente de color gris), pelo corto y pelo largo, estos dos últimos pueden ser de color negro con rojo fuego, marrón chocolate y dorado.\nEl dachshund de tamaño estándar se desarrolló para oler, perseguir y ahuyentar a los tejones (de ahí su nombre, que en alemán significa 'perro tejonero') y otros animales que viven en madrigueras, mientras que el dachshund en miniatura fue criado para cazar animales pequeños como conejos y ratones."
  }
]